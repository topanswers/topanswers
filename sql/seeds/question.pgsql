--
-- Data for Name: question; Type: TABLE DATA; Schema: db; Owner: postgres
--

COPY "db".question (community_id, account_id, question_id, question_at, question_title, question_markdown, question_room_id, question_change_at, question_votes, license_id, codelicense_id, question_poll_id, question_poll_major_id, question_poll_minor_id, question_se_question_id, question_flags, question_crew_flags, question_active_flags, question_se_imported_at, question_permit_later_license, question_permit_later_codelicense, kind_id, question_sesite_id) FROM stdin;
1	115	240	2019-12-01 21:13:03.66515+00	Will there be an inbox?	When I started using TopAnswers I noticed, and was initially confused by, the inbox for chat replies.  At the time I assumed it was a general inbox; all I'd done so far was chat, so it made sense that it was full of chat stuff.\r\n\r\nI've now posted some questions, some of which have gotten answers, but I've seen no notifications.  Should I have?  Or haven't we built an inbox for Q&A responses yet?  If we haven't, will we?\r\n\r\nAs the body of questions grows, and also as time goes on and questions can thus be inactive long enough to be out of view, being alerted that somebody has provided an answer seems pretty important.\r\n	248	2019-12-01 21:13:03.66515+00	5	4	1	315	713	1651	\N	0	0	0	\N	f	f	2	\N
1	811	627	2020-01-28 13:55:01.946573+00	Post chat pane "status bar"	In the main site view, the bottom of the chat pane shows the room icon and has a selector for which chat one wants to participate in (I presume, as no site has multiple rooms as of yet).\r\n\r\nHowever, on a post page, only an avatar is shown with no selector. I get that there can only be one chat room per post, but maybe the empty space next to the icon could hold the question title, further emphasising that this chat is for the particular post?\r\n\r\nP.S. Could the drop-down be given the full width of the pane?	642	2020-01-28 13:55:01.946573+00	0	1	1	702	1759	3425	\N	0	0	0	\N	f	f	2	\N
1	759	642	2020-01-30 01:23:15.654991+00	How to order Questions?	Likely this is just a scenario where I'm not yet familiar enough with the UI, but how exactly do I order qeustions?\r\n\r\nWith that in mind, how is the current order currently determined?  \r\n\r\nAt first I thought it was by when the question was posted, but the following screenshot doesn't seem to indicate that being the case.\r\n\r\n![2020-01-29 19_18_13-The Tavern - TopAnswers.png](/image?hash=7f7c2c867eff8627281e33577d3fddbdde49ede7cedd515714cc9398303fee96)	657	2020-01-30 01:23:15.654991+00	3	4	1	717	1835	3554	\N	0	0	0	\N	f	f	2	\N
1	131	231	2019-12-01 18:22:44.881301+00	Please don't assume Javascript	I know I'm the odd one out sometimes in these matters, but I'd like to suggest that the site should be possible to **read** without having Javascript and XmlHttpRequest turned on.\r\n\r\nCurrently, browsing to https://topanswers.xyz without Javascript and XHR on (for example, using uMatrix in a default-block configuration) results in a completely blank page. The site _looks_ functional, but as if there's nothing there.\r\n\r\nViewing a question having disabled Javascript in the browser results in a wall of unformatted text.\r\n\r\nYes, people in general browse with Javascript-capable browsers and everything turned on; but there is a not-insignificant minority (and I suspect that it's larger among technical people) who have their browsers' ability to run unknown third-party code locked down. By all means use Javascript to _enhance_ the experience, but please make it possible to at least _read_ the site without running Javascript and making extra asynchronous requests to the server.	239	2019-12-01 18:28:42.226247+00	7	4	1	306	1280	3327	\N	0	0	0	\N	f	f	2	\N
2	756	570	2020-01-13 16:44:27+00	Incorrect Result with Apply	When selecting from a nested query in an `OUTER APPLY` statement the nested query seems to be evaluated only once in certain circumstances.\r\n\r\nIs this the expected behavior or am I missing something in the documentation or is this a bug in SQL Server?\r\n\r\nAlso, is there any possibility to force evaluation of the nested query for every row?\r\n\r\n## Test Case 1\r\n\r\nEvaluates nested `FROM` query for every row in `VALUES` (expected behaviour)\r\n\r\n```\r\nSELECT\r\n    v,\r\n    v2\r\nFROM\r\n    (VALUES (1), (2), (3), (4)) AS inner_query(v)\r\n    OUTER APPLY (\r\n        SELECT\r\n            MAX(inner_v2) AS v2\r\n\t\tFROM (\r\n            SELECT \r\n                15 AS id,\r\n                v AS inner_v2\r\n        ) AS outer_query\r\n        GROUP BY id\r\n    ) AS outer_apply\r\n```\r\n\r\nResult:\r\n\r\n| v | v2|\r\n|---|---|\r\n| 1 | 1 |\r\n| 2 | 2 |\r\n| 3 | 3 |\r\n| 4 | 4 |\r\n\r\n## Test Case 2\r\n\r\nIt also evaluates nested `FROM` query for every row in `VALUES` (expected behaviour)\r\n\r\n```\r\nSELECT\r\n    v,\r\n    v2\r\nFROM\r\n    (VALUES (1), (2), (3), (4)) AS inner_query(v)\r\n    OUTER APPLY (\r\n        SELECT\r\n            MAX(inner_v2) AS v2\r\n        FROM (\r\n            SELECT \r\n                15 AS id,\r\n                v AS inner_v2\r\n            UNION ALL\r\n            SELECT\r\n                id AS id,\r\n                TestCaseTemp2.v AS inner_v2\r\n            FROM\r\n                (VALUES (1337, 0)) AS TestCaseTemp2(id, v)\r\n            WHERE TestCaseTemp2.v != 0\r\n        ) AS outer_query\r\n        GROUP BY id\r\n    ) AS outer_apply;\r\n```\r\n\r\nResult:\r\n\r\n| v | v2|\r\n|---|---|\r\n| 1 | 1 |\r\n| 2 | 2 |\r\n| 3 | 3 |\r\n| 4 | 4 |\r\n\r\n## Test Case 3\r\n\r\nEvaluates nested `FROM` query only once\r\n\r\n```\r\nCREATE TABLE TestCaseTemp\r\n(\r\n    id int,\r\n    v int\r\n);\r\nINSERT INTO TestCaseTemp VALUES (1337, 0);\r\n\r\nSELECT\r\n    v,\r\n    v2\r\nFROM\r\n    (VALUES (1), (2), (3), (4)) AS inner_query(v)\r\n    OUTER APPLY (\r\n        SELECT\r\n            MAX(inner_v2) AS v2\r\n        FROM (\r\n            SELECT \r\n                15 AS id,\r\n                v AS inner_v2\r\n            UNION ALL\r\n            SELECT\r\n                id AS id,\r\n                TestCaseTemp.v AS inner_v2\r\n            FROM\r\n                TestCaseTemp\r\n            WHERE TestCaseTemp.v != 0\r\n        ) AS outer_query\r\n        GROUP BY id\r\n    ) AS outer_apply;\r\n\r\nDROP TABLE TestCaseTemp;\r\n```\r\n\r\nResult:\r\n\r\n| v | v2|\r\n|---|---|\r\n| 1 | 1 |\r\n| 2 | 1 |\r\n| 3 | 1 |\r\n| 4 | 1 |\r\n	584	2020-01-14 14:47:49.989278+00	0	4	1	645	1478	3288	257326	0	0	0	2020-01-14 14:39:37.582556+00	f	f	1	1
2	751	333	2019-12-05 21:08:10+00	Why/when does SQL Server evaluate the probe side of an inner hash join when the build side was empty?	## Setup\r\n\r\n    DROP TABLE IF EXISTS #EmptyTable, #BigTable\r\n    \r\n    CREATE TABLE #EmptyTable(A int);\r\n    CREATE TABLE #BigTable(A int);\r\n    \r\n    INSERT INTO #BigTable\r\n    SELECT TOP 10000000 CRYPT_GEN_RANDOM(3)\r\n    FROM   sys.all_objects o1,\r\n           sys.all_objects o2,\r\n           sys.all_objects o3;\r\n\r\n## Query\r\n\r\n    WITH agg\r\n         AS (SELECT DISTINCT a\r\n             FROM   #BigTable)\r\n    SELECT *\r\n    FROM   #EmptyTable E\r\n           INNER HASH JOIN agg B\r\n                        ON B.A = E.A;\r\n\r\n## Execution Plan\r\n\r\n[![enter image description here][1]][1]\r\n\r\n## Problem\r\n\r\nThis is a simplified repro for a phenomenon I hadn't noticed before today. My expectation for an inner hash join would be that if the build input is empty the probe side should not be executed as the join can return no rows. The above example contradicts that and reads the 10 million rows from the table. This adds 2.196 seconds to the execution time of the query (99.9%).\r\n\r\n## Additional Observations\r\n\r\n1. With `OPTION (MAXDOP 1)` the execution plan reads no rows from `#BigTable`. The `ActualExecutions` is `0` for all operators on the inside of the hash join.\r\n2. For the query `SELECT * FROM #EmptyTable E INNER HASH JOIN #BigTable B ON B.A = E.A`- I get a parallel plan, the scan operator on the inside of the hash join does have `ActualExecutions` of DOP but still no rows are read. This plan has no repartition streams operator (or aggregate)\r\n\r\n## Question\r\n\r\nWhat's going on here? Why does the original plan exhibit the problem and the other cases don't?\r\n\r\n  [1]: https://i.stack.imgur.com/XC3O9.png	341	2019-12-05 22:50:54.557009+00	0	4	1	408	767	1553	254947	0	0	0	2019-12-05 22:50:54.557009+00	f	f	1	1
2	178	284	2019-06-26 18:41:16+00	for xml path('') output	When I run the following\r\n\r\n    select t.type\r\n\t  from (values ('Green'),('Blue'),('Red')) as t(type)\r\n\t   for xml path('')\r\n\r\nI receive this output\r\n\r\n``` xml\r\n<type>Green</type>\r\n<type>Blue</type>\r\n<type>Red</type>\r\n```\r\n\r\nIf I run the following\r\n\r\n    select t.type + '/'\r\n\t  from (values ('Green'),('Blue'),('Red')) as t(type)\r\n\t   for xml path('')\r\n\r\nI receive this output\r\n\r\n``` none\r\nGreen/Blue/Red/\r\n```\r\n\r\nWhy does adding the concatenation in the select lead to the removal of the type tags and output on one line in the xml file? Running SQL Server 2012.	292	2019-12-04 14:28:15.869114+00	0	4	1	359	615	868	241485	0	0	0	2019-12-04 14:26:06.150377+00	f	f	1	1
1	701	500	2019-12-31 15:33:56.922845+00	Is there a way to find questions which need answers?	I'm interested in supporting a SE alternative, and would like to contribute to the Databases community.  Is there a practical way for me to find questions which need answers?  The first few pages of questions all have answers, but it's hard to tell which ones have *good* answers, or where the OP is hoping for something more.	512	2019-12-31 15:33:56.922845+00	11	4	1	575	2106	2668	\N	0	0	0	\N	f	f	2	\N
2	155	245	2019-02-01 09:02:33+00	Why is the auto created statistic on this column empty?	**Info**\r\n\r\nMy question relates to a moderately big table (~40GB data space) that is a heap  \r\n*(Unfortunately, I am not allowed to add a clustered index to the table by the application owners)*\r\n\r\nAn auto created statistic on an Identity column (`ID`) was created, but is empty. \r\n\r\n - Auto create stats & auto update stats are on\r\n - Modifications have happened in the table\r\n - There are other (auto created) statistics that are getting updated\r\n - There is another statistic on the same column created by an index (duplicate)\r\n - Build: 12.0.5546\r\n\r\n\r\nThe duplicate statistic is getting updated:\r\n[![enter image description here][1]][1]\r\n\r\n**The actual question**\r\n\r\nTo my understanding, all stats could be used and modifications are tracked, even if there are two statistics on exactly the same columns (duplicates), so why does this statistic remain empty?\r\n\r\n\r\n\r\n**Stats Info**\r\n\r\n[![enter image description here][2]][2]\r\n\r\n**DB stat info**\r\n\r\n[![enter image description here][3]][3]\r\n\r\n**Table Size**\r\n\r\n[![enter image description here][4]][4]\r\n\r\n**Column Information where the statistic is created on**\r\n\r\n[![enter image description here][5]][5]\r\n\r\n    [ID] [int] IDENTITY(1,1) NOT NULL\r\n*Identity column*\r\n\r\n    select * from sys.stats  \r\n    where name like '%_WA_Sys_0000000A_6B7099F3%';\r\n[![enter image description here][6]][6]\r\n*Auto created*\r\n\r\n\r\n**Getting some info on another statistic**\r\n\r\n    select * From sys.dm_db_stats_properties (1802541555, 3)  \r\n\r\n[![enter image description here][7]][7]\r\n\r\nIn comparison with my empty stat:\r\n\r\n[![enter image description here][8]][8]\r\n\r\nStats + Histogram from "generate scripts":\r\n\r\n    /****** Object:  Statistic [_WA_Sys_0000000A_6B7099F3]    Script Date: 2/1/2019 10:18:19 AM ******/\r\n    \r\n        CREATE STATISTICS [_WA_Sys_0000000A_6B7099F3] ON [dbo].[table]([ID]) WITH STATS_STREAM = 0x01000000010000000000000000000000EC03686B0000000040000000000000000000000000000000380348063800000004000A00000000000000000000000000\r\n\r\n\r\n**When creating a copy of the stats, no data is inside**\r\n\r\n    CREATE STATISTICS [_WA_Sys_0000000A_6B7099F3_TEST] ON [dbo].[table]([ID]) WITH STATS_STREAM = 0x01000000010000000000000000000000EC03686B0000000040000000000000000000000000000000380348063800000004000A00000000000000000000000000\r\n\r\n\r\n\r\n[![enter image description here][9]][9]\r\n\r\n**When manually updating the stat they do get updated.** \r\n\r\n    UPDATE STATISTICS [dbo].[Table]([_WA_Sys_0000000A_6B7099F3_TEST])\r\n\r\n[![enter image description here][10]][10]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/HTEDK.png\r\n  [2]: https://i.stack.imgur.com/qNvrU.png\r\n  [3]: https://i.stack.imgur.com/VSCrc.png\r\n  [4]: https://i.stack.imgur.com/Ae4NZ.png\r\n  [5]: https://i.stack.imgur.com/jvofZ.png\r\n  [6]: https://i.stack.imgur.com/KCfa7.png\r\n  [7]: https://i.stack.imgur.com/EdQki.png\r\n  [8]: https://i.stack.imgur.com/A7scr.png\r\n  [9]: https://i.stack.imgur.com/k9Z8Y.png\r\n  [10]: https://i.stack.imgur.com/qAfFn.png	253	2019-12-02 19:02:37.232926+00	0	4	1	320	537	702	228656	0	0	0	2019-12-02 19:02:37.232926+00	f	f	1	1
2	529	434	2017-02-08 21:03:07+00	Why doesn't this invalid subquery raise a syntax error?	Is there any explanation so as to why SSMS did not raise any compilation error for the below delete query with a subquery that is invalid on its own?\r\n\r\nBelow is the sequence of steps to reproduce the behavior:\r\n\r\n    CREATE TABLE [dbo].[delete_test]\r\n    (\r\n        [id] [int] IDENTITY(1,1) NOT NULL,\r\n    \t[name] [nchar](10) NOT NULL\r\n    ) ON [PRIMARY];\r\n\r\n\r\nSeed data:\r\n\r\n    insert dbo.delete_test(name) values(N'a'),(N'b'),(N'c'),(N'd'),(N'e');\r\n\r\nCreate a backup table:\r\n\r\n    select id, name\r\n      into dbo.delete_test_backup \r\n      from dbo.delete_test where id > 2;\r\n\r\nBelow statement should ideally not execute and, even if it executes, should only delete rows with `id` values that have been backed up :\r\n\r\n    delete dbo.delete_test where id in (\r\n    \tselect id delete_test_backup \r\n    );\r\n\r\nThe message shows that 5 rows were deleted, but I expected only 3 rows to be deleted:\r\n\r\n    (5 row(s) affected)\r\n\r\nAttempt to execute the sub-query as a standalone fails :\r\n\r\n    select id delete_test_backup ;\r\n\r\n> Invalid column name 'id'\r\n\r\nBelow one executes as expected :\r\n\r\n    select id from dbo.delete_test_backup ;\r\n\r\n> Output :\r\n\r\n    id\r\n    --\r\n    3\r\n    4\r\n    5	445	2019-12-12 22:39:05.889224+00	0	4	1	509	1041	1988	163647	0	0	0	2019-12-12 22:39:05.889224+00	f	f	1	1
2	192	298	2017-04-04 19:07:07+00	CPU clock speed versus CPU core count - higher GHz, or more cores for SQL Server?	We are beginning to provision a set of physical servers for a virtual cluster of SQL Server 2016 nodes within VMware. We will be utilizing Enterprise Edition licenses.\r\n\r\nWe plan on setting up 6 nodes, but there is a bit of a debate on what the ideal way to provision the physical servers with regards to CPU clock speed versus CPU core count.\r\n\r\nI know this is largely dependent on transaction volume and number of databases stored among other software-specific factors, but is there a general rule of thumb that is advised?\r\n\r\nFor instance, is a dual 8-core, 3.2 GHz physical server (16 cores) more preferential to a dual 16-core, 2.6 GHz server (32 cores)?\r\n\r\nHas anyone come across a white paper that further delves into this type of topic?	306	2019-12-04 22:50:17.867201+00	0	4	1	373	655	1539	169123	0	0	0	2019-12-04 22:50:17.867201+00	f	f	1	1
2	79	407	2016-06-16 07:31:39+00	Execution plan percentages don't add up to 100%	I am referring to SQL Server query execution plans to take query cost and then optimize the required things by looking at the plan. \r\n\r\nBut the total of individual query costs adds up to more than 100%. \r\n\r\nThis is my query:\r\n\r\n    DECLARE @date SMALLDATETIME\r\n    \r\n    SELECT Reffd AS NAME\r\n    \t,(\r\n    \t\tSELECT (\r\n    \t\t\t\t(\r\n    \t\t\t\t\tSELECT count(*)\r\n    \t\t\t\t\tFROM [cal_reg].[dbo].[customer]\r\n    \t\t\t\t\tWHERE upper(Reffd) = upper(main.reffd)\r\n    \t\t\t\t\t\tAND cast(DATE AS SMALLDATETIME) = @date\r\n    \t\t\t\t\t) + (\r\n    \t\t\t\t\tSELECT count(*)\r\n    \t\t\t\t\tFROM [cal_reg].[dbo].[rep]\r\n    \t\t\t\t\tWHERE upper(Reffd) = upper(main.Reffd)\r\n    \t\t\t\t\t\tAND cast(DATE AS SMALLDATETIME) = @date\r\n    \t\t\t\t\t)\r\n    \t\t\t\t)\r\n    \t\t) AS Completed\r\n    \t,(\r\n    \t\tSELECT (\r\n    \t\t\t\t(\r\n    \t\t\t\t\tSELECT count(*)\r\n    \t\t\t\t\tFROM [cal_reg].[dbo].[customer]\r\n    \t\t\t\t\tWHERE upper([call Attnd]) = upper(main.[Reffd])\r\n    \t\t\t\t\t\tAND cast(DATE AS SMALLDATETIME) = @date\r\n    \t\t\t\t\t) + (\r\n    \t\t\t\t\tSELECT count(*)\r\n    \t\t\t\t\tFROM [cal_reg].[dbo].[rep]\r\n    \t\t\t\t\tWHERE upper([Call Attnd]) = upper(main.reffd)\r\n    \t\t\t\t\t\tAND cast(DATE AS SMALLDATETIME) = @date\r\n    \t\t\t\t\t)\r\n    \t\t\t\t)\r\n    \t\t) AS Attended\r\n    \t,(\r\n    \t\tSELECT (\r\n    \t\t\t\t(\r\n    \t\t\t\t\tSELECT (\r\n    \t\t\t\t\t\t\t(\r\n    \t\t\t\t\t\t\t\tSELECT count(*)\r\n    \t\t\t\t\t\t\t\tFROM [cal_reg].[dbo].[customer]\r\n    \t\t\t\t\t\t\t\tWHERE upper(Reffd) = upper(main.reffd)\r\n    \t\t\t\t\t\t\t\t\tAND cast(DATE AS SMALLDATETIME) = @date\r\n    \t\t\t\t\t\t\t\t) + (\r\n    \t\t\t\t\t\t\t\tSELECT count(*)\r\n    \t\t\t\t\t\t\t\tFROM [cal_reg].[dbo].[rep]\r\n    \t\t\t\t\t\t\t\tWHERE upper(Reffd) = upper(main.Reffd)\r\n    \t\t\t\t\t\t\t\t\tAND cast(DATE AS SMALLDATETIME) = @date\r\n    \t\t\t\t\t\t\t\t)\r\n    \t\t\t\t\t\t\t)\r\n    \t\t\t\t\t)\r\n    \t\t\t\t) + (\r\n    \t\t\t\tSELECT (\r\n    \t\t\t\t\t\t(\r\n    \t\t\t\t\t\t\tSELECT count(*)\r\n    \t\t\t\t\t\t\tFROM [cal_reg].[dbo].[customer]\r\n    \t\t\t\t\t\t\tWHERE upper([call Attnd]) = upper(main.[Reffd])\r\n    \t\t\t\t\t\t\t\tAND cast(DATE AS SMALLDATETIME) = @date\r\n    \t\t\t\t\t\t\t) + (\r\n    \t\t\t\t\t\t\tSELECT count(*)\r\n    \t\t\t\t\t\t\tFROM [cal_reg].[dbo].[rep]\r\n    \t\t\t\t\t\t\tWHERE upper([Call Attnd]) = upper(main.reffd)\r\n    \t\t\t\t\t\t\t\tAND cast(DATE AS SMALLDATETIME) = @date\r\n    \t\t\t\t\t\t\t)\r\n    \t\t\t\t\t\t)\r\n    \t\t\t\t)\r\n    \t\t) AS Total\r\n    \t,'' AS f6row\r\n    \t,'' AS f8row\r\n    FROM [cal_reg].[dbo].[customer] AS main\r\n    WHERE cast(DATE AS SMALLDATETIME) = @date\r\n    \tAND upper(reffd) IN (\r\n    \t\tSELECT upper(shname)\r\n    \t\tFROM common.dbo.Password_table\r\n    \t\t)\r\n    GROUP BY reffd\r\n\r\nAnd for that I am getting total of 104% as shown below:\r\n \r\n[![enter image description here][1]][1]\r\n+\r\n[![enter image description here][2]][2]\r\n\r\nMy question is it possible to solve that more than 100% cost error or are there any other ways from which I can tell my query is running efficiently ?\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/zrWwb.png\r\n  [2]: https://i.stack.imgur.com/Rr7oM.png	418	2019-12-11 04:31:44.157005+00	0	4	1	482	955	1685	141377	0	0	0	2019-12-11 04:31:44.157005+00	f	f	1	1
2	17	402	2018-02-27 11:37:24+00	Implicit conversion causing error part of the time	I have been scratching my head about this one for the last day or so - I just can't see why a procedure works in one environment, but then fails in another due to a conversion error (same data, same code).\r\n\r\nThe plan for (a heavily cut down version of) the working server is here: https://www.brentozar.com/pastetheplan/?id=B1jZWTfOf\r\n\r\n(This doesn't seem to be working, so I've uploaded the plan XML to pastebin here: [https://pastebin.com/47Q6nniw][1])\r\n\r\nA bit of background for the relevant columns causing the problem:\r\n\r\nThe propertyInst table holds a column called ValueStr, which is an nvarchar data type. The GeneralLedgerCode table has a column id of data type int. Our developers are attempting to join between the two tables. The ValueStr column holds text data, XML data, int data, decimal data etc. As a matter of course we decided to add a function to ensure only integers are parsed (ISNUMERIC(ValueStr) = 1). What we didn't realise is that this would then also try to convert any decimal values - this is causing a conversion error in production of the nature:\r\n\r\nConversion failed when converting the nvarchar value '0.1' to data type int.\r\n\r\nMy question here is, given that the Table Scan of the propertyInst table will be picking up all numeric values including decimals, why is the implicit conversion in the scalar operator then not failing with the same conversion error? The output of the ISNUMERIC query returns some of these decimals. I can't see how the attached plan is working at all.\r\n\r\nDoes an implicit conversion operator never fail outright, and is the conversion error coming from the probe residual in the hash match operator? Then again, how could the value even make it to the operator when it would fail the implicit conversion?\r\n\r\nAs a small assistance, the plan image is here, with the Compute Scalar I'm asking about circled.\r\n\r\n[![Plan][2]][2]\r\n\r\nIn addition, I can see that there are no actual rows logged against the scalar operator - does this mean the engine decided not to use that particular operator at runtime due to the conversion error?\r\n\r\n[![Plan2][3]][3]\r\n\r\nAny help would be appreciated.\r\n\r\n\r\n  [1]: https://pastebin.com/47Q6nniw\r\n  [2]: https://i.stack.imgur.com/S93iV.png\r\n  [3]: https://i.stack.imgur.com/bNzJW.png	413	2019-12-11 04:18:19.135795+00	0	4	1	477	947	1680	198901	0	0	0	2019-12-11 04:18:19.135795+00	f	f	1	1
2	38	424	2019-12-11 23:10:46.203808+00	Non-determinisitic sort & repeatable behavior in parallel plans	Using `OFFSET ... NEXT` over a non-unique sort column results in predictable & repeatable variance under different execution context in the presence of of parallelism. Why does this behavior occur and is there a productive inference that can be made about the optimizer based on this behavior?\r\n\r\n#### Some Background\r\n\r\nI received a support ticket saying something like...\r\n\r\n> I'm auditing some financial statements for `$time_period` and getting inconsistent results. When I execute `$stored_procedure`, I get $100; but when I run the `$ad-hoc-code` found _inside_ the SP with the same parameters, I get $200. _Why is SQL wrong?!_ \r\n\r\nThe underlying proc was paginating results by sorting on a non-unique column. Job done right? I closed the ticket as "_Don't sort on a non-unique column_".\r\n\r\n#### But why was it repeatable?\r\n\r\nIn reproducing & minifying the behavior, I was able to consistently induce... \r\n\r\n* a `ScanDirection="FORWARD"` for a **STORED-PROC EXECUTION** ([execution plan][ptp-STORED_PROC]) and\r\n* a `ScanDirection="BACKWARD"` for an **AD-HOC F5** ([execution plan][ptp-AD_HOC_F5]). \r\n\r\n...given the following pre-requisites...\r\n\r\n* Non-unique clustered index\r\n* With a non-clustered index\r\n* In a parallel plan\r\n* With sufficient padding data \r\n  * ~60k rows is the smallest dataset I've minified to as yet\r\n      * give me some credit I was starting vs ~200 million rows\r\n\r\n...which causes reproducible variance give the repro dataset. \r\n\r\nThese are the most noticable variances in [the diff of the SQLPlans][sqlplan-diff], although there are others (but I'm gonna feel real bad if this is somehow parameter sniffing & I just don't know how to read plans).\r\n\r\nThe fact that I can reproducably cause this specific change in the execution plan at this level is a little disorienting. To repeat from the **Summary**...\r\n\r\n> Why does this behavior occur and is there a productive inference that can be made about the optimizer based on this behavior?\r\n\r\n\r\n## [Reproduction][gist-repro]\r\n\r\n> TODO: fix [fiddle][fiddle-repro] batching (55k more to go 😬)\r\n> \r\n> TODO: plain-language describe stats histogram of psuedononimized data\r\n\r\nvery long fiddle here: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=4726813c2d4d6c23fd5d7a50756de4b6&hide=2\r\n\r\n## Other Trivia\r\n\t\r\n1. "_In the wild_", column `[xx]` was a `datetime2(7)` column (presumably some enterprising dev thought clustering in this was was equivalent to uniqueness). I modified it to `int` for readability while minifying the repro\r\n2. This question has sat as a stub for about a year as of posting. If you care for the deep cut, start [here][1]\r\n\r\n[ptp-STORED_PROC]: \r\nhttps://www.brentozar.com/pastetheplan/?id=r15VllyRB\r\n[ptp-AD_HOC_F5]: https://www.brentozar.com/pastetheplan/?id=SJcS-lJAB\r\n[gist-repro]: https://gist.github.com/petervandivier/a5497e7a1e8e52635499f10996c9355a\r\n[1]: https://github.com/petervandivier/hello-world/commit/8f3143efff657dfe167493e137105c55b7cbf90a\r\n[fiddle-repro]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=49b7f1c1d154473b7dd42c73c6b07f0d\r\n[sqlplan-diff]: https://gist.githubusercontent.com/petervandivier/a5497e7a1e8e52635499f10996c9355a/raw/5496f4fee31118b518382b1f48e194712eaa24c2/sqlplan.diff\r\n\r\n	435	2019-12-12 13:40:51.884368+00	0	4	1	499	1030	3402	\N	0	0	0	\N	f	f	1	\N
2	14	465	2018-12-04 21:15:03+00	Should the filtering column(s) always be in the keys / includes?	I'm considering creating a filtered index in my [copy of the Stack Overflow database][1].  Something like this, for example:\r\n\r\n    CREATE UNIQUE NONCLUSTERED INDEX IX_DisplayName_Filtered\r\n        ON dbo.Users (DisplayName)\r\n        WHERE Reputation > 400000;\r\n\r\nShould I always add the column in the filtering expression (`Reputation` in this example) to the key or includes for the index, or is having it in the filtering expression good enough?\r\n\r\n[1]: https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/	476	2019-12-18 21:54:18.936364+00	0	4	1	540	1124	2390	224149	0	0	0	2019-12-18 21:54:18.936364+00	f	f	1	1
2	14	482	2018-12-18 15:42:29+00	Why is this stream aggregate necessary?	Check out this query.  It's pretty simple (see the end of the post for table and index definitions, and a repro script):\r\n\r\n    SELECT MAX(Revision)\r\n    FROM dbo.TheOneders\r\n    WHERE Id = 1 AND 1 = (SELECT 1);\r\n\r\n> Note: the "AND 1 = (SELECT 1) is just to keep this query from being auto-parameterized, which I felt like was confusing the issue - it actually gets the same plan with or without that clause though\r\n\r\nAnd here's the plan ([paste the plan link)][1]:\r\n\r\n[![plan with a stream agg][2]][2]\r\n\r\nSince there is a "top 1" there, I was surprised to see the stream aggregate operator.  It doesn't seem necessary to me, since there is guaranteed to only be one row.\r\n\r\nTo test that theory, I tried out this logically equivalent query:\r\n\r\n    SELECT MAX(Revision)\r\n    FROM dbo.TheOneders\r\n    WHERE Id = 1\r\n    GROUP BY Id;\r\n\r\nHere's the plan for that one ([paste the plan link][4]):\r\n\r\n[![plan without a stream agg][3]][3]\r\n\r\nSure enough, the group by plan is able to get by without the stream aggregate operator.\r\n\r\nNotice that both queries read "backwards" from the end of the index and do a "top 1" to get the max revision.\r\n\r\nWhat am I missing here?  **Is the stream aggregate actually doing work in the first query, or should it be able to be eliminated (and it's just a limitation of the optimizer that it's not)?**\r\n\r\nBy the way, I realize this is not an incredibly practical problem (both queries report 0 ms of CPU and elapsed time), I'm just curious about the internals / behavior being exhibited here.\r\n\r\n---\r\n\r\nHere's the setup code I ran before running the two queries above:\r\n\r\n    DROP TABLE IF EXISTS dbo.TheOneders;\r\n    GO\r\n    \r\n    CREATE TABLE dbo.TheOneders\r\n    (\r\n        Id INT NOT NULL,\r\n        Revision SMALLINT NOT NULL,\r\n        Something NVARCHAR(23),\r\n    \r\n        CONSTRAINT PK_TheOneders PRIMARY KEY NONCLUSTERED (Id, Revision)\r\n    );\r\n    GO\r\n    \r\n    INSERT INTO dbo.TheOneders\r\n        (Id, Revision, Something)\r\n    SELECT DISTINCT TOP 1000 \r\n        1, m.message_id, 'Do...'\r\n    FROM sys.messages m\r\n    ORDER BY m.message_id\r\n    OPTION (MAXDOP 1);\r\n    \r\n    INSERT INTO dbo.TheOneders\r\n        (Id, Revision, Something)\r\n    SELECT DISTINCT TOP 100 \r\n        2, m.message_id, 'Do that thing you do...'\r\n    FROM sys.messages m\r\n    ORDER BY m.message_id\r\n    OPTION (MAXDOP 1);\r\n    GO\r\n\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=HyQibq8gE\r\n  [2]: https://i.stack.imgur.com/jRU17.png\r\n  [3]: https://i.stack.imgur.com/3WQGs.png\r\n  [4]: https://www.brentozar.com/pastetheplan/?id=SkCN7qLxV	494	2019-12-24 10:49:50.279812+00	0	4	1	557	1179	2666	225279	0	0	0	2019-12-24 10:49:50.279812+00	f	f	1	1
2	98	273	2017-09-27 18:57:44+00	Does SQL Server support GREATEST and LEAST, if not what is the common workaround?	Reviewing [this question](https://dba.stackexchange.com/q/186906/2639) it seems like that's a lot of work that shouldn't be needed. They're trying to extend a range with a date. In other databases, you would just use `greatest` and `least`..\r\n\r\n    least(extendDate,min), greatest(extendDate,max)\r\n\r\nWhen I try to use these though, I get\r\n\r\n    'least' is not a recognized built-in function name.\r\n    'greatest' is not a recognized built-in function name.\r\n\r\nThat would cover extension in either direction. \r\n\r\nFor the purposes of the question, you would still have to do exclusive range replacement.\r\n\r\nI'm just wondering how SQL Server users implement query patterns to mimic `least` and `greatest` functionality. \r\n\r\n* [PostgreSQL `GREATEST`/`LEAST`](https://www.postgresql.org/docs/current/static/functions-conditional.html#FUNCTIONS-GREATEST-LEAST)\r\n* [MySQL `GREATEST`/`LEAST`](https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html)\r\n* MariaDB [`GREATEST`](https://mariadb.com/kb/en/library/greatest/) [`LEAST`](https://mariadb.com/kb/en/library/least/)\r\n* DB2 [`GREATEST`](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_9.7.0/com.ibm.db2.luw.sql.ref.doc/doc/r0052623.html) [`LEAST`](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_9.7.0/com.ibm.db2.luw.sql.ref.doc/doc/r0052624.html)\r\n* Oracle [`GREATEST`](https://docs.oracle.com/database/121/SQLRF/functions078.htm#SQLRF00645) [`LEAST`](https://docs.oracle.com/database/121/SQLRF/functions099.htm#SQLRF00657)\r\n\r\nDo you unroll the conditions into `CASE` statements or is there an extension, third party add-on, or license from Microsoft that enables this functionality?	281	2019-12-04 14:17:44.036055+00	0	4	1	348	601	1508	187090	0	0	0	2019-12-04 14:17:44.036055+00	f	f	1	1
2	622	449	2013-04-04 18:05:52+00	Gaps and islands: client solution vs T-SQL query	Can a T-SQL solution for gaps and islands run faster than a C# solution running on the client?\r\n\r\nTo be specific, let us provide some test data:\r\n\r\n    CREATE TABLE dbo.Numbers\r\n      (\r\n        n INT NOT NULL\r\n              PRIMARY KEY\r\n      ) ; \r\n    GO \r\n    \r\n    INSERT  INTO dbo.Numbers\r\n            ( n )\r\n    VALUES  ( 1 ) ; \r\n    GO \r\n    DECLARE @i INT ; \r\n    SET @i = 0 ; \r\n    WHILE @i < 21 \r\n      BEGIN \r\n        INSERT  INTO dbo.Numbers\r\n                ( n \r\n                )\r\n                SELECT  n + POWER(2, @i)\r\n                FROM    dbo.Numbers ; \r\n        SET @i = @i + 1 ; \r\n      END ;  \r\n    GO\r\n    \r\n    CREATE TABLE dbo.Tasks\r\n      (\r\n        StartedAt SMALLDATETIME NOT NULL ,\r\n        FinishedAt SMALLDATETIME NOT NULL ,\r\n        CONSTRAINT PK_Tasks PRIMARY KEY ( StartedAt, FinishedAt ) ,\r\n        CONSTRAINT UNQ_Tasks UNIQUE ( FinishedAt, StartedAt )\r\n      ) ;\r\n    GO\r\n    \r\n    INSERT  INTO dbo.Tasks\r\n            ( StartedAt ,\r\n              FinishedAt\r\n            )\r\n            SELECT  DATEADD(MINUTE, n, '20100101') AS StartedAt ,\r\n                    DATEADD(MINUTE, n + 2, '20100101') AS FinishedAt\r\n            FROM    dbo.Numbers\r\n            WHERE   ( n < 500000\r\n                      OR n > 500005\r\n                    )\r\n    GO\r\n\r\nThis first set of test data has exactly one gap:\r\n\r\n    SELECT  StartedAt ,\r\n            FinishedAt\r\n    FROM    dbo.Tasks\r\n    WHERE   StartedAt BETWEEN DATEADD(MINUTE, 499999, '20100101')\r\n                      AND     DATEADD(MINUTE, 500006, '20100101')\r\n\r\n\r\nThe second set of test data has 2M -1 gaps, a gap between each two adjacent intervals:\r\n\r\n    TRUNCATE TABLE dbo.Tasks;\r\n    GO\r\n    \r\n    INSERT  INTO dbo.Tasks\r\n            ( StartedAt ,\r\n              FinishedAt\r\n            )\r\n            SELECT  DATEADD(MINUTE, 3*n, '20100101') AS StartedAt ,\r\n                    DATEADD(MINUTE, 3*n + 2, '20100101') AS FinishedAt\r\n            FROM    dbo.Numbers\r\n            WHERE   ( n < 500000\r\n                      OR n > 500005\r\n                    )\r\n    GO\r\n\r\nCurrently I am running 2008 R2, but 2012 solutions are very welcome.\r\nI have posted my C# solution as an answer.\r\n	460	2019-12-15 06:20:35.081438+00	0	4	1	524	1086	2134	39272	0	0	0	2019-12-15 06:20:35.081438+00	f	f	1	1
1	811	622	2020-01-26 20:31:23.290321+00	What's up with the scoll bars?	Is there some sort of scaling going on? I keep getting these mangled scroll bar buttons. Other sites never show this behaviour at the same zoom and resolution level (magnified 4 times):\r\n\r\n![funky-scroll.png](/image?hash=ba94bada2b2f9ffef24c9b97d86f54ff5579ae14fd784063eecfddf12066c3a0)	637	2020-01-26 20:31:23.290321+00	2	1	1	697	1745	3395	\N	0	0	0	\N	f	f	2	\N
2	8	110	2015-07-09 13:23:09+00	sp_cursoropen and parallelism	I'm running into a performance problem with a query that I can't seem to get my head around.\r\n\r\nI pulled the query out of a cursor definition.\r\n\r\nThis query takes seconds to execute\r\n\r\n    SELECT A.JOBTYPE\r\n    FROM PRODROUTEJOB A\r\n    WHERE ((A.DATAAREAID=N'IW')\r\n    AND ((A.CALCTIMEHOURS<>0)\r\n    AND (A.JOBTYPE<>3)))\r\n    AND EXISTS (SELECT 'X'\r\n    FROM PRODROUTE B\r\n    WHERE ((B.DATAAREAID=N'IW')\r\n    AND (((((B.PRODID=A.PRODID)\r\n    AND ((B.PROPERTYID=N'PR1526157') OR (B.PRODID=N'PR1526157')))\r\n    AND (B.OPRNUM=A.OPRNUM))\r\n    AND (B.OPRPRIORITY=A.OPRPRIORITY))\r\n    AND (B.OPRID=N'GRIJZEN')))\r\n    AND NOT EXISTS (SELECT 'X'\r\n    FROM ADUSHOPFLOORROUTE C\r\n    WHERE ((C.DATAAREAID=N'IW')\r\n    AND ((((((C.WRKCTRID=A.WRKCTRID)\r\n    AND (C.PRODID=B.PRODID))\r\n    AND (C.OPRID=B.OPRID))\r\n    AND (C.JOBTYPE=A.JOBTYPE))\r\n    AND (C.FROMDATE>{TS '1900-01-01 00:00:00.000'}))\r\n    AND ((C.TODATE={TS '1900-01-01 00:00:00.000'}))))))\r\n    GROUP BY A.JOBTYPE\r\n    ORDER BY A.JOBTYPE\r\n\r\nThe actual execution plan looks like this.\r\n\r\n![enter image description here][1]\r\n\r\n\r\nNoticing the server wide setting was set to MaxDOP 1 I tried playing around with maxdop settings.\r\n\r\nAdding `OPTION (MAXDOP 0)` to the query, or changing the server settings results in much better performance and this query plan.\r\n\r\n![enter image description here][2]\r\n\r\nHowever, the application in question (Dynamics AX) doesn't execute queries like this, it uses cursors.\r\n\r\nThe actual code captured is this.\r\n\r\n    declare @p1 int\r\n    set @p1=189527589\r\n    declare @p3 int\r\n    set @p3=16\r\n    declare @p4 int\r\n    set @p4=1\r\n    declare @p5 int\r\n    set @p5=2\r\n    exec sp_cursoropen @p1 output,N'SELECT A.JOBTYPE FROM PRODROUTEJOB A WHERE ((A.DATAAREAID=N''IW'') AND ((A.CALCTIMEHOURS<>0) AND (A.JOBTYPE<>3))) AND EXISTS (SELECT ''X'' FROM PRODROUTE B WHERE ((B.DATAAREAID=N''IW'') AND (((((B.PRODID=A.PRODID) AND ((B.PROPERTYID=N''PR1526157'') OR (B.PRODID=N''PR1526157''))) AND (B.OPRNUM=A.OPRNUM)) AND (B.OPRPRIORITY=A.OPRPRIORITY)) AND (B.OPRID=N''GRIJZEN''))) AND NOT EXISTS (SELECT ''X'' FROM ADUSHOPFLOORROUTE C WHERE ((C.DATAAREAID=N''IW'') AND ((((((C.WRKCTRID=A.WRKCTRID) AND (C.PRODID=B.PRODID)) AND (C.OPRID=B.OPRID)) AND (C.JOBTYPE=A.JOBTYPE)) AND (C.FROMDATE>{TS ''1900-01-01 00:00:00.000''})) AND ((C.TODATE={TS ''1900-01-01 00:00:00.000''})))))) GROUP BY A.JOBTYPE ORDER BY A.JOBTYPE ',@p3 output,@p4 output,@p5 output\r\n    select @p1, @p3, @p4, @p5\r\n\r\nresulting in this execution plan (and unfortunately the same multiple-second execution times).\r\n\r\n![enter image description here][3]\r\n\r\nI've tried several things such as dropping cached plans, adding options in the query inside the cursor definition, ... But none of them seem to get me a parallel plan.\r\n\r\nI've also searched google for quite a bit looking for parallelism limitations of cursors but can't seem to find any limitations.\r\n\r\nAm I missing something obvious here?\r\n\r\nThe actual SQL build is `SQL Server 2008 (SP1) - 10.0.2573.0 (X64)` which i realise is unsupported, but I cannot upgrade this instance as I see fit. I would need to transfer the database to another server and that would mean pulling a fairly large uncompressed backup over a slow WAN.\r\n\r\nTrace flag 4199 doesn't make a difference, and neither does OPTION (RECOMPILE).\r\n\r\nThe cursor properties are: \r\n\r\n    API | Fast_Forward | Read Only | Global (0)\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/HfDJC.png\r\n  [2]: http://i.stack.imgur.com/m6TLE.png\r\n  [3]: http://i.stack.imgur.com/ofCcr.png	117	2019-11-25 23:43:20.457+00	0	4	1	185	270	266	106454	0	0	0	2019-11-25 23:43:20.457+00	f	f	1	1
2	287	406	2018-05-25 07:39:19+00	Why am I not getting minimal logging when inserting to indexed tables	I'm testing minimal logging inserts in different scenarios and from what I've read `INSERT INTO SELECT` into a heap with a non clustered index using `TABLOCK` and SQL Server 2016+ should minimally log, however in my case when doing this I'm getting full logging. My database is in the simple recovery model and I successfully get minimally logged inserts on a heap with no indexes and `TABLOCK`.\r\n\r\nI'm using an old backup of the Stack Overflow database to test on and have created a replicate of the Posts table with the following schema...\r\n\r\n    CREATE TABLE [dbo].[PostsDestination](\r\n    \t[Id] [int] NOT NULL,\r\n    \t[AcceptedAnswerId] [int] NULL,\r\n    \t[AnswerCount] [int] NULL,\r\n    \t[Body] [nvarchar](max) NOT NULL,\r\n    \t[ClosedDate] [datetime] NULL,\r\n    \t[CommentCount] [int] NULL,\r\n    \t[CommunityOwnedDate] [datetime] NULL,\r\n    \t[CreationDate] [datetime] NOT NULL,\r\n    \t[FavoriteCount] [int] NULL,\r\n    \t[LastActivityDate] [datetime] NOT NULL,\r\n    \t[LastEditDate] [datetime] NULL,\r\n    \t[LastEditorDisplayName] [nvarchar](40) NULL,\r\n    \t[LastEditorUserId] [int] NULL,\r\n    \t[OwnerUserId] [int] NULL,\r\n    \t[ParentId] [int] NULL,\r\n    \t[PostTypeId] [int] NOT NULL,\r\n    \t[Score] [int] NOT NULL,\r\n    \t[Tags] [nvarchar](150) NULL,\r\n    \t[Title] [nvarchar](250) NULL,\r\n    \t[ViewCount] [int] NOT NULL\r\n    )\r\n    CREATE NONCLUSTERED INDEX ndx_PostsDestination_Id ON PostsDestination(Id)\r\n\r\nI then try to copy the posts table into this table...\r\n\r\n    INSERT INTO PostsDestination WITH(TABLOCK)\r\n    SELECT * FROM Posts ORDER BY Id \r\n\r\nFrom looking at fn_dblog and the log file usage I can see I'm not getting minimal logging from this. I've read that versions before 2016 require trace flag 610 to minimally log to indexed tables, I've also tried setting this but still no joy.\r\n\r\nI'm guessing I'm missing something here?\r\n\r\n## More information\r\n\r\nTo add more info I'm using the following procedure that I've written to try to detect minimal logging, maybe I've got something here wrong...\r\n\r\n    /*\r\n    \tExample Usage...\r\n    \r\n    \tEXEC sp_GetLogUseStats\r\n       @Sql = '\r\n          INSERT INTO PostsDestination\r\n          SELECT TOP 500000 * FROM Posts ORDER BY Id ',\r\n       @Schema = 'dbo',\r\n       @Table = 'PostsDestination',\r\n       @ClearData = 1\r\n    \r\n    */\r\n    \r\n    CREATE PROCEDURE [dbo].[sp_GetLogUseStats]\r\n    (\t\r\n       @Sql NVARCHAR(400),\r\n       @Schema NVARCHAR(20),\r\n       @Table NVARCHAR(200),\r\n       @ClearData BIT = 0\r\n    )\r\n    AS\r\n    \r\n    IF @ClearData = 1\r\n       BEGIN\r\n       TRUNCATE TABLE PostsDestination\r\n       END\r\n    \r\n    /*Checkpoint to clear log (Assuming Simple/Bulk Recovery Model*/\r\n    CHECKPOINT\t\r\n    \r\n    /*Snapshot of logsize before query*/\r\n    CREATE TABLE #BeforeLogUsed(\r\n       [Db] NVARCHAR(100),\r\n       LogSize NVARCHAR(30),\r\n       Used NVARCHAR(50),\r\n       Status INT\r\n    )\r\n    INSERT INTO #BeforeLogUsed\r\n    EXEC('DBCC SQLPERF(logspace)')\r\n    \r\n    /*Run Query*/\r\n    EXECUTE sp_executesql @SQL\r\n    \r\n    /*Snapshot of logsize after query*/\r\n    CREATE TABLE #AfterLLogUsed(\t\r\n       [Db] NVARCHAR(100),\r\n       LogSize NVARCHAR(30),\r\n       Used NVARCHAR(50),\r\n       Status INT\r\n    )\r\n    INSERT INTO #AfterLLogUsed\r\n    EXEC('DBCC SQLPERF(logspace)')\r\n    \r\n    /*Return before and after log size*/\r\n    SELECT \r\n       CAST(#AfterLLogUsed.Used AS DECIMAL(12,4)) - CAST(#BeforeLogUsed.Used AS DECIMAL(12,4)) AS LogSpaceUsersByInsert\r\n    FROM \r\n       #BeforeLogUsed \r\n       LEFT JOIN #AfterLLogUsed ON #AfterLLogUsed.Db = #BeforeLogUsed.Db\r\n    WHERE \r\n       #BeforeLogUsed.Db = DB_NAME()\r\n    \r\n    /*Get list of affected indexes from insert query*/\r\n    SELECT \r\n       @Schema + '.' + so.name + '.' +  si.name AS IndexName\r\n    INTO \r\n       #IndexNames\r\n    FROM \r\n       sys.indexes si \r\n       JOIN sys.objects so ON si.[object_id] = so.[object_id]\r\n    WHERE \r\n       si.name IS NOT NULL\r\n       AND so.name = @Table\r\n    /*Insert Record For Heap*/\r\n    INSERT INTO #IndexNames VALUES(@Schema + '.' + @Table)\r\n    \r\n    /*Get log recrod sizes for heap and/or any indexes*/\r\n    SELECT \r\n       AllocUnitName,\r\n       [operation], \r\n       AVG([log record length]) AvgLogLength,\r\n       SUM([log record length]) TotalLogLength,\r\n       COUNT(*) Count\r\n    INTO #LogBreakdown\r\n    FROM \r\n       fn_dblog(null, null) fn\r\n       INNER JOIN #IndexNames ON #IndexNames.IndexName = allocunitname\r\n    GROUP BY \r\n       [Operation], AllocUnitName\r\n    ORDER BY AllocUnitName, operation\r\n    \r\n    SELECT * FROM #LogBreakdown\r\n    SELECT AllocUnitName, SUM(TotalLogLength)  TotalLogRecordLength \r\n    FROM #LogBreakdown\r\n    GROUP BY AllocUnitName\r\n\r\nInserting into a heap with no indexes and TABLOCK using following code...\r\n\r\n    EXEC sp_GetLogUseStats\r\n       @Sql = '\r\n          INSERT INTO PostsDestination\r\n          SELECT * FROM Posts ORDER BY Id ',\r\n       @Schema = 'dbo',\r\n       @Table = 'PostsDestination',\r\n       @ClearData = 1\r\n\r\nI get these results\r\n\r\n[![enter image description here][1]][1]\r\n\r\nAt 0.0024mb log file growth, very small log record sizes and very few of them I'm happy that this is using minimal logging.\r\n\r\nIf I then create a non clustered index on id...\r\n\r\n\r\n    CREATE INDEX ndx_PostsDestination_Id ON PostsDestination(Id)\r\n\r\nThen run my same insert again...\r\n\r\n[![enter image description here][2]][2]\r\n\r\n\r\nNot only am I not getting minimal logging on the non clustered index but I've also lost it on the heap. After doing some more tests it seems if I make ID clustered it does minimally log but from what I've read 2016+ should minimally log to a heap with non clustered index when tablock is used. \r\n\r\nI've reported the behaviour to Microsoft on the [SQL Server UserVoice](https://feedback.azure.com/forums/908035-sql-server/suggestions/34407871-sql-server-2017-minimal-logging-not-behaving-as-d) and will update if I get a response. I've also written up the full details of the minimal log scenarios that I couldn't get to work at https://gavindraper.com/2018/05/29/SQL-Server-Minimal-Logging-Inserts/\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/pvmcA.png\r\n  [2]: https://i.stack.imgur.com/k8G7D.png\r\n  [3]: https://feedback.azure.com/forums/908035-sql-server/suggestions/34407871-sql-server-2017-minimal-logging-not-behaving-as-do	417	2019-12-11 04:29:41.686013+00	0	4	1	481	953	3153	207814	0	0	0	2019-12-11 04:28:29.005212+00	f	f	1	1
2	132	227	2018-08-31 14:22:20+00	Actual rows read in a table scan is multiplied by the number of threads used for the scan	I'm running into quite a strange problem. I'm running the same script to generate data and do some matching later on, on an older 2008R2 instance. The last query (an `UPDATE`) does a single table scan and returns all of the 250.000 rows, whereas on a newer 2017 instance, the table is scanned in parallel and each of the 4 threads reads 250.000 rows, and returns 1 million "actual rows read".\r\n\r\nI changed the compatibility mode to 2008 in my 2017 instance and the actuals stayed the same, at 1.000.000.\r\n\r\nIs there any valid reason for why this would happen or does this seem like it should be a Connect item?\r\n\r\nPlans contain the same operators, but one of them does the scan in parallel and instead of splitting the 250.000 rows per each of the 4 threads (and only read 62.500 rows on each thread) all the threads read 250k each *4 = 1.000.000\r\n\r\nBoth execution plans can be found at pastetheplan:\r\n\r\n- [**2017 version here**][1]\r\n- [**2008R2 version here**][2]\r\n\r\nAlso, the full script I was running can be found down below:\r\n\r\n    create table #targets (id int identity(1,1), start_point int, end_point int, refference_type_id int, bla1 int, bla2 int, bla3 int, bla4 int, bla5 int, bla6 int, bla7 int, bla8 int, bla9 bit, assignedTouch varchar(10));\r\n    \r\n    ;with cte as (\r\n    \tselect\r\n    \t\t1 sp\r\n    \t\t, abs(checksum(newid())) % 11 + 2 ep\r\n    \t\t, 1 rn\r\n    \t\t, abs(checksum(newid())) % 3 b\r\n    \t\t, abs(checksum(newid())) % 3 c\r\n    \t\t, abs(checksum(newid())) % 3 d\r\n    \t\t, abs(checksum(newid())) % 3 e\r\n    \t\t, abs(checksum(newid())) % 3 f\r\n    \t\t, abs(checksum(newid())) % 3 g\r\n    \t\t, abs(checksum(newid())) % 3 g2\r\n    \t\t, abs(checksum(newid())) % 3 h\r\n    \t\t, abs(checksum(newid())) % 3 x3\r\n    \tunion all\r\n    \tselect\r\n    \t\tsp\r\n    \t\t, abs(checksum(newid())) % 11 + 2 ep\r\n    \t\t, rn + 1\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \t\t, abs(checksum(newid())) % 8\r\n    \tfrom cte\r\n    \twhere rn < 250000)\r\n    insert into #targets\r\n    select *\r\n    \t, 'Unassigned' [Default State]\r\n    from cte\r\n    option (maxrecursion 0)\r\n    \r\n    select top 250 *\r\n    \t, char(abs(checksum(newid())) % 85 + 65) [class]\r\n    into #matching\r\n    from #targets\r\n    where end_point in ( 11, 14, 22, 33 )\r\n    order by abs(checksum(newid())) % 13\r\n    \r\n    update #matching\r\n    set bla8 = case \r\n    \t\t\twhen bla1 - bla2 > 0\r\n    \t\t\t\tthen NULL\r\n    \t\t\telse bla8\r\n    \t\tend\r\n    \t, bla7 = case \r\n    \t\t\twhen bla2 - bla3 > 0\r\n    \t\t\t\tthen NULL\r\n    \t\t\telse bla7\r\n    \t\tend\r\n    \t, bla6 = case \r\n    \t\t\twhen bla4 - bla5 > 0\r\n    \t\t\t\tthen NULL\r\n    \t\t\telse bla6\r\n    \t\tend\r\n    \r\n    create nonclustered index nc_assignedTouch on #targets (assignedTouch);\r\n    \r\n    update t\r\n    set assignedTouch = m.class\r\n    from #targets t\r\n    \tinner join #matching m\r\n    \t\ton t.start_point = isnull(m.start_point, t.start_point)\r\n    \t\t\tand t.bla1 = isnull(m.bla1, t.bla1)\r\n    \t\t\tand t.bla2 = isnull(m.bla2, t.bla2)\r\n    \t\t\tand t.bla3 = isnull(m.bla3, t.bla3)\r\n    \t\t\tand t.bla4 = isnull(m.bla4, t.bla4)\r\n    \t\t\tand t.bla5 = isnull(m.bla5, t.bla5)\r\n    \t\t\tand t.bla6 = isnull(m.bla6, t.bla6)\r\n    \t\t\tand t.bla7 = isnull(m.bla7, t.bla7)\r\n    \t\t\tand t.bla8 = isnull(m.bla8, t.bla8)\r\n    \t\t\tand t.bla9 = isnull(m.bla9, t.bla9)\r\n    where t.assignedTouch = 'Unassigned';\r\n\r\nsp_configure info:\r\n\r\n[![enter image description here][3]][3]\r\n\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=SkSy66IPX\r\n  [2]: https://www.brentozar.com/pastetheplan/?id=rJ3LA6UDQ\r\n  [3]: https://i.stack.imgur.com/FpvSA.png	235	2019-12-01 17:48:05.842749+00	0	4	1	302	479	577	216422	0	0	0	2019-12-01 17:48:05.842749+00	f	f	1	1
1	821	623	2020-01-26 20:54:19.980854+00	UI suggestions	I have some comments about bits where I found the UI to be unintuitive or unnatural. I'm using a laptop (1920x1080).\r\n\r\n\r\n![TopAnswers top left.png](/image?hash=eb9f934cac0a48f4c68f7e9d57b0f245430b1542ccff6b460a5c580e8f43d018)\r\n\r\nOften I'm browsing a question and want to return to view other questions on the site. Currently, this can be done with the "TopAnswers" link at the top left, but I'd expect that to go to a general main page. It would be nice if the site name under it linked to its question page. I also expected that re-selecting the current site from the dropdown would take me to its question page, but it just leaves me on the question.\r\n\r\n![TopAnswers question UI snippet.png](/image?hash=3437d601949d291c457c12ef3bf529955a9162bcc43dbe806f91bcd6e4fa7779)\r\n\r\nWith one answer here, it looks like  Adám was the one who posted the answer, because his name and icon are immediately above it. The name and icon of the actual answerer Skillmon are all the way to the right, where they are easy to miss.\r\n\r\n![TopAnswers top right.png](/image?hash=16a1c4c6c586ca10d1421bcf5c3742c56ee1e2b84e58e14b437f2faed7063967)\r\n\r\nIt took me surprisingly long to find the "submit" button for my first post. It seems to be hidden among drop-downs that looks kind-of similar to it. I also expected it to be at the bottom below the edit window, continuing the downward flow when typing out an answer.\r\n\r\n![TopAnswers question UI snippet2.png](/image?hash=4aa964d5b452fcf0d7e164b722f4d9aaf34a3746cbdba57bccabb6bedb068808)\r\n\r\nThe license link is much too prominent here, with the hyperlink color and underline making it stand out more than the poster and date.\r\n\r\n![TopAnswers chat snippet.png](/image?hash=27c96554af52b71c1313d105ea6e234528cf80f81a72beb8e955c81bdc403974)\r\n\r\nI think the time of the post should appear without mouseover. The "X minutes later" often doesn't help with this because I don't know that message was posted either.	638	2020-01-26 20:54:19.980854+00	5	4	1	698	1810	3389	\N	0	0	0	\N	f	f	2	\N
4	289	414	2019-12-08 19:49:54+00	Move \\insertnavigation to the left in later sections in beamer footer	I am inserting the list of section names in the footer using:\r\n\r\n```\r\n\\vskip2pt\\insertnavigation{\\paperwidth}\\vskip-2pt\r\n```\r\nThe complete code for the footer (using the metropolis theme) is\r\n```\r\n\\makeatletter\r\n\\setbeamertemplate{footline}{%\r\n  \\begin{beamercolorbox}[wd=\\textwidth, sep=3ex]{footline}%\r\n    \\usebeamerfont{page number in head/foot}%\r\n    \\usebeamertemplate*{frame footer}\r\n    \\hfill%\r\n    \\usebeamertemplate*{frame numbering}\r\n  \\end{beamercolorbox}%\r\n\r\n  \\begin{beamercolorbox}[colsep=1.5pt]{upper separation line head}\r\n  \\end{beamercolorbox}\r\n  \\begin{beamercolorbox}{section in head/foot}\r\n    \\vskip2pt\\insertnavigation{\\paperwidth}\\vskip-2pt\r\n  \\end{beamercolorbox}%\r\n  \\begin{beamercolorbox}[colsep=1.5pt]{lower separation line head}\r\n  \\end{beamercolorbox}\r\n}\r\n\\makeatother\r\n```\r\n\r\nHowever I have too many sections and it escapes the end of the footer. See picture of slide:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nTo fix this I would like to move the navigation to the left as the current section increases. Something like:\r\n\r\n```\r\n\\vskip-{\\sectionnumber}pt\\insertnavigation{\\paperwidth}\\vskip-2pt\r\n```\r\nwhere `\\sectionnumber` is the number of the current section (made up the command).\r\n\r\n  [1]: https://i.stack.imgur.com/yvKTR.png	425	2019-12-11 15:53:39.440869+00	0	4	1	489	1073	1730	519739	0	0	0	2019-12-11 15:53:39.440869+00	f	f	1	2
2	637	451	2019-12-16 09:57:27+00	Sometimes the updated rows are not locked within instead of update trigger	Here is as small repro:\r\n\r\n    create table dbo.t (id int primary key, v int);\r\n    insert into dbo.t values (1, 1), (2, 2);\r\n    \r\n    create table dbo.s (id int primary key, v int);\r\n    insert into dbo.s values (1, 10);\r\n    go\r\n    \r\n    create trigger dbo.tr_t__iou\r\n    on dbo.t\r\n    instead of update\r\n    as\r\n    begin\r\n     set nocount on;\r\n    \r\n     exec sp_lock @@spid;\r\n    end;\r\n    go\r\n    \r\n    update dbo.t set v = 10 where id = 1;\r\n    \r\n    update t\r\n     set\r\n      v = 10\r\n    from\r\n     dbo.s s join\r\n     dbo.t t on t.id = s.id;\r\n    \r\n    update t\r\n     set\r\n      v = 10\r\n    from\r\n     (values (1, 10)) s(id, v) join\r\n     dbo.t t on t.id = s.id;\r\n    go\r\n    \r\n    drop table dbo.t, dbo.s;\r\n    go\r\n\r\n`sp_lock` within the `trigger` reports U-`key lock` on the affected row in the first and the last cases, but in the second case there is no `lock` at all, how can it be explained?	462	2019-12-16 13:02:52.984238+00	0	4	1	526	1097	2137	255692	0	0	0	2019-12-16 13:02:52.984238+00	f	f	1	1
2	108	192	2017-07-21 14:46:49+00	Strange behaviour with sample sizes for statistics updates	I've been playing around investigating sampling thresholds with statistics updates on SQL Server (2012) and noticed some curious behaviour. Basically the number of rows sampled seems to vary under some circumstances - even with the same set of data. \r\n\r\nI run this query:\r\n\r\n    --Drop table if exists\r\n    IF (OBJECT_ID('dbo.Test')) IS NOT NULL DROP TABLE dbo.Test;\r\n    \r\n    --Create Table for Testing\r\n    CREATE TABLE dbo.Test(Id INT IDENTITY(1,1) CONSTRAINT PK_Test PRIMARY KEY CLUSTERED, TextValue VARCHAR(20) NULL);\r\n\r\n    --Insert enough data so we have more than 8Mb (the threshold at which sampling kicks in)\r\n    INSERT INTO dbo.Test(TextValue) \r\n    SELECT TOP 1000000 'blahblahblah'\r\n    FROM sys.objects a, sys.objects b, sys.objects c, sys.objects d;  \r\n\r\n    --Create Index on TextValue\r\n    CREATE INDEX IX_Test_TextValue ON dbo.Test(TextValue);\r\n\r\n    --Update Statistics without specifying how many rows to sample\r\n    UPDATE STATISTICS dbo.Test IX_Test_TextValue;\r\n\r\n    --View the Statistics\r\n    DBCC SHOW_STATISTICS('dbo.Test', IX_Test_TextValue) WITH STAT_HEADER;\r\n\r\n\r\nWhen I look at the output of the SHOW_STATISTICS I'm finding that the "Rows Sampled" varies with each full execution (i.e. the table gets dropped, recreated and repopulated). \r\n\r\nFor example:\r\n\r\nRows Sampled\r\n\r\n - 318618 \r\n - 319240 \r\n - 324198 \r\n - 314154\r\n\r\nMy expectation was that this figure would be the same each time as the table is identical. By the way, I don't get this behaviour if I just delete the data and re-insert it. \r\n \r\nIt's not a critical question, but I'd be interested in understanding what's going on.	199	2019-11-29 08:45:09.931863+00	0	4	1	267	405	442	180461	0	0	0	2019-11-29 08:45:09.931863+00	f	f	1	1
4	650	453	2019-12-16 13:09:14+00	Latex Beamer same reference on different slides with footcite	I use the customized footcite command from https://tex.stackexchange.com/questions/123104/inline-citations-with-only-author-title-and-year:\r\n\r\n    \\newcommand{\\customcite}[1]{\\citeauthor{#1}, \\citetitle{#1}, \\citeyear{#1}}\r\n\r\nMy problem is now, when I have the same reference on multiple slides, the footnote number is increased. I would like to have the same number for the same source on every slide where it appears.\r\n\r\nI found something with `savefootnote`, but this seems to be very complicated: https://tex.stackexchange.com/questions/27763/beamer-multiple-references-to-the-same-footnote. Also, it is only possible to store one reference at a time, but I would like to have 3 references each on 2 pages.\r\n\r\nMWE:\r\n\r\n    \\documentclass[11pt]{beamer}\r\n    %\\documentclass[handout,11pt]{beamer}\r\n    \\usepackage[utf8]{inputenc}\r\n    \\usepackage[T1]{fontenc}\r\n    \\usetheme{Madrid}\r\n    \r\n    \r\n    % literature\r\n    % only for this example, otherwise in .bib file\r\n    \\usepackage{filecontents}\r\n    \r\n    \\begin{filecontents}{\\jobname.bib}\r\n    @article{Harshman1970,\r\n    \tauthor = {and others Harshman, Richard A},\r\n    \tdoi = {10.1134/S0036023613040165},\r\n    \tfile = {:C$\\backslash$:/Users/Isi/Documents/Research/Tensors/TensorsDocumentation/Literature/Harshman{\\_}CPD.pdf:pdf},\r\n    \tissn = {00360236},\r\n    \tjournal = {UCLA Working Papers in Phonetics},\r\n    \tpages = {1--84},\r\n    \ttitle = {{Foundations of the PARAFAC procedure: Models and conditions for an" explanatory" multimodal factor analysis}},\r\n    \tvolume = {16},\r\n    \tyear = {1970}\r\n    }\r\n    @article{Hitchcock1927,\r\n    \tauthor = {Hitchcock, Frank L.},\r\n    \tdoi = {10.1002/sapm192761164},\r\n    \tfile = {:C$\\backslash$:/Users/Isi/Documents/Research/Tensors/TensorsDocumentation/Literature/Hitchcock{\\_}cpd.pdf:pdf},\r\n    \tissn = {0097-1421},\r\n    \tjournal = {Journal of Mathematics and Physics},\r\n    \tnumber = {1-4},\r\n    \tpages = {164--189},\r\n    \ttitle = {{The Expression of a Tensor or a Polyadic as a Sum of Products}},\r\n    \tvolume = {6},\r\n    \tyear = {1927}\r\n    }\r\n    @article{Carroll1970,\r\n    \tdoi = {10.1007/BF02310791},\r\n    \tfile = {:C$\\backslash$:/Users/Isi/Documents/Research/Tensors/TensorsDocumentation/Literature/Carrol{\\_}Chang{\\_}CPD.pdf:pdf},\r\n    \tissn = {00333123},\r\n    \tjournal = {Psychometrika},\r\n    \tnumber = {3},\r\n    \tpages = {283--319},\r\n    \ttitle = {{Analysis of individual differences in multidimensional scaling via an n-way generalization of "Eckart-Young" decomposition}},\r\n    \tvolume = {35},\r\n    \tyear = {1970}\r\n    }\r\n    \r\n    \\end{filecontents}\r\n    \r\n    \\usepackage[style=verbose,backend=bibtex]{biblatex}\r\n    \\addbibresource{\\jobname.bib}\r\n    \\newcommand{\\customfootcite}[1]{\\footnote{\\citeauthor{#1}, \\citetitle{#1}, \\citeyear{#1}}}\r\n    \r\n    \r\n    \r\n    \\begin{document}\r\n    \t\\setbeamertemplate{navigation symbols}{}\r\n    \t\r\n    \t\\begin{frame}{Slide 1\\customfootcite{Harshman1970}\\customfootcite{Hitchcock1927}\\customfootcite{Carroll1970}}\r\n    \t\tsome text\r\n    \t\\end{frame}\r\n    \r\n    \t\\begin{frame}{Slide 2\\customfootcite{Harshman1970}\\customfootcite{Hitchcock1927}\\customfootcite{Carroll1970}}\r\n    \tsome figure\r\n    \t\\end{frame}\r\n    \r\n    \\end{document}\r\n\r\nGives this:\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\nI want both slides to have the same numbers.\r\n\r\nThank you in advance!\r\n\r\n  [1]: https://i.stack.imgur.com/80baB.png	464	2019-12-16 22:33:31.384173+00	0	4	1	528	1134	2149	520652	0	0	0	2019-12-16 22:33:31.384173+00	f	f	1	2
2	12	149	2016-05-08 14:07:58+00	Logical reads different when accessing the same LOB data	Here are three simple tests that read the same data, yet report very different logical reads:\r\n\r\n### Setup\r\n\r\nThe following script creates a test table with 100 identical rows, each containing an *xml* column with enough data to ensure it is stored off row. In my test database, the length of the *xml* generated is 20,204 bytes for each row.\r\n\r\n    -- Conditional drop\r\n    IF OBJECT_ID(N'dbo.XMLTest', N'U') IS NOT NULL\r\n        DROP TABLE dbo.XMLTest;\r\n    GO\r\n    -- Create test table\r\n    CREATE TABLE dbo.XMLTest\r\n    (\r\n        ID integer IDENTITY PRIMARY KEY,\r\n        X xml NULL\r\n    );\r\n    GO\r\n    -- Add 100 wide xml rows\r\n    DECLARE @X xml;\r\n    \r\n    SET @X =\r\n    (\r\n        SELECT TOP (100) *\r\n        FROM  sys.columns AS C\r\n        FOR XML \r\n            PATH ('row'),\r\n            ROOT ('root'),\r\n            TYPE\r\n    );\r\n    \r\n    INSERT dbo.XMLTest\r\n        (X)\r\n    SELECT TOP (100)\r\n        @X\r\n    FROM  sys.columns AS C;\r\n    \r\n    -- Flush dirty buffers\r\n    CHECKPOINT;\r\n\r\n### Tests\r\n\r\nThe following three tests read the *xml* column with:\r\n\r\n1. A plain `SELECT` statement\r\n2. Assigning the *xml* to a variable\r\n3. Using `SELECT INTO` to create a temporary table\r\n\r\n```\r\n-- No row count messages or graphical plan\r\n-- Show I/O statistics\r\nSET NOCOUNT ON;\r\nSET STATISTICS XML OFF;\r\nSET STATISTICS IO ON;\r\nGO\r\nPRINT CHAR(10) + '=== Plain SELECT ===='\r\n    \r\nDBCC DROPCLEANBUFFERS WITH NO_INFOMSGS;\r\n    \r\nSELECT XT.X \r\nFROM dbo.XMLTest AS XT;\r\nGO\r\nPRINT CHAR(10) + '=== Assign to a variable ===='\r\n    \r\nDBCC DROPCLEANBUFFERS WITH NO_INFOMSGS;\r\n    \r\nDECLARE @X xml;\r\n    \r\nSELECT\r\n    @X = XT.X\r\nFROM dbo.XMLTest AS XT;\r\nGO\r\nPRINT CHAR(10) + '=== SELECT INTO ===='\r\n    \r\nIF OBJECT_ID(N'tempdb..#T', N'U') IS NOT NULL\r\n    DROP TABLE #T;\r\n    \r\nDBCC DROPCLEANBUFFERS WITH NO_INFOMSGS;\r\n    \r\nSELECT \r\n    XT.X\r\nINTO #T\r\nFROM dbo.XMLTest AS XT\r\nGO\r\nSET STATISTICS IO OFF;\r\n```\r\n\r\n### Results\r\n\r\nThe output is:\r\n\r\n```none\r\n=== Plain SELECT ====\r\nTable 'XMLTest'. Scan count 1, logical reads 3, physical reads 1, read-ahead reads 0, \r\n    lob logical reads 795, lob physical reads 37, lob read-ahead reads 796.\r\n\r\n=== Assign to a variable ====\r\nTable 'XMLTest'. Scan count 1, logical reads 3, physical reads 1, read-ahead reads 0, \r\n    lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n\r\n=== SELECT INTO ====\r\nTable 'XMLTest'. Scan count 1, logical reads 3, physical reads 1, read-ahead reads 0, \r\n    lob logical reads 300, lob physical reads 37, lob read-ahead reads 400.\r\n```\r\n\r\n### Questions\r\n\r\n* Why are the LOB reads so different?\r\n* Surely the exact same data was read in each test?\r\n	156	2019-11-27 10:24:40.600492+00	0	4	1	224	340	1466	137907	0	0	0	2019-11-27 10:21:41.455569+00	f	f	1	1
4	786	587	2018-08-16 15:39:57+00	Progress bar and sections/subsections 	After my reading of a nice topic <https://tex.stackexchange.com/questions/59742/progress-bar-for-latex-beamer> on progress bar, I want to know if it is possible to do a progress bar which take into account all the whole slides and also the number of slides in the current subsection.\r\n\r\nTo be more precise, I would like a very simple progress bar as in <https://tex.stackexchange.com/questions/413760/change-the-color-or-the-progress-bar-indicator-in-the-beamer-metropolis-theme>, but when we can distinguish the different subsection. The aim of this is to allow people to know where we are in the subsection and in the entire presentation.\r\n\r\nTo be more precise with what I imagine, the principle would be the same as the one with the circle progress bar, you can tell how many slides remains in the current subsection and in the entire presentation. So I would like to do the same but with a progress bar, maybe by adding some vertical bars indicating the limits between the differents sections (subsections).\r\n\r\n	601	2020-01-20 16:38:26.676776+00	0	4	1	662	1565	3156	446300	0	0	0	2020-01-20 16:35:54.034268+00	f	f	1	2
2	606	443	2017-01-10 15:56:06+00	Is it possible to bulk insert data into a table that has columns encrypted with Always Encrypted?	In SSMS we are attempting to bulk insert from a csv file into a table that has a column encrypted using SQL Server 2016's Always Encrypted feature.\r\n\r\nThis is the command we're using:\r\n\r\n    INSERT INTO membersE\r\n    SELECT *\r\n    FROM OPENROWSET(\r\n    \tBULK 'c:\\members.csv', \r\n    \tFORMATFILE = 'c:\\membersEFormat.xml',\r\n    \tFIRSTROW = 2\r\n    \t) m\r\n\r\nThis returns the typical error you get when attempting to insert into an encrypted column:\r\n\r\n>Msg 206, Level 16, State 2, Line 6  \r\nOperand type clash: varbinary is incompatible with varchar(50) encrypted with (encryption_type = 'DETERMINISTIC', encryption_algorithm_name = 'AEAD_AES_256_CBC_HMAC_SHA_256', column_encryption_key_name = 'CEK_Auto1', column_encryption_key_database_name = 'DATABASE') collation_name = 'Latin1_General_BIN2'\r\n\r\nWe understand that you can't insert into an encrypted column via SSMS and that you need to use a .NET 4.6.1+ client, but we'd like to know if bulk insert operations are not possible as well?	454	2019-12-14 06:56:24.233429+00	0	4	1	518	1068	2122	160577	0	0	0	2019-12-14 06:54:17.74107+00	f	f	1	1
2	78	53	2017-03-31 20:29:59+00	Why does TSQL return the wrong value for POWER(2.,64.)?	`select POWER(2.,64.)` returns `18446744073709552000` instead of `18446744073709551616`.  It seems to have only 16 digits of precision (rounding the 17th).  \r\n\r\nEven making the precision explicit `select power(cast(2 as numeric(38,0)),cast(64 as numeric(38,0)))` it still returns the rounded result.\r\n\r\nThis seems like a pretty basic operation for it to be flaking out arbitrarily at 16 digits of precision like this.  The highest it can calculate correctly is only `POWER(2.,56.)`, failing for `POWER(2.,57.)`.  What is going on here?\r\n\r\nWhat's really terrible is that `select 2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.*2.;` actually returns the right value.  So much for terseness.	60	2019-11-16 23:45:22.274541+00	0	4	1	128	259	106	168815	0	0	0	2019-11-16 23:44:49.128804+00	f	f	1	1
2	12	416	2019-08-29 22:54:29+00	Forced plans on readable secondaries	If a plan is forced on the primary in an Availability Group, is it applied to queries run on a secondary?\r\n\r\nI'm looking for answers that cover both possibilities for plan forcing:\r\n\r\n* [Plan Guides][1]\r\n* [Query Store Forced Plan][2]\r\n\r\nI have read the following that suggest QS forced plans do not carry over, but cannot find anything authoritative in the documentation, or anything about plan guides.\r\n\r\n* [Query Store and Availability Groups][3] by Erin Stellato\r\n* [Query Data Store Forced Plan behavior on AlwaysOn Readable Secondary][4] by Vikas Rana\r\n\r\nConclusive evidence of forcing would be the presence of `Use Plan` or `PlanGuideName` and `PlanGuideDB` properties in the secondary's execution plan.\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/performance/plan-guides\r\n  [2]: https://docs.microsoft.com/en-us/sql/relational-databases/performance/monitoring-performance-by-using-the-query-store#force-a-plan-for-a-query-apply-forcing-policy\r\n  [3]: https://www.sqlskills.com/blogs/erin/query-store-and-availability-groups/\r\n  [4]: https://blogs.msdn.microsoft.com/vikas_rana/2017/10/13/query-data-store-forced-plan-behavior-on-alwayson-readable-secondary/	427	2019-12-11 16:14:35.229045+00	0	4	1	491	995	1738	246591	0	0	0	2019-12-11 16:14:35.229045+00	f	f	1	1
2	12	262	2019-07-17 20:47:55+00	What does SQL Server do for a hash aggregate bailout?	A question that arose in a chat discussion:\r\n\r\nI know *hash join* bailout switches internally to a sort of nested loops thing.\r\n\r\nWhat does SQL Server do for a *hash aggregate* bailout (if it can happen at all)?	270	2019-12-05 11:07:47.823782+00	0	4	1	337	672	1715	243132	0	0	0	2019-12-04 11:43:09.351463+00	f	f	1	1
2	625	450	2012-10-11 13:24:23+00	Return a column per date in a range	Let's say I have Table A: *BookingsPerPerson*\r\n\r\n    Person_Id    ArrivalDate    DepartureDate\r\n    123456       2012-01-01     2012-01-04\r\n    213415       2012-01-02     2012-01-07\r\n\r\nWhat I need to achieve with a view is the following:\r\n\r\n    Person_Id    ArrivalDate    DepartureDate    Jan-01    Jan-02    Jan-03    Jan-04    Jan-05    Jan-06    Jan-07\r\n    123456       2012-01-01     2012-01-04       1         1         1         1\r\n    213415       2012-01-02     2012-01-07                 1         1         1         1         1         1\r\n\r\nThe system is for events, so each hotel booking could take anything between 1 to 15 days but no more than that. Any ideas would be very much appreciated. 	461	2019-12-15 06:24:20.243612+00	0	4	1	525	1088	2136	25809	0	0	0	2019-12-15 06:24:20.243612+00	f	f	1	1
4	254	362	2019-11-27 21:41:11+00	Deactivate sidebar for AtBeginSection-slides	I have a beamer-project, where it's required to have a sidebar, but not for the `\\AtBeginSection`-slides.\r\n\r\nWhat i've tried is to look at the definition of the sidebar-theme (`kpsewhich beamerouterthemesidebar.sty`) and define a new, empty template that i can switch to locally in the `\\AtBeginSection`-block. This does what i want, but it doesn't work locally. I get an error, if i include the `\\setbeamertemplate` inside the `\\AtBeginSection`-block.\r\n\r\nMWE:\r\n\r\n``` latex\r\n\\documentclass{beamer}\r\n\r\n\\useoutertheme[%\r\nwidth=3cm,\r\nheight=1cm,\r\nhideothersubsections,\r\n]{sidebar}\r\n\r\n% This works but disables sidebar globally\r\n\\makeatletter\r\n\\defbeamertemplate{sidebar \\beamer@sidebarside}{dummy}{}\r\n\\setbeamertemplate{sidebar \\beamer@sidebarside}[dummy] % Comment out for local option\r\n\\makeatother\r\n\r\n\\AtBeginSection[]{\r\n  % This throws an error:\r\n  % \\makeatletter\r\n  % \\setbeamertemplate{sidebar \\beamer@sidebarside}[dummy]\r\n  % \\makeatother\r\n  \\begin{frame}\r\n    {\\thesection\\\\[.4ex]\\insertsectionhead}\r\n  \\end{frame}\r\n}\r\n\r\n\\begin{document}\r\n\r\n\\section{First section}\r\n\r\n\\begin{frame}{Title}\r\n  Some content\r\n\\end{frame}\r\n\r\n\\begin{frame}{Title}\r\n  Some content\r\n\\end{frame}\r\n\r\n\\section{Second section}\r\n\r\n\\begin{frame}{Title}\r\n  Some content\r\n\\end{frame}\r\n\r\n\\begin{frame}{Title}\r\n  Some content\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```	371	2019-12-06 12:03:15.359878+00	0	4	1	437	999	1732	518333	0	0	0	2019-12-06 12:03:15.359878+00	f	f	1	2
2	660	462	2013-02-27 21:29:33+00	Is there a better option than Union All for multiple selects from the same row?	Example Schema:\r\n\r\n    CREATE TABLE [dbo].[Base](\r\n    [ID] [int] IDENTITY(1,1) NOT NULL,\r\n    [Option1ID] [int] NULL,\r\n    [Option2ID] [int] NULL,\r\n    [Option3ID] [int] NULL,\r\n    [Option1Name] [varchar] NULL,\r\n    [Option2Name] [varchar] NULL,\r\n    [Option3Name] [varchar] NULL,\r\n    [Option1LName] [varchar] NULL,\r\n    [Option2LName] [varchar] NULL,\r\n    [Option3LName] [varchar] NULL,)\r\n\r\nIs there a way to get results that show up like:\r\n\r\n    ID | OptionID | OptionName | OptionLName\r\n\r\nI have tried to achieve this using UNION ALL but this means going over the same row 3 times in my example, in my real problem I have to do it 10 times. I cannot normalize the table due to legacy code. Is there a way to only go over the Base row once?	473	2019-12-18 07:41:46.554375+00	0	4	1	537	1119	2225	35620	0	0	0	2019-12-18 07:41:46.554375+00	f	f	1	1
2	611	445	2016-06-30 13:34:40+00	Is it legal for SQL Server to fill PERSISTED columns with data that does not match the definition?	I'm following up on [this question][1] about strange values in a `PERSISTED` computed column. The answer there makes a few guesses about how this behavior came to be.\r\n\r\nI'm asking the following: Is this not an outright bug? Are `PERSISTED` columns ever allowed to behave this way?\r\n\r\n    DECLARE @test TABLE (\r\n    \tCol1 INT,\r\n    \tContains2 AS CASE WHEN 2 IN (Col1) THEN 1 ELSE 0 END PERSISTED) --depends on Col1\r\n    \r\n    INSERT INTO @test (Col1) VALUES\r\n    \t(ABS(CHECKSUM(NEWID()) % 5)),\r\n    \t(ABS(CHECKSUM(NEWID()) % 5)),\r\n    \t(ABS(CHECKSUM(NEWID()) % 5)),\r\n    \t(ABS(CHECKSUM(NEWID()) % 5)),\r\n    \t(ABS(CHECKSUM(NEWID()) % 5))\r\n    \r\n    SELECT * FROM @test --shows impossible data\r\n\r\n    UPDATE @test SET Col1 = Col1*1 --"fix" the data by rewriting it\r\n\r\n    SELECT * FROM @test --observe fixed data\r\n    \r\n    /*\r\n    Col1\tContains2\r\n\t2\t0\r\n\t2\t0\r\n\t0\t1\r\n\t4\t0\r\n\t3\t0\r\n\r\n    Col1\tContains2\r\n\t2\t1\r\n\t2\t1\r\n\t0\t0\r\n\t4\t0\r\n\t3\t0\r\n    */\r\n\r\nNote, that the data appears "impossible" because the values of the computed column do not correspond to its definition.\r\n\r\nIt is well known that non-deterministic functions in queries can behave strangely but here this seems to violate the contract of persisted computed columns and, therefore, should be illegal.\r\n\r\nInserting random numbers might be a contrived scenario but what if we were inserting `NEWID()` values or `SYSUTCDATETIME()`? I think this is a relevant issue that might practically manifest itself.\r\n\r\n  [1]: https://stackoverflow.com/questions/38120684/inconsistent-results-with-newid-and-persistent-computed-column	456	2019-12-14 07:10:06.505421+00	0	4	1	520	1070	2123	142675	0	0	0	2019-12-14 07:10:06.505421+00	f	f	1	1
2	680	481	2011-09-19 15:06:59+00	Does SQL Server read all of a COALESCE function even if the first argument is not NULL?	I'm using a T-SQL `COALESCE` function where the first argument will not be null on about 95% of the times it is ran. If the first argument is `NULL`, the second argument is quite a lengthy process:\r\n\r\n    SELECT COALESCE(c.FirstName\r\n                    ,(SELECT TOP 1 b.FirstName\r\n                      FROM TableA a \r\n                      JOIN TableB b ON .....)\r\n                    )\r\n\r\nIf, for example, `c.FirstName = 'John'`, would SQL Server still run the sub-query?\r\n\r\nI know with the VB.NET `IIF()` function, if the second argument is True, the code still reads the third argument (even though it won't be used).	493	2019-12-24 00:59:40.146614+00	0	4	1	556	1176	2417	12941	0	0	0	2019-12-24 00:59:40.146614+00	f	f	1	1
1	709	581	2020-01-16 21:43:09.751051+00	Answer Snippet Formatting Inconsistency	When viewing a community, question headers are displayed with answer snippets.  It appears that there was an attempt to preserve markdown format in the snippet (see **nix** bold below the second question). It also appears that the markdown format isn't entirely preserved (see the second answer to the top question).  \r\n\r\nSince the markdown headers don't map nicely to answer snippets, and people are likely to use headers at the top of answers, it might be best to convert headers to bold for the answer snippet, or perhaps just strip out any markdown characters that doesn't map nicely to a snippet.\r\n\r\n![bold_format.png](/image?hash=56b42c48264767f3b7a6f3c532e7c8889aa818bafd599dea8f4d351a57cf0190)	595	2020-01-17 16:37:07.457286+00	5	4	1	656	1610	3341	\N	0	0	0	\N	f	f	2	\N
2	80	467	2019-12-19 18:23:15+00	Why would a table with a Clustered Columnstore Index have many open rowgroups?	I was experiencing some performance issues with a query yesterday and upon further investigation, I noticed what I believe is odd behavior with a clustered columnstore index that I'm trying to get to the bottom of. \r\n\r\nThe table is \r\n\r\n    CREATE TABLE [dbo].[NetworkVisits](\r\n    \t[SiteId] [int] NOT NULL,\r\n    \t[AccountId] [int] NOT NULL,\r\n    \t[CreationDate] [date] NOT NULL,\r\n    \t[UserHistoryId] [int] NOT NULL\r\n    )\r\n\r\nwith the index:\r\n\r\n    CREATE CLUSTERED COLUMNSTORE INDEX [CCI_NetworkVisits] \r\n       ON [dbo].[NetworkVisits] WITH (DROP_EXISTING = OFF, COMPRESSION_DELAY = 0) ON [PRIMARY]\r\n\r\nThe table currently has 1.3 Billion rows in it and we are constantly inserting new rows to it. When I say constantly, I mean all the time. It's a steady stream of inserting one row at a time to the table. \r\n\r\n    Insert Into NetworkVisits (SiteId, AccountId, CreationDate, UserHistoryId)\r\n    Values (@SiteId, @AccountId, @CreationDate, @UserHistoryId)\r\n\r\nExecution plan [here](https://www.brentozar.com/pastetheplan/?id=HJCeiEt0H)\r\n\r\nI also have a scheduled job that runs every 4 hours to delete duplicate rows from the table. The query is:\r\n\r\n    With NetworkVisitsRows\r\n      As (Select SiteId, UserHistoryId, Row_Number() Over (Partition By SiteId, UserHistoryId\r\n                                        Order By CreationDate Asc) RowNum\r\n            From NetworkVisits\r\n           Where CreationDate > GETUTCDATE() - 30)\r\n    DELETE\r\n    FROM NetworkVisitsRows\r\n    WHERE RowNum > 1\r\n    Option (MaxDop 48)\r\n\r\nThe execution plan has been pasted [here](https://www.brentozar.com/pastetheplan/?id=BkVWc4YRH).\r\n\r\nWhile digging into the issue, I noticed that the `NetworkVisits` table had roughly 2000 rowgroups in it, with about 800 of them being in an open state and no where near the max allowed (1048576). Here is a small sample of what I was seeing:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nI ran a reorganize on the index, which compressed all but 1 rowgroup, but this morning I checked again and we again have multiple open rowgroups - the one that was created yesterday after the reorganize, then 3 others each created roughly around the time the deletion job ran:\r\n\r\n    TableName\t\tIndexName\t\t\ttype_desc\t\t\t\tstate_desc\ttotal_rows\tdeleted_rows\tcreated_time\r\n    NetworkVisits\tCCI_NetworkVisits\tCLUSTERED COLUMNSTORE\tOPEN\t\t36754\t\t0\t\t\t\t2019-12-18 18:30:54.217\r\n    NetworkVisits\tCCI_NetworkVisits\tCLUSTERED COLUMNSTORE\tOPEN\t\t172103\t\t0\t\t\t\t2019-12-18 20:02:06.547\r\n    NetworkVisits\tCCI_NetworkVisits\tCLUSTERED COLUMNSTORE\tOPEN\t\t132628\t\t0\t\t\t\t2019-12-19 04:03:10.713\r\n    NetworkVisits\tCCI_NetworkVisits\tCLUSTERED COLUMNSTORE\tOPEN\t\t397718\t\t0\t\t\t\t2019-12-19 08:02:13.063\r\n\r\nI'm trying to determine what possibly could be causing this to create new rowgroups instead of using the existing one. \r\n\r\nIs it possibly memory pressure or contention between the insert and the delete? Is this behavior documented anywhere?\r\n\r\nWe're running SQL Server 2017 CU 16 Enterprise Edition on this server. \r\n\r\nThe `INSERT` is MAXDOP 0, the `DELETE` is MAXDOP 48. The only closed rowgroups are the ones from the initial `BULKLOAD` and then the `REORG_FORCED` that I did yesterday, so the trim reasons in `sys.dm_db_column_store_row_group_physical_stats` are `REORG` and `NO_TRIM` respectively. There are no closed rowgroups beyond those. There are no updates being run against this table. We average about 520 executions per minute on the insert statement. There is no partitioning on the table.\r\n\r\nI am aware of trickle inserts. We do the same thing elsewhere and are not experiencing the same issue with multiple open row groups. Our suspicion is it has to do with the delete. Each newly created row group is around the time of the scheduled deletion job. There are only two delta stores showing deleted rows. We don't actually delete a lot of data from this table, for example during one execution yesterday it deleted 266 rows.\r\n\r\n  [1]: https://i.stack.imgur.com/KNG0O.jpg	478	2019-12-19 20:31:03.544793+00	0	4	1	542	1144	2884	255985	0	0	0	2019-12-19 19:28:20.7549+00	f	f	1	1
2	121	209	2018-06-08 09:53:20+00	SQL Server splits A <> B into A < B OR A > B, yielding strange results if B is non-deterministic	We have encountered an interesting issue with SQL Server. Consider the following repro example:\r\n\r\n    CREATE TABLE #test (s_guid uniqueidentifier PRIMARY KEY);\r\n    INSERT INTO #test (s_guid) VALUES ('7E28EFF8-A80A-45E4-BFE0-C13989D69618');\r\n    \r\n    SELECT s_guid FROM #test\r\n    WHERE s_guid = '7E28EFF8-A80A-45E4-BFE0-C13989D69618'\r\n      AND s_guid <> NEWID();\r\n    \r\n    DROP TABLE #test;\r\n\r\n[fiddle](http://rextester.com/REASM51008)\r\n\r\nPlease forget for a moment that the `s_guid <> NEWID()` condition seems entirely useless - this is just a minimal repro example. Since the probability of `NEWID()` matching some given constant value is extremely small, it should evaluate to TRUE every time.\r\n\r\nBut it doesn't. Running this query *usually* returns 1 row, but *sometimes* (quite frequently, more than 1 time out of 10) returns 0 rows. I have reproduced it with SQL Server 2008 on my system, and you can reproduce it on-line with the fiddle linked above (SQL Server 2014).\r\n\r\nLooking at the execution plan reveals that the query analyzer apparently splits the condition into `s_guid < NEWID() OR s_guid > NEWID()`:\r\n\r\n[![query plan screenshot][1]][1]\r\n\r\n...which completely explains why it fails sometimes (if the first generated ID is smaller and the second one larger than the given ID).\r\n\r\nIs SQL Server allowed to evaluate `A <> B` as `A < B OR A > B`, even if one of the expressions is non-deterministic? If yes, where is it documented? Or did we find a bug?\r\n\r\nInterestingly, `AND NOT (s_guid = NEWID())` yields the same execution plan (and the same random result).\r\n\r\nWe found this issue when a developer wanted to optionally exclude a particular row and used:\r\n\r\n    s_guid <> ISNULL(@someParameter, NEWID())\r\n\r\nas a "shortcut" for: \r\n\r\n    (@someParameter IS NULL OR s_guid <> @someParameter)\r\n\r\nI am looking for documentation and/or confirmation of a bug. The code is not all that relevant so workarounds are not required.\r\n\r\n  [1]: https://i.stack.imgur.com/OwK4n.png	216	2019-11-30 13:01:28.694525+00	0	4	1	284	434	520	209084	0	0	0	2019-11-30 13:01:28.694525+00	f	f	1	1
2	619	448	2015-09-16 19:59:24+00	Why is it taking longer to create an index after column size increases?	Our vendor changed column widths on almost every column in the entire database.  The database is around 7TB, 9000+ tables. We are trying to create an index on a table that has 5.5billion rows. Before the vendor's upgrade we could create the index in 2 hours. Now it takes days. What they have done is increase any varchar(xx) size to varchar(256).  So most columns used to be varchar(18) or varchar(75), etc.\r\n\r\nAnyway the primary key consists of 6 columns that combined width was 126 characters. Now after the upgrade, the primary key is 1283 characters which violates SQL Servers limit of 900 characters.  The entire table column width went from a total combined varchar count of 1049 to a total combined varchar count of 4009.\r\n\r\nThere is not an increase in data, the table doesn't take up any more "space" than it did before all the column width increase, but performance to create something as simple as an index is now taking an unreasonable amount of time.\r\n\r\nCan anyone explain why it is taking so much longer to create and index when the only thing done was increasing the size of the columns?\r\n\r\nThe index we are trying to create is nonclustered since the pk is the clustered index.  After several attempts to create the index, we gave up. I think it ran 4 or 5 days without completion.\r\n\r\nI tried this in a non-production environment by taking a file system snapshot and brought the database up on a quieter server.	459	2019-12-15 06:16:31.435593+00	0	4	1	523	1085	2263	115290	0	0	0	2019-12-15 06:16:31.435593+00	f	f	1	1
2	37	542	2019-07-22 19:52:10+00	“Invalid Protection Option”	In the following error message, the reason given is "Invalid Protection Option".  What does that indicate?  \r\n\r\n> SQL Server detected a logical consistency-based I/O error: invalid protection option\r\n\r\nFor the purposes of this question, I don't need to know anything about "how to run DBCC", or "check for corruption".  I got that.  I'm just curious about the "root cause" piece, and what could possibly cause *this* flavor of the logical consistency-based I/O error.	555	2020-01-10 03:19:14.636988+00	0	4	1	617	1390	2942	243488	0	0	0	2020-01-10 03:19:14.636988+00	f	f	1	1
2	671	470	2013-12-03 20:09:31+00	Is it possible to view LRU-K values in SQL Server?	In SQL Server's `sys.dm_os_memory_cache_entries`, it is possible to view both the original cost of an entry in the cache as well as the current cost of the cache entry (`original_cost` and `current_cost` respectively). The DMV `sys.dm_os_buffer_descriptors` contains a record of the pages that are currently in memory as well as some metadata about the pages. One interesting chunk of info not available in the DVM are the LRU-K values for the data pages.\r\n\r\nIs it possible to get the LRU-K values for data pages in buffer pool in SQL Server? If so, how?	481	2019-12-22 09:17:40.843198+00	0	4	1	545	1148	2735	54377	0	0	0	2019-12-22 09:17:40.843198+00	f	f	1	1
2	12	256	2019-06-05 14:08:22+00	CROSS APPLY produces outer join	In answer to [SQL counting distinct over partition](https://dba.stackexchange.com/q/239788), Erik Darling posted this code to work around for the lack of `COUNT(DISTINCT) OVER ()`:\r\n\r\n```\r\nSELECT      *\r\nFROM        #MyTable AS mt\r\nCROSS APPLY (   SELECT COUNT(DISTINCT mt2.Col_B) AS dc\r\n                FROM   #MyTable AS mt2\r\n                WHERE  mt2.Col_A = mt.Col_A\r\n                -- GROUP BY mt2.Col_A \r\n            ) AS ca;\r\n```\r\n\r\nThe query uses `CROSS APPLY` (not `OUTER APPLY`) so why is there an **outer** join in the execution plan instead of an **inner** join?\r\n\r\n[![enter image description here][1]][1]\r\n\r\nAlso why does uncommenting the group by clause result in an inner join?\r\n\r\n[![enter image description here][2]][2]\r\n\r\nI don't think the data is important but copying from that given by kevinwhat on the other question:\r\n\r\n```\r\ncreate table #MyTable (\r\nCol_A varchar(5),\r\nCol_B int\r\n)\r\n\r\ninsert into #MyTable values ('A',1)\r\ninsert into #MyTable values ('A',1)\r\ninsert into #MyTable values ('A',2)\r\ninsert into #MyTable values ('A',2)\r\ninsert into #MyTable values ('A',2)\r\ninsert into #MyTable values ('A',3)\r\n\r\ninsert into #MyTable values ('B',4)\r\ninsert into #MyTable values ('B',4)\r\ninsert into #MyTable values ('B',5)\r\n```\r\n\r\n  [1]: https://i.stack.imgur.com/oMRf9.png\r\n  [2]: https://i.stack.imgur.com/DYdNO.png	264	2019-12-04 01:13:26.915764+00	0	4	1	331	558	956	239865	0	0	0	2019-12-04 01:12:22.465652+00	f	f	1	1
1	31	26	2019-11-13 09:13:44.09962+00	Could we please use a sans-serif typeface?	This is a usability question: could we please use a typeface that is a better fit for screens?  Something sans serif would be very welcome.	33	2019-11-18 19:08:56.421316+00	3	4	1	28	438	3447	\N	0	0	0	\N	f	f	2	\N
2	95	15	2019-11-06 13:35:06+00	Are repeated expressions calculated more than once?	Take the following example:\r\n\r\n     SELECT <CalculationA> As ColA,\r\n            <CalculationB> As ColB,\r\n            <CalculationA> + <CalculationB> As ColC\r\n     FROM TableA\r\n\r\nWould CalculationA and CalculationB, each be calculated twice?   \r\nOr would the optimizer be clever enough to calculate them once and use the result twice?\r\n\r\nI would like to perform a test to see the result for myself, however, I am not sure how I could check something like this.\r\n\r\nMy assumption is that it would perform the calculation twice.   \r\nIn which case, depending upon the calculations involved, might it be better to use a derived table, or nested view? Consider the following:\r\n\r\n     SELECT TableB.ColA,\r\n            TableB.ColB,\r\n            TableB.ColA + TableB.ColB AS ColC,\r\n     FROM(    \r\n           SELECT <CalculationA> As ColA,\r\n                  <CalculationB> As ColB\r\n           FROM TableA\r\n         ) As TableB\r\n\r\nIn this case, I would hope that the calculations would only be performed once?	22	2019-11-27 23:35:53.526114+00	0	4	1	25	373	260	252661	0	0	0	2019-11-09 10:26:11.712944+00	f	f	1	1
2	747	556	2019-04-26 10:08:30+00	Can SQL Server create collisions in system generated constraint names?	I have an application which creates millions of tables in a SQL Server 2008 database (non clustered). I am looking to upgrade to SQL Server 2014 (clustered), but am hitting an error message when under load:\r\n\r\n>There is already an object named ‘PK__tablenameprefix__179E2ED8F259C33B’ in the database\r\n\r\nThis is a system generated constraint name. It looks like a randomly generated 64-bit number. Is it possible that I am seeing collisions due to the large number of tables? Assuming I have 100 million tables, I calculate less than a 1-in-1-trillion chance of a collision when adding the next table, but that assumes a uniform distribution. Is it possible that SQL Server changed its name generation algorithm between version 2008 and 2014 to increase the odds of collision?\r\n\r\nThe other significant difference is that my 2014 instance is a clustered pair, but I am struggling to form a hypothesis for why that would generate the above error. \r\n\r\nI know creating millions of tables is insane. This is black box 3rd party code over which I have no control. Despite the insanity, it worked in version 2008 and now doesn’t in version 2014. \r\n\r\nOn closer inspection, the generated suffix always seems to start with 179E2ED8 - meaning the random part is actually only a 32-bit number and the odds of collisions are a mere 1-in-50 every time a new table is added, which is a much closer match to the error rate I’m seeing!	569	2020-01-12 13:57:51.691279+00	0	4	1	631	1427	2945	236768	0	0	0	2020-01-12 13:56:57.51136+00	f	f	1	1
4	96	367	2015-03-24 10:51:34+00	LuaLaTeX and hyperref ate my document (properties). How do I get them back?	I use `hyperref` for a number of reasons, but among other things it sets up the document properties up well. As long as I compile with XeLaTeX that is. As soon as I compile with LuaLaTeX, everything is a jumble. There are many more fields with issues in my real project, but here is an MWE using just the title field:\r\n\r\n    \\documentclass{scrartcl}\r\n    \\usepackage{polyglossia}\r\n    \\setmainlanguage{turkish}\r\n    \\makeatletter\r\n    \\usepackage[hidelinks]{hyperref}\r\n    \\AtBeginDocument{%\r\n    \t\\hypersetup{%\r\n    \t\tpdftitle = {\\@title}\r\n    \t}\r\n    }{}\r\n    \\makeatother\r\n    \\title{RAB'BİN GÜNÜ}\r\n    \\begin{document}\r\n    Title in document properties should match: RAB'BİN GÜNÜ \r\n    \\end{document}\r\n\r\nCompile with `xelatex` gives me:\r\n \r\n> **Title:** RAB’BİN GÜNÜ\r\n\r\nBut `lualatex` eats Unicode for lunch and has indigestion:\r\n\r\n> **Title:** RAB'BÄ°N GÃœNÃœ\r\n\r\nHow do I get proper Unicode characters into the document property fields when compiling with LuaLaTeX?	376	2019-12-06 12:46:23.724473+00	0	4	1	442	797	1116	234780	0	0	0	2019-12-06 12:46:23.724473+00	f	f	1	2
4	167	357	2013-09-09 09:46:42+00	How to number slides with an increasing number of digits of pi?	How to number frames with an increasing number of digits of pi? Illustrative, I want the frame numbers to be like\r\n\r\n- 3 (on the first slide)\r\n\r\n- 3.1 (on the second slide)\r\n\r\n- 3.14 (on the third slide)\r\n\r\n... and so on.\r\n\r\nThis is a question, which occupies me for quite some time; actually since a talk I gave on March 14th (also known as pi-day :).\r\n\r\nMy present workaround is, to insert the frame numbers by hand – which is of course quite annoying when changing the order of slides. \r\n\r\n    \\documentclass[t]{beamer}\r\n\r\n    \\usepackage[absolute,overlay]{textpos}\r\n    \\setbeamertemplate{navigation symbols}{}\r\n\r\n    \\newcommand{\\pifoot}[1]{\r\n        \\begin{textblock*}{120mm}(0mm,84.3mm)\r\n            \\raggedleft #1\r\n        \\end{textblock*}\r\n    }\r\n\r\n    \\begin{document}\r\n\r\n        \\begin{frame}\r\n            \\pifoot{3}\r\n        \\end{frame}\r\n\r\n        \\begin{frame}\r\n            \\pifoot{3.1}\r\n        \\end{frame}\r\n\r\n        \\begin{frame}\r\n            \\pifoot{3.14}\r\n        \\end{frame}\r\n\r\n    \\end{document}\r\n\r\n**EDIT:**\r\n\r\nBased on Reds great answer I compiled an unpretentious solution. It abstains new counters in favour of the current page number.\r\n\r\n    \\documentclass[t]{beamer}\r\n    \r\n    \\usepackage[absolute,overlay]{textpos}\r\n    \\setbeamertemplate{navigation symbols}{}\r\n    \\usepackage{xstring}\r\n    \r\n    \\setbeamertemplate{footline}{%\r\n    \t\\begin{picture}(54,12.5)(0,0)\r\n    \t    \\put(0.9,0.52){%\r\n    \t    \t\\begin{minipage}[b][12.5mm][c]{112.5mm}\r\n    \t\t         \\raggedleft\r\n                     3\\StrLeft{.141592653589793238462643383279502884197169399375105820974944592307816406286 208998628034825342117067982148086513282306647093844609550582231725359408128481}{\\arabic{framenumber}}\r\n    \t     \t\\end{minipage}\r\n            }\r\n    \t\\end{picture}\r\n    }\r\n    \r\n    \\begin{document}\r\n        \\begin{frame}\r\n        \\end{frame}\r\n        \r\n        \\begin{frame}\r\n        \\end{frame}\r\n    \\end{document} \r\n\r\n  	366	2019-12-06 09:51:37.862792+00	0	4	1	432	768	2166	132370	0	0	0	2019-12-06 09:51:37.862792+00	f	f	1	2
4	672	473	2019-12-22 16:13:35+00	question about the special rule mentioned after 20.5	I don't quite understand the special rule that follows texbook exercise 20.5:\r\n\r\n[![screenshot][1]][1]\r\n\r\nIf I understand correctly (which I don't), if the opening parenthesis `{`which starts the replacement text is immediately preceded by a hash character `#` like this `#{` then this opening parenthesis is at the same time a delimiter which marks the beginning of the replacement text and indicates that this parenthesis is also part of the parameter text (which it can never be in the general case). \r\n\r\nBut then, I don't understand why this `\\def\\a#1#{\\hbox to #1}` definition applied to the `\\a3pt{x}` example is extended to `\\hbox to 3pt{x}`. \r\n\r\nWhy is there the "x" in `3pt{x}`?\r\n\r\nI don't understand the explanation given:\r\n\r\n> because the argument of \\a is delimited by a left brace\r\n\r\nTranslated with www.DeepL.com/Translator (free version)\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/QBy0y.png	485	2019-12-22 19:44:42.681057+00	0	4	1	548	1157	2410	521447	0	0	0	2019-12-22 19:44:42.681057+00	f	f	1	2
4	167	438	2019-12-13 09:59:09.565827+00	How to define custom colours in l3draw	By default `l3draw` only supports `red`, `yellow`, `blue`, `green`, `white`, `black`, `cyan` and `magenta` (and combinations thereof). \r\n\r\nHow to define more colours?	449	2019-12-13 09:59:09.565827+00	0	4	3	513	1060	3429	\N	0	0	0	\N	f	f	1	\N
1	168	410	2019-12-11 08:42:58.408769+00	A way to highlight specific users	Because some of us had some bad experiences with some specific users on other sites in the past, it might help if there was a way to mark posts of specific users, so it might be easier to ignore them or to live with them in peace.\r\n\r\nThe more drastic approach would be to hide every post of such users, but since the aim of this site is to serve not only as a community but as a repository of knowledge for future visitors, this will be contraproductive.	421	2019-12-11 13:36:07.657425+00	3	4	1	485	1021	2372	\N	0	0	0	\N	f	f	2	\N
4	787	588	2019-08-10 18:38:55+00	How to add progress bar with section titles?	Many presentations include some kind of a visualizer where the listeners can easily see how far the presentation has gone.\r\n\r\nA great option to do so is adding a `progress bar` into the slide layout.\r\n\r\n----------\r\n\r\n**Minimum Working Example (MWE):**\r\n\r\nThe user [Gonzalo Medina][2] has posted a nice approach on how to display a triangle above a line:\r\n\r\n[![Screenshot of the code from user Gonzalo Medina][3]][3]\r\n\r\n    \\documentclass{beamer}\r\n    \\usepackage{tikz}\r\n    \\usetikzlibrary{calc}\r\n    \r\n    \\definecolor{pbgray}{HTML}{575757}% background color for the progress bar\r\n    \r\n    \\makeatletter\r\n    \\def\\progressbar@progressbar{} % the progress bar\r\n    \\newcount\\progressbar@tmpcounta% auxiliary counter\r\n    \\newcount\\progressbar@tmpcountb% auxiliary counter\r\n    \\newdimen\\progressbar@pbht %progressbar height\r\n    \\newdimen\\progressbar@pbwd %progressbar width\r\n    \\newdimen\\progressbar@tmpdim % auxiliary dimension\r\n    \r\n    \\progressbar@pbwd=\\linewidth\r\n    \\progressbar@pbht=1pt\r\n    \r\n    % the progress bar\r\n    \\def\\progressbar@progressbar{%\r\n    \r\n        \\progressbar@tmpcounta=\\insertframenumber\r\n        \\progressbar@tmpcountb=\\inserttotalframenumber\r\n        \\progressbar@tmpdim=\\progressbar@pbwd\r\n        \\multiply\\progressbar@tmpdim by \\progressbar@tmpcounta\r\n        \\divide\\progressbar@tmpdim by \\progressbar@tmpcountb\r\n    \r\n      \\begin{tikzpicture}[very thin]\r\n        \\draw[pbgray!30,line width=\\progressbar@pbht]\r\n          (0pt, 0pt) -- ++ (\\progressbar@pbwd,0pt);\r\n        \\draw[draw=none]  (\\progressbar@pbwd,0pt) -- ++ (2pt,0pt);\r\n    \r\n        \\draw[fill=pbgray!30,draw=pbgray] %\r\n           ( $ (\\progressbar@tmpdim, \\progressbar@pbht) + (0,1.5pt) $ ) -- ++(60:3pt) -- ++(180:3pt) ;\r\n    \r\n        \\node[draw=pbgray!30,text width=3.5em,align=center,inner sep=1pt,\r\n          text=pbgray!70,anchor=east] at (0,0) {\\insertframenumber/\\inserttotalframenumber};\r\n      \\end{tikzpicture}%\r\n    }\r\n    \r\n    \\addtobeamertemplate{headline}{}\r\n    {%\r\n      \\begin{beamercolorbox}[wd=\\paperwidth,ht=5ex,center,dp=1ex]{white}%\r\n        \\progressbar@progressbar%\r\n      \\end{beamercolorbox}%\r\n    }\r\n    \\makeatother\r\n    \r\n    \\begin{document}\r\n    \r\n    \t\\section{Introduction}\r\n    \t\t\r\n    \t\t\\begin{frame}\r\n    \t\t\t\\frametitle{Introduction}\r\n    \t\t\ttest\r\n    \t\t\\end{frame}\r\n    \r\n    \t\\section{Motivation}\r\n    \r\n    \t\t\\begin{frame}\r\n    \t\t\t\\frametitle{Motivation}\r\n    \t\t\ttest\r\n    \t\t\\end{frame}\r\n    \r\n    \t\\section{Methodology}\r\n    \r\n    \t\t\\begin{frame}\r\n    \t\t\t\\frametitle{Experiments}\r\n    \t\t\ttest\r\n    \t\t\\end{frame}\r\n    \t\t\r\n    \t\t\\begin{frame}\r\n    \t\t\t\\frametitle{Statistics}\r\n    \t\t\ttest\r\n    \t\t\\end{frame}\r\n    \r\n    \t\\section{Results}\r\n    \t\t\r\n    \t\t\\begin{frame}\r\n    \t\t\t\\frametitle{Results 1}\r\n    \t\t\ttest\r\n    \t\t\\end{frame}\r\n    \t\t\r\n    \t\t\\begin{frame}\r\n    \t\t\t\\frametitle{Results 2}\r\n    \t\t\ttest\r\n    \t\t\\end{frame}\r\n    \r\n    \t\\section{Conclusion}\r\n    \r\n    \t\t\\begin{frame}\r\n    \t\t\t\\frametitle{Conclusion}\r\n    \t\t\ttest\r\n    \t\t\\end{frame}\r\n    \r\n    \\end{document}\r\n\r\n----------\r\n\r\n**Question:**\r\n\r\nThis looks nice indeed, but I would prefer to add the `section titles` into the timeline as well.\r\n\r\n[![Screenshot of the desired progres bar][1]][1]\r\n\r\nHereby you can see:\r\n\r\n - *Introduction* and *Motivation* has already been presented\r\n - *Methodology* is currently presented\r\n - *Results* and *Conclusion* will be presented afterwards\r\n\r\nWould it be possible to extend the code so the section titles will appear in the timeline as well?\r\n\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/PF7Wt.png\r\n  [2]: https://tex.stackexchange.com/a/59749/78784\r\n  [3]: https://i.stack.imgur.com/nkSSU.png	602	2020-01-20 17:22:08.192601+00	0	4	1	663	1572	3157	503702	0	0	0	2020-01-20 16:36:27.065081+00	f	f	1	2
2	193	299	2019-09-27 11:00:16+00	Determining if auto stats update is in progress	In our production system, queries sometimes 'stall'. Whilst stalled, no incrementing resource use (CPU, Reads) is shown in sp_whoisactive, and there is no blocking.\r\n\r\nIn retrospective diagnosis, we can see that sys.dm_db_stats_properties shows last_updated around the time the query was 'stalled'.\r\n\r\nWhat we would like to do is - when we see a stalled query - then determine what auto stats updates are in progress.\r\n\r\nBecause we want to do this ad-hoc, and also because we don't want to impact production performance, using profiler is probably not an option for us.\r\n\r\n*(If there is no way of doing an ad-hoc determination, then maybe we'll have to consider Extended Events, or some other lower-impact pre-emptive tracking).*\r\n\r\nOur version is 2014, however answers for later versions would also be useful.	307	2019-12-04 22:54:07.949449+00	0	4	1	374	656	1540	249775	0	0	0	2019-12-04 22:54:07.949449+00	f	f	1	1
2	206	314	2018-09-13 23:55:15+00	Temp Table Filtered Index Creation is Blocked When Stored Procedure is Run Multiple Times Concurrently	Are there any known blocking issues in SQL Server 2014 that are specific to creating one or more filtered indexes on a temp table after the initial table creation? (i.e. not an inline index creation)\r\n\r\nI am not permitted to post the actual code; so, I have changed table/column names to protect the innocent.  The representative but very rough logic below is included at the beginning of a stored procedure.\r\n\r\nThe blocking occurs when the stored procedure is run multiple times concurrently, I believe.  According to my DMV query, the stored procedure is being blocked on the 1st nonclustered index creation statement.  \r\n\r\n    CREATE TABLE #temp_table_name_goes_here (\r\n    \t[the_first_col] BIGINT NULL\r\n    \t,[the_second_col] INT NULL\r\n    \t,[another_col] VARCHAR(20) NULL\r\n    \t);\r\n    \r\n    CREATE INDEX tmp_indx_temp_table_name_goes_here_1 ON #temp_table_name_goes_here (the_first_col)\r\n    WHERE the_first_col IS NOT NULL;\r\n    \r\n    CREATE INDEX tmp_indx_temp_table_name_goes_here_2 ON #temp_table_name_goes_here (the_second_col)\r\n    WHERE the_second_col IS NOT NULL; 	322	2019-12-05 16:44:29.83561+00	0	4	1	389	690	1546	217573	0	0	0	2019-12-05 16:44:29.83561+00	f	f	1	1
2	190	295	2017-12-19 16:35:17+00	How to multiply rows for a column that contains negative and zero values?	I'm trying to get the Product of all rows for a specific column in a grouped by query.  Most examples I've found point me towards combining `exp`, `sum` and `log`\r\n\r\n    exp(sum(log([Column A])))\r\n\r\nThe problem I'm having is that the column contains some zeros for values and thus I'm getting this error when zeros get passed to the `log` function:\r\n>An invalid floating point operation occurred.\r\n\r\nI thought I could work around this by using a `case` expression, but that just doesn't work the way I would think it should, as it seems to evaluate all cases...\r\n\r\n    select \r\n      Name,\r\n      Product = case \r\n        when min([Value]) = 0 then 0 \r\n        when min([Value]) <> 0 then exp(sum(log(I))) -- trying to get the product of all rows in this column\r\n      end\r\n     from ids\r\n     group by Name\r\n\r\n\r\n[SqlFiddle](http://sqlfiddle.com/#!6/5e18f/1)\r\n\r\nGiven the following result set:\r\n\r\n    Id  Name  Value\r\n    _________________________________\r\n    1   a     1\r\n    2   a     2\r\n    3   b     0\r\n    4   b     1\r\n\r\nI would expect to get the following rows:\r\n\r\n    Name  Product\r\n    _____________\r\n    a     2\r\n    b     0\r\n\r\nSo in summary...\r\nHow do you multiply rows in a column that can contain negative or zero valued numbers?\r\n\r\n	303	2019-12-04 22:43:48.322832+00	0	4	1	370	652	1534	193551	0	0	0	2019-12-04 22:43:48.322832+00	f	f	1	1
1	167	574	2020-01-15 11:47:32.008015+00	Duplicate questions	How will duplicate questions be treated?\r\n\r\nPersonally I liked the approach from SO to link to the existing post to avoid spreading out possible answers over multiple places, but leave the duplicate around to act as a signpost for future users with the same problem.	588	2020-01-15 11:47:32.008015+00	9	4	1	649	2115	3912	\N	0	0	0	\N	f	f	2	\N
2	12	96	2014-12-17 10:12:48+00	Excessive sort memory grant	Why is this simple query granted so much memory?\r\n\r\n```sql\r\n-- Demo table\r\nCREATE TABLE dbo.Test\r\n(\r\n    TID integer IDENTITY NOT NULL,\r\n    FilterMe integer NOT NULL,\r\n    SortMe integer NOT NULL,\r\n    Unused nvarchar(max) NULL,\r\n    \r\n    CONSTRAINT PK_dbo_Test_TID\r\n    PRIMARY KEY CLUSTERED (TID)\r\n);\r\nGO\r\n-- 100,000 example rows\r\nINSERT dbo.Test WITH (TABLOCKX)\r\n    (FilterMe, SortMe)\r\nSELECT TOP (100 * 1000)\r\n    CHECKSUM(NEWID()) % 1000,\r\n    CHECKSUM(NEWID())\r\nFROM sys.all_columns AS AC1\r\nCROSS JOIN sys.all_columns AS AC2;\r\nGO    \r\n-- Query\r\nSELECT\r\n    T.TID,\r\n    T.FilterMe,\r\n    T.SortMe,\r\n    T.Unused\r\nFROM dbo.Test AS T \r\nWHERE \r\n    T.FilterMe = 567\r\nORDER BY \r\n    T.SortMe;\r\n```\r\n\r\nFor an estimated 50 rows, the optimizer reserves almost 500 MB for the sort:\r\n\r\n![Estimated plan][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/5De1C.png	103	2019-11-24 11:05:45.302622+00	0	4	2	171	233	1456	86383	0	0	0	2019-11-24 11:04:46.070196+00	f	f	1	1
2	617	447	2013-10-24 08:31:33+00	Index on Persisted Computed column needs key lookup to get columns in the computed expression	I have a persisted computed column on a table which is simply made up concatenated columns, e.g.\r\n\r\n\tCREATE TABLE dbo.T \r\n\t(\t\r\n\t\tID INT IDENTITY(1, 1) NOT NULL CONSTRAINT PK_T_ID PRIMARY KEY,\r\n\t\tA VARCHAR(20) NOT NULL,\r\n\t\tB VARCHAR(20) NOT NULL,\r\n\t\tC VARCHAR(20) NOT NULL,\r\n\t\tD DATE NULL,\r\n\t\tE VARCHAR(20) NULL,\r\n\t\tComp AS A + '-' + B + '-' + C PERSISTED NOT NULL \r\n\t);\r\n\r\nIn this `Comp` is not unique, and D is the valid from date of each combination of `A, B, C`, therefore I use the following query to get the end date for each `A, B, C` (basically the next start date for the same value of Comp):\r\n\r\n\tSELECT\tt1.ID,\r\n\t\t\tt1.Comp,\r\n\t\t\tt1.D,\r\n\t\t\tD2 = (\tSELECT\tTOP 1 t2.D\r\n\t\t\t\t\tFROM\tdbo.T t2\r\n\t\t\t\t\tWHERE\tt2.Comp = t1.Comp\r\n\t\t\t\t\tAND\t\tt2.D > t1.D\r\n\t\t\t\t    ORDER BY t2.D\r\n\t\t\t\t)\r\n\tFROM\tdbo.T t1\r\n\tWHERE\tt1.D IS NOT NULL -- DON'T CARE ABOUT INACTIVE RECORDS\r\n\tORDER BY t1.Comp;\r\n\r\nI then added an index to the computed column to assist in this query (and also others):\r\n\r\n\tCREATE NONCLUSTERED INDEX IX_T_Comp_D ON dbo.T (Comp, D) WHERE D IS NOT NULL;\r\n\r\nThe query plan however surprised me. I would have thought that since I have a where clause stating that `D IS NOT NULL` and I am sorting by `Comp`, and not referencing any column outside of the index that the index on the computed column could be used to scan t1 and t2, but I saw a clustered index scan. \r\n\r\n![enter image description here][1]\r\n\r\nSo I forced the use of this index to see if it yielded a better plan:\r\n\r\n\tSELECT\tt1.ID,\r\n\t\t\tt1.Comp,\r\n\t\t\tt1.D,\r\n\t\t\tD2 = (\tSELECT\tTOP 1 t2.D\r\n\t\t\t\t\tFROM\tdbo.T t2\r\n\t\t\t\t\tWHERE\tt2.Comp = t1.Comp\r\n\t\t\t\t\tAND\t\tt2.D > t1.D\r\n\t\t\t\t    ORDER BY t2.D\r\n\t\t\t\t)\r\n\tFROM\tdbo.T t1 WITH (INDEX (IX_T_Comp_D))\r\n\tWHERE\tt1.D IS NOT NULL\r\n\tORDER BY t1.Comp;\r\n\r\nWhich gave this plan\r\n\r\n![enter image description here][2]\r\n\r\nThis shows that a Key lookup is being used, the details of which are:\r\n\r\n![enter image description here][3]\r\n\r\nNow, according to the SQL-Server documentation:\r\n\r\n>You can create an index on a computed column that is defined with a deterministic, but imprecise, expression if the column is marked PERSISTED in the CREATE TABLE or ALTER TABLE statement. This means that the Database Engine stores the computed values in the table, and updates them when any other columns on which the computed column depends are updated. The Database Engine uses these persisted values when it creates an index on the column, and when the index is referenced in a query. This option enables you to create an index on a computed column when Database Engine cannot prove with accuracy whether a function that returns computed column expressions, particularly a CLR function that is created in the .NET Framework, is both deterministic and precise.\r\n\r\nSo if, as the docs say *"the Database Engine stores the computed values in the table"*, and the value is also being stored in my index, why is a Key Lookup required to get A, B and C when they are not referenced in the query at all? I assume they are being used to calculate Comp, but why? Also, why can the query use the index on `t2`, but not on `t1`?\r\n\r\n*[Queries and DDL on SQL Fiddle](http://sqlfiddle.com/#!3/e47a1/2)*\r\n\r\n\r\n*N.B. I have tagged SQL Server 2008 because this is the version that my main problem is on, but I also get the same behaviour in 2012.*\r\n\r\n  [1]: https://i.stack.imgur.com/NrGjj.png\r\n  [2]: https://i.stack.imgur.com/AkBTW.png\r\n  [3]: https://i.stack.imgur.com/ygAJv.png	458	2019-12-15 06:12:59.391979+00	0	4	1	522	1083	2264	52129	0	0	0	2019-12-15 06:12:59.391979+00	f	f	1	1
4	841	474	2019-10-23 09:56:33+00	Expandable test for an empty token list—methods, performance, and robustness	With &epsilon;-TeX, the go-to method for testing if a `<token-list>` is empty is the following test:\r\n\r\n    \\if\\relax\\detokenize{<token-list>}\\relax\r\n      % empty\r\n    \\else\r\n      % not empty\r\n    \\fi\r\nThe method is fool-proof as long as the `<token-list>` can be safely `\\detokenize`d, which is the case when it is grabbed as argument to some other macro which does the testing.\r\n\r\nNow looking at the `expl3` sources I found the test to actually be (modulo `_` and `:`)\r\n\r\n    \\expandafter\\ifx\\expandafter\\qnil\\detokenize{#1}\\qnil\r\n      % empty\r\n    \\else\r\n      % not empty\r\n    \\fi\r\n\r\nwhere `\\qnil` are “quarks” defined with `\\def\\qnil{\\qnil}`, which means that `\\ifx\\qnil<token>` will only be true if `<token>` is `\\qnil`, which will be the case _only if_ `#1` is empty; otherwise `<token>` will be any other (catcode-10 or 12) token which will make the test return false.\r\n\r\nBut this condition is also true for the first test: `\\if\\relax<token>` will only be true if `<token>` is another control sequence, which will never be the case if there's _anything_ inside the `\\detokenize`.\r\n\r\n### Or is it?\r\n\r\nIs there a reason for the second method being preferred over the first? Is there an edge-case in which one of them would fail?\r\n\r\nBoth methods, as far as I can tell, apply the same treatment to the input token list, and are both robust regarding weird arguments, such as `\\iftrue\\else\\fi` (which would otherwise be a problem) because in either case the `<token-list>` is `\\detokenize`d, so the argument can be virtually anything.\r\n\r\n---\r\n\r\n### Motivation:\r\n\r\nI’m working on some code that will use this test and should be executed a few hundred times for each function call, so performance is important. According to my tests the first method is slightly (very, _very_ slightly) faster than the second:\r\n```latex\r\n\\RequirePackage{l3benchmark}\r\n\\ExplSyntaxOn\r\n\\prg_new_conditional:Npnn \\pho_tl_if_empty:n #1 { TF }\r\n  {\r\n    \\if:w \\scan_stop: \\tl_to_str:n {#1} \\scan_stop:\r\n      \\prg_return_true:\r\n    \\else:\r\n      \\prg_return_false:\r\n    \\fi:\r\n  }\r\n\\cs_new:Npn \\pho_test:N #1\r\n  {\r\n    \\benchmark_tic:\r\n    \\int_step_inline:nn { 999999 }\r\n      {\r\n        #1 { } { } { } % Empty\r\n        #1 { X } { } { } % non-empty\r\n        #1 { \\iftrue \\else \\fi } { } { } % just in case\r\n      }\r\n    \\benchmark_toc:\r\n  }\r\n\\pho_test:N \\pho_tl_if_empty:nTF\r\n\\pho_test:N \\tl_if_empty:nTF\r\n\\stop\r\n```\r\noutput:\r\n```none\r\n(l3benchmark) + TIC\r\n(l3benchmark) + TOC: 2.17 s\r\n(l3benchmark) + TIC\r\n(l3benchmark) + TOC: 2.32 s\r\n```\r\n.&thinsp;.&thinsp;. Yes, those are 15 hundredths of a second in one million repetitions :-)\r\n\r\nThus, the motivation here is to know whether I can use the (in)significantly faster method without sacrificing robustness. The _real_ motivation is to know in what way this type of choice may come to bite me in the future.	486	2019-12-22 19:47:43.640436+00	0	4	1	549	1156	2409	513337	0	0	0	2019-12-22 19:47:43.640436+00	f	f	1	2
2	184	285	2019-05-13 17:09:36+00	Substring join or additional table, which is faster?	I have a case that come up regularly at work. I have many tables that use a 4 character string as a foreign key : `G191`\r\n\r\nThe `G` is a sort of category, the `19` is the year, and the `1` is an instance.\r\nGetting all rows of category `G` is something we do very often. Usually like:\r\n\r\n```\r\nSELECT * FROM [Table] \r\nWHERE Left([ID], 1) = 'G'\r\n```\r\n\r\nThere is a way to get this effect without manipulating a string, by joining to a table where this category is defined:\r\n\r\n```\r\nSELECT * FROM [Table]\r\nJOIN [Categories] ON [Table].CategoryID = [Categories].CategoryID\r\nWHERE [Categories].Letter = 'G'\r\n```\r\n\r\nMy co worker insists that the first way is more performant, and rolls his eyes at me for doing it the second way.\r\n\r\nWhich one is better? Does joining by another table really add more work then checking the first character of a string?	293	2019-12-04 14:29:02.786986+00	0	4	1	360	616	1527	238042	0	0	0	2019-12-04 14:29:02.786986+00	f	f	1	1
2	750	560	2013-10-04 13:32:08+00	Why does @@dbts increase after backup/restore?	I have a SQL Server 2005 database, containing some tables which have a Timestamp (or RowVersion) column.\r\nI need to know the value of the current timestamp, so I use the following query: `SELECT CAST(@@dbts AS BIGINT);`\r\n\r\nThis will return, for example, **10505**.\r\n\r\nImmediately afterwards, without updating, inserting, ... anything, I do a `BACKUP DATABASE` and a `RESTORE DATABASE` and I run the `SELECT` query again.\r\nOnly this time, the result is **14000**, while none of the timestamps in the tables have increased.\r\n\r\nWhy/how does this happen?\r\n	573	2020-01-12 14:19:28.702029+00	0	4	1	635	1436	2949	51026	0	0	0	2020-01-12 14:19:28.702029+00	f	f	1	1
1	2	383	2019-12-08 20:21:58.06475+00	A big change you (hopefully) won't notice	### Will it scale?\r\n\r\n[![](https://imgs.xkcd.com/comics/datacenter_scale.png)](https://xkcd.com/1737)\r\n\r\nRight now, you could fit the TopAnswers server on your mobile phone with room to spare, but perhaps one day that won't be the case. It would be nice if everything doesn't grind to a halt on that day.\r\n\r\nSo we have to think about how we will scale, based on reasonable worst-case[^1] hand-wavy back of the envelope calculations.\r\n\r\nLet's say we decide we want to keep on going even if we become as busy as Stack Overflow. How do we even begin to plan for that? Some would say we need to [use a particular language](https://forum.codidact.org/t/should-we-join-forces-with-topanswers/330/2?u=jackdouglas):\r\n\r\n> I’m pretty confident in saying that PHP wont scale to the levels we are looking for, so I think we should continue our efforts on the asp.net core project.\r\n\r\n[or](https://topanswers.xyz/transcript?room=4&id=7417#c7417) [database](https://topanswers.xyz/transcript?room=4&id=7423#c7423) (possibly in jest ;)):\r\n\r\n> but will it scale\r\n\r\n> It's on PostgreSQL so no\r\n\r\nOf course those choices matter in their own ways, but we've been thinking about another particular angle, which is mostly independent[^2] of the technology choices we make: ***database state changes***, a.k.a. 'writes'. An important fact of any 'library of knowledge' that is seeing actual use, is that there will be a *lot more* reads than writes. That asymmetry needs to be considered — if we ignore it, all the people who make the content great will suddenly start suffering at the expense of the many more who are benefitting from that great content.\r\n\r\nSo, although it is early days, we have started to plan for this. We've just released a major update splitting TopAnswers into two. From now on there is topanswers.xyz (which everyone will know about and use) and post.topanswers.xyz (which no-one will know about and will only be used by the topanswers source code itself). The important distinction is that every operation that *does not change the database* will use topanswers.xyz and an HTTP GET, and every operation that *changes the database* will use post.topanswers.xyz and an HTTP POST.\r\n\r\nWe are telling you this now because:\r\n\r\n1. we probably broke some things with the update (please let us know about any regressions).\r\n2. you might be interested :)\r\n\r\nWhen necessary, we will use the replication features of postgres to offload the [vast majority](https://stackexchange.com/sites#questionsperday) of activity off the master server (which will continue to handle writes). The url change makes this easy, and with some sort of DNS load balancing on the main topanswers.xyz url, we can take it a step further and have as many read-only replicas  as we need including, perhaps, one geographically near *you*.\r\n\r\nSo, we made a big change, it was a bit more difficult than we thought (forgot about CORS, had various issues with cookies etc), but hopefully you didn't notice. Now we'll get back to making some changes you've actually asked for!\r\n\r\n\r\n[^1]: Or 'best-case' depending on how you look at it!\r\n[^2]: Given that we really don't want to think about any multi-master replication technology unless we absolutely have to. And we probably don't.	394	2019-12-08 20:59:12.181779+00	11	1	1	458	895	2680	\N	0	0	0	\N	f	f	3	\N
1	702	665	2020-02-01 13:37:45.997455+00	Does a "tag-only-edit" bump questions?	Certain actions bring a Q&A back to the top of the top page of TopAnswers, in particular:\r\n\r\n- adding an answer;\r\n- editing the question or any answer.\r\n\r\nBut it seems as if **tag edits**, either adding or deleting tags, does *not* bump the Q&A in the site's sort order.\r\n\r\nIs this correct?\r\n\r\nI actually hope it is, as the re-tagging changes nothing in terms of Q&A content, only refines the "meta" information about it. (I always thought tag-edits on SE that bumped Q&As were a bit of a pain, frankly.)	680	2020-02-01 13:37:45.997455+00	6	1	1	740	1975	3764	\N	0	0	0	\N	f	f	2	\N
4	168	664	2020-01-31 17:09:02.219254+00	Add a new question type "Contest"	Since from time to time the more ducky people of the TeX community like to have a small fun-contest (example: https://topanswers.xyz/tex?q=533) I think it would be nice to have a new question type for this.\r\n\r\nThe CodeGolf.TA community has the question type "Code Golf" for new questions which is nicely displayed on the question list similar to the Meta questions.\r\n\r\nI'd like to have the question type "Contest" in TeX.TA in addition to the other types ("Question", "Meta Question", and "Blog Post").	679	2020-01-31 17:09:02.219254+00	11	6	3	739	1925	3749	\N	0	0	0	\N	f	f	2	\N
1	115	242	2019-12-02 01:47:25.617106+00	A proposal for less crowding in chat pane (particularly on question lists)	I was chatting (:-) ) wtih Jack about how the question-list page felt crowded to me and particularly that the chat pane felt over-emphasized to me.  Even though the pane is narrower than the main pane (where questions or the question list go), I think I'm seeing it as emphasized more because it's more *dense* -- it's a column of text with very little whitespace.  You don't want wasted space in a chat interface, so just spreading things out in that pane wouldn't be the right approach.\r\n\r\nAnother issue that came up while we were chatting was migrating between rooms.  I'd lost track of where we were and didn't feel I had an easy way to see what rooms I was in and how many messages were waiting for me since I'd last been there.\r\n\r\nThese ideas came together, and after clearing it with Jack (I don't want to be a pushy newcomer), I drew some sketches.  The main ideas here are:\r\n\r\n- Have a distinct notifications area, and not just for chat.  (I didn't draw it here, but I imagine this being collapsible and only shown if you *have* waiting notifications.)\r\n\r\n- Be able to toggle between showing chat and seeing a list of rooms you're in.  Clicking on any of those rooms would take you there.\r\n\r\n- The room list, like on SE, would show gravatars of who's there.\r\n\r\n- The list would show the number of new messages since last you were in that room, like chat in an inactive tab on SE.  Like the SE rooms list, it would also show you how long ago someone talked.\r\n\r\n- The sections (main page, notifications, chat) have clear boundaries with some padding, as compared to current chat notifications which just overlay the chat with little padding.  (That confused me when I first saw it; I thought I'd somehow opened the chat in two places.)\r\n\r\nPlease forgive my crude drawings to try to illustrate this point.  These are not proper wireframes or anything like that, just Paintbrush.\r\n\r\nCollapsed view:\r\n\r\n![chat-mockup.png](/image?hash=f682054fccb04ba63f0594a752114d0e4b339b3cf9327e42135825ed07f8c3f5)\r\n\r\nExpanded view (active chat room):\r\n\r\n![chat-mockup-expand.png](/image?hash=4503f131f5bdeda784c284a448ae6c20d58ea4babcd7ebe059bff065ba58808e)	250	2019-12-02 02:15:38.825163+00	6	4	1	317	2102	3890	\N	0	0	0	\N	f	f	2	\N
2	603	436	2018-05-08 20:11:16+00	How to control position of startup predicate in the execution plan	For two last statements of the code below produce actual execution plan. You can see that startup predicate on `@Par1` is placed in a different position that completely changes the actual number of rows that come from `test_fn1` function. I need to control this behaviour.\r\n\r\n    create or alter function dbo.test_fn1(@Par1 varchar(100), @Par2 varchar(1))\r\n    returns @t table(item varchar(100))\r\n    as\r\n    begin\r\n      insert into @t (item) select value from STRING_SPLIT(@Par1, @Par2);\r\n      return\r\n    end\r\n    GO\r\n    create or alter function dbo.test_fn(@Par1 varchar(100), @Par2 varchar(100))\r\n    returns table\r\n    as\r\n    return (\r\n      select s.*\r\n        from dbo.test_fn1(@Par2,';') x\r\n        inner join sys.objects s on s.name = x.item\r\n        where @Par1 = 'CASE1'\r\n    )\r\n    GO\r\n    create or alter function dbo.test_fnx(@Par1 varchar(100), @Par2 \r\n       varchar(100))\r\n    returns table\r\n    as\r\n    return (\r\n      select s.*\r\n        from sys.objects s\r\n        inner join dbo.test_fn1(@Par2,';') x ON s.name = x.item\r\n        where @Par1 = 'CASE1'\r\n    )\r\n    GO\r\n    declare @Par1 varchar(100), @Par2 varchar(100)\r\n    select @Par1 = 'CASE2', @Par2 = 'test1;test2'\r\n\r\n    select * from dbo.test_fn(@Par1, @Par2)\r\n    select * from dbo.test_fnx(@Par1, @Par2)\r\n\r\nBelow is the proper plan where startup predicate makes the query does not work on any data.\r\n[![enter image description here][1]][1]\r\n\r\nHere is the plan that show wrong behaviour of placing startup predicate. In both cases we start from the same function and only order in T-SQL is changed.\r\n[![enter image description here][2]][2]\r\n\r\nTested on SQL Server 2016 SP2.\r\n\r\nIs there any whitepaper or documentation on how SQL Server is placing startup predicates? \r\n\r\n\r\n  [1]: https://i.stack.imgur.com/QxJAJ.png\r\n  [2]: https://i.stack.imgur.com/6QA8C.png	447	2019-12-13 09:50:14.331608+00	0	4	1	511	1047	2269	206191	0	0	0	2019-12-13 09:50:14.331608+00	f	f	1	1
2	87	120	2016-02-27 15:56:57+00	How to reset statistics after UPDATE STATISTICS … WITH ROWCOUNT	For query tuning and testing purposes, you can manually assign a rowcount and pagecount to a table's index statistics by running `UPDATE STATISTICS`. But how do you recompute/reset the statistics to the table's actual contents?\r\n\r\n    --- Create a table..\r\n    CREATE TABLE dbo.StatTest (\r\n        i      int NOT NULL,\r\n        CONSTRAINT PK_StatTest PRIMARY KEY CLUSTERED (i)\r\n    );\r\n    GO\r\n\r\n    --- .. and give it a thousand-or-so rows:\r\n    DECLARE @i int=1;\r\n    INSERT INTO dbo.StatTest (i) VALUES (@i);\r\n    \r\n    WHILE (@i<1000) BEGIN;\r\n        INSERT INTO dbo.StatTest (i) SELECT @i+i FROM dbo.StatTest;\r\n        SET @i=@i*2;\r\n    END;\r\n\r\nA dummy query:\r\n\r\n    SELECT i%100, COUNT(*) FROM dbo.StatTest GROUP BY i%100;\r\n\r\n... will return the following query plan (the row estimate in the Index Scan is 1024 rows).\r\n\r\n[![10 000 rows][1]][1]\r\n\r\nRun the `UPDATE STATISTICS` command..\r\n\r\n    UPDATE STATISTICS dbo.StatTest WITH ROWCOUNT=10000000;\r\n\r\n... and the plan looks like this, now with an estimate of 10 million rows:\r\n\r\n[![10 million rows][2]][2]\r\n\r\nHow do I reset the rowcount to the actual contents of the table without using `WITH ROWCOUNT`?\r\n\r\nI've tried `WITH FULLSCAN`, `WITH RESAMPLE` and `WITH SAMPLE n ROWS`, but the statistics rowcount remains 10 million rows. Inserting a row or even deleting all of the rows doesn't update the statistics, because the change is too small.\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/e8Mtl.png\r\n  [2]: http://i.stack.imgur.com/Dh3HI.png	127	2019-11-26 16:07:16.465711+00	0	4	1	195	286	349	130660	0	0	0	2019-11-26 16:07:16.465711+00	f	f	1	1
2	657	460	2012-02-13 08:10:42+00	What's the easiest way to create a temp table in SQL Server that can hold the result of a stored procedure?	Many times I need to write something like the following when dealing with SQL Server.\r\n\r\n    create table #table_name\r\n    (\r\n        column1 int,\r\n        column2 varchar(200)\r\n        ...\r\n    )\r\n    \r\n    insert into #table_name\r\n    execute some_stored_procedure;\r\n\r\nBut create a table which has the exact syntax as the result of a stored procedure is a tedious task. For example, the result of [`sp_helppublication`][1] has 48 columns! I want to know if there is an easier way to do this?\r\n\r\n\r\n  [1]: https://msdn.microsoft.com/en-us/library/ms189782.aspx	471	2019-12-18 07:30:06.95171+00	0	4	1	535	1117	2222	12739	0	0	0	2019-12-18 07:29:21.513412+00	f	f	1	1
2	656	459	2017-02-18 22:11:19+00	Why does this seek on BIGINT col have extra constant scan, compute scalar, and nested loops operators?	When I look at the actual exection plan of some of my queries I notice that literal constants used in a `WHERE` clause show up as a nested chain of *compute scalar* and *constant scan*.\r\n\r\n[![sql studio screenshot][1]][1]\r\n\r\nTo reproduce this, I use the following table\r\n\r\n    CREATE TABLE Table1 (\r\n\t    [col1] [bigint] NOT NULL,\r\n\t    [col2] [varchar](50) NULL,\r\n\t    [col3] [char](200) NULL\r\n    )\r\n    CREATE NONCLUSTERED INDEX IX_Table1 ON Table1 (col1 ASC)\r\n\r\nWith some data in it:\r\n\r\n    INSERT INTO Table1(col1) VALUES (1),(2),(3),\r\n                                   (-9223372036854775808),\r\n                                   (9223372036854775807),\r\n                                   (2147483647),(-2147483648)\r\n\r\nWhen I run the following (nonsense) query:\r\n\r\n    SELECT a.col1, a.col2\r\n      FROM Table1 a, Table1 b\r\n      WHERE b.col1 > 2147483648\r\n\r\nI see that it will do a Nested Loop drawing in the result of Index Seek and a scalar calculation (from a constant).\r\n\r\nNote that the literal is larger than maxint. It does help to write `CAST(2147483648 as BIGINT)`. Any idea why SQL Server is defrering that to the execution plan and is there a shorter way to avoid it than using the cast? Does it affect bound parameters to prepared statements (from jtds JDBC) as well?\r\n\r\nThe scalar calculation is not always done (seems to be *index seek* specific). And sometimes the query analyser does not show it graphically but as `col1 < scalar(expr1000)` in the predicate properties.\r\n\r\n*I have seen this with SSMS 2016 (13.0.16100.1) and SQL Server 2014 Express Edition 64bit on Windows 7, but I guess it is a general behavior.*\r\n\r\n  [1]: https://i.stack.imgur.com/0uaGH.png	470	2019-12-18 07:22:00.56519+00	0	4	1	534	1115	2220	164808	0	0	0	2019-12-18 07:20:41.925645+00	f	f	1	1
2	126	223	2018-07-27 15:20:55+00	Is sql_variant compatible with First Normal Form (1NF)?	1NF rule says that we should not keep data of different types in one single column. Does it mean that sql_variant is not compatible with first normal form hence should not be used?	231	2019-12-01 15:14:07.78371+00	0	4	1	298	471	559	213421	0	0	0	2019-12-01 15:14:07.78371+00	f	f	1	1
2	2	146	2012-08-15 10:08:11+00	How can I generate a random bytea	I would like to be able to generate random `bytea` fields of arbitrary length (<1Gb) for populating test data.\r\n\r\nWhat is the best way of doing this?	153	2019-11-27 01:21:52.456218+00	0	4	1	221	323	311	22512	0	0	0	2019-11-27 01:21:52.456218+00	f	f	1	1
2	71	100	2014-11-05 19:35:31+00	Why is cost threshold for parallelism ignored?	SQL Server 2012 SP2 Enterprise Edition. Users complaining of slowness. Monitoring tool shows highest wait event is `CXPACKET`. \r\n\r\nInstance settings \r\n\r\n* MAXDOP: 8\r\n* Cost Threshold for Parallelism: 175\r\n\r\nRan `sp_BlitzCache` (from the Brent Ozar toolset, those guys rock) for further diagnosis and results show queries with cost under 175 going parallel.  \r\n\r\nAnyone ever see Cost Threshold for Parallelism being ignored?	107	2019-11-25 10:16:16.112028+00	0	4	1	175	245	355	81980	0	0	0	2019-11-25 10:16:16.112028+00	f	f	1	1
2	37	550	2017-08-31 14:13:06+00	BACKUP MASTER KEY failing with cannot find symmetric master key because it does not exist	I'm trying to backup the master key for a TDE database, but SQL Server says there isn't one. It's a bit weird, but I'm sure I'm just doing something wrong. I'm sysadmin on the server, so I should be able to see everything.\r\n\r\nThis is the statement that is failing:\r\n\r\n    USE [my_db];\r\n    BACKUP MASTER KEY\r\n    TO FILE = 'C:\\master_key'\r\n    ENCRYPTION BY PASSWORD = 'some_killer_password';\r\n\r\nAnd the error message returned:\r\n\r\n>Msg 15151, Level 16, State 1, Line 11  \r\n>Cannot find the symmetric key 'master key', because it does not exist or you do not have permission.\r\n\r\nI've used the following to see the details about the database encryption key, and the associated certificate, however the certificate details from the `sys.certificates` table is empty.\r\n\r\n    USE [my_db];\r\n    SELECT DatabaseName = d.name\r\n    \t, ddek.encryptor_type\r\n        , ddek.opened_date\r\n        , c.name\r\n        , c.cert_serial_number\r\n        , c.pvt_key_encryption_type_desc\r\n        , c.subject\r\n    FROM sys.dm_database_encryption_keys ddek\r\n    \tINNER JOIN sys.databases d ON ddek.database_id = d.database_id\r\n        LEFT JOIN sys.certificates c ON ddek.encryptor_thumbprint = c.thumbprint\r\n    WHERE d.name <> 'tempdb' /* tempdb is auto-encrypted by SQL Server */\r\n\r\n```\r\n╔══════════════╦════════════════╦═════════════════════════╦══════╦════════════════════╦══════════════════════════════╦═════════╗\r\n║ DatabaseName ║ encryptor_type ║ opened_date             ║ name ║ cert_serial_number ║ pvt_key_encryption_type_desc ║ subject ║\r\n╠══════════════╬════════════════╬═════════════════════════╬══════╬════════════════════╬══════════════════════════════╬═════════╣\r\n║ my_db        ║ CERTIFICATE    ║ 2017-09-20 11:24:13.590 ║ NULL ║ NULL               ║ NULL                         ║ NULL    ║\r\n╚══════════════╩════════════════╩═════════════════════════╩══════╩════════════════════╩══════════════════════════════╩═════════╝\r\n```\r\n\r\nSo, I can see the database encryption key in `[my_db]`, and I can see it's encrypted by a certificate, but the certificate doesn't exist?	563	2020-01-10 22:02:30.636769+00	0	4	1	625	1405	2943	184805	0	0	0	2020-01-10 22:00:08.815729+00	f	f	1	1
2	127	224	2018-08-03 16:49:04+00	Are duplicates in Read Commited standard behaviour?	This refers to this question: https://dba.stackexchange.com/questions/185897/duplicate-records-returned-from-table-with-no-duplicates\r\n\r\nThe "read commited" isolation level assumes that one transaction could read same record differently, but it does not explicitly imply that records could be duplicated. I agree that two statements in the same transactions could return different data sets, but returning same data set with record duplication sounds insane.\r\n\r\nAs far as I know only MS SQL Server behaves this way.\r\n\r\nPlease prove or disprove if it conforms to "Read Committed" isolation level defined by ANSI SQL standard.\r\n	232	2019-12-01 15:19:00.724119+00	0	4	1	299	542	721	214021	0	0	0	2019-12-01 15:19:00.724119+00	f	f	1	1
2	658	461	2016-01-06 20:14:54+00	What does the position of the ON clause actually mean?	The normal `JOIN ... ON ...` syntax is well known. But it is also possible to position the `ON` clause separately from the `JOIN` that it corresponds to. This is something that is rarely seen in practice, not found in tutorials and I have not found *any* web resource that even mentions that this is possible.\r\n\r\nHere is a script to play around with:\r\n\r\n    SELECT *\r\n    INTO #widgets1\r\n    FROM (VALUES (1), (2), (3)) x(WidgetID)\r\n    \r\n    \r\n    SELECT *\r\n    INTO #widgets2\r\n    FROM (VALUES (1, 'SomeValue1'), (2, 'SomeValue2'), (3, 'SomeValue3')) x(WidgetID, SomeValue)\r\n    \r\n    SELECT *\r\n    INTO #widgetProperties\r\n    FROM (VALUES\r\n        (1, 'a'), (1, 'b'),\r\n        (2, 'a'), (2, 'b'))\r\n    x(WidgetID, PropertyName)\r\n    \r\n    \r\n    --q1\r\n    SELECT w1.WidgetID, w2.SomeValue, wp.PropertyName\r\n    FROM #widgets1 w1\r\n    LEFT JOIN #widgets2 w2 ON w2.WidgetID = w1.WidgetID\r\n    LEFT JOIN #widgetProperties wp ON w2.WidgetID = wp.WidgetID AND wp.PropertyName = 'b'\r\n    ORDER BY w1.WidgetID\r\n    \r\n    \r\n    --q2\r\n    SELECT w1.WidgetID, w2.SomeValue, wp.PropertyName\r\n    FROM #widgets1 w1\r\n    LEFT JOIN #widgets2 w2 --no ON clause here\r\n    JOIN #widgetProperties wp\r\n     ON w2.WidgetID = wp.WidgetID AND wp.PropertyName = 'b'\r\n     ON w2.WidgetID = w1.WidgetID\r\n    ORDER BY w1.WidgetID\r\n    \r\n    \r\n    --q3\r\n    SELECT w1.WidgetID, w2.SomeValue, wp.PropertyName\r\n    FROM #widgets1 w1\r\n    LEFT JOIN (\r\n        #widgets2 w2 --no SELECT or FROM here\r\n        JOIN #widgetProperties wp\r\n        ON w2.WidgetID = wp.WidgetID AND wp.PropertyName = 'b')\r\n    ON w2.WidgetID = w1.WidgetID\r\n    ORDER BY w1.WidgetID\r\n\r\nq1 looks normal. q2 and q3 have these unusual positionings of the `ON` clause.\r\n\r\nThis script does not necessarily make much sense. It was hard for me to contrive a meaningful scenario.\r\n\r\nSo what do these unusual syntax patterns mean? How is this defined? I noticed that not all positions and orderings for the two `ON` clauses are allowed. What are the rules governing this?\r\n\r\nAlso is it ever a good idea to write queries like this?	472	2019-12-18 07:33:52.556292+00	0	4	1	536	1118	2394	125422	0	0	0	2019-12-18 07:33:52.556292+00	f	f	1	1
2	80	84	2013-12-03 14:11:52+00	Why does SQL Server require the datatype length to be the same when using UNPIVOT?	When applying the [`UNPIVOT`][1] function to data that is not normalized, SQL Server requires that the datatype and length be the same.  I understand why the datatype must be the same but why does UNPIVOT require the length to be the same?\r\n\r\nLet's say that I have the following sample data that I need to unpivot:\r\n\r\n    CREATE TABLE People\r\n    (\r\n    \tPersonId int, \r\n    \tFirstname varchar(50), \r\n    \tLastname varchar(25)\r\n    )\r\n    \r\n    INSERT INTO People VALUES (1, 'Jim', 'Smith');\r\n    INSERT INTO People VALUES (2, 'Jane', 'Jones');\r\n    INSERT INTO People VALUES (3, 'Bob', 'Unicorn');\r\n\r\nIf I attempt to UNPIVOT the `Firstname` and `Lastname` columns similar to:\r\n\r\n    select PersonId, ColumnName, Value  \r\n    from People\r\n    unpivot\r\n    (\r\n      Value \r\n      FOR ColumnName in (FirstName, LastName)\r\n    ) unpiv;\r\n\r\nSQL Server generates the error:\r\n\r\n> Msg 8167, Level 16, State 1, Line 6\r\n\r\n>The type of column "Lastname" conflicts with the type of other columns specified in the UNPIVOT list.\r\n\r\nIn order to resolve the error, we must use a subquery to first cast the `Lastname` column to have the same length as `Firstname`:\r\n\r\n    select PersonId, ColumnName, Value  \r\n    from\r\n    (\r\n      select personid, \r\n        firstname, \r\n        cast(lastname as varchar(50)) lastname\r\n      from People\r\n    ) d\r\n    unpivot\r\n    (\r\n      Value FOR \r\n      ColumnName in (FirstName, LastName)\r\n    ) unpiv;\r\n\r\nSee [SQL Fiddle with Demo][2]\r\n\r\n\r\nPrior to UNPIVOT being introduced in SQL Server 2005, I would use a `SELECT` with `UNION ALL` to unpivot the `firstname`/`lastname` columns and the query would run without the need to convert the columns to the same length:\r\n\r\n    select personid, 'firstname' ColumnName, firstname value\r\n    from People\r\n    union all\r\n    select personid, 'LastName', LastName\r\n    from People;\r\n\r\nSee [SQL Fiddle with Demo][3].  \r\n\r\nWe are also able to successfully unpivot the data using `CROSS APPLY` without having the same length on the datatype:\r\n\r\n    select PersonId, columnname, value\r\n    from People\r\n    cross apply\r\n    (\r\n    \tselect 'firstname', firstname union all\r\n    \tselect 'lastname', lastname\r\n    ) c (columnname, value);\r\n\r\nSee [SQL Fiddle with Demo][4].\r\n\r\nI have read through MSDN but I didn't find anything explaining the reasoning for forcing the length on the datatype to be the same.  \r\n\r\nWhat is the logic behind requiring the same length when using UNPIVOT?  \r\n\r\n\r\n  [1]: http://msdn.microsoft.com/en-us/library/ms177410%28v=sql.105%29.aspx\r\n  [2]: http://sqlfiddle.com/#!3/e49ef/2\r\n  [3]: http://sqlfiddle.com/#!3/e49ef/3\r\n  [4]: http://sqlfiddle.com/#!3/e49ef/8	91	2019-11-23 08:16:28.030593+00	0	4	1	159	193	1452	54353	0	0	0	2019-11-23 08:16:28.030593+00	f	f	1	1
2	17	226	2018-08-21 08:04:22+00	Bad row estimate following Compute Scalar operator in plan	I'm struggling to understand where a row estimate is coming from in an execution plan.\r\n\r\n[Paste the plan link][1]\r\n\r\n\r\n    declare\r\n    @BatchKey INT = 1, @ParentBatchKey INT = 1,\r\n    @QuoteRef varchar(50) = 'Q00018249',\r\n    @MpanRef varchar(50) = '1425431100004'\r\n    \r\n    \r\n    SELECT DISTINCT\r\n            ISNULL(c.ContractReference,-1) AS [ContractReference] ,\r\n            ISNULL(d_cd.ContractDetailsKey,-1) AS [ContractDetailsKey] ,\r\n            -1 AccountManagerKey,\r\n            -1 SegmentationKey,\r\n            ISNULL(d_tpi.TpiKey,-1) AS [TpiKey] ,\r\n            ISNULL(d_cu.CustomerKey,-1) AS [CustomerKey] ,\r\n            ISNULL(d_p.ProductKey,-1) AS [ProductKey] ,\r\n            -1 as PayPointKey,\r\n            -1 AS [GspBandingKey], --Not used in Junifer ESOB\r\n            ISNULL(d_pps.[ProductPricingStructureKey],-1) AS [ProductPricingStructureKey],\r\n            ISNULL(d_tou.TouBandingKey,-1) AS [PricingStructureBandingKey],\r\n            -1 AS [VolumePointCategoryKey],\r\n            ISNULL(d_ppc.PowerPeriodCategoryKey,-1) AS [PowerPeriodCategoryKey],\r\n            ISNULL(d_pcat.[PriceComponentAggregationTypeKey],-1) AS [PriceComponentAggregationTypeKey],\r\n            -1 AS [MarginRateBandingKey], --Not used in Junifer ESOB\r\n            -1 AS [DuosUrcBandingKey], --Not used in Junifer ESOB\r\n            -1 AS [ConsumptionToleranceKey],\r\n            ISNULL(d_mp.MeterPointKey,-1) AS [MeterPointKey] ,\r\n            ISNULL(d.DateKey,-1) AS [ForecastDateKey] ,\r\n            -1 AS [ForecastEFADateKey], \r\n            ISNULL(d_cw.DateKey,-1) AS [ContractWonDateKey] ,\r\n            ISNULL(f.SiteVolumeKwh,0) AS [SiteVolume] ,\r\n            ISNULL(f.GspVolumeKwh,0) AS [GspVolume] ,\r\n            ISNULL(f.NbpVolumeKwh,0) AS [NbpVolume],\r\n            @BatchKey,\r\n            @ParentBatchKey,\r\n            CAST(f.ForecastKey as NVARCHAR(100)) AS [SourceId]\r\n    FROM \r\n            [Electricity].[Forecast] f \r\n                  INNER JOIN Electricity.ContractMeterPoint cmp ON cmp.MeterPointKey = f.MeterPointKey and cmp.ContractKey = f.ContractKey  \r\n                  INNER JOIN Electricity.Contract c on c.ContractKey = cmp.ContractKey \r\n            INNER JOIN Electricity.MeterPoint mp ON mp.MeterPointKey = cmp.MeterPointKey\r\n    \r\n            --INNER JOIN Electricity.ContractMeterPoint cmp ON cmp.MeterPointKey = mp.MeterPointKey and cmp.ContractKey = c.ContractKey \r\n            INNER JOIN Electricity.ProductBundle pb ON c.ProductBundleKey = pb.ProductBundleKey\r\n            LEFT JOIN Electricity.Quote q ON c.QuoteKey = q.QuoteKey\r\n            LEFT JOIN Gdf.Tender t ON q.TenderKey = t.TenderKey\r\n            LEFT JOIN Gdf.Customer cu ON q.CustomerKey = cu.CustomerKey\r\n            LEFT JOIN Electricity.ProductBundleAggregationType pbat ON pbat.ProductName = pb.BundleName\r\n            LEFT JOIN Dimensional_DW.DimensionElectricity.Product d_p ON d_p.ProductDurableKey = pb.ProductBundleKey\r\n            LEFT JOIN Dimensional_DW.Dimension.Tpi d_tpi ON d_tpi.TpiDurableKey = c.TpiKey\r\n            LEFT JOIN Dimensional_DW.DimensionElectricity.ProductPricingStructure d_pps ON d_pps.ProductPricingStructureDurableKey = f.PriceStructureKey\r\n            LEFT JOIN Dimensional_DW.DimensionElectricity.TouBanding d_tou ON d_tou.TouBandingDurableKey = f.PriceRateKey\r\n            LEFT JOIN Dimensional_DW.DimensionElectricity.MeterPoint d_mp ON d_mp.MeterPointDurableKey = cmp.MeterPointKey\r\n            LEFT JOIN Dimensional_DW.DimensionElectricity.PriceComponentAggregationType d_pcat ON d_pcat.[TnuosAggregationType] =pbat.[TNUoSAggType] AND d_pcat.[DuosAggregationType] =pbat.[DUoSFixedAvailAggType] AND d_pcat.[DuosUrcAggregationType] =pbat.[DUoSURCAggType] AND d_pcat.[BsuosAggregationType] =pbat.[BSUoSAggType] AND d_pcat.[ROAggregationType] =pbat.[ROAggType]\r\n            LEFT JOIN Dimensional_DW.Dimension.Date AS d ON d.DateKey = CAST(CONVERT(NVARCHAR(8), f.DeliveryDate, 112) AS INT) \r\n            LEFT JOIN Dimensional_DW.Dimension.Date AS d_cw ON d_cw.DateKey = CAST(CONVERT(NVARCHAR(8), c.QuoteWonDate, 112) AS INT) \r\n            LEFT JOIN Dimensional_DW.DimensionElectricity.PowerPeriodCategory d_ppc ON d_ppc.HhPeriod = f.Period\r\n            LEFT JOIN Dimensional_DW.Dimension.Customer d_cu ON d_cu.CustomerDurableKey = cu.CustomerKey\r\n            LEFT JOIN Dimensional_DW.DimensionElectricity.ContractDetails d_cd ON d_cd.ContractDetailsDurableKey = c.ContractKey\r\n    \r\n    WHERE  1=1\r\n       and     c.ContractReference = @QuoteRef\r\n       and c.QuoteWonDate IS NOT NULL \r\n       and c.QuoteKey <> -1\r\n               --(SELECT distinct C.ContractKey FROM Electricity.Contract WHERE ContractReference = @QuoteRef and c.QuoteWonDate IS NOT NULL and c.QuoteKey <> -1)\r\n                    --(SELECT distinct C1.ContractKey FROM Electricity.Contract c1 WHERE c1.ContractReference = @QuoteRef and c1.QuoteWonDate IS NOT NULL and c1.QuoteKey <> -1)\r\n            and mp.MpanID = @MpanRef\r\n                  --and c.ContractKey = 18235\r\n                  --and d.DateKey =  20180522\r\n                  order by [ForecastDateKey]\r\n\r\n\r\nMy problem is around nodeId 26, the scalar operator:\r\n\r\n[![enter image description here][2]][2]\r\n\r\nI'm unsure as to how the row estimate of 5 is being generated - this seems to then cascade down the plan to most other operators - the nested loop operators estimated execution counts further down the plan seem to all indicate ~5 estimated, then ~35k actual.\r\n\r\nWhy would the scalar operator be fed an estimate of ~14000 rows, then estimate an output of 5? Is this a problem or a red herring? Is it anything to do with the conversions it is performing? I can understand that affecting a join, but why would it affect the output of the conversion?\r\n\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=HkWoHrY8Q "Paste the plan link"\r\n  [2]: https://i.stack.imgur.com/zN1vX.png	234	2019-12-01 15:27:17.099254+00	0	4	1	301	475	1488	215450	0	0	0	2019-12-01 15:27:17.099254+00	f	f	1	1
2	176	275	2017-09-06 21:38:50+00	SQL Server--If logic in stored procedure and the plan cache	SQL Server 2012 and 2016 Standard:\r\n\r\nIf I put `if-else` logic in a stored procedure to execute one of two branches of code, depending on the value of a parameter, **does the engine cache the latest version?**\r\n\r\nAnd if on the following execution, the value of the parameter changes, **will it re-compile and re-cache the stored procedure**, since a different branch of the code must be executed? (This query is quite expensive to compile.)	283	2019-12-04 14:18:44.815994+00	0	4	1	350	603	1510	185252	0	0	0	2019-12-04 14:18:44.815994+00	f	f	1	1
2	197	303	2019-08-30 19:58:23+00	Inserting with implicit type conversion causes warning for cardinality estimates	I noticed this while doing some performance testing recently. When I insert a value into a column that will require an implicit conversion (e.g. `bigint` into `nvarchar`), I get a warning: \r\n\r\n> Type conversion in expression `(CONVERT_IMPLICIT(nvarchar(50),[tempdb].[dbo].[#MyFunIntTable].[EvenCoolerColumn],0))` may affect "Cardinality Estimate" in query plan choice.\r\n\r\nBeing a concerned citizen, I checked all of the obvious suspects and eventually dug into the XML to confirm that it was actually warning about the insert into the table. The problem is, I can't figure out why this would ever affect cardinality estimates. If I were doing this in a join or somewhere with a little more logic it would make sense, but there shouldn't be a cardinality estimate mismatch for the actual insert operation, right?\r\n\r\nI noticed that this happened when it was more than just a trivial query - as soon as more than one value is inserted, or we're pulling a value from a table, we hit this.\r\n\r\nI'm literally not doing anything with this column. I'm not using it in a filter, or a sort, or a grouping, or a join, or in a function - any of these things makes the scenario more complicated. All I'm doing is inserting a `bigint` into a `nvarchar`, which should never impact a meaningful cardinality estimate that I can think of.\r\n\r\nWhat I'm specifically looking for out of an answer is:\r\n\r\n  1. An explanation of why I get this warning despite nothing of interest going on - is it just that SQL Server will be conservative and report even when it won't affect plan choice?\r\n  2. What cardinality estimate is actually at risk here, and what operation would change based off of inaccuracies in that cardinality estimate?\r\n  3. Is there a scenario where this could affect plan choice? Obviously if I start joining or filtering on the converted column it could, but as-is?\r\n  4. Is there anything that can be done to prevent it from warning, besides changing data types (assume this is a requirement of how the data models interact)\r\n\r\nI recreated it with the below simple example ([paste the plan][1])\r\n\r\n    DROP TABLE IF EXISTS #MyFunStringTable;\r\n    DROP TABLE IF EXISTS #MyFunIntTable;\r\n    \r\n    CREATE TABLE #MyFunStringTable\r\n    (\r\n      SuperCoolColumn nvarchar(50) COLLATE DATABASE_DEFAULT NULL\r\n    );\r\n    \r\n    CREATE TABLE #MyFunIntTable\r\n    (\r\n      EvenCoolerColumn bigint NULL\r\n    );\r\n    \r\n    INSERT INTO #MyFunIntTable\r\n    ( EvenCoolerColumn )\r\n    VALUES\r\n    ( 1 ),\r\n    ( 2 ),\r\n    ( 3 ),\r\n    ( 4 ),\r\n    ( 5 );\r\n    \r\n    INSERT INTO #MyFunStringTable\r\n    ( SuperCoolColumn )\r\n      SELECT EvenCoolerColumn\r\n        FROM #MyFunIntTable;\r\n    \r\n    INSERT INTO #MyFunStringTable\r\n    ( SuperCoolColumn )\r\n    VALUES\r\n    ( 1 );\r\n    \r\n    INSERT INTO #MyFunStringTable\r\n    ( SuperCoolColumn )\r\n    VALUES\r\n    ( 1 ),\r\n    ( 2 );\r\n    \r\n    INSERT INTO #MyFunStringTable\r\n    ( SuperCoolColumn )\r\n      SELECT 1;\r\n    \r\n    INSERT INTO #MyFunStringTable\r\n    ( SuperCoolColumn )\r\n    SELECT 1\r\n    UNION ALL\r\n    SELECT 2;\r\n    \r\n    INSERT INTO #MyFunStringTable\r\n    ( SuperCoolColumn )\r\n      SELECT 1\r\n        FROM #MyFunIntTable;\r\n\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=H1CjyZvSS	311	2019-12-05 02:37:27.766529+00	0	4	1	378	662	1544	246677	0	0	0	2019-12-05 02:36:50.266909+00	f	f	1	1
2	174	315	2019-05-28 13:06:40+00	Improve OR inside INNER JOIN	I have [this query plan](https://www.brentozar.com/pastetheplan/?id=HJEioh56N). It shows a *nested loop (inner join)* with 97% cost.\r\n\r\nI'm sure the problem is the `OR` inside the second join, because I changed something here and there and I could get rid of it, but I would like to be sure what would be the best way to deal with data like these. These tables have millions of rows.\r\n\r\n### Table Definition\r\n\r\n\r\n    CREATE TABLE [DBO].[TABLE1](\r\n    \t[F1] [int] NOT NULL,\r\n    \t[F1] [varchar](16) NOT NULL,\r\n    \t[F3] [money] NOT NULL,\r\n    \t[F4] [money] NOT NULL)\r\n    \t\r\n\r\nI created this index:\r\n\r\n```\r\nCREATE NONCLUSTERED INDEX IX_TB1 ON DBO.TABLE1\r\n(\r\n    F1,\r\n    F2\r\n)\r\n```\r\n\r\nThe index seek is now 14% cost, but got a *Hash Join* with 82% cost.	323	2019-12-05 21:19:22.512947+00	0	4	1	390	718	2273	239210	0	0	0	2019-12-05 16:44:47.153347+00	f	f	1	1
2	86	112	2012-09-24 19:08:09+00	How (and why) does TOP impact an execution plan?	For a moderately complex query I am trying to optimize, I noticed that removing the `TOP n` clause changes the execution plan.  I would have guessed that when a query includes `TOP n` the database engine would run the query ignoring the the `TOP` clause, and then at the end just shrink that result set down to the *n* number of rows that was requested.  The graphical execution plan seems to indicate this is the case -- `TOP` is the "last" step.  But it appears there is more going on.\r\n\r\n**My question is, how (and why) does a TOP n clause impact the execution plan of a query?**\r\n\r\nHere is a simplified version of what is going on in my case:\r\n\r\nThe query is matching rows from two tables, A and B.  \r\n\r\nWithout the `TOP` clause, the optimizer estimates there will be 19k rows from table A and 46k rows from table B.  The actual number of rows returned is 16k for A and 13k for B.  A hash match is used to join these two results sets for a total of 69 rows (then a sort is applied).  This query happens very quickly.\r\n\r\nWhen I add `TOP 1001` the optimizer does not use a hash match; instead it first sorts the results from table A (same estimate/actual of 19k/16k) and does a nested loop against table B.  The estimated number of rows for table B is now 1, and the strange thing is that the `TOP n` directly affects the estimated number of executions (index seek) against B -- it appears to always be *2n+1*, or in my case 2003.  This estimate changes accordingly if I change `TOP n`.  Of course, since this is a nested join the actual number of executions is 16k (the number of rows from table A) and this slows down the query.\r\n\r\nThe actual scenario is a bit more complex but this captures the basic idea/behavior.  Both tables are searched using index seeks.  This is SQL Server 2008 R2 Enterprise edition.	119	2019-11-26 10:47:11.268705+00	0	4	1	187	275	1462	24832	0	0	0	2019-11-26 10:47:11.268705+00	f	f	1	1
2	81	101	2014-12-11 11:28:53+00	Get minimal logging when loading data into temporary tables	Even after reading [The Data Loading Performance Guide][1] , I am still unsure if its necessary to add the TABLOCK table hint to an empty temporary table, defined with a clustered index in order to get minimal logging.\r\n\r\nObviously the temp table is created in TempDB which operates in the SIMPLE recovery mode so I would have thought that it was a perfect candidate for minimal logging, however I can not find a passage to confirm it. \r\n\r\nIs a temporary table a candidate for minimal logging, and if so is it worth adding the TABLOCK hint as recommended for permanent tables?\r\n\r\n  [1]: http://technet.microsoft.com/en-us/library/dd425070%28v=sql.100%29.aspx	108	2019-11-25 12:08:52.08641+00	0	4	1	176	247	243	85892	0	0	0	2019-11-25 12:08:52.08641+00	f	f	1	1
2	112	195	2017-08-03 23:18:41+00	Why does the READPAST hint cause indexed views to be ignored?	I am investigating using the `READPAST` hint to reduce resource locking in our application's financial subsystem.\r\n\r\nIt seemed like a good way to go because financial transaction records are only ever added, never updated or deleted. The only rows that would ever be skipped are brand new rows inserted inside of a transaction; they effectively don't exist to the outside world until the transaction is committed.\r\n\r\nHowever, I noticed worse performance on queries that utilize indexed views that I had put the `READPAST` hint on. Comparing the query plans, it looks like with the hint, the query optimizer chooses to not use the indexed view and instead falls back to treating it like a regular view.\r\n\r\nI'm not sure why that would be; I imagine indexed views to be like any other index in that keys can be locked during operations and adding `READPAST` would work similarly.\r\n\r\n    SELECT TOP 1 isa.InvoiceId\r\n    FROM Financial_InvoiceSummaryAmounts isa WITH (READPAST)\r\n    WHERE isa.TotalOwedAmount = 0.0\r\n\r\n[![enter image description here][1]][1]\r\n\r\n    SELECT TOP 1 isa.InvoiceId\r\n    FROM Financial_InvoiceSummaryAmounts isa\r\n    WHERE isa.TotalOwedAmount = 0.0\r\n\r\n[![enter image description here][2]][2]\r\n\r\nAdding a `NOEXPAND` hint as well does seem to work, but I am interested in learning more about possibly why `READPAST` caused the query optimizer to make that choice in the first place (as part of a full answer).\r\n\r\n  [1]: https://i.stack.imgur.com/aR9zV.png\r\n  [2]: https://i.stack.imgur.com/l0UeJ.png	202	2019-11-29 12:35:49.012838+00	0	4	1	270	409	465	182646	0	0	0	2019-11-29 12:35:49.012838+00	f	f	1	1
2	200	306	2017-10-05 21:57:06+00	Do irrelevant columns affect query time of select statements?	Say you have a table of one million rows and run:\r\n\r\n    select order_value from store.orders\r\n\r\nDoes it make a difference whether that table has one column, two columns, or a hundred columns, in actual query time? I mean all columns other than `order_value`.\r\n\r\nRight now I'm pushing data to a data warehouse. Sometimes I dump columns into the table that "may be used in the future, someday", but they aren't being queried right now, by anything. Would these 'extraneous' columns affect select statements that do not include them, directly or indirectly (no `select *` I mean)?	314	2019-12-05 21:31:53.38221+00	0	4	1	381	722	1140	187807	0	0	0	2019-12-05 13:48:12.917398+00	f	f	1	1
2	12	91	2014-12-17 14:33:48+00	Retrieving n rows per group	I often need to select a number of rows from each group in a result set.\r\n\r\nFor example, I might want to list the 'n' highest or lowest recent order values per customer.\r\n\r\nIn more complex cases, the number of rows to list might vary per group (defined by an attribute of the grouping/parent record). This part is definitely optional/for extra credit and not intended to dissuade people from answering.\r\n\r\nWhat are the main options for solving these types of problems in SQL Server 2005 and later? What are the main advantages and disadvantages of each method?\r\n\r\n**AdventureWorks examples (for clarity, optional)**\r\n\r\n1. List the five most recent recent transaction dates and IDs from the `TransactionHistory` table, for each product that starts with a letter from M to R inclusive.\r\n2. Same again, but with `n` history lines per product, where `n` is five times the `DaysToManufacture` Product attribute.\r\n3. Same, for the special case where exactly one history line per product is required (the single most recent entry by `TransactionDate`, tie-break on `TransactionID`.	98	2019-11-24 09:51:29.291337+00	0	4	1	166	346	1469	86415	0	0	0	2019-11-24 09:51:29.291337+00	f	f	1	1
2	2	200	2011-12-24 15:38:28+00	If an 'after' DDL trigger causes an error, is the DDL rolled back?	It [has been suggested](http://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:7072180788422#7084625440626) that DDL is logically performed something like this:\r\n\r\n\r\n    begin\r\n        COMMIT;\r\n        perform any appropriate pre-DDL trigger code;\r\n        do the ddl;\r\n        perform any appropriate post-DDL trigger code;\r\n        COMMIT;\r\n    exception\r\n        when others then\r\n             ROLLBACK;\r\n             raise;\r\n    end;\r\n\r\nWhich would suggest that any error in a trigger would cause the DDL to be rolled back. Is this the case?	207	2019-11-29 17:53:06.394681+00	0	4	1	275	532	1986	9700	1	-1	2	2019-11-29 17:53:06.394681+00	f	f	1	1
2	16	92	2018-04-20 20:10:14+00	Why is my query suddenly slower than it was yesterday?	[Salutations]\r\n\r\n**(check one)**\r\n\r\n    [ ] Well trained professional, [ ] Casual reader, [ ] Hapless wanderer,\r\n\r\nI have a **(check all that apply)**\r\n\r\n    [ ] query [ ] stored procedure [ ] database thing maybe  \r\n\r\nthat was running fine **(if applicable)** \r\n\r\n    [ ] yesterday [ ] in recent memory [ ] at some point \r\n\r\nbut is suddenly slower now.\r\n\r\nI've already checked to make sure it's not being blocked, and that it's not the victim of some long running maintenance task, report, or other out of band process.\r\n\r\nWhat is the problem, what should I do, and what information can I provide to get some help?\r\n\r\n    [*Insert appropriate closing remarks*]\r\n	99	2019-11-24 10:07:04.470366+00	0	4	1	167	263	1459	204565	0	0	0	2019-11-24 10:07:04.470366+00	f	f	1	1
1	2	472	2019-12-22 12:58:48.029125+00	I want new features!	You might be wondering if James and I are putting our feet up for Christmas, because there haven't been many new features added in the last week.\r\n\r\nWe have been working on TopAnswers though; on something essential but not very visible. After importing 170k messages into a chat room here, we noticed quite a performance hit in some areas, which prompted a rethink of a lot of our logic. Mostly that's meant writing custom SQL and wrapping it in a function rather than directly querying views from the application layer[^1].\r\n\r\nThis work is nearly done, and you might have noticed page load times are a bit quicker as a result.\r\n\r\nYou may also have noticed various regressions — unfortunately this kind of re-jigging is prone to that. There will probably be a few more today and tomorrow until it's finished. Please let us know if you spot anything broken.\r\n\r\nHopefully we can soon get back to doing what we really want to spend time doing: adding features like [pagination for the question list](https://topanswers.xyz/meta?q=456#question) and [improving the look of the notification and chat panels](https://topanswers.xyz/meta?q=242).\r\n\r\n\r\n[^1]: because predicates weren't getting pushed down the way I hoped	483	2019-12-22 12:58:48.029125+00	6	1	1	547	1154	2421	\N	0	0	0	\N	f	f	3	\N
2	194	301	2019-07-02 15:49:10+00	How can I make this execution plan more efficient?	I have worked out all the implicit conversions, but I still see mentions of it in the plan. I have attached the plan, and any recommendation will help.\r\n\r\n```\r\nselect cardholder_index, sum(value) as [RxCost] \r\ninto #rxCosts \r\nfrom RiskPredictionStatistics with (nolock) where model_name = 'prescription_cost_12_months'\r\n and model_set_name = 'rx_updated' and run_id in (select value from #runIds) \r\n and exists (select 1 from StringContainsHelper with (nolock) where IntValue = cardholder_index and ReferenceId = @stringContainsHelperRefId)\r\ngroup by cardholder_index\r\n```\r\n\r\nhttps://www.brentozar.com/pastetheplan/?id=SyHl9xFeS\r\n\r\n	309	2019-12-04 22:56:11.219036+00	0	4	1	376	658	1147	241904	0	0	0	2019-12-04 22:56:11.219036+00	f	f	1	1
2	98	278	2017-05-23 16:55:20+00	SQL Server returns “Arithmetic overflow error converting expression to data type int.”	When I run this command with `SUM()`\r\n\r\n    SELECT COUNT(*) AS [Records], SUM(t.Amount) AS [Total]\r\n    FROM   dbo.t1 AS t\r\n    WHERE  t.Id > 0\r\n           AND t.Id < 101;\r\n\r\nI'm getting,\r\n\r\n    Arithmetic overflow error converting expression to data type int.\r\n\r\nAny idea on what is the cause of it?\r\n\r\nI'm just following the instructions in [this answer](https://dba.stackexchange.com/a/174093/2639).	286	2019-12-04 14:23:40.554148+00	0	4	1	353	606	871	174353	0	0	0	2019-12-04 14:23:40.554148+00	f	f	1	1
2	749	559	2011-12-01 02:25:00+00	Shared Lock issued on IsolationLevel.ReadUncommitted	I read that if I use IsolationLevel.ReadUncommitted, the query should not issue any locks. However, when I tested this, I saw the following lock:\r\n\r\n>Resource_Type: HOBT  \r\nRequest_Mode: S (Shared)\r\n\r\nWhat is a HOBT lock? Something related to HBT (Heap or Binary Tree lock)?\r\n\r\nWhy would I still get a S lock?\r\n\r\nHow do I avoid shared locking when querying without turning on the isolation level snapshot option?\r\n\r\nI am testing this on SQLServer 2008, and the snapshot option is set to off. The query only performs a select.\r\n\r\nI can see that Sch-S is required, although SQL Server seems not to be showing it in my lock query. How come it still issues a Shared Lock?  According to:\r\n\r\n[SET TRANSACTION ISOLATION LEVEL (Transact-SQL)][1]\r\n\r\n>Transactions running at the `READ UNCOMMITTED` level do not issue shared locks to prevent other transactions from modifying data read by the current transaction.\r\n\r\nSo I am a little confused.\r\n\r\n\r\n  [1]: http://msdn.microsoft.com/en-us/library/ms173763.aspx	572	2020-01-12 14:15:28.914682+00	0	4	1	634	1434	2986	8627	0	0	0	2020-01-12 14:15:28.914682+00	f	f	1	1
2	189	294	2019-06-28 09:24:41+00	Dynamic SQL query - how do I add an int to the code?	Consider:\r\n\r\n    Declare @stringsvar varchar(1000)\r\n    Declare @Emp_id int \r\n    DECLARE  @strvar SYSNAME = 'Employee_test'\r\n    \r\n    SET @stringsvar = ('select * from' + ' ' + @strvar + ' where emp_id' + ' =' + @Emp_id)\r\n    Print @stringsvar\r\n\r\nThe above query is giving an error as mentioned below:\r\n\r\n> Msg 245, Level 16, State 1, Line 17 Conversion failed when converting\r\n> the nvarchar value 'select * from Employee_test where emp_id =' to\r\n> data type int.\r\n\r\nWhat needs to be done in this case?\r\n\r\n	302	2019-12-04 22:41:58.558415+00	0	4	1	369	651	931	241607	0	0	0	2019-12-04 22:41:58.558415+00	f	f	1	1
2	61	147	2016-04-11 08:42:22+00	SQL Server sample Update of Statistics misses highest RANGE_HI_KEY on ascending key column	I'm trying to understand how Statistics sampling works and whether or not the below is expected behaviour on sampled statistics updates.\r\n\r\nWe have a large table partitioned by date with a couple of billion rows. The partition date is the prior business date and so is an ascending key. We only load data into this table for the prior day.\r\n\r\nThe data load runs overnight, so on Friday 8th April we loaded data for the 7th.\r\n\r\nAfter each run we update statistics, although take a sample, rather than a `FULLSCAN`.\r\n\r\nMaybe I am being naïve, but I would have expected SQL Server identify the highest key and lowest key in the range to ensure it got an accurate range sample. According to [this article][1]:  \r\n\r\n> For the first bucket, the lower boundary is the smallest value of the column on which the histogram is generated.\r\n\r\nHowever, it doesn't mention the last bucket/largest value. \r\n\r\nWith the sampled Statistics update on the morning of the 8th, the sample missed the highest value in the table (the 7th).\r\n\r\n[![enter image description here][2]][2]\r\n\r\nAs we do a lot of querying on data from the prior day, this resulted in inaccurate cardinality estimation and a number of queries timing out.\r\n\r\nShould SQL Server not identify the highest value for that key and use that as the maximum `RANGE_HI_KEY`? Or is this just one of the limits of update without using `FULLSCAN`?\r\n\r\nVersion SQL Server 2012 SP2-CU7. We cannot currently upgrade due to a change in `OPENQUERY` behaviour in SP3 that was rounding down numbers in a linked server query between SQL Server and Oracle.\r\n\r\n\r\n  [1]: https://www.sqlservercentral.com/blogs/practicalsqldba/2013/06/27/sql-server-part-2-all-about-sql-server-statistics-histogram/\r\n  [2]: https://i.stack.imgur.com/hdKC9.png	154	2019-11-27 12:55:30.608416+00	0	4	1	222	339	345	134945	0	0	0	2019-11-27 10:05:14.652044+00	f	f	1	1
4	167	507	2020-01-02 14:02:49.111522+00	regex to replace TikZ's old circle, ellipse and arc syntax with new one	I have a document that heavily uses the old TikZ syntax for circles, ellipses and arcs:\r\n\r\n```\r\n\\draw (0,0) circle (0.45);\r\n\\draw (0,0) ellipse (1.4 and 4);\r\n\\draw (0.65,0.61) arc(180:360:0.35 and 0.35); \r\n\\draw (0.65,0.61) arc(180:360:0.35);\r\n```\r\n\r\nHow can I use texstudios search and replace with regex to change it to the new syntax?\r\n\r\n```\r\n\\draw (0,0) circle[radius=0.45];\r\n\\draw (0,0) ellipse[x radius=1.4, y radius=4];\r\n\\draw (0.65,0.61) arc[start angle=180, end angle=360, x radius=0.35, y radius =0.35]\r\n\\draw (0.65,0.61) arc[start angle=180, end angle=360, radius=0.35];\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n	519	2020-01-04 13:44:13.816166+00	0	4	3	582	1299	2568	\N	0	0	0	\N	f	f	1	\N
2	755	565	2017-11-15 10:02:21+00	Ignore accents in 'where'	In our database we have multiple entries with caron/hatschek.\r\nNow our users want to find entries including caron/hatschek when they search for entries without. I'll show this by a simple example:\r\n\r\nIn our database we have the entry (contact with name)\r\n\r\n    Millière\r\n\r\nso this name is correct in the country the person lives in.\r\n\r\nIn our country we do not have any characters with caron/hatschek, therefore our user searches for `Milliere`. No results come up, as `è` does obviously not match `e`.\r\n\r\nI have no idea how this could be realized as `é`,`è`,`ê` and many more are available (and this is only an example for letter `e`...).\r\n\r\n(The other way would be much easier, as I could simply string replace all letters with caron/hatschek with the basic one. Obviously, our users do want the correct version of the name in the database, not the crippled one.)	578	2020-01-14 10:09:55.191267+00	0	4	1	640	1454	2999	190969	0	0	0	2020-01-14 10:09:55.191267+00	f	f	1	1
2	2	178	2014-05-27 10:05:13+00	How do I decompose ctid into page and row numbers?	Each row in a table has a [system column](http://www.postgresql.org/docs/9.3/static/ddl-system-columns.html) `ctid` of type `tid` that represents the physical location of the row:\r\n\r\n\r\n<>http://dbfiddle.uk/?rdbms=postgres_9.4&fiddle=98abf5bffd03364076bd4cd4b30feb6e\r\n\r\nWhat's the best way of getting just the page number as from the `ctid` in the most appropriate type (eg `integer`, `bigint` or `numeric(1000,0)`)?\r\n\r\nThe [only way I can think of](http://dbfiddle.uk/?rdbms=postgres_9.4&fiddle=1dc8740cf72ca8a8c7c1a26113e6d442) is very ugly.	185	2019-11-27 23:16:31.631815+00	0	4	1	253	370	382	65964	0	0	0	2019-11-27 23:11:30.339339+00	f	f	1	1
2	16	267	2016-03-16 20:16:45+00	SARGable WHERE clause for two date columns	I have what is, to me, an interesting question on SARGability. In this case, it's about using a predicate on the difference between two date columns. Here's the setup:\r\n\r\n    USE [tempdb]\r\n    SET NOCOUNT ON\t\r\n    \r\n    IF OBJECT_ID('tempdb..#sargme') IS NOT NULL\r\n    BEGIN\r\n    DROP TABLE #sargme\r\n    END\r\n\r\n    SELECT TOP 1000\r\n    IDENTITY (BIGINT, 1,1) AS ID,\r\n    CAST(DATEADD(DAY, [m].[severity] * -1, GETDATE()) AS DATE) AS [DateCol1],\r\n    CAST(DATEADD(DAY, [m].[severity], GETDATE()) AS DATE) AS [DateCol2]\r\n    INTO #sargme\r\n    FROM sys.[messages] AS [m]\r\n    \r\n    ALTER TABLE [#sargme] ADD CONSTRAINT [pk_whatever] PRIMARY KEY CLUSTERED ([ID])\r\n    CREATE NONCLUSTERED INDEX [ix_dates] ON [#sargme] ([DateCol1], [DateCol2])\r\n\r\nWhat I'll see pretty frequently, is something like this:\r\n\r\n    /*definitely not sargable*/\r\n    SELECT\r\n        * ,\r\n        DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2])\r\n    FROM\r\n        [#sargme] AS [s]\r\n    WHERE\r\n        DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2]) >= 48;\r\n\r\n...which definitely isn't SARGable. It results in an index scan, reads all 1000 rows, no good. Estimated rows stink. You'd never put this in production.\r\n\r\n[![No sir, I didn't like it.][1]][1]\r\n\r\nIt would be nice if we could materialize CTEs, because that would help us make this, well, more SARGable-er, technically speaking. But no, we get the same execution plan as up top.\r\n\r\n    /*would be nice if it were sargable*/\r\n    WITH    [x] AS ( SELECT\r\n                    * ,\r\n                    DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2]) AS [ddif]\r\n                   FROM\r\n                    [#sargme] AS [s])\r\n         SELECT\r\n            *\r\n         FROM\r\n            [x]\r\n         WHERE\r\n            [x].[ddif] >= 48;\r\n\r\nAnd of course, since we are not using constants, this code changes nothing, and is not even half SARGable. No fun. Same execution plan. \r\n\r\n    /*not even half sargable*/\r\n    SELECT\r\n        * ,\r\n        DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2])\r\n    FROM\r\n        [#sargme] AS [s]\r\n    WHERE\r\n        [s].[DateCol2] >= DATEADD(DAY, 48, [s].[DateCol1])\r\n\r\nIf you're feeling lucky, and you're obeying all the ANSI SET options in your connection strings, you could add a computed column, and search on it...\r\n\r\n    ALTER TABLE [#sargme] ADD [ddiff] AS \r\n    DATEDIFF(DAY, DateCol1, DateCol2) PERSISTED\r\n\r\n    CREATE NONCLUSTERED INDEX [ix_dates2] ON [#sargme] ([ddiff], [DateCol1], [DateCol2])\r\n    \r\n    SELECT [s].[ID] ,\r\n           [s].[DateCol1] ,\r\n           [s].[DateCol2]\r\n    FROM [#sargme] AS [s]\r\n    WHERE [ddiff] >= 48\r\n\r\nThis will get you an index seek with three queries. The odd man out is where we add 48 days to DateCol1. The query with `DATEDIFF` in the `WHERE` clause, the `CTE`, and the final query with a predicate on the computed column all give you a much nicer plan with much nicer estimates, and all that.\r\n\r\n[![I could live with this.][2]][2]\r\n\r\nWhich brings me to the question: in a single query, is there a SARGable way to perform this search? \r\n\r\nNo temp tables, no table variables, no altering the table structure, and no views. \r\n\r\nI'm fine with self-joins, CTEs, subqueries, or multiple passes over the data. Can work with any version of SQL Server.\r\n\r\nAvoiding the computed column is an artificial limitation because I'm more interested in a query solution than anything else.\r\n\r\n  [1]: https://i.stack.imgur.com/vvmAp.jpg\r\n  [2]: https://i.stack.imgur.com/noBNR.jpg\r\n  [3]: http://blogfabiano.com/2012/07/03/statistics-used-in-a-cached-query-plan/\r\n  [4]: https://i.stack.imgur.com/80oNF.png	275	2019-12-04 14:11:28.845848+00	0	4	1	342	610	1522	132437	0	0	0	2019-12-04 14:11:28.845848+00	f	f	1	1
2	68	83	2014-02-13 18:36:41+00	Why does query error with empty result set in SQL Server 2012?	When running the following queries in MS SQL Server 2012 the second query fails but not the first. Also, when run without the where clauses both queries will fail. I am at a loss why either would fail since both should have empty result sets. Any help/insight is appreciated.\r\n\r\n    create table #temp\r\n    (id\t\tint primary key)\r\n    \r\n    create table #temp2\r\n    (id\t\tint)\r\n    \r\n    select 1/0\r\n    from #temp\r\n    where id = 1\r\n    \r\n    select 1/0\r\n    from #temp2\r\n    where id = 1	90	2019-11-23 08:10:55.45685+00	0	4	1	158	194	1453	58888	0	0	0	2019-11-23 08:10:55.45685+00	f	f	1	1
2	757	571	2018-09-07 18:13:45+00	Why filtered index on IS NULL value is not used?	Assume we have a table definition like this:\r\n\r\n    CREATE TABLE MyTab (\r\n    \tID INT IDENTITY(1,1) CONSTRAINT PK_MyTab_ID PRIMARY KEY\r\n    \t,GroupByColumn NVARCHAR(10) NOT NULL\r\n    \t,WhereColumn DATETIME NULL\r\n    \t)\r\n\r\nAnd a filtered non-clustered index like this:\r\n\r\n    CREATE NONCLUSTERED INDEX IX_MyTab_GroupByColumn ON MyTab \r\n    \t(GroupByColumn)\r\n    WHERE (WhereColumn IS NULL) \r\n\r\nWhy this index is not "covering" for this query:\r\n\r\n    SELECT \r\n    \tGroupByColumn\r\n    \t,COUNT(*)\r\n    FROM MyTab\r\n    WHERE WhereColumn IS NULL\r\n    GROUP BY GroupByColumn\r\n\r\nI'm getting this execution plan:\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\nThe KeyLookup is for the WhereColumn IS NULL predicate.\r\n\r\nHere is the plan: [https://www.brentozar.com/pastetheplan/?id=SJcbLHxO7][2]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/JTu56.gif\r\n  [2]: https://www.brentozar.com/pastetheplan/?id=SJcbLHxO7	585	2020-01-14 15:20:28.470956+00	0	4	1	646	1471	3007	217046	0	0	0	2020-01-14 15:20:28.470956+00	f	f	1	1
2	207	334	2018-10-16 09:45:24+00	Why is "SELECT *" an antipattern?	On multiple questions I see people saying in the comments and answers that `select * from table` is almost always an antipattern, without any explaination why.\r\n\r\nAlthough I can sort of deduce why it *is* an antipattern, I may be looking over a detail that someone else with better understanding of the issue noticed.\r\n\r\nWhy do people say that `select *` is an antipattern?	342	2019-12-06 09:29:35.516931+00	0	4	1	409	763	2270	220216	0	0	0	2019-12-05 23:57:32.501488+00	f	f	1	1
4	96	345	2015-03-24 07:09:16+00	Why does LuaLaTeX's \\IfFileExists fail where XeLaTeX succeeds?	I have a series of files that I've been compiling with XeLaTeX. I recently attempted to compile with LuaLaTex and it made a royal mess of things. I traced the issue to the use of `\\IfFileExists` in one of my macros. The bash script that is handling compile jobs out of the queue is doing a number of things such as retrieving and formatting LilyPond files for inclusion it the TeX document. If it is able to get everything together it creates a flag file in the build directory so TeX knows it can go ahead with processing music at that point.\r\n\r\nThis MWE acts as expected in XeLaTex but fails in LuaLaTex even when the file exists:\r\n\r\n\t\\documentclass{minimal}\r\n\r\n\t\\begin{document}\r\n\r\n\tSong XX:\r\n\r\n\t\\IfFileExists{XX-has-music}{%\r\n\t\tTypeset block for when music exists.\r\n\t}{%\r\n\t\tTypeset block for when music doesn't exist.\r\n\t}\r\n\r\n\t\\end{document}\r\n\r\nLuaLaTex doesn't seem to have a problem with file paths as even running `\\input` on things in the same directory shows that it can find things there. What's going on?\r\n	354	2019-12-06 06:43:23.818063+00	0	4	1	420	742	1100	234750	0	0	0	2019-12-06 06:43:23.818063+00	f	f	1	2
2	62	148	2016-04-22 09:46:24+00	Where do this Constant Scan and Left Outer Join come from in a trivial SELECT query plan?	I have this table:\r\n\r\n    CREATE TABLE [dbo].[Accounts] (\r\n        [AccountId] UNIQUEIDENTIFIER UNIQUE NOT NULL DEFAULT NEWID(),\r\n        -- WHATEVER other columns\r\n    );\r\n    GO\r\n    CREATE UNIQUE CLUSTERED INDEX [AccountsIndex]\r\n        ON [dbo].[Accounts]([AccountId] ASC);\r\n    GO\r\n\r\nThis query:\r\n\r\n    DECLARE @result UNIQUEIDENTIFIER\r\n    SELECT @result = AccountId FROM Accounts WHERE AccountId='guid-here'\r\n\r\nexecutes with a query plan consisting of a single Index Seek - as expected:\r\n\r\n    SELECT <---- Clustered Index Seek\r\n\r\nThis query does the same:\r\n\r\n    DECLARE @result UNIQUEIDENTIFIER\r\n    SET @result = (SELECT AccountId FROM Accounts WHERE AccountId='guid-here')\r\n\r\nbut it's executed with a plan where result of Index Seek is Left Outer Joined with result of some Constant Scan and then fed into Compute Scalar:\r\n\r\n    SELECT <--- Compute Scalar <--- Left Outer Join <--- Constant Scan\r\n                                          ^\r\n                                          |------Clustered Index Seek\r\n\r\nWhat's that extra magic? What does that Constant Scan followed by Left Outer Join do?\r\n    \r\n\r\n\r\n    	155	2019-11-27 10:14:49.512855+00	0	4	1	223	944	1463	136210	0	0	0	2019-11-27 10:14:49.512855+00	f	f	1	1
2	667	179	2012-05-22 21:19:38+00	Can spatial index help a “range - order by - limit” query	Asking this question, specifically for Postgres, as it has good supoort for R-tree/spatial indexes.\r\n\r\nWe have the following table with a tree structure (Nested Set model) of words and their frequencies:\r\n\r\n    lexikon\r\n    -------\r\n    _id   integer  PRIMARY KEY\r\n    word  text\r\n    frequency integer\r\n    lset  integer  UNIQUE KEY\r\n    rset  integer  UNIQUE KEY\r\n\r\nAnd the query:\r\n\r\n    SELECT word\r\n    FROM lexikon\r\n    WHERE lset BETWEEN @Low AND @High\r\n    ORDER BY frequency DESC\r\n    LIMIT @N\r\n\r\nI suppose a covering index on `(lset, frequency, word)` would be useful but I feel it may not perform well if there are too many `lset` values in the `(@High, @Low)` range. \r\n\r\nA simple index on `(frequency DESC)` may also be sufficient sometimes, when a search using that index yields early the `@N` rows that match the range condition. \r\n\r\nBut it seems that performance depends a lot on the parameter values.\r\n\r\nIs there a way to make it perform fast, regardless of whether the range `(@Low, @High)` is wide or narrow and regardless of whether the top frequency words are luckily in the (narrow) selected range?\r\n\r\nWould an R-tree/spatial index help?\r\n\r\nAdding indexes, rewriting the query, re-designing the table, there is no limitation.	186	2019-11-27 23:25:25.137945+00	0	4	1	254	372	381	18300	0	0	0	2019-11-27 23:25:25.137945+00	f	f	1	1
2	16	269	2019-03-28 23:15:23+00	In SQL Server, How Does Parallelism Change Memory Grants?	I've heard conflicting things about memory grants for parallel select queries:\r\n\r\n- Memory grants get multiplied by DOP\r\n- Memory grants get divided by DOP\r\n\r\nWhich is it? 	277	2019-12-04 14:13:57.661603+00	0	4	1	344	595	1504	233445	0	0	0	2019-12-04 14:13:57.661603+00	f	f	1	1
2	83	94	2012-12-17 21:17:56+00	MERGE a subset of the target table	I am trying to use a `MERGE` statement to insert or delete rows from a table, but I only want to act on a subset of those rows. The documentation for `MERGE` has a pretty strongly worded warning:\r\n\r\n> It is important to specify only the columns from the target table that are used for matching purposes. That is, specify columns from the target table that are compared to the corresponding column of the source table. Do not attempt to improve query performance by filtering out rows in the target table in the ON clause, such as by specifying AND NOT target_table.column_x = value. Doing so may return unexpected and incorrect results.\r\n\r\nbut this is exactly what it appears I have to do to make my `MERGE` work.\r\n\r\nThe data I have is a standard many-to-many join table of items to categories (e.g. which items are included in which categories) like so:\r\n\r\n    CategoryId   ItemId\r\n    ==========   ======\r\n    1            1\r\n    1            2\r\n    1            3\r\n    2            1\r\n    2            3\r\n    3            5\r\n    3            6\r\n    4            5\r\n\r\nWhat I need to do is to effectively replace all rows in a specific category with a new list of items. My initial attempt to do this looks like this:\r\n\r\n    MERGE INTO CategoryItem AS TARGET\r\n    USING (\r\n      SELECT ItemId FROM SomeExternalDataSource WHERE CategoryId = 2\r\n    ) AS SOURCE\r\n    ON SOURCE.ItemId = TARGET.ItemId AND TARGET.CategoryId = 2\r\n    WHEN NOT MATCHED BY TARGET THEN\r\n        INSERT ( CategoryId, ItemId )\r\n        VALUES ( 2, ItemId )\r\n    WHEN NOT MATCHED BY SOURCE AND TARGET.CategoryId = 2 THEN\r\n        DELETE ;\r\n\r\nThis *appears* to be working in my tests, but I am doing exactly what MSDN explicitly warns me not to do. This makes me concerned that I will run into unexpected problems later on, but I cannot see any other way to make my `MERGE` only affect rows with the specific field value (`CategoryId = 2`) and ignore rows from other categories.\r\n\r\nIs there a "more correct" way to achieve this same result? And what are the "unexpected or incorrect results" that MSDN is warning me about?	101	2019-11-24 10:18:24.825701+00	0	4	1	169	226	2681	30633	0	0	0	2019-11-24 10:18:24.825701+00	f	f	1	1
2	70	90	2012-12-05 02:11:22+00	Why Does the Transaction Log Keep Growing or Run Out of Space?	This one seems to be a common question in most forums and all over the web, it is asked here in many formats that typically sound like this:\r\n\r\n> In SQL Server -\r\n> \r\n> \r\n\r\n> - What are some reasons the transaction log grows so large?\r\n> - Why is my log file so big?\r\n> - What are some ways to prevent this problem from occurring? \r\n> - What do I do when I get myself on track with the underlying cause and want to put\r\n> my transaction log file to a healthy size?	97	2019-11-24 09:25:55.47698+00	0	4	1	165	341	1467	29829	0	0	0	2019-11-24 09:25:55.47698+00	f	f	1	1
2	764	573	2015-04-20 11:59:35+00	SQL Server unpredictable select results (dbms error?)	Below is simple example, which returns strange results, that are unpredictable and we cannot explain it in our team. Are we doing something wrong or is it SQL Server error?\r\n\r\nAfter some investigation we reduced the search area to **union clause in subquery**, which selects one record from "men" table\r\n\r\nIt works as expected in SQL Server 2000 (returns 12 rows), but in 2008 and 2012 it returns only one row.\r\n\r\n\r\n    create table dual (dummy int)\r\n      \r\n    insert into dual values (0)\r\n    \r\n    create table men (\r\n    man_id int,\r\n    wife_id int )\r\n    \r\n    -- there are 12 men, 6 married \r\n    insert into men values (1, 1)\r\n    insert into men values (2, 2)\r\n    insert into men values (3, null)\r\n    insert into men values (4, null)\r\n    insert into men values (5, null)\r\n    insert into men values (6, 3)\r\n    insert into men values (7, 5)\r\n    insert into men values (8, 7)\r\n    insert into men values (9, null)\r\n    insert into men values (10, null)\r\n    insert into men values (11, null)\r\n    insert into men values (12, 9)\r\n    \r\nThis returns only one row:   1\t1\t2\r\n\r\n    select \r\n    man_id,\r\n    wife_id,\r\n    (select count( * ) from \r\n        (select dummy from dual\r\n         union select men.wife_id  ) family_members\r\n    ) as family_size\r\n    from men\r\n    --where wife_id = 2 -- uncomment me and try again\r\n\r\nUncomment last line and it gives: 2\t2\t2\r\n\r\nThere is a lot of odd behaviors:\r\n\r\n - After series of drops, creates, truncates and inserts on "men" table **it sometimes works** (returns 12 rows)\r\n - When you change "union select men.wife_id" to "union all select men.wife_id" or "union select isnull(men.wife_id, null)" (!!!) it **returns 12** rows (as expected). \r\n - The strange behavior seems to be unrelated to datatype of column "wife_id". We observed it on development system with much greater data sets. \r\n - "where wife_id > 0" returns 6 rows\r\n - we also observes strange behavior of views with this kind of statements. SELECT * returns subset of rows, SELECT TOP 1000 returns all	587	2020-01-15 08:30:16.881492+00	0	4	1	648	1477	3045	98903	0	0	0	2020-01-15 08:30:16.881492+00	f	f	1	1
2	79	75	2013-07-24 11:51:17+00	LIKE uses index, CHARINDEX does not?	This question is related to [my old question][1]. The below query was taking 10 to 15 seconds to execute:\r\n\r\n    SELECT [customer].[Customer name],[customer].[Sl_No],[customer].[Id]\r\n    FROM [company].dbo.[customer]\r\n    WHERE (Charindex('123456789',CAST([company].dbo.[customer].[Phone no] AS VARCHAR(MAX)))>0) \r\n\r\nIn some articles I saw that using `CAST` and `CHARINDEX` will not benefit from indexing. There are also some articles that say using `LIKE '%abc%'` will not benefit from indexing while `LIKE 'abc%'` will:\r\n\r\n* http://bytes.com/topic/sql-server/answers/81467-using-charindex-vs-like-where\r\n* https://stackoverflow.com/questions/803783/sql-server-index-any-improvement-for-like-queries\r\n* http://www.sqlservercentral.com/Forums/Topic186262-8-1.aspx#bm186568\r\n\r\nIn my case I can rewrite the query as:\r\n\r\n    SELECT [customer].[Customer name],[customer].[Sl_No],[customer].[Id]\r\n    FROM [company].dbo.[customer]\r\n    WHERE [company].dbo.[customer].[Phone no]  LIKE '%123456789%'\r\n\r\nThis query gives the same output as the previous one. I have created a nonclustered index for column `Phone no`. When I execute this query it runs in just **1 second**. This is a huge change compared with **14 seconds** previously.\r\n\r\nHow does `LIKE '%123456789%'` benefit from indexing?\r\n\r\nWhy do the listed articles state that it will not improve performance?\r\n\r\nI tried rewriting the query to use `CHARINDEX`, but performance is still slow. Why does `CHARINDEX` not benefit from the indexing as it appears the `LIKE` query does?\r\n\r\nQuery using `CHARINDEX`:\r\n\r\n    SELECT [customer].[Customer name],[customer].[Sl_No],[customer].[Id]\r\n    FROM [Company].dbo.[customer]\r\n    WHERE ( Charindex('9000413237',[Company].dbo.[customer].[Phone no])>0 ) \r\n\r\nExecution plan:\r\n\r\n![enter image description here][2]\r\n\r\nQuery using `LIKE`:\r\n\r\n    SELECT [customer].[Customer name],[customer].[Sl_No],[customer].[Id]\r\n    FROM [Company].dbo.[customer]\r\n    WHERE[Company].dbo.[customer].[Phone no] LIKE '%9000413237%'\r\n\r\nExecution plan:\r\n\r\n![LIKE query plan][3]\r\n\r\n\r\n  [1]: https://dba.stackexchange.com/questions/46835/query-taking-non-consistent-time-to-execute?noredirect=1#comment83125_46835\r\n  [2]: http://i.stack.imgur.com/7gSwu.png\r\n  [3]: http://i.stack.imgur.com/2bHFM.png	82	2019-11-25 17:06:21.190383+00	0	4	1	150	260	1458	46917	0	0	0	2019-11-20 12:21:34.743968+00	f	f	1	1
1	43	79	2019-11-22 09:36:59.76997+00	Why can't I use my real name which contains an apostrophe?	My real name is "Colin 't Hart" and I would like to use it as my name here too. All the O'Reilly's of this world would have the same problem.	86	2019-11-22 09:36:59.76997+00	4	4	1	154	177	262	\N	0	0	0	\N	f	f	2	\N
2	76	93	2016-11-17 22:39:48+00	Authoritative source that <> and != are identical in performance in SQL Server	Consider [this answer](https://stackoverflow.com/a/15179434/57611) on SO that reassures the asker about the `<>` operator that:\r\n\r\n> `<>` is ... the same as `!=`.\r\n\r\nBut then a commenter pipes up and says:\r\n\r\n> It's true that they are, functionally, the same. However, how the SQL optimizer uses them is very different. =/!= are simply evaluated as true/false whereas <> means the engine has to look and see if the value is greater than or less than, meaning more performance overhead. Just something to consider when writing queries that may be expensive.\r\n\r\nI am confident this is false, but In order to address potential skeptics, I wonder if anyone can provide an authoritative or canonical source to prove that these operators are not just functionally the same, but identical in all aspects?	100	2019-11-24 10:13:44.933484+00	0	4	1	168	264	1460	155650	0	0	0	2019-11-24 10:13:44.933484+00	f	f	1	1
2	186	288	2019-08-28 14:32:39+00	SQL Server - How to achieve READCOMMITED and NOLOCK at the same time?	If I have a **User** table:\r\n\r\n    id | name   | age\r\n    1  | Mateus | 27\r\n\r\nThe first transaction executes an update, and leaves the transaction open, without committing or rolling back:  \r\n`update User set name = 'John' where id = 1;`\r\n\r\nMeanwhile, the second transaction executes a select:  \r\n`select * from User where id = 1;`  \r\nThis command will wait until the first transaction releases the lock, either by commit or rollback, unless the second transaction uses a table hint `with(nolock)`, like so:  \r\n`select * from User with(nolock) where id = 1;`  \r\nThat will return the record without locking the transaction, but it will return the uncommitted value `John` instead of the original `Mateus`.\r\n\r\nFrom what I know, there are only two ways to return a locked record without locking the current transaction, one can use `with(nolock)` that will return the record but with the uncommitted value, and `with(readpast)` that will just not return the record.\r\n\r\nIs there a way I can return the record, without locking the table, and returning its "old" values?	296	2019-12-04 14:31:22.8128+00	0	4	1	363	619	1531	246453	0	0	0	2019-12-04 14:31:22.8128+00	f	f	1	1
2	85	85	2013-12-18 18:38:41+00	Moving primary key constraint from one index to another	I have a SQL Server database, and have run [`sp_blitz`][1] against it. This pulled out that there are a couple of heaps on moderate size tables (a few hundred thousand rows in one case).\r\n\r\nMost of these tables have a primary key (one does not, but I'll deal with that later). The primary key itself has an unusual name - i.e. not the default one, when most of the tables have a default primary key name (`PK_tablename`).\r\n\r\nThere is an index with a name matching the primary key, which is unique and non-clustered.\r\n\r\nI can rename the primary key, but I then think I should be creating a clustered index. If I do this, then I'll have duplicate indexes, so it would make sense to remove the non-clustered index. However, it's being used for the primary key.\r\n\r\nIf I move the primary key constraint from the old non-clustered index to the new clustered index:\r\n\r\n1. Does this make sense?\r\n2. Are there any things I should be aware of?\r\n3. What is the best method to do this?\r\n\r\n  [1]: http://www.brentozar.com/blitz/	92	2019-11-23 08:21:17.407913+00	0	4	1	160	198	179	55262	0	0	0	2019-11-23 08:21:17.407913+00	f	f	1	1
2	12	95	2016-06-04 06:35:05+00	Is there any benefit to SCHEMABINDING a function beyond Halloween Protection?	It is well-known that `SCHEMABINDING` a function can [avoid an unnecessary spool][1] in update plans:\r\n\r\n>If you are using simple T-SQL UDFs that do not touch any tables (i.e. do not access data), make sure you specify the `SCHEMABINDING` option during creation of the UDFs. This will make the UDFs schema-bound and ensure that the query optimizer does not generate any unnecessary spool operators for query plans involving these UDFs.\r\n\r\nAre there any other advantages of `SCHEMABINDING` a function, even if it does not access data?\r\n\r\n  [1]: https://blogs.msdn.microsoft.com/sqlprogrammability/2006/05/12/improving-query-plans-with-the-schemabinding-option-on-t-sql-udfs/	102	2019-11-24 10:28:03.068347+00	0	4	1	170	229	1455	140381	0	0	0	2019-11-24 10:28:03.068347+00	f	f	1	1
2	12	250	2019-02-25 10:22:55+00	Single-row INSERT…SELECT much slower than separate SELECT	Given the following heap table with 400 rows numbered from 1 to 400:\r\n\r\n```\r\nDROP TABLE IF EXISTS dbo.N;\r\nGO\r\nSELECT \r\n    SV.number\r\nINTO dbo.N \r\nFROM master.dbo.spt_values AS SV\r\nWHERE \r\n    SV.[type] = N'P'\r\n    AND SV.number BETWEEN 1 AND 400;\r\n```\r\n\r\nand the following settings:\r\n\r\n```\r\nSET NOCOUNT ON;\r\nSET STATISTICS IO, TIME OFF;\r\nSET STATISTICS XML OFF;\r\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\r\n```\r\n\r\nThe following `SELECT` statement completes in around **6 seconds** ([demo][1], [plan][2]):\r\n\r\n```\r\nDECLARE @n integer = 400;\r\n\r\nSELECT\r\n    c = COUNT_BIG(*) \r\nFROM dbo.N AS N\r\nCROSS JOIN dbo.N AS N2\r\nCROSS JOIN dbo.N AS N3\r\nWHERE \r\n    N.number <= @n\r\n    AND N2.number <= @n\r\n    AND N3.number <= @n\r\nOPTION\r\n    (OPTIMIZE FOR (@n = 1));\r\n```\r\n\r\n*Note: The `OPTIMIZE FOR` clause is just for the sake of producing a sensibly-sized repro that captures the essential details of the real problem, including a cardinality misestimate that can arise for a variety of reasons.*\r\n\r\nWhen the single-row output is written to a table, it takes **19 seconds** ([demo][3], [plan][4]):\r\n\r\n```\r\nDECLARE @T table (c bigint NOT NULL);\r\n\r\nDECLARE @n integer = 400;\r\n\r\nINSERT @T\r\n    (c)\r\nSELECT\r\n    c = COUNT_BIG(*) \r\nFROM dbo.N AS N\r\nCROSS JOIN dbo.N AS N2\r\nCROSS JOIN dbo.N AS N3\r\nWHERE \r\n    N.number <= @n\r\n    AND N2.number <= @n\r\n    AND N3.number <= @n\r\nOPTION\r\n    (OPTIMIZE FOR (@n = 1));\r\n```\r\n\r\nThe execution plans appear identical aside from the insert of one row.\r\n\r\nAll the extra time seems to be consumed by CPU usage.\r\n\r\nWhy is the `INSERT` statement so much slower?\r\n\r\n  [1]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=3a1563d5014e69c90fc13e4fa33459d7\r\n  [2]: https://www.brentozar.com/pastetheplan/?id=ByCrEMbIE\r\n  [3]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=35e46790034e0345c64ea2ca6fa78570\r\n  [4]: https://www.brentozar.com/pastetheplan/?id=HJLT4fb84	258	2019-12-03 12:35:26.825773+00	0	4	1	325	545	1494	230651	0	0	0	2019-12-03 12:34:27.918737+00	f	f	1	1
1	115	246	2019-12-02 20:15:54.668955+00	Can we make chat more accessible for those with vision challenges?	As a user with some vision challenges, but not so many that I can't use sites like this at all or have to resort to screen readers, I'm feeling challenged by some aspects of the chat UI.  I understand that it's a work in progress, and I'm here to ask for a few things that I think are small implementation efforts but would have large accessibility impact.\r\n\r\nFirst some context.  Because zoom affects the whole page, I have to find a balance among Q&A text, chat-message text, and UI elements.  If I zoom the page enough to read the smallest things, the biggest things becaome *huge* and that impedes readability too.  Larger isn't always better; the eye has a field of vision, a "chunk" that can be taken in at once, that is measured in physical dimensions, but the brain has a cognitive "chunk" size too.  That's why you don't really want to read text one word at a time, for instance; you can *see* it just fine, but it's too irritating to *process*.  On the computer I'm typing this on, I've balanced all of this to land at a zoom level of 110%, which isn't all that much.  The text I'm typing in this edit window and the preview next to it are nice sizes for me.  (For completeness, I'm also using the sans-serif font.)\r\n\r\nNow, enter chat.  Chat *messages* are a legible size and while lines are short (becuase of the width of the pane), they're not *too* short (for chat; long-form writing would be different).  The *rest* of the chat UI, however, is tiny and hard for me to use:\r\n\r\n- The icons next to messages are cryptic little gray things.  Fortunately they mostly have tooltips now (thanks for adding those!), though there's one (I think "..."?) that doesn't.  **Tooltips make the illegible accessible**; they're really important.  They're not a silver bullet -- they don't help on touch interfaces -- but they're a simple thing that often helps, so please always use tooltips for graphical elements.  (This is a general principle, not restricted to chat.)\r\n\r\n- Messages are preceeded by *very* tiny text that includes the author, timestamp, and who it's replying to (if applicable).  I can't read that.  I've developed the heuristic that if the line of text seems long it's probably a reply and hovering over the message shows me the nice highlighting for replies, but otherwise that text isn't helping me.  *That color-coding for reply hover is great* -- just wanted to say that.\r\n\r\n- Messages are also preceeded by the author's gravatar.  You might think that mitigates against the tiny text, but those gravatars are very small too.  Please don't rely on them.  Even on SE chat where they're bigger than here, I often can't tell people apart if their gravatars are similar -- and by "similar" I mean things that are more aobut color and shape and less about details.  Many headshots look the same to me, and there are a few dogs on SE that I can't tell apart without extra effort.  **To mitigate this**, it'd be great if the tooltip for those gravatars could be expanded to include the name -- "So and so, 32 stars" rather than "30 stars", and it's still short enough to not get in the way.  I just noticed that you do that in the column of people in the room over on the right; if you could do it everywhere that'd be great.  (Hey, go ahead and do it for the gravaters in the question list too, for consistency and that extra bit of help.)\r\n\r\n- That stuff I said about tiny text applies to chat notifications, too. I've already accidentally dismissed messages when I was trying to jump to them.  I hope we're going to improve notifications in other ways, so I won't say much about them here.\r\n\r\nFinally, I'd like to point out some things that work well in chat -- thanks, and please keep them!\r\n\r\n- Black text, white background -- nice and readable.  There was a time this wouldn't have been remarkable, but alas... thank you for not falling for the "light gray text on white is good enough for everyone, right?" meme.\r\n\r\n- That color highlighting for threads is great!  First, that you have it at all and that it goes both directions, and second, that the color choice manages to draw attention without either impeding legibility or being harsh.\r\n\r\n- The text size for messages plays well with the size for Q&A.  It's smaller, befitting chat/comments, but it looks like it's only one notch down, so those of us who are more limited in what we can see can still see it all.\r\n\r\n\r\n	254	2019-12-02 20:15:54.668955+00	8	4	1	321	919	2954	\N	0	0	0	\N	f	f	2	\N
1	709	580	2020-01-16 21:16:56.864788+00	How to Find Profile Reputation	It has been mentioned in other Q/A posts that stars increase voting power at a rate of `floor(log10(stars)) + 1`, and stars are community-specific (i.e. they don't transfer between communities).\r\n\r\nAs a user, it would be nice to be able to somehow figure out the number of stars.  Perhaps that information could be embedded in the user profile on each community?	594	2020-01-16 21:16:56.864788+00	4	4	1	655	1522	3130	\N	0	0	0	\N	f	f	2	\N
1	167	457	2019-12-17 14:45:00.027586+00	Keep original date of imported posts	It would be nice if imported posts would show when they were originally written, not just the time stamp when they were imported.\r\n\r\nThis would help to get some context:\r\n\r\n- of the chronological order of multiple answers, which one is the most recent, etc.\r\n\r\n- in case the answer makes references like "... this will be included in the next release version" \r\n\r\n\r\n\r\n	468	2019-12-18 07:13:17.20661+00	11	4	1	532	1315	2638	\N	0	0	0	\N	f	f	2	\N
2	16	669	2017-10-04 01:22:12+00	How does Batch Mode Memory Grant Feedback Work?	Batch Mode Memory Grant Feedback is part of a [family of features][1] in the 2017 query processor that consists of:\r\n\r\n - [Batch Mode Adaptive Joins][2] \r\n - [Interleaved Execution][3]\r\n - Batch Mode Memory Grant Feedback\r\n\r\nSo how does Batch Mode Memory Grant Feedback work?\r\n\r\n\r\n  [1]: https://blogs.technet.microsoft.com/dataplatforminsider/2017/09/28/enhancing-query-performance-with-adaptive-query-processing-in-sql-server-2017/\r\n  [2]: https://topanswers.xyz/databases?q=668\r\n  [3]: https://topanswers.xyz/databases?q=672	684	2020-02-03 03:15:26.837926+00	0	4	1	744	1948	3695	187582	0	0	0	2020-02-02 15:58:04.30153+00	f	f	1	1
2	92	158	2016-12-05 09:56:41+00	Where is Read Committed insufficient?	[Wikipedia says][1] that Read Committed is prone to non-repeatable reads. However, I can simply cache the first read result in my transaction (make a snapshot) and release the DB lock to let other transactions update the read rows. Effectively, I achieve repeated reads at the cost of RC isolation and, thus, higher performance than Repeatable Read offers. Nothing bad can happen and I can feel as safe as when using the Repeatable Read isolation, right? No, I guess that holding a read lock for the data I have read helps to prevent some undesirable scenarios in banking or booking reservations, probably. Which ones?\r\n\r\nWhat I am looking for is an example where my emulation of RR fails. I have demonstrated that I can make the reads repeatable simply by caching the data item on its first read access. I want an example where my emulation is not sufficient. What is bad about releasing the lock immediately after reading under this strategy?\r\n\r\nThe Wikipedia article and the name *Repeatable Read* seem to imply to me that that is all what we need. But I suspect that *Repeatable Read*, by taking the shared lock, sacrifices performance for the sake of providing something more important than simple consistency of reads, since the latter can simply be achieved with a simple *Read Committed + cache (snapshot)* combination. I guess the reader locks persist for the whole transaction in *2PL* and we have shared/read locks as in *multiple readers/single-writer* for a greater purpose than just to provide repeatable reads within a single transaction and, consequently, `Repeatable Read` is a misnomer that hides the fact of locking, which actually sacrifices the performance for something larger than consistent reads.\r\n\r\n\r\n  [1]: https://en.wikipedia.org/wiki/Isolation_(database_systems)#Non-repeatable_reads	165	2019-11-27 19:18:50.971929+00	0	4	1	233	349	1470	157193	0	0	0	2019-11-27 19:18:50.971929+00	f	f	1	1
2	163	257	2019-06-18 19:24:15+00	Update Intersect hangs	We have this really weird behavior we just started experiencing with our update intersect statements.  These were working fine, but now we are ingesting a pretty wide data source in terms of columns and slows gradually till hanging indefinitely. \r\n\r\nThe query below will get longer and longer as we add data in sets of like 20K rows (which is very small), and hang around 70K rows.  There are no indexes being used, we drop them before we ingest the data.\r\n\r\nHere is the statement:\r\n\r\n    UPDATE Staging.[TdDailyPerformance]\r\n    SET [SYS_OPERATION] = 'U'\r\n    FROM (\r\n        SELECT [HashCode]           \r\n        FROM Staging.[TdDailyPerformance]\r\n        INTERSECT\r\n        SELECT [HashCode]\r\n        FROM [IdMatch].[TdDailyPerformance]\r\n    ) AS A\r\n\r\nExecution plan:\r\n\r\n[![ExecutionPlan_1][1]][1]\r\n\r\n[![enter image description here][2]][2]\r\n\r\nNow this query works in many other places on our server but not here.  The interesting thing is that no matter whether the `INTERSECT` returns rows, the query hangs forever (I test this by running the intersect independently - it takes less than 2ms.).\r\n\r\nIt seems like according to SQL it shouldn't work, but it does.  If the `HasCode` in the `Staging` table exists already in the `IdMatch` table it updates the `[SYS_OPERATION]` of the `Staging` table to be "U".  We use this several places and it only started failing on this one dataset recently.\r\n\r\nAny ideas what might be causing this?\r\n\r\nNo blocking, as far as we can see.  The only wait types on the transaction are `CXPACKET` which is what I would expect from the QP.  I've queried `sp_who2`, looked at All Transactions and Activity Monitor to identify blocks and have found nothing.  I haven't traced.\r\n\r\nMostly, our tests work out now, so that it has 0 rows when it hangs. But we have verified that it also hangs with 1-100 rows that `INTERSECT`.\r\n\r\n`IdMatch` has no `HashCodes` that exist in `Staging`, but both tables have around 70K rows at time of hang. So to be clear, both tables have around 70K, but the intersection on `HashCode` is 0 rows.\r\n\r\nWe've tested with indexes. We get overall poor performance before we hit the query in question. The index just fragments too quickly to be helpful.\r\n\r\n### Table definitions\r\n\r\nStaging.TdDailyPerformance\r\n\r\n    CREATE TABLE [Staging].[TdDailyPerformance]\r\n    (\r\n    \t[ID] INT NOT NULL,\r\n    \t[SYS_OPERATION] CHAR(8) NULL,\r\n    \t[HashCode] BINARY(65) NULL,\r\n    \t[Ad Environment] NVARCHAR(1024) NULL,\r\n    \t[Ad Format] NVARCHAR(10) NULL,\r\n    \t[Ad Group] NVARCHAR(1024) NULL,\r\n    \t[Ad Group ID] NVARCHAR(32) NULL,\r\n    \t[Ad Server Creative Placement ID] NVARCHAR(1024) NULL,\r\n    \t[Ad Server Name] NVARCHAR(1024) NULL,\r\n    \t[Advertiser] NVARCHAR(1024) NULL,\r\n    \t[Advertiser Currency Code] NVARCHAR(32) NULL,\r\n    \t[Advertiser ID] NVARCHAR(32) NULL,\r\n    \t[App] NVARCHAR(1024) NULL,\r\n    \t[Audience] NVARCHAR(1024) NULL,\r\n    \t[Audience ID] NVARCHAR(32) NULL,\r\n    \t[Browser] NVARCHAR(30) NULL,\r\n    \t[Campaign] NVARCHAR(1024) NULL,\r\n    \t[Campaign ID] NVARCHAR(32) NULL,\r\n    \t[Carrier ID] INT NULL,\r\n    \t[Carrier Name] NVARCHAR(1024) NULL,\r\n    \t[Category ID] NVARCHAR(1024) NULL,\r\n    \t[Category Name] NVARCHAR(1024) NULL,\r\n    \t[City] NVARCHAR(50) NULL,\r\n    \t[Country] NVARCHAR(50) NULL,\r\n    \t[Creative] NVARCHAR(1024) NULL,\r\n    \t[Creative Duration In Seconds] INT NULL,\r\n    \t[Creative ID] NVARCHAR(32) NULL,\r\n    \t[Date] NVARCHAR(1024) NULL,\r\n    \t[Deal ID] NVARCHAR(128) NULL,\r\n    \t[Device Make] NVARCHAR(32) NULL,\r\n    \t[Device Type] NVARCHAR(15) NULL,\r\n    \t[Fold] NVARCHAR(128) NULL,\r\n    \t[Language] NVARCHAR(1024) NULL,\r\n    \t[Market Type] NVARCHAR(32) NULL,\r\n    \t[Media Type] NVARCHAR(32) NULL,\r\n    \t[Metro] NVARCHAR(128) NULL,\r\n    \t[Metro Code] INT NULL,\r\n    \t[Operating System] NVARCHAR(32) NULL,\r\n    \t[Operating System Family] NVARCHAR(1024) NULL,\r\n    \t[Partner ID] NVARCHAR(32) NULL,\r\n    \t[Partner Name] NVARCHAR(32) NULL,\r\n    \t[Recency Group] NVARCHAR(32) NULL,\r\n    \t[Recency Group End In Minutes] INT NULL,\r\n    \t[Recency Group Start In Minutes] INT NULL,\r\n    \t[Region] NVARCHAR(128) NULL,\r\n    \t[Site] NVARCHAR(128) NULL,\r\n    \t[Site List Name] NVARCHAR(1024) NULL,\r\n    \t[Site/Category Bid Factor] NVARCHAR(1024) NULL,\r\n    \t[Supply Vendor] NVARCHAR(25) NULL,\r\n    \t[Supply Vendor Publisher Id] NVARCHAR(64) NULL,\r\n    \t[Timezone] NVARCHAR(1024) NULL,\r\n    \t[Video Playback Type] NVARCHAR(32) NULL,\r\n    \t[Whitelist Site] NVARCHAR(32) NULL,\r\n    \t[Partner Currency Code] NVARCHAR(1024) NULL,\r\n    \t[Additional Fee Cost (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Additional Fee Cost (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Additional Fee Cost (USD)] DECIMAL(37,15) NULL,\r\n    \t[Advertiser Cost (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Advertiser Cost (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Advertiser Cost (USD)] DECIMAL(37,15) NULL,\r\n    \t[All Last Click + View Conversions] INT NULL,\r\n    \t[Bids] INT NULL,\r\n    \t[Clicks] INT NULL,\r\n    \t[Companion Clicks] INT NULL,\r\n    \t[Companion Impressions] INT NULL,\r\n    \t[Data Cost (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Data Cost (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Data Cost (USD)] DECIMAL(37,15) NULL,\r\n    \t[Fee Features Cost (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Fee Features Cost (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Fee Features Cost (USD)] DECIMAL(37,15) NULL,\r\n    \t[IAS Display Fully In View 0 Seconds] INT NULL,\r\n    \t[IAS Display Fully In View 1 Second] INT NULL,\r\n    \t[IAS Display Fully In View 15 Seconds] INT NULL,\r\n    \t[IAS Display Fully In View 5 Seconds] INT NULL,\r\n    \t[IAS Display In View 1 Second] INT NULL,\r\n    \t[IAS Display In View 15 Seconds] INT NULL,\r\n    \t[IAS Display In View 5 Seconds] INT NULL,\r\n    \t[IAS High Risk Impression Count] INT NULL,\r\n    \t[IAS Low Risk Impression Count] INT NULL,\r\n    \t[IAS Moderate Risk Impression Count] INT NULL,\r\n    \t[IAS Non GVIT Impression Count] INT NULL,\r\n    \t[IAS Suspicious Activity] INT NULL,\r\n    \t[IAS Total Impression Count] INT NULL,\r\n    \t[IAS Very High Risk Impression Count] INT NULL,\r\n    \t[IAS Video 25% Complete] INT NULL,\r\n    \t[IAS Video 50% Complete] INT NULL,\r\n    \t[IAS Video 75% Complete] INT NULL,\r\n    \t[IAS Video Completed Views] INT NULL,\r\n    \t[IAS Video Muted] INT NULL,\r\n    \t[IAS Video Viewable 25% Complete] INT NULL,\r\n    \t[IAS Video Viewable 50% Complete] INT NULL,\r\n    \t[IAS Video Viewable 75% Complete] INT NULL,\r\n    \t[IAS Video Viewable Completed Views] INT NULL,\r\n    \t[Impressions] INT NULL,\r\n    \t[In-banner Player Impressions] INT NULL,\r\n    \t[Large Player Impressions] INT NULL,\r\n    \t[Media Cost (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Media Cost (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Media Cost (USD)] DECIMAL(37,15) NULL,\r\n    \t[Medium Player Impressions] INT NULL,\r\n    \t[MOAT Display In View Time 10 Seconds] INT NULL,\r\n    \t[MOAT Display In View Time 15 Seconds] INT NULL,\r\n    \t[MOAT Display In View Time 30 Seconds] INT NULL,\r\n    \t[MOAT Display In View Time 5 Seconds] INT NULL,\r\n    \t[MOAT Display On Screen] INT NULL,\r\n    \t[MOAT Display Universal Interaction] INT NULL,\r\n    \t[MOAT Display Universal Interaction Time 10 Seconds] INT NULL,\r\n    \t[MOAT Display Universal Interaction Time 15 Seconds] INT NULL,\r\n    \t[MOAT Display Universal Interaction Time 30 Seconds] INT NULL,\r\n    \t[MOAT Display Universal Interaction Time 5 Seconds] INT NULL,\r\n    \t[MOAT Video Audible and Visible on Complete] INT NULL,\r\n    \t[MOAT Video In View Time 3 Seconds] INT NULL,\r\n    \t[MOAT Video In View Time 5 Seconds] INT NULL,\r\n    \t[MOAT Video Visible on Complete] INT NULL,\r\n    \t[Non-USD Currency Cost (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Non-USD Currency Cost (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Non-USD Currency Cost (USD)] DECIMAL(37,15) NULL,\r\n    \t[Partner Cost (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Partner Cost (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Partner Cost (USD)] DECIMAL(37,15) NULL,\r\n    \t[Player 25% Complete] INT NULL,\r\n    \t[Player 50% Complete] INT NULL,\r\n    \t[Player 75% Complete] INT NULL,\r\n    \t[Player Audible Event] INT NULL,\r\n    \t[Player Close] INT NULL,\r\n    \t[Player Collapse] INT NULL,\r\n    \t[Player Completed Views] INT NULL,\r\n    \t[Player Engaged Views] INT NULL,\r\n    \t[Player Errors] INT NULL,\r\n    \t[Player Expansion] INT NULL,\r\n    \t[Player Full Screen] INT NULL,\r\n    \t[Player Invitation Accept] INT NULL,\r\n    \t[Player Mute] INT NULL,\r\n    \t[Player Pause] INT NULL,\r\n    \t[Player Playing Event] INT NULL,\r\n    \t[Player Resume] INT NULL,\r\n    \t[Player Rewind] INT NULL,\r\n    \t[Player Skip] INT NULL,\r\n    \t[Player Starts] INT NULL,\r\n    \t[Player Total Playing Seconds] INT NULL,\r\n    \t[Player Unmute] INT NULL,\r\n    \t[Player Views] INT NULL,\r\n    \t[Predictive Clearing Savings (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Predictive Clearing Savings (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Predictive Clearing Savings (USD)] DECIMAL(37,15) NULL,\r\n    \t[Profit (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Profit (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Profit (USD)] DECIMAL(37,15) NULL,\r\n    \t[Sampled Tracked Impressions] INT NULL,\r\n    \t[Sampled Viewed Impressions] INT NULL,\r\n    \t[Small Player Impressions] INT NULL,\r\n    \t[Total Audible Seconds] INT NULL,\r\n    \t[Total Bid Amount (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[Total Bid Amount (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[Total Bid Amount (USD)] DECIMAL(37,15) NULL,\r\n    \t[Total Custom CPA Conversions] INT NULL,\r\n    \t[Total Seconds In View] INT NULL,\r\n    \t[TTD Cost (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[TTD Cost (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[TTD Cost (USD)] DECIMAL(37,15) NULL,\r\n    \t[TTD Margin (Adv Currency)] DECIMAL(37,15) NULL,\r\n    \t[TTD Margin (Partner Currency)] DECIMAL(37,15) NULL,\r\n    \t[TTD Margin (USD)] DECIMAL(37,15) NULL,\r\n    \t[Video In View Event] INT NULL,\r\n    \t[White Ops SIVT Bids Avoided] INT NULL,\r\n    \r\n    \t[CreatedOn] DATETIME2 NULL CONSTRAINT df_Staging_TdDailyPerformance_CreatedOn DEFAULT SYSUTCDATETIME(),\r\n    \t[ModifiedOn] DATETIME2 NULL CONSTRAINT df_Staging_TdDailyPerformance_ModifiedOn DEFAULT SYSUTCDATETIME(),\r\n    \t[Retired] BIT NULL CONSTRAINT df_Staging_TdDailyPerformance_Retired DEFAULT 0\r\n    )\r\n\r\nIdMatch.TdDailyPerformance\r\n\r\n    CREATE TABLE [IdMatch].[TdDailyPerformance]\r\n    (\r\n    \t[ID] INT IDENTITY(1,1) NOT NULL,\r\n    \t[HashCode] BINARY(65) NOT NULL,\r\n    \t[Ad Environment] NVARCHAR(1024) NULL,\r\n    \t[Ad Format] NVARCHAR(10) NULL,\r\n    \t[Ad Group] NVARCHAR(1024) NULL,\r\n    \t[Ad Group ID] NVARCHAR(32) NULL,\r\n    \t[Ad Group Integer ID] INT NULL,\r\n    \t[Ad Server Creative Placement ID] NVARCHAR(1024) NULL,\r\n    \t[Ad Server Name] NVARCHAR(1024) NULL,\r\n    \t[Advertiser] NVARCHAR(1024) NULL,\r\n    \t[Advertiser Currency Code] NVARCHAR(1024) NULL,\r\n    \t[Advertiser ID] NVARCHAR(32) NULL,\r\n    \t[App] NVARCHAR(1024) NULL,\r\n    \t[Browser] NVARCHAR(30) NULL,\r\n    \t[Campaign] NVARCHAR(1024) NULL,\r\n    \t[Campaign ID] NVARCHAR(32) NULL,\r\n    \t[Carrier ID] INT NULL,\r\n    \t[Carrier Name] NVARCHAR(1024) NULL,\r\n    \t[Category ID] NVARCHAR(1024) NULL,\r\n    \t[Category Name] NVARCHAR(1024) NULL,\r\n    \t[City] NVARCHAR(50) NULL,\r\n    \t[Country] NVARCHAR(50) NULL,\r\n    \t[Creative] NVARCHAR(1024) NULL,\r\n    \t[Creative Duration In Seconds] INT NULL,\r\n    \t[Creative ID] NVARCHAR(32) NULL,\r\n    \t[Date] NVARCHAR(1024) NULL,\r\n    \t[Device Make] NVARCHAR(32) NULL,\r\n    \t[Device Type] NVARCHAR(15) NULL,\r\n    \t[Fold] NVARCHAR(128) NULL,\r\n    \t[Language] NVARCHAR(1024) NULL,\r\n    \t[Market Type] NVARCHAR(32) NULL,\r\n    \t[Media Type] NVARCHAR(32) NULL,\r\n    \t[Metro] NVARCHAR(128) NULL,\r\n    \t[Metro Code] INT NULL,\r\n    \t[Operating System] NVARCHAR(32) NULL,\r\n    \t[Operating System Family] NVARCHAR(1024) NULL,\r\n    \t[Partner ID] NVARCHAR(32) NULL,\r\n    \t[Partner Name] NVARCHAR(32) NULL,\r\n    \t[Region] NVARCHAR(128) NULL,\r\n    \t[Site] NVARCHAR(128) NULL,\r\n    \t[Site List Name] NVARCHAR(1024) NULL,\r\n    \t[Site/Category Bid Factor] NVARCHAR(1024) NULL,\r\n    \t[Supply Vendor] NVARCHAR(25) NULL,\r\n    \t[Supply Vendor Integer ID] INT NULL,\r\n    \t[Supply Vendor Publisher Id] NVARCHAR(64) NULL,\r\n    \t[Timezone] NVARCHAR(1024) NULL,\r\n    \t[Video Playback Type] NVARCHAR(32) NULL,\r\n    \t[Whitelist Site] NVARCHAR(32) NULL,\r\n    \t[CreatedOn] DATETIME2 NOT NULL CONSTRAINT df_IdMatch_TdDailyPerformance_CreatedOn DEFAULT SYSUTCDATETIME(),\r\n    \t[ModifiedOn] DATETIME2 NOT NULL CONSTRAINT df_IdMatch_TdDailyPerformance_ModifiedOn DEFAULT SYSUTCDATETIME(),\r\n    \t[Retired] BIT NOT NULL CONSTRAINT df_IdMatch_TdDailyPerformance_Retired DEFAULT 0\r\n    )\r\n    GO\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/UhbMw.png\r\n  [2]: https://i.stack.imgur.com/HuavC.png	265	2019-12-04 01:22:25.306181+00	0	4	1	332	560	752	240843	0	0	0	2019-12-04 01:22:25.306181+00	f	f	1	1
2	126	258	2019-06-20 05:24:29+00	Can we put an equal sign (=) after aggregate functions in Transact-SQL?	I have encountered a script like this:\r\n    \r\n    set @sum = sum = (case when somecol1 is null then \r\n        DATEDIFF(d,[somecol2],somecol3) else 0 end)\r\n\r\nI can't understand the meaning of the equal sign (=) after the second keyword sum. When I run the query it is not showing any errors both with the equal sign and without. \r\n\r\nI want to know the purpose of putting an equal sign after the keyword `sum`. Is that a mistake or not?	266	2019-12-04 01:25:44.368988+00	0	4	1	333	563	955	240995	0	0	0	2019-12-04 01:25:12.307882+00	f	f	1	1
2	104	186	2016-09-01 09:18:07+00	Cardinality Estimation for >= and > for intra step statistics value	I am trying to understand how SQL Server try to estimate for 'greater than' and 'greater than equal to' where clauses in SQL Server 2014.\r\n\r\nI think I do understand the cardinality estimation when it hits the step for example if I do \r\n\r\n\t\tselect * from charge where charge_dt >= '1999-10-13 10:47:38.550'\r\n\r\nThe cardinality estimation is 6672 which can be easily calculated as \r\n32(EQ_ROWS) + 6624(RANGE_ROWS) + 16 (EQ_ROWS) = 6672 (histogram in below screenshot)\r\n\r\n [![enter image description here][1]][1]\r\n\r\nBut when I do\r\n\r\n\t\tselect * from charge where charge_dt >= '1999-10-13 10:48:38.550' \r\n(increased the time to 10:48 so its not a step)\r\n\r\nthe estimate is 4844.13.\r\n\r\nHow is that calculated?\r\n\r\n  [1]: http://i.stack.imgur.com/1p14c.png	193	2019-11-28 13:56:15.212097+00	0	4	1	261	387	1472	148523	0	0	0	2019-11-28 13:56:15.212097+00	f	f	1	1
2	752	629	2020-01-28 14:27:50.850492+00	What is the best way to get a database with only 10 rows per table?	My wife has a request for a copy of a database where each table only has 10 rows in it.\r\n\r\nI suggested she loop through the tables, something like this:\r\n\r\n    declare @table varchar(255),\r\n    @stgtable varchar(255),\r\n    @columns varchar(max),\r\n    @sql nvarchar(max)\r\n\r\n    declare c cursor for\r\n\r\n    select table_name from INFORMATION_SCHEMA.tables\r\n\r\n    open c\r\n    fetch from c into @table\r\n\r\n    while @@FETCH_STATUS = 0\r\n    begin\r\n\r\n    set @sql = 'alter table dbo.' + @table + ' nocheck constraint all'\r\n\r\n    print @sql\r\n    --exec sp_executesql @sql\r\n\r\n    select @columns = stuff( (\r\n        select ', ' + Column_Name from information_schema.columns\r\n        where Table_Name = @table\r\n        for xml path ('')),1,2,'')\r\n\r\n    set @sql = 'use sampledb; select top 10 ' + @columns + ' into ' + @table + ' from \t\r\n    reportDB.dbo.' + @table \r\n\r\n    print @sql\r\n    --exec sp_executesql @sql\r\n\r\n    fetch next from c into @table\r\n    end\r\n\r\n    close c\r\n    deallocate c\r\n\r\nbut this seems kind of loopy, if you know what I mean.\r\n\r\n**QUESTION** is there a more straightforward way?	644	2020-01-28 19:26:50.639861+00	0	4	1	704	1792	3410	\N	0	0	0	\N	f	f	1	\N
1	811	626	2020-01-28 13:45:32.853352+00	Issues with starring chat messages	The starring system has a few issues:\r\n* Stars mean something different in chat (pin this) than on posts (like this).\r\n* Starring a chat message does not update the "starred" pane until page reload.\r\n* In the starred pane, unstarring is jarring:\r\n    * Hovering over a star I put, doesn't revert it to black outline mode to indicate that clicking will unstar it.\r\n    * Starring and unstarring has no visible effect until page reload.\r\n* Stars in chat have [too low contrast](https://contrastchecker.com/?c=bdbdbd&b=333333#dbdbdb) due to the brighter background.	641	2020-01-28 13:45:32.853352+00	2	1	1	701	1758	3453	\N	0	0	0	\N	f	f	2	\N
2	902	707	2015-05-22 15:56:43+00	Trace Flag 4199 - Enable globally?	This may fall under the category of opinion, but I'm curious if people are using [trace flag 4199][1] as a startup parameter for SQL Server.  For those that have used it, under what circumstances did you experience query regression?\r\n\r\nIt certainly seems like a potential performance benefit across the board, I'm considering enabling it globally in our non-production environment and letting it sit for a couple months to ferret out any issues.  \r\n\r\nAre the fixes in 4199 rolled into the optimizer by default in 2014 (or 2016)?  Although I understand the case for not introducing unexpected plan changes, it seems odd to keep all these fixes hidden between versions.\r\n\r\nWe're using 2008, 2008R2 and mostly 2012.\r\n\r\n\r\n  [1]: https://support.microsoft.com/en-us/kb/974006	722	2020-02-13 01:40:18.080429+00	0	4	1	782	2172	4013	102292	0	0	0	2020-02-13 01:40:18.080429+00	f	f	1	1
2	119	206	2018-02-27 11:22:12+00	When is the SQL Server database ready to accept queries?	In the SQL Server error log file I found the following lines:\r\n\r\n    2018-02-22 14:10:58.95 spid17s     Starting up database 'msdb'.\r\n    2018-02-22 14:10:58.95 spid16s     Starting up database 'ReportServer'.\r\n    2018-02-22 14:10:58.95 spid18s     Starting up database 'ReportServerTempDB'.\r\n    2018-02-22 14:10:58.95 spid19s     Starting up database 'XYZ'.\r\n\r\nIf I check for the status of the database XYZ before this time, it is `ONLINE` using the following statement:\r\n\r\n    SELECT state_desc FROM sys.databases WHERE name='XYZ'\r\n\r\n...but when I try to connect to this database using a C# application, it can't connect to the database.\r\n\r\nThe error is:\r\n\r\n>Login failed for user 'asd'.  \r\nReason: Failed to open the explicitly specified database.\r\n\r\nI tried three different users (Windows user, sa, SQL Server user defined for the application). The problem happens when I run the application in the start up of the OS, but if I start it manually after the start up, no errors happen, so I think all SQL Server settings and firewall settings are correct.\r\n\r\nI also checked before this that the service status is running.\r\n\r\nWhat else should I check to make sure the database is actually online and ready for queries?\r\n\r\nI'm looking for a key that tells me it's ok to query the database, instead of delaying for a time (even not based on a clear reason).\r\n\r\nI thought of scanning the error log for the text "Starting up database 'XYZ'", but this means I have to add a setting for the application for the path of the SQL Server error log. It also means reading the file many times until I find this phrase.	213	2019-11-30 12:52:20.896138+00	0	4	1	281	430	1485	198898	0	0	0	2019-11-30 12:51:50.036589+00	f	f	1	1
2	45	251	2019-02-26 01:27:45+00	Why is a temp table a more efficient solution to the Halloween Problem than an eager spool?	Consider the following query that inserts rows from a source table only if they aren't already in the target table:\r\n\r\n    INSERT INTO dbo.HALLOWEEN_IS_COMING_EARLY_THIS_YEAR WITH (TABLOCK)\r\n    SELECT maybe_new_rows.ID\r\n    FROM dbo.A_HEAP_OF_MOSTLY_NEW_ROWS maybe_new_rows\r\n    WHERE NOT EXISTS (\r\n    \tSELECT 1\r\n    \tFROM dbo.HALLOWEEN_IS_COMING_EARLY_THIS_YEAR halloween\r\n    \tWHERE maybe_new_rows.ID = halloween.ID\r\n    )\r\n    OPTION (MAXDOP 1, QUERYTRACEON 7470);\r\n\r\nOne possible plan shape includes a merge join and an eager spool. The eager spool operator is present to solve the [Halloween Problem][1]:\r\n\r\n[![first plan][2]][2]\r\n\r\nOn my machine, the above code executes in about 6900 ms. Repro code to create the tables is included at the bottom of the question. If I'm dissatisfied with performance I might try to load the rows to be inserted into a temp table instead of relying on the eager spool. Here's one possible implementation:\r\n\r\n    DROP TABLE IF EXISTS #CONSULTANT_RECOMMENDED_TEMP_TABLE;\r\n    CREATE TABLE #CONSULTANT_RECOMMENDED_TEMP_TABLE (\r\n    \tID BIGINT,\r\n    \tPRIMARY KEY (ID)\r\n    );\r\n    \r\n    INSERT INTO #CONSULTANT_RECOMMENDED_TEMP_TABLE WITH (TABLOCK)\r\n    SELECT maybe_new_rows.ID\r\n    FROM dbo.A_HEAP_OF_MOSTLY_NEW_ROWS maybe_new_rows\r\n    WHERE NOT EXISTS (\r\n    \tSELECT 1\r\n    \tFROM dbo.HALLOWEEN_IS_COMING_EARLY_THIS_YEAR halloween\r\n    \tWHERE maybe_new_rows.ID = halloween.ID\r\n    )\r\n    OPTION (MAXDOP 1, QUERYTRACEON 7470);\r\n    \r\n    INSERT INTO dbo.HALLOWEEN_IS_COMING_EARLY_THIS_YEAR WITH (TABLOCK)\r\n    SELECT new_rows.ID\r\n    FROM #CONSULTANT_RECOMMENDED_TEMP_TABLE new_rows\r\n    OPTION (MAXDOP 1);\r\n\r\nThe new code executes in about 4400 ms. I can get actual plans and use Actual Time Statistics™ to examine where time is spent at the operator level. Note that asking for an actual plan adds significant overhead for these queries so totals will not match the previous results.\r\n\r\n``` none\r\n╔═════════════╦═════════════╦══════════════╗\r\n║  operator   ║ first query ║ second query ║\r\n╠═════════════╬═════════════╬══════════════╣\r\n║ big scan    ║ 1771        ║ 1744         ║\r\n║ little scan ║ 163         ║ 166          ║\r\n║ sort        ║ 531         ║ 530          ║\r\n║ merge join  ║ 709         ║ 669          ║\r\n║ spool       ║ 3202        ║ N/A          ║\r\n║ temp insert ║ N/A         ║ 422          ║\r\n║ temp scan   ║ N/A         ║ 187          ║\r\n║ insert      ║ 3122        ║ 1545         ║\r\n╚═════════════╩═════════════╩══════════════╝\r\n```\r\n\r\nThe query plan with the eager spool seems to spend significantly more time on the insert and spool operators compared to the plan that uses the temp table.\r\n\r\nWhy is the plan with the temp table more efficient? Isn't an eager spool mostly just an internal temp table anyway? I believe I am looking for answers that focus on internals. I'm able to see how the call stacks are different but can't figure out the big picture.\r\n\r\nI am on SQL Server 2017 CU 11 in case someone wants to know. Here is code to populate the tables used in the above queries:\r\n\r\n    DROP TABLE IF EXISTS dbo.HALLOWEEN_IS_COMING_EARLY_THIS_YEAR;\r\n    \r\n    CREATE TABLE dbo.HALLOWEEN_IS_COMING_EARLY_THIS_YEAR (\r\n    ID BIGINT NOT NULL,\r\n    PRIMARY KEY (ID)\r\n    );\r\n    \r\n    INSERT INTO dbo.HALLOWEEN_IS_COMING_EARLY_THIS_YEAR WITH (TABLOCK)\r\n    SELECT TOP (20000000) ROW_NUMBER() OVER (ORDER BY (SELECT NULL))\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2\r\n    CROSS JOIN master..spt_values t3\r\n    OPTION (MAXDOP 1);\r\n    \r\n    \r\n    DROP TABLE IF EXISTS dbo.A_HEAP_OF_MOSTLY_NEW_ROWS;\r\n    \r\n    CREATE TABLE dbo.A_HEAP_OF_MOSTLY_NEW_ROWS (\r\n    ID BIGINT NOT NULL\r\n    );\r\n    \r\n    INSERT INTO dbo.A_HEAP_OF_MOSTLY_NEW_ROWS WITH (TABLOCK)\r\n    SELECT TOP (1900000) 19999999 + ROW_NUMBER() OVER (ORDER BY (SELECT NULL))\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2;\r\n\r\n\r\n  [1]: https://sqlperformance.com/2013/02/t-sql-queries/halloween-problem-part-1\r\n  [2]: https://i.stack.imgur.com/sB7G3.png	259	2019-12-03 12:37:15.126668+00	0	4	1	326	547	1495	230722	0	0	0	2019-12-03 12:36:10.302855+00	f	f	1	1
2	751	300	2012-02-18 21:30:15+00	Why is using a table variable more than twice as fast as a #temp table in this specific case?	I was looking at the article here \r\n[Temporary Tables vs. Table Variables and Their Effect on SQL Server Performance][1] and on SQL Server 2008 was able to reproduce similar results to those shown there for 2005. \r\n\r\nWhen executing the stored procedures (definitions below) with only 10 rows the table variable version out performs the temporary table version by more than two times.\r\n\r\nI cleared the procedure cache and ran both stored procedures 10,000 times then repeated the process for another 4 runs. Results below (time in ms per batch)\r\n                 \r\n    T2_Time     V2_Time\r\n    ----------- -----------\r\n    8578        2718      \r\n    6641        2781    \r\n    6469        2813   \r\n    6766        2797\r\n    6156        2719\r\n\r\nMy question is: **What is the reason for the better performance of the table variable version?**\r\n\r\nI've done some investigation. e.g. Looking at the performance counters with \r\n\r\n    SELECT cntr_value\r\n    from sys.dm_os_performance_counters\r\n    where counter_name = 'Temp Tables Creation Rate';\r\n\r\nconfirms that in both cases the temporary objects are being cached after the first run [as expected][2] rather than created from scratch again for every invocation.\r\n\r\nSimilarly tracing the `Auto Stats`, `SP:Recompile`, `SQL:StmtRecompile`events in Profiler (screenshot below) shows that these events only occur once (on the first invocation of the `#temp` table stored procedure) and the other 9,999 executions do not raise any of these events. (The table variable version does not get any of these events)\r\n\r\n![Trace][3]\r\n\r\nThe slightly greater overhead of the first run of the stored procedure can in no way account for the big overall difference however as it still only takes a few ms to clear the procedure cache and run both procedures once so I don't believe either statistics or recompiles can be the cause. \r\n\r\n\r\n**Create Required Database Objects**\r\n\r\n    CREATE DATABASE TESTDB_18Feb2012;\r\n    \r\n    GO\r\n    \r\n    USE TESTDB_18Feb2012;\r\n    \r\n    CREATE TABLE NUM \r\n      ( \r\n         n INT PRIMARY KEY, \r\n         s VARCHAR(128) \r\n      ); \r\n    \r\n    WITH NUMS(N) \r\n         AS (SELECT TOP 1000000 ROW_NUMBER() OVER (ORDER BY $/0) \r\n             FROM   master..spt_values v1, \r\n                    master..spt_values v2) \r\n    INSERT INTO NUM \r\n    SELECT N, \r\n           'Value: ' + CONVERT(VARCHAR, N) \r\n    FROM   NUMS \r\n    \r\n    GO\r\n    \r\n    CREATE PROCEDURE [dbo].[T2] @total INT \r\n    AS \r\n      CREATE TABLE #T \r\n        ( \r\n           n INT PRIMARY KEY, \r\n           s VARCHAR(128) \r\n        ) \r\n    \r\n      INSERT INTO #T \r\n      SELECT n, \r\n             s \r\n      FROM   NUM \r\n      WHERE  n%100 > 0 \r\n             AND n <= @total \r\n    \r\n      DECLARE @res VARCHAR(128) \r\n    \r\n      SELECT @res = MAX(s) \r\n      FROM   NUM \r\n      WHERE  n <= @total \r\n             AND NOT EXISTS(SELECT * \r\n                            FROM   #T \r\n                            WHERE  #T.n = NUM.n) \r\n    GO\r\n    \r\n    CREATE PROCEDURE [dbo].[V2] @total INT \r\n    AS \r\n      DECLARE @V TABLE ( \r\n        n INT PRIMARY KEY, \r\n        s VARCHAR(128)) \r\n    \r\n      INSERT INTO @V \r\n      SELECT n, \r\n             s \r\n      FROM   NUM \r\n      WHERE  n%100 > 0 \r\n             AND n <= @total \r\n    \r\n      DECLARE @res VARCHAR(128) \r\n    \r\n      SELECT @res = MAX(s) \r\n      FROM   NUM \r\n      WHERE  n <= @total \r\n             AND NOT EXISTS(SELECT * \r\n                            FROM   @V V \r\n                            WHERE  V.n = NUM.n) \r\n                            \r\n    \r\n    GO\r\n \r\n**Test Script**\r\n   \r\n    SET NOCOUNT ON;\r\n    \r\n    DECLARE @T1 DATETIME2,\r\n            @T2 DATETIME2,\r\n            @T3 DATETIME2,  \r\n            @Counter INT = 0\r\n    \r\n    SET @T1 = SYSDATETIME()\r\n            \r\n    WHILE ( @Counter < 10000)\r\n    BEGIN\r\n    EXEC dbo.T2 10\r\n    SET @Counter += 1\r\n    END\r\n    \r\n    SET @T2 = SYSDATETIME()\r\n    SET @Counter = 0\r\n            \r\n    WHILE ( @Counter < 10000)\r\n    BEGIN\r\n    EXEC dbo.V2 10\r\n    SET @Counter += 1\r\n    END\r\n    \r\n    SET @T3 = SYSDATETIME()\r\n    \r\n    SELECT DATEDIFF(MILLISECOND,@T1,@T2) AS T2_Time,\r\n           DATEDIFF(MILLISECOND,@T2,@T3) AS V2_Time\r\n\r\n\r\n\r\n\r\n  [1]: http://www.codeproject.com/Articles/18972/Temporary-Tables-vs-Table-Variables-and-Their-Effe\r\n  [2]: http://www.sqlmag.com/article/sql-server/caching-of-temporary-objects\r\n  [3]: https://i.stack.imgur.com/AiSQ5.png	308	2019-12-04 22:55:11.782673+00	0	4	1	375	657	1542	13392	0	0	0	2019-12-04 22:55:11.782673+00	f	f	1	1
1	709	515	2020-01-04 02:17:42.58078+00	How do new sites get added?	I'm not a DBA and know next to nothing about databases. At the moment I can only really contribute to Meta.  \r\n\r\nI'd love to contribute and ask/answer questions about Unix/Linux/Python/C/C++/Git/Ansible/Go/Servers/Filesystems/Networking/etc.  I understand that there are only two sites at the moment, but what does the process of adding more look like.  \r\n\r\nI understand if the site wants to be careful expanding too quickly without a proper meta model in place, but at the same time I'm using SE until different sites are added.	528	2020-01-04 02:17:42.58078+00	3	4	1	590	1548	3145	\N	0	0	0	\N	f	f	2	\N
1	167	483	2019-12-24 16:46:39.21702+00	Add link to the answer/question to comment	If one uses the "comment" link below questions or answers\r\n\r\n![Screen Shot 2019-12-24 at 17.32.38.png](/image?hash=6247d9a5a5f18cc63fb928d1f717761e52ebaa69525b873c8f0403d36e40aadd)\r\n\r\nit would be nice if the resulting comment would have a link back to the post, for example by adding "replying to <link to post>" like it is displayed if one replies to other chat messages\r\n\r\n![Screen Shot 2019-12-24 at 17.32.43.png](/image?hash=09b8be4362a8d051505ded303be364810728bd080879699f8b9971d5ef1e4f47)\r\n\r\nEspecially if there are multiple answers (maybe even from the same person), this would make it clearer which post the message is about.\r\n\r\n\r\n	495	2019-12-24 16:46:39.21702+00	8	4	1	558	2103	3947	\N	0	0	0	\N	f	f	2	\N
2	16	266	2017-03-27 15:30:57+00	What are different ways to replace ISNULL() in a WHERE clause that uses only literal values?	**What this isn't about:**\r\n\r\nThis is not a question about [catch-all queries][1] that accept user input or use variables.\r\n\r\nThis is strictly about queries where `ISNULL()` is used in the `WHERE` clause to replace `NULL` values with a canary value for comparison to a predicate, and different ways to rewrite those queries to be [SARGable][2] in SQL Server.\r\n\r\n**Why don't you have a seat over there?**\r\n\r\nOur example query is against a local copy of the Stack Overflow database on SQL Server 2016, and looks for users with a `NULL` age, or an age < 18.\r\n\r\n    SELECT COUNT(*)\r\n    FROM dbo.Users AS u\r\n    WHERE ISNULL(u.Age, 17) < 18;\r\n\r\nThe query plan shows a Scan of a quite thoughtful nonclustered index.\r\n\r\n[![Nuts][3]][3]\r\n\r\nThe scan operator shows (thanks to additions to actual execution plan XML in more recent versions of SQL Server) that we read every stinkin' row.\r\n\r\n[![Nuts][4]][4]\r\n\r\nOverall, we do 9157 reads and use about half a second of CPU time:\r\n\r\n    Table 'Users'. Scan count 1, logical reads 9157, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 485 ms,  elapsed time = 483 ms.\r\n\r\n**The question:**\r\nWhat are ways to rewrite this query to make it more efficient, and perhaps even SARGable?\r\n\r\nFeel free to offer other suggestions. I don't think my answer is necessarily *the* answer, and there are enough smart people out there to come up with alternatives that may be better.\r\n\r\nIf you want to play along on your own computer, head over here to [download the SO database][5].\r\n\r\nThanks!\r\n\r\n\r\n  [1]: http://sqlinthewild.co.za/index.php/2009/03/19/catch-all-queries/\r\n  [2]: https://en.wikipedia.org/wiki/Sargable\r\n  [3]: https://i.stack.imgur.com/WNwbT.jpg\r\n  [4]: https://i.stack.imgur.com/x1IPP.jpg\r\n  [5]: http://brentozar.com/go/querystack	274	2019-12-04 14:01:10.851099+00	0	4	1	341	592	1502	168276	0	0	0	2019-12-04 14:01:10.851099+00	f	f	1	1
1	854	647	2020-01-30 10:35:07.301662+00	Delete chat messages?	Is it possible to delete a message I wrote in the chat?	662	2020-01-30 10:35:26.124961+00	12	4	1	722	1873	4174	\N	0	0	0	\N	f	f	2	\N
4	234	551	2020-01-12 04:51:47.967378+00	Moderation on TopAnswers TeX	From [this discussion](https://topanswers.xyz/meta?q=182#question) on the main site it seems to emerge that each site is supposed to come up with its own moderation policies. This question is to ask what the policies on [TopAnswers TeX](https://topanswers.xyz/tex) should be. \r\n\r\nAmong the many subquestions that come to my mind, the perhaps most important ones could be\r\n\r\n1. How will we determine the moderators?\r\n2. If we decide that this will be elections, how often will these elections take place?\r\n3. How many do we need (in the beginning)?\r\n4. What can one do if one feels that the actions of a moderator are inappropriate?	564	2020-01-12 04:51:47.967378+00	6	4	3	626	1415	3026	\N	0	0	0	\N	f	f	2	\N
4	167	458	2019-12-17 15:13:34.377394+00	Changing colour of beamer blocks	Beamer has several different types of blocks (e.g. `block`, `alertblock`, `exampleblock`, `theorem`, `proof`, ...). How to change the colour for each type individually?\r\n\r\n	469	2019-12-17 15:25:43.134731+00	0	4	3	533	1111	2183	\N	0	0	0	\N	f	f	1	\N
1	811	646	2020-01-30 10:18:16.720239+00	Make tables use proportional font	|`Tables`|are|\r\n|-|-\r\n|`currently`|monospaced|\r\n|`make them`|proportional|	661	2020-01-30 10:18:16.720239+00	4	1	1	721	1971	3969	\N	0	0	0	\N	f	f	2	\N
2	8	708	2017-03-30 14:42:55+00	Empty blocking process in blocked process report	I'm collecting blocked process reports using Extended Events, and for some reason in some reports the `blocking-process` node is empty. This is the full xml:\r\n\r\n\r\n\r\n    <blocked-process-report monitorLoop="383674">\r\n     <blocked-process>\r\n      <process id="processa7bd5b868" taskpriority="0" logused="106108620" waitresource="KEY: 6:72057613454278656 (8a2f7bc2cd41)" waittime="25343" ownerId="1051989016" transactionname="user_transaction" lasttranstarted="2017-03-20T09:30:38.657" XDES="0x21f382d9c8" lockMode="X" schedulerid="7" kpid="15316" status="suspended" spid="252" sbid="0" ecid="0" priority="0" trancount="2" lastbatchstarted="2017-03-20T09:39:15.853" lastbatchcompleted="2017-03-20T09:39:15.850" lastattention="1900-01-01T00:00:00.850" clientapp="Microsoft Dynamics AX" hostname="***" hostpid="1348" loginname="***" isolationlevel="read committed (2)" xactid="1051989016" currentdb="6" lockTimeout="4294967295" clientoption1="671088672" clientoption2="128056">\r\n       <executionStack>\r\n        <frame line="1" stmtstart="40" sqlhandle="0x02000000f7def225b0edaecd8744b453ce09bdcff9b291f50000000000000000000000000000000000000000" />\r\n        <frame line="1" sqlhandle="0x0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000" />\r\n       </executionStack>\r\n       <inputbuf>\r\n    (@P1 bigint,@P2 int)DELETE FROM DIMENSIONFOCUSUNPROCESSEDTRANSACTIONS WHERE ((PARTITION=5637144576) AND ((FOCUSDIMENSIONHIERARCHY=@P1) AND (STATE=@P2)))   </inputbuf>\r\n      </process>\r\n     </blocked-process>\r\n     <blocking-process>\r\n      <process />\r\n     </blocking-process>\r\n    </blocked-process-report>\r\n\r\nThe index definition for the index this hobt_id belongs to is\r\n\r\n    CREATE UNIQUE CLUSTERED INDEX [I_7402FOCUSDIMENSIONHIERARCHYIDX] ON [dbo].[DIMENSIONFOCUSUNPROCESSEDTRANSACTIONS]\r\n    (\r\n    \t[PARTITION] ASC,\r\n    \t[FOCUSDIMENSIONHIERARCHY] ASC,\r\n    \t[STATE] ASC,\r\n    \t[GENERALJOURNALENTRY] ASC\r\n    )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, SORT_IN_TEMPDB = OFF, IGNORE_DUP_KEY = OFF, DROP_EXISTING = OFF, ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]\r\n    GO\r\n\r\nThere is no partitioning involved, this is the table definition:\r\n\r\n    CREATE TABLE [dbo].[DIMENSIONFOCUSUNPROCESSEDTRANSACTIONS](\r\n    \t[FOCUSDIMENSIONHIERARCHY] [bigint] NOT NULL DEFAULT ((0)),\r\n    \t[GENERALJOURNALENTRY] [bigint] NOT NULL DEFAULT ((0)),\r\n    \t[STATE] [int] NOT NULL DEFAULT ((0)),\r\n    \t[RECVERSION] [int] NOT NULL DEFAULT ((1)),\r\n    \t[PARTITION] [bigint] NOT NULL DEFAULT ((5637144576.)),\r\n    \t[RECID] [bigint] NOT NULL,\r\n     CONSTRAINT [I_7402RECID] PRIMARY KEY NONCLUSTERED \r\n    (\r\n    \t[RECID] ASC\r\n    )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]\r\n    ) ON [PRIMARY]\r\n    \r\n    GO\r\n    \r\n    ALTER TABLE [dbo].[DIMENSIONFOCUSUNPROCESSEDTRANSACTIONS]  WITH CHECK ADD CHECK  (([RECID]<>(0)))\r\n    GO\r\n\r\nThere are no triggers or foreign keys defined on any of the tables in the entire database.\r\n\r\nThe exact SQL Server build is:\r\n\r\n> Microsoft SQL Server 2012 (SP3-CU4) (KB3165264) - 11.0.6540.0 (X64)  \r\n> Jun 23 2016 17:45:11   Copyright (c) Microsoft Corporation  Enterprise\r\n> Edition: Core-based Licensing (64-bit) on Windows NT 6.3 <X64> (Build\r\n> 14393: ) (Hypervisor)\r\n\r\nThe extended events is fairly simple, just logging the blocked process reports:\r\n\r\n    CREATE EVENT SESSION [Dynperf_Blocking_Data] ON SERVER \r\n    ADD EVENT sqlserver.blocked_process_report(\r\n        ACTION(package0.collect_system_time,sqlserver.client_hostname,sqlserver.context_info)),\r\n    ADD EVENT sqlserver.lock_escalation(\r\n        ACTION(package0.collect_system_time,sqlserver.client_hostname,sqlserver.context_info)),\r\n    ADD EVENT sqlserver.xml_deadlock_report(\r\n        ACTION(package0.collect_system_time,sqlserver.client_hostname,sqlserver.context_info)) \r\n    ADD TARGET package0.event_file(SET filename=N'F:\\SQLTrace\\Dynamics_Blocking.xel',max_file_size=(100),max_rollover_files=(10))\r\n    WITH (MAX_MEMORY=32768 KB,EVENT_RETENTION_MODE=ALLOW_SINGLE_EVENT_LOSS,MAX_DISPATCH_LATENCY=5 SECONDS,MAX_EVENT_SIZE=0 KB,MEMORY_PARTITION_MODE=PER_NODE,TRACK_CAUSALITY=ON,STARTUP_STATE=ON)\r\n    GO\r\n\r\nThe database is configured in Read Committed Snapshot Isolation, and *max degree of parallelism* is set to 1. This is the server configuration:\r\n\r\n    +------------------------------------+-------+\r\n    |                name                | value |\r\n    +------------------------------------+-------+\r\n    | access check cache bucket count    |     0 |\r\n    | access check cache quota           |     0 |\r\n    | Ad Hoc Distributed Queries         |     0 |\r\n    | affinity I/O mask                  |     0 |\r\n    | affinity mask                      |     0 |\r\n    | affinity64 I/O mask                |     0 |\r\n    | affinity64 mask                    |     0 |\r\n    | Agent XPs                          |     1 |\r\n    | allow updates                      |     0 |\r\n    | backup compression default         |     1 |\r\n    | blocked process threshold (s)      |     2 |\r\n    | c2 audit mode                      |     0 |\r\n    | clr enabled                        |     0 |\r\n    | common criteria compliance enabled |     0 |\r\n    | contained database authentication  |     0 |\r\n    | cost threshold for parallelism     |     5 |\r\n    | cross db ownership chaining        |     0 |\r\n    | cursor threshold                   |    -1 |\r\n    | Database Mail XPs                  |     1 |\r\n    | default full-text language         |  1033 |\r\n    | default language                   |     0 |\r\n    | default trace enabled              |     1 |\r\n    | disallow results from triggers     |     0 |\r\n    | EKM provider enabled               |     0 |\r\n    | filestream access level            |     0 |\r\n    | fill factor (%)                    |     0 |\r\n    | ft crawl bandwidth (max)           |   100 |\r\n    | ft crawl bandwidth (min)           |     0 |\r\n    | ft notify bandwidth (max)          |   100 |\r\n    | ft notify bandwidth (min)          |     0 |\r\n    | index create memory (KB)           |     0 |\r\n    | in-doubt xact resolution           |     0 |\r\n    | lightweight pooling                |     0 |\r\n    | locks                              |     0 |\r\n    | max degree of parallelism          |     1 |\r\n    | max full-text crawl range          |     4 |\r\n    | max server memory (MB)             | 65536 |\r\n    | max text repl size (B)             | 65536 |\r\n    | max worker threads                 |     0 |\r\n    | media retention                    |     0 |\r\n    | min memory per query (KB)          |  1024 |\r\n    | min server memory (MB)             |     0 |\r\n    | nested triggers                    |     1 |\r\n    | network packet size (B)            |  4096 |\r\n    | Ole Automation Procedures          |     0 |\r\n    | open objects                       |     0 |\r\n    | optimize for ad hoc workloads      |     1 |\r\n    | PH timeout (s)                     |    60 |\r\n    | precompute rank                    |     0 |\r\n    | priority boost                     |     0 |\r\n    | query governor cost limit          |     0 |\r\n    | query wait (s)                     |    -1 |\r\n    | recovery interval (min)            |     0 |\r\n    | remote access                      |     1 |\r\n    | remote admin connections           |     0 |\r\n    | remote login timeout (s)           |    10 |\r\n    | remote proc trans                  |     0 |\r\n    | remote query timeout (s)           |   600 |\r\n    | Replication XPs                    |     0 |\r\n    | scan for startup procs             |     1 |\r\n    | server trigger recursion           |     1 |\r\n    | set working set size               |     0 |\r\n    | show advanced options              |     1 |\r\n    | SMO and DMO XPs                    |     1 |\r\n    | transform noise words              |     0 |\r\n    | two digit year cutoff              |  2049 |\r\n    | user connections                   |     0 |\r\n    | user options                       |     0 |\r\n    | xp_cmdshell                        |     0 |\r\n    +------------------------------------+-------+\r\n\r\nI ran a server side trace for a while and I get the same empty nodes in a trace file as I do using extended events.  \r\nThis blocked process report was captured using a server side trace on another server also running Dynamics AX, so it's not specific to this server or build.\r\n\r\n    <blocked-process-report monitorLoop="1327922">\r\n     <blocked-process>\r\n      <process id="processbd9839848" taskpriority="0" logused="1044668" waitresource="KEY: 5:72057597098328064 (1d7966fe609a)" waittime="316928" ownerId="3415555263" transactionname="user_transaction" lasttranstarted="2017-03-27T07:59:29.290" XDES="0x1c1c0c3b0" lockMode="U" schedulerid="3" kpid="25236" status="suspended" spid="165" sbid="0" ecid="0" priority="0" trancount="2" lastbatchstarted="2017-03-27T07:59:47.873" lastbatchcompleted="2017-03-27T07:59:47.873" lastattention="2017-03-27T07:58:01.490" clientapp="Microsoft Dynamics AX" hostname="***" hostpid="11072" loginname="***" isolationlevel="read committed (2)" xactid="3415555263" currentdb="5" lockTimeout="4294967295" clientoption1="671088672" clientoption2="128056">\r\n       <executionStack>\r\n        <frame line="1" stmtstart="236" stmtend="676" sqlhandle="0x020000004d6830193d42a167edd195c201f40bb772e9ece20000000000000000000000000000000000000000"/>\r\n       </executionStack>\r\n       <inputbuf>\r\n    (@P1 numeric(32,16),@P2 int,@P3 bigint,@P4 nvarchar(5),@P5 nvarchar(36),@P6 int,@P7 numeric(32,16),@P8 bigint,@P9 int)UPDATE PRODCALCTRANS SET REALCOSTAMOUNT=@P1,RECVERSION=@P2 WHERE (((((((PARTITION=@P3) AND (DATAAREAID=@P4)) AND (COLLECTREFPRODID=@P5)) AND (COLLECTREFLEVEL=@P6)) AND (LINENUM=@P7)) AND (RECID=@P8)) AND (RECVERSION=@P9))   </inputbuf>\r\n      </process>\r\n     </blocked-process>\r\n     <blocking-process>\r\n      <process/>\r\n     </blocking-process>\r\n    </blocked-process-report>\r\n\r\nDoes anybody have an explanation for these reports? What is blocking the query?\r\n\r\nIs there any way to find out what was happening if I'm looking at the reports after the locks have long gone?\r\n\r\nOne thing that might be useful to add is that these queries are run via `sp_cursorprepare`and `sp_cursorexecute`\r\n\r\nSo far I haven't been able to reproduce it, it seems to happen randomly but very often.\r\n\r\nIt happens on several instances (of different builds) and several tables/queries, all related to Dynamics AX.\r\n\r\nThere are no index or other database maintenance jobs occurring in the background at the time.\r\n\r\nUsing the code provided in the [answer by srutzky](https://dba.stackexchange.com/a/169309/39384) I was able to capture some logging related to this blocked process report:\r\n\r\n    <blocked-process-report monitorLoop="1621637">\r\n     <blocked-process>\r\n      <process id="processd06909c28" taskpriority="0" logused="0" waitresource="KEY: 5:72057597585719296 (d2d87c26d920)" waittime="78785" ownerId="4436575948" transactionname="user_transaction" lasttranstarted="2017-04-13T07:39:17.590" XDES="0x3219d034e0" lockMode="U" schedulerid="3" kpid="133792" status="suspended" spid="106" sbid="0" ecid="0" priority="0" trancount="2" lastbatchstarted="2017-04-13T07:39:17.657" lastbatchcompleted="2017-04-13T07:39:17.657" lastattention="1900-01-01T00:00:00.657" clientapp="Microsoft Dynamics AX" hostname="****" hostpid="11800" loginname="****" isolationlevel="read committed (2)" xactid="4436575948" currentdb="5" lockTimeout="4294967295" clientoption1="671088672" clientoption2="128056">\r\n       <executionStack>\r\n        <frame line="1" stmtstart="72" stmtend="256" sqlhandle="0x0200000076a6a92ab1256af09321b056ab243f187342f9960000000000000000000000000000000000000000"/>\r\n        <frame line="1" sqlhandle="0x0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"/>\r\n       </executionStack>\r\n       <inputbuf>\r\n    (@P1 int,@P2 int,@P3 bigint,@P4 int)UPDATE PRODROUTEJOB SET JOBSTATUS=@P1,RECVERSION=@P2 WHERE ((RECID=@P3) AND (RECVERSION=@P4))   </inputbuf>\r\n      </process>\r\n     </blocked-process>\r\n     <blocking-process>\r\n      <process/>\r\n     </blocking-process>\r\n    </blocked-process-report>\r\n\r\nThis is found in the logging tables for the same resource around that time: [Gist because of character limit](https://gist.github.com/anonymous/9c3381e0bfdef4ed5f125a3174bff680)\r\n\r\nFurther investigation shows that just before and after the report with an empty blocking process I have reports for the same resourceid that do have blocking process nodes:\r\n\r\n    <blocked-process-report monitorLoop="1621636">\r\n     <blocked-process>\r\n      <process id="processd06909c28" taskpriority="0" logused="0" waitresource="KEY: 5:72057597585719296 (d2d87c26d920)" waittime="73765" ownerId="4436575948" transactionname="user_transaction" lasttranstarted="2017-04-13T07:39:17.590" XDES="0x3219d034e0" lockMode="U" schedulerid="3" kpid="133792" status="suspended" spid="106" sbid="0" ecid="0" priority="0" trancount="2" lastbatchstarted="2017-04-13T07:39:17.657" lastbatchcompleted="2017-04-13T07:39:17.657" lastattention="1900-01-01T00:00:00.657" clientapp="Microsoft Dynamics AX" hostname="***" hostpid="11800" loginname="***" isolationlevel="read committed (2)" xactid="4436575948" currentdb="5" lockTimeout="4294967295" clientoption1="671088672" clientoption2="128056">\r\n       <executionStack>\r\n        <frame line="1" stmtstart="72" stmtend="256" sqlhandle="0x0200000076a6a92ab1256af09321b056ab243f187342f9960000000000000000000000000000000000000000"/>\r\n        <frame line="1" sqlhandle="0x0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"/>\r\n       </executionStack>\r\n       <inputbuf>\r\n    (@P1 int,@P2 int,@P3 bigint,@P4 int)UPDATE PRODROUTEJOB SET JOBSTATUS=@P1,RECVERSION=@P2 WHERE ((RECID=@P3) AND (RECVERSION=@P4))   </inputbuf>\r\n      </process>\r\n     </blocked-process>\r\n     <blocking-process>\r\n      <process status="sleeping" spid="105" sbid="0" ecid="0" priority="0" trancount="1" lastbatchstarted="2017-04-13T07:40:31.417" lastbatchcompleted="2017-04-13T07:40:31.423" lastattention="1900-01-01T00:00:00.423" clientapp="Microsoft Dynamics AX" hostname="**" hostpid="11800" loginname="**" isolationlevel="read committed (2)" xactid="4436165115" currentdb="5" lockTimeout="4294967295" clientoption1="671088672" clientoption2="128056">\r\n       <executionStack/>\r\n       <inputbuf>\r\n    (@P1 bigint,@P2 nvarchar(5),@P3 bigint,@P4 bigint,@P5 nvarchar(11),@P6 int,@P7 nvarchar(21),@P8 datetime2)SELECT T1.REGDATETIME,T1.REGDATETIMETZID,T1.WORKERPILOT,T1.WORKER,T1.WRKCTRIDPILOT,T1.REGTYPE,T1.PROFILEDATE,T1.JOBID,T1.JOBIDABS,T1.MATCHRECIDSTARTSTOP,T1.JOBACTIVE,T1.RESNO,T1.STARTITEMS,T1.GOODITEMS,T1.SCRAPITEMS,T1.FINISHEDCODE,T1.TMPGOODITEMS,T1.TMPSCRAPITEMS,T1.SYSMRPUPDATEREQUEST,T1.ERROR,T1.ERRORTXT,T1.TMPSTARTITEMS,T1.AUTOSTAMP,T1.ERRORSPECIFICATION,T1.COSTCATEGORY,T1.ONCALLACTIVITY,T1.TERMINALID,T1.PDSCWGOODITEMS,T1.PDSCWSCRAPITEMS,T1.PDSCWSTARTITEMS,T1.RETAILTERMINALID,T1.MODIFIEDDATETIME,T1.RECVERSION,T1.PARTITION,T1.RECID FROM JMGTERMREG T1 WHERE (((PARTITION=@P1) AND (DATAAREAID=@P2)) AND (((((WORKER=@P3) OR ((WORKER=@P4) AND (WRKCTRIDPILOT=@P5))) AND (REGTYPE=@P6)) AND (JOBID=@P7)) AND (REGDATETIME&gt;=@P8))) ORDER BY T1.REGDATETIME   </inputbuf>\r\n      </process>\r\n     </blocking-process>\r\n    </blocked-process-report>\r\n\r\n\r\n\r\nUsing the new script provided by [srutzky](https://dba.stackexchange.com/users/30859/srutzky) new data has been collected.\r\n[It's posted on github](https://gist.github.com/anonymous/c2427cb3acb208a4ad37124970297347) because of the maximum post length.\r\n\r\nSince the originally posted data didn't have both session id's some new data has been posted [on github again](https://gist.github.com/anonymous/4c8df0a69cf5e2bd0a0741aed8b10019)\r\n\r\nNew data including the connections [on github](https://gist.github.com/anonymous/6f5375019dafed8ce3d5934622ea4ad5)	723	2020-02-13 13:48:12.356717+00	0	4	1	783	2177	4010	168646	0	0	0	2020-02-13 13:48:12.356717+00	f	f	1	1
2	174	272	2018-04-13 14:18:07+00	Why does SQL Server return some rows while still executing the query, and sometimes not?	There are queries where when we hit "execute", it shows some rows and it keeps growing, but the query is not over yet. Yet sometimes, it waits until the end of the query.\r\n\r\nWhy does this happen? Is there a way to control this? \r\n\r\n	280	2019-12-04 14:17:09.226265+00	0	4	1	347	600	1507	203877	0	0	0	2019-12-04 14:17:09.226265+00	f	f	1	1
1	96	347	2019-12-06 07:01:14.579293+00	Re-importing questions fails ungracefully	Obviously I wouldn't expect re-importing a question from SE to work the same way twice. It's nice that it won't allow the same post to be moved over more than once. But this message isn't a great way to show that:\r\n\r\n> ERROR: already imported CONTEXT: PL/pgSQL function _error(integer,text) line 1 at RAISE SQL function "new_sequestion" statement 1 \r\n\r\nThis isn't the only time I've been dumped raw output from PostgreSQL. The site should probably catch and display these more gracefully in general, and with friendlier messages for cases like this where we know what happened and expect users to make the mistake.\r\n\r\nIn the mean time for re-imports I would suggest using that as a chance to add any answers that were not imported last time. Don't muck with any that were, but if new answer ids are in the list, use the re-import of a known question to add answers.	356	2019-12-06 07:02:38.421083+00	4	4	1	422	1091	2229	\N	0	0	0	\N	f	f	2	\N
2	165	260	2019-07-01 15:03:44+00	SQL Server error 242 with ANSI datetime	I'm trying to debug a MS SQL Server error 242:\r\n\r\n>The conversion of a char data type to a datetime data type resulted in an out-of-range datetime value.\r\n\r\nThe error is originated by this statement:\r\n\r\n```sql\r\nCREATE TABLE db.schema.test (\r\n\tColumn1 datetime NULL\r\n) GO\r\n\r\nINSERT INTO db.schema.test (Column1)\r\nVALUES (convert(varchar,convert(datetime,{D '2019-06-30'}),102));\r\n```\r\n\r\nThis is my server version:\r\n\r\n    Microsoft SQL Server 2017 (RTM-CU15) (KB4498951) - 14.0.3162.1 (X64) \r\n    \tMay 15 2019 19:14:30 \r\n    \tCopyright (C) 2017 Microsoft Corporation\r\n    \tStandard Edition (64-bit) on Linux (Debian GNU/Linux 9 (stretch))\r\n\r\nAnd the language `@@LANGUAGE` is `Italiano`.\r\n\r\nThe same statements works on \r\n\r\n    Microsoft SQL Server 2017 (RTM-CU10) (KB4342123) - 14.0.3037.1 (X64)\r\n       Jul 27 2018 09:40:27\r\n       Copyright (C) 2017 Microsoft Corporation\r\n       Standard Edition (64-bit) on Windows Server 2012 R2 Standard 6.3 <X64> (Build 9600: ) (Hypervisor) \r\n\r\nWith language: `us_english`\r\n\r\nWithout without making explicit convert style it works\r\n\r\n```sql\r\nINSERT INTO db.schema.test (Column1)\r\nVALUES (convert(varchar,convert(datetime,{D '2019-06-30'})));\r\n```\r\n\r\nConvert style 102 is ANSI `yyyy.mm.dd`, shouldn't be recognized by both English and Italian SQL Server?	268	2019-12-04 03:30:26.999569+00	0	4	1	335	572	1499	241799	0	0	0	2019-12-04 03:30:26.999569+00	f	f	1	1
2	16	268	2019-09-04 18:40:19+00	Which costing factors go into the optimizer choosing different types of spools?	Spoolum\r\n--\r\nIn SQL Server there are several kinds of spools. The two that I'm interested are [Table Spool][1]s and [Index spools][2], *outside of modification queries*.\r\n\r\nRead only queries, particularly on the inner side of a Nested Loops join, may use either a Table or Index spool to potentially reduce I/O and improve query performance. These spools can be [Eager][3] or [Lazy][4]. Just like you and me.\r\n\r\nMy questions are:\r\n\r\n - Which factors go into the choice of Table vs. Index Spool\r\n - Which factors go into the choice between Eager and Lazy Spools\r\n\r\n\r\n  [1]: https://sqlserverfast.com/epr/table-spool/\r\n  [2]: https://sqlserverfast.com/epr/index-spool/\r\n  [3]: https://www.red-gate.com/simple-talk/sql/learn-sql-server/operator-of-the-week-spools-eager-spool/\r\n  [4]: https://www.red-gate.com/simple-talk/sql/learn-sql-server/showplan-operator-of-the-week-lazy-spool/	276	2019-12-04 14:13:22.586496+00	0	4	1	343	594	1503	246992	0	0	0	2019-12-04 14:13:22.586496+00	f	f	1	1
2	12	229	2018-09-01 14:22:32+00	Why doesn't join elimination work with sys.query_store_plan?	The following is a simplification of a performance problem encountered with the Query Store:\r\n\r\n    CREATE TABLE #tears\r\n    (\r\n        plan_id bigint NOT NULL\r\n    );\r\n    \r\n    INSERT #tears (plan_id) \r\n    VALUES (1);\r\n    \r\n    SELECT\r\n        T.plan_id\r\n    FROM #tears AS T\r\n    LEFT JOIN sys.query_store_plan AS QSP\r\n        ON QSP.plan_id = T.plan_id;\r\n\r\nThe `plan_id` column is documented as being the primary key of [`sys.query_store_plan`][1], but the execution plan does not use [join elimination][2] as would be expected:\r\n\r\n0. No attributes are being projected from the DMV.\r\n0. The DMV primary key `plan_id` cannot duplicate rows from the temporary table\r\n0. A `LEFT JOIN` is used, so no rows from `T` can be eliminated.\r\n\r\n[Execution plan][3]\r\n\r\n[![plan graphic][4]][4]\r\n\r\nWhy is this, and what can be done to obtain join elimination here?\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-query-store-plan-transact-sql?view=sql-server-2017\r\n  [2]: https://blogs.msmvps.com/robfarley/2008/11/08/join-simplification-in-sql-server/\r\n  [3]: https://www.brentozar.com/pastetheplan/?id=HyObYM_w7\r\n  [4]: https://i.stack.imgur.com/VHXax.png	237	2019-12-01 18:04:57.190024+00	0	4	1	304	482	723	216485	0	0	0	2019-12-01 18:04:57.190024+00	f	f	1	1
2	12	241	2019-01-28 12:15:34+00	When does SQL Server warn about an Excessive Memory Grant?	What are the conditions that produce an *"Excessive Grant"* execution plan warning?\r\n\r\n>The query memory grant detected "ExcessiveGrant", which may impact the reliability. Grant size: Initial 5128 KB, Final 5128 KB, Used 16 KB.\r\n\r\n### SSMS\r\n\r\n[![SSMS plan][1]][1]  \r\n\r\n[![SSMS properties][2]][2]  \r\n\r\n### Plan Explorer\r\n\r\n[![Plan Explorer][3]][3]  \r\n\r\n### Showplan xml\r\n\r\n```xml\r\n<Warnings>\r\n    <MemoryGrantWarning GrantWarningKind="Excessive Grant"\r\n        RequestedMemory="5128" GrantedMemory="5128" MaxUsedMemory="16" />\r\n</Warnings>\r\n```\r\n\r\n  [1]: https://i.stack.imgur.com/99Lwu.png\r\n  [2]: https://i.stack.imgur.com/1oGc6.png\r\n  [3]: https://i.stack.imgur.com/0lJLp.png	249	2019-12-01 21:44:03.116329+00	0	4	1	316	508	1491	228237	0	0	0	2019-12-01 21:44:03.116329+00	f	f	1	1
2	205	313	2017-03-16 22:31:56+00	Getting a list of active objects that use cursors but don't explicitly close/deallocate them	Sometimes our developers will write a query that uses cursors, but doesn't close them out explicitly. I'm trying to generate a list of active objects in my production database that use cursors but don't explicitly close/deallocate them. To do this I've written a simple statement that does the job, but is extremely slow:\r\n\r\n    select distinct name, \r\n                definition\r\n    from   SYS.SQL_MODULES \r\n       inner join SYS.OBJECTS O \r\n               on SQL_MODULES.OBJECT_ID = O.OBJECT_ID \r\n    where  SQL_MODULES.DEFINITION like '%open%'\r\n       and SQL_MODULES.DEFINITION like '%declare % cursor%'\r\n       and ( SQL_MODULES.DEFINITION not like '%close%' \r\n              or SQL_MODULES.DEFINITION not like '%deallocate%' )\r\n\r\nCurrently this takes something like 3 minutes to run. Is there a better way to get the information I'm looking for?	321	2019-12-05 16:44:06.653866+00	0	4	1	388	719	1549	167405	0	0	0	2019-12-05 16:44:06.653866+00	f	f	1	1
2	45	189	2017-05-18 13:47:56+00	Why doesn't this query use an index spool?	I'm asking this question in order to better understand the optimizer's behavior and to understand the limits around index spools. Suppose that I put integers from 1 to 10000 into a heap:\r\n\r\n    CREATE TABLE X_10000 (ID INT NOT NULL);\r\n    truncate table X_10000;\r\n    \r\n    INSERT INTO X_10000 WITH (TABLOCK)\r\n    SELECT TOP 10000 ROW_NUMBER() OVER (ORDER BY (SELECT NULL))\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2;\r\n\r\nAnd force a nested loop join with `MAXDOP 1`:\r\n\r\n    SELECT *\r\n    FROM X_10000 a\r\n    INNER JOIN X_10000 b ON a.ID = b.ID\r\n    OPTION (LOOP JOIN, MAXDOP 1);\r\n\r\nThis is a rather unfriendly action to take towards SQL Server. Nested loop joins often aren't a good choice when both tables don't have any relevant indexes. Here's the plan:\r\n\r\n[![bad query][1]][1]\r\n\r\nThe query takes 13 seconds on my machine with 100000000 rows fetched from the table spool. However, I don't see why the query has to be slow. The query optimizer has the ability to create indexes on the fly through [index spools][2]. This query seems like it would be a perfect candidate for an index spool.\r\n\r\nThe following query returns the same results as the first one, has an index spool, and finishes in less than a second:\r\n\r\n    SELECT *\r\n    FROM X_10000 a\r\n    CROSS APPLY (SELECT TOP (9223372036854775807) b.ID FROM X_10000 b WHERE a.ID = b.ID) ca\r\n    OPTION (LOOP JOIN, MAXDOP 1);\r\n\r\n[![workaround 1][3]][3]\r\n\r\nThis query also has an index spool and finishes in less than a second:\r\n\r\n    SELECT *\r\n    FROM X_10000 a\r\n    INNER JOIN X_10000 b ON a.ID >= b.ID AND a.ID <= b.ID\r\n    OPTION (LOOP JOIN, MAXDOP 1);\r\n\r\n[![workaround 2][4]][4]\r\n\r\nWhy doesn't the original query have an index spool? Is there any set of documented or undocumented hints or trace flags that will give it an index spool? I did find [this related question][5], but it doesn't fully answer my question and I can't get the mysterious trace flag to work for this query.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/9QxCV.png\r\n  [2]: https://www.simple-talk.com/sql/learn-sql-server/operator-of-the-week-index-spool/\r\n  [3]: https://i.stack.imgur.com/aUZwZ.png\r\n  [4]: https://i.stack.imgur.com/WJIit.png\r\n  [5]: https://dba.stackexchange.com/questions/30158/forcing-an-index-spool\r\n	196	2019-11-28 16:36:19.304702+00	0	4	1	264	398	1478	173939	0	0	0	2019-11-28 16:36:19.304702+00	f	f	1	1
2	664	466	2013-03-13 10:09:36+00	How do I insert one or multiple default rows?	If I create a table with an `identity` column as primary key, and all the other columns have default values, for example:\r\n\r\n    create table rr\r\n    (\r\n        id int identity(1,1) primary key, \r\n        dt datetime default getdate()\r\n    );	477	2019-12-19 13:07:00.55653+00	0	4	1	541	1137	2391	36553	0	0	0	2019-12-19 13:05:18.202805+00	f	f	1	1
2	90	297	2013-02-04 07:58:37+00	Cast to date is sargable but is it a good idea?	In SQL Server 2008 the [date](http://msdn.microsoft.com/en-us/library/bb630352.aspx) datatype was added.\r\n\r\nCasting a `datetime` column to `date` is [sargable](https://dba.stackexchange.com/questions/162263/what-does-the-word-sargable-really-mean) and can use an index on the `datetime` column.\r\n\r\n    select *\r\n    from T\r\n    where cast(DateTimeCol as date) = '20130101';\r\n\r\nThe other option you have is to use a range instead.\r\n\r\n    select *\r\n    from T\r\n    where DateTimeCol >= '20130101' and\r\n          DateTimeCol < '20130102'\r\n\r\n\r\nAre these queries equally good or should one be preferred over the other?\r\n\r\n	305	2019-12-04 22:47:38.941886+00	0	4	1	372	654	1537	34047	0	0	0	2019-12-04 22:47:38.941886+00	f	f	1	1
2	187	289	2019-08-27 11:35:04+00	Force SQL Server to use fragmented indexes?	I have a quite large table (>10M rows) with frequent crud operations. It has proper indexes, but they get fragmented quickly. Without a periodic index reorganize/rebuild maintenance plan, the index fragmentation can easily go over 90%. \r\n\r\nRight now I've addressed this by reorganizing the indexes on a daily basis and a rebuild on a weekly basis. I've also played around with fill factors etc to keep the fragmentation lower.\r\n\r\nMy main problem is that when the indexes gets too fragmented, SQL Server ignores the indexes and performs full table scans instead. When that happens, it almost kills the application since repeatedly scans of such large table is really heavy. I'm quite sure using a fragmented index would be faster in those cases.\r\n\r\nIs there a way to let SQL Server use the index regardless of its fragmentation? 	297	2019-12-04 14:31:40.879292+00	0	4	1	364	620	897	246355	0	0	0	2019-12-04 14:31:40.879292+00	f	f	1	1
2	185	286	2018-09-05 17:57:27+00	Best way to put commas into large numbers	I've started a new job and it involves looking at a bunch of big numbers. Is there an easy way to add commas to an `int` or `decimal` field to make it readable?\r\n\r\nFor example, SQL Server outputs the column on the left, but for my own sanity, I need it to look like the one on the right:\r\n\r\n    2036150 -> 2,036,150  \r\n\r\n...or would I have to write some heinous \r\n\r\n    left(right(vandalized_data),6),3) + ',' + right(left(vandalized_data),6),3)\r\n\r\nfunction?\r\n\r\nThe perfect thing would be commas in the display grid, then plain integers in the output.	294	2019-12-04 14:29:33.585979+00	0	4	1	361	617	847	216816	0	0	0	2019-12-04 14:29:33.585979+00	f	f	1	1
2	183	282	2019-09-28 06:57:53+00	Does SQL Server's serializable isolation level lock entire table	Me and a colleague of mine discussed the implications of use of the serializable isolation level. He said it locked the entire table, but I disagreed to that telling him it potentially could but it tries to apply range locks and it doesn't apply true serialization as explained here: [The Serializable Isolation Level][1].\r\n\r\nI can't find anything in the docs either for the "locks entire table": [SET TRANSACTION ISOLATION LEVEL][2]. \r\n\r\nThe doc states a bunch of things regarding range locks, so in theory you could lock the entire table by simply having a range lock that locks the entire range of possible values in the table, but it doesn't lock the table.\r\n\r\nAm I completely wrong here?  Does it in fact lock the entire table (or tables)? \r\n\r\n\r\n  [1]: https://sqlperformance.com/2014/04/t-sql-queries/the-serializable-isolation-level\r\n  [2]: https://docs.microsoft.com/en-us/sql/t-sql/statements/set-transaction-isolation-level-transact-sql?view=sql-server-2017	290	2019-12-04 14:25:33.785697+00	0	4	1	357	611	1523	249838	0	0	0	2019-12-04 14:25:33.785697+00	f	f	1	1
1	854	648	2020-01-30 11:06:44.465239+00	New question copies previous one	I asked a question before, then I clicked to ask another question and it filled in both the title and content from the previous question, rather than leaving them empty. What's going on?!	663	2020-01-30 11:06:44.465239+00	10	4	1	723	1902	3608	\N	0	0	0	\N	f	f	2	\N
4	290	415	2016-09-04 12:23:27+00	Long headline with Beamer/Darmstadt	I like the Darmstadt theme in Beamer, but my presentation has to many sections for the headline. I tried to find an elegant solution, without success so far.\r\n\r\nDo you know if it is possible to show only a part of the headline? My idea is to have a headline that shows as many sections as possible around (ie before and/or after) the current section.\r\n\r\nThank you for your help.\r\n\r\nCheers\r\n\r\nHere is a MWE:\r\n\r\n    \\documentclass{beamer}\r\n\r\n    \\usetheme{Darmstadt}\r\n\r\n    \\begin{document}\r\n\r\n    \\section{Section 1}\r\n    \\subsection{Subsection 1.1}\r\n    \\begin{frame}{Frame 1.11}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 1.12}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 1.13}\r\n    ...\r\n    \\end{frame}\r\n    \\subsection{Subsection 1.2}\r\n    \\begin{frame}{Frame 1.21}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 1.22}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 1.23}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 2}\r\n    \\begin{frame}{Frame 2}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 3}\r\n    \\begin{frame}{Frame 3}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 4}\r\n    \\begin{frame}{Frame 4}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 5}\r\n    \\begin{frame}{Frame 5}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 6}\r\n    \\begin{frame}{Frame 6}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 7}\r\n    \\begin{frame}{Frame 7}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 8}\r\n    \\begin{frame}{Frame 8}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 9}\r\n    \\subsection{Subsection 9.1}\r\n    \\begin{frame}{Frame 9.11}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 9.12}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 9.13}\r\n    ...\r\n    \\end{frame}\r\n    \\subsection{Subsection 9.2}\r\n    \\begin{frame}{Frame 9.21}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 9.22}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 9.23}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \r\n    \\section{Section 10}\r\n    \\begin{frame}{Frame 10}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 11}\r\n    \\begin{frame}{Frame 11}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 12}\r\n    \\begin{frame}{Frame 12}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 13}\r\n    \\begin{frame}{Frame 13}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 14}\r\n    \\begin{frame}{Frame 14}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 15}\r\n    \\begin{frame}{Frame 15}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 16}\r\n    \\subsection{Subsection 16.1}\r\n    \\begin{frame}{Frame 16.11}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 16.12}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 16.13}\r\n    ...\r\n    \\end{frame}\r\n    \\subsection{Subsection 16.2}\r\n    \\begin{frame}{Frame 16.21}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 16.22}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 16.23}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 17}\r\n    \\begin{frame}{Frame 17}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 18}\r\n    \\begin{frame}{Frame 18}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 19}\r\n    \\subsection{Subsection 19.1}\r\n    \\begin{frame}{Frame 19.11}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 19.12}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 19.13}\r\n    ...\r\n    \\end{frame}\r\n    \\subsection{Subsection 19.2}\r\n    \\begin{frame}{Frame 19.21}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 19.22}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 19.23}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 20}\r\n    \\begin{frame}{Frame 20}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\end{document}	426	2019-12-11 15:54:46.759713+00	0	4	1	490	990	1734	327914	0	0	0	2019-12-11 15:54:46.759713+00	f	f	1	2
2	906	709	2019-11-17 12:22:24+00	After upgrading to SQL Server 2019, function throws “insufficient memory” error	I have moved an SQL Server 2012 Database (8GB in size) to a newly setup SQL Server 2019 virtual machine with the same memory and CPU configuration and changed the compatibility level to SQL Server 2019. \r\n\r\nEverything in my application works fine except for one stored procedure that consists of one big SQL query with two parameters (and no fancy options). When this SP executed, it lets the memory of the SQL Server process go up to the specified max level and then returns an error:\r\n\r\n> "There is insufficient memory to run this query"\r\n\r\nWhen I execute the SQL query (inside the stored procedure) in a separate query window of SSMS, it executes in no-time and returns the expected 300 rows. Also, when I change the DB's compatibility level to "SQL Server 2017" and execute the stored procedure, everything is ok.\r\n\r\nI first thought it might be a parameter sniffing issue, but none of the workarounds helped (e.g. `OPTION (RECOMPILE)`).\r\n\r\nI have drilled down the problem to the call of a scalar valued function. Every time I call this function, the memory error occurs.\r\n\r\nHere is the DDL of the function (sorry, partly in German):\r\n\r\n\tCREATE FUNCTION [dbo].[GetWtmTime] (\r\n\t\t@WorkTimeModelID uniqueidentifier,\r\n\t\t@Date DATETIME,\r\n\t\t@SequenceNo TINYINT)\r\n\t  RETURNS VARCHAR(5)\r\n\tAS\r\n\tBEGIN\r\n\t\t-- SET DATEFIRST 7; has to be executed before calling this function\r\n\t\tDECLARE @WtmTime VARCHAR(5)\r\n\t\tDECLARE @WtmWeeks INT\r\n\t\tDECLARE @WtmTakeHolidays BIT\r\n\t\tDECLARE @WtmMaxMemberCount TINYINT\r\n\t\tSELECT @WtmWeeks = AnzahlWochen\r\n\t\t\t , @WtmTakeHolidays = ÜbernimmtFeiertage \r\n\t\t\t , @WtmMaxMemberCount = MaxAnzahlMitglieder\r\n\t\tFROM Arbeitszeitmodelle \r\n\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID;\r\n\t\tIF @WtmWeeks = 1\r\n\t\tBEGIN\r\n\t\t\tIF (dbo.IstFeiertag(@Date, 0) = 1     -- Holiday\r\n\t\t\t\tAND @WtmMaxMemberCount = 1)\r\n\t\t\tBEGIN\r\n\t\t\t\tIF @WtmTakeHolidays = 0\r\n\t\t\t\tBEGIN \r\n\t\t\t\t\tIF @Date >= '20130901'\r\n\t\t\t\t\t\tSET @WtmTime = 'KD'\r\n\t\t\t\t\tELSE\r\n\t\t\t\t\t\tSET @WtmTime = 'ZA';\r\n\t\t\t\tEND ELSE\r\n\t\t\t\tBEGIN\r\n\t\t\t\t\tIF EXISTS ( SELECT *\r\n\t\t\t\t\t\t\t\tFROM AzmWochen\r\n\t\t\t\t\t\t\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\t\t\t\t\t\t\tAND Folgenummer = @SequenceNo\r\n\t\t\t\t\t\t\t\t\tAND AzmZeitMo IN ('KD','T')\r\n\t\t\t\t\t\t\t\t\tAND AzmZeitDi IN ('KD','T')\r\n\t\t\t\t\t\t\t\t\tAND AzmZeitMi IN ('KD','T')\r\n\t\t\t\t\t\t\t\t\tAND AzmZeitDo IN ('KD','T')\r\n\t\t\t\t\t\t\t\t\tAND AzmZeitFr IN ('KD','T')\r\n\t\t\t\t\t\t\t\t\tAND AzmZeitSa IN ('KD','T')\r\n\t\t\t\t\t\t\t\t\tAND AzmZeitSo IN ('KD','T') )\r\n\t\t\t\t\t\tSET @WtmTime = 'T';\r\n\t\t\t\t\tELSE\r\n\t\t\t\t\t\tSET @WtmTime = 'G';\r\n\t\t\t\tEND\r\n\t\t\tEND ELSE IF DATEPART(dw, @Date) = 1\t\t\t\t-- Sunday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitSo FROM AzmWochen \r\n\t\t\t\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\t\t\t\tAND Folgenummer = @SequenceNo;\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 2\t\t\t\t-- Monday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitMo FROM AzmWochen \r\n\t\t\t\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\t\t\t\tAND Folgenummer = @SequenceNo;\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 3\t\t\t\t-- Tuesday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitDi FROM AzmWochen \r\n\t\t\t\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\t\t\t\tAND Folgenummer = @SequenceNo;\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 4\t\t\t\t-- Wednesday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitMi FROM AzmWochen \r\n\t\t\t\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\t\t\t\tAND Folgenummer = @SequenceNo;\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 5\t\t\t\t-- Thursday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitDo FROM AzmWochen \r\n\t\t\t\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\t\t\t\tAND Folgenummer = @SequenceNo;\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 6\t\t\t\t-- Friday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitFr FROM AzmWochen \r\n\t\t\t\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\t\t\t\tAND Folgenummer = @SequenceNo;\r\n\t\t\tELSE \t\t\t\t\t\t\t\t\t\t\t-- Saturday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitSa FROM AzmWochen \r\n\t\t\t\t\tWHERE ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\t\t\t\tAND Folgenummer = @SequenceNo;\r\n\t\tEND ELSE\r\n\t\tBEGIN\r\n\t\t\tDECLARE @NUMWEEKS INT\r\n\t\t\tSELECT @NUMWEEKS = DATEDIFF(week, CONVERT(CHAR(10), '01.01.2000', 104), @Date)\r\n\t\t\tIF DATEPART(dw, @Date) = 1 \r\n\t\t\t\tSET @NUMWEEKS = @NUMWEEKS - 1;\r\n\t\t\tDECLARE @WEEKNUMBER INT \r\n\t\t\tIF @NUMWEEKS % 2 = 0\r\n\t\t\t\tSET @WEEKNUMBER = 1\r\n\t\t\tELSE\r\n\t\t\t\tSET @WEEKNUMBER = 2;\r\n\t\t\tIF DATEPART(dw, @Date) = 1\t\t\t\t-- Sunday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitSo FROM AzmWochen \r\n\t\t\t\t\tWHERE Folgenummer = @SequenceNo AND Wochennummer = @WEEKNUMBER \r\n\t\t\t\t\t  AND ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 2\t\t-- Monday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitMo FROM AzmWochen \r\n\t\t\t\t\tWHERE Folgenummer = @SequenceNo AND Wochennummer = @WEEKNUMBER \r\n\t\t\t\t\t  AND ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 3\t\t-- Tuedsay\r\n\t\t\t\tSELECT @WtmTime = AzmZeitDi FROM AzmWochen \r\n\t\t\t\t\tWHERE Folgenummer = @SequenceNo AND Wochennummer = @WEEKNUMBER \r\n\t\t\t\t\t  AND ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 4\t\t-- Wednesday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitMi FROM AzmWochen \r\n\t\t\t\t\tWHERE Folgenummer = @SequenceNo AND Wochennummer = @WEEKNUMBER \r\n\t\t\t\t\t  AND ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 5\t\t-- Thursday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitDo FROM AzmWochen \r\n\t\t\t\t\tWHERE Folgenummer = @SequenceNo AND Wochennummer = @WEEKNUMBER \r\n\t\t\t\t\t  AND ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\tELSE IF DATEPART(dw, @Date) = 6\t\t-- Friday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitFr FROM AzmWochen \r\n\t\t\t\t\tWHERE Folgenummer = @SequenceNo AND Wochennummer = @WEEKNUMBER \r\n\t\t\t\t\t  AND ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\t\tELSE\t\t\t\t\t\t\t\t\t-- Saturday\r\n\t\t\t\tSELECT @WtmTime = AzmZeitSa FROM AzmWochen \r\n\t\t\t\t\tWHERE Folgenummer = @SequenceNo AND Wochennummer = @WEEKNUMBER \r\n\t\t\t\t\t  AND ArbeitszeitmodellID = @WorkTimeModelID\r\n\t\tEND\r\n\t\tIF @Date >= '20130901' AND @WtmTime = 'ZA'\r\n\t\t\tSET @WtmTime = 'KD';\r\n\r\n\t\tRETURN @WtmTime;\r\n\tEND\r\n\r\n\r\n\tCREATE FUNCTION [dbo].[IstFeiertag] (\r\n\t\t@Datum DATETIME,\r\n\t\t@IstEvangelisch BIT)\r\n\t  RETURNS INT\r\n\tAS\r\n\tBEGIN\r\n\t\tDECLARE @I INT\r\n\t\tDECLARE @Y INT\r\n\t\tDECLARE @A INT\r\n\t\tDECLARE @B INT\r\n\t\tSET @I = DATEPART(year, @Datum) / 100 - DATEPART(year, @Datum) / 400 + 4;\r\n\t\tSET @Y = @I - DATEPART(year, @Datum) / 300 + 11;\r\n\t\tSET @A = (((DATEPART(year, @Datum) % 19) * 19) + @Y) % 30;\r\n\t\tSET @B = (((DATEPART(year, @Datum) % 4) * 2 + 4 * DATEPART(year, @Datum) + 6 * @A + @I) % 7) + @A - 9;\r\n\t\tDECLARE @OstTag INT\r\n\t\tDECLARE @OstMon INT\r\n\t\tIF @B < 1\r\n\t\tBEGIN\r\n\t\t\tSET @OstTag = 31 + @B\r\n\t\t\tSET @OstMon = 3\r\n\t\tEND ELSE\r\n\t\tBEGIN\r\n\t\t\tIF ((@B = 26) OR ((@A = 28) AND (@B = 25) AND ((11 * (@Y + 1) % 30) < 19)))\r\n\t\t\tBEGIN\r\n\t\t\t\tSET @B = @B - 7;\r\n\t\t\tEND\r\n\t\t\tSET @OstTag = @B\r\n\t\t\tSET @OstMon = 4\r\n\t\tEND\r\n\r\n\t\tDECLARE @Ostersonntag DATETIME\r\n\t\tSET @Ostersonntag = dbo.CreateDate(DATEPART(year, @Datum), @OstMon, @OstTag)\r\n\r\n\t\tIF @Datum >= @Ostersonntag\r\n\t\tBEGIN\r\n\t\t\tDECLARE @TAGE INT\r\n\t\t\tSET @TAGE = DATEDIFF(day, @Ostersonntag, @Datum)\r\n\t\t\tIF @TAGE = 0 OR @TAGE = 1 OR @TAGE = 39 OR @TAGE = 50 OR @TAGE = 60\r\n\t\t\tBEGIN\r\n\t\t\t\tRETURN 1\r\n\t\t\tEND\r\n\t\tEND\r\n\t\tDECLARE @TEMP INT \r\n\t\tSET @TEMP = DATEPART(month, @Datum) * 100 + DATEPART(day, @Datum)\r\n\t\tIF @TEMP = 101 OR @TEMP = 106 OR @TEMP = 501 OR @TEMP = 815 OR @TEMP = 1026\r\n\t\t\t OR @TEMP = 1101 OR @TEMP = 1208 OR @TEMP = 1225 OR @TEMP = 1226\r\n\t\tBEGIN\r\n\t\t\tRETURN 1\r\n\t\tEND \r\n\t\tRETURN 0\r\n\tEND\r\n\tGO\r\n\r\n\r\n\tCREATE FUNCTION [dbo].[CreateDate] (\r\n\t\t@Year int, \r\n\t\t@Month int, \r\n\t\t@Day int)\r\n\t  RETURNS DATETIME\r\n\tAS\r\n\tBEGIN\r\n\t\tdeclare @d datetime;\r\n\t\tset @d = dateadd(year,(@Year - 1753),'1/1/1753');\r\n\t\tset @d = dateadd(month,@Month - 1,@d);\r\n\t\treturn dateadd(day,@Day - 1,@d)\r\n\tEND\r\n\tGO\r\n\r\nThese are the table definitions (in German):\r\n\r\n\r\n    CREATE TABLE [dbo].[Arbeitszeitmodelle]\r\n    (\r\n    \t[ArbeitszeitmodellID] uniqueidentifier ROWGUIDCOL NOT NULL \r\n    \t\tCONSTRAINT [DF_Arbeitszeitmodelle_ArbeitszeitmodellID] DEFAULT (newid()) \r\n    \t\tCONSTRAINT [PK_Arbeitszeitmodelle_ArbeitszeitmodellID] PRIMARY KEY CLUSTERED,\r\n    \t[Name] nvarchar(25) NOT NULL,\t\r\n    \t[MaxAnzahlMitglieder]  tinyint NOT NULL \r\n\t\tCONSTRAINT [CK_Arbeitszeitmodelle_MaxAnzahlMitglieder] CHECK (([MaxAnzahlMitglieder] > 0) AND ([MaxAnzahlMitglieder] < 10)), \r\n    \t[AnzahlWochen] tinyint NOT NULL\r\n\t\tCONSTRAINT [CK_Arbeitszeitmodelle_AnzahlWochen] CHECK (([AnzahlWochen] > 0) AND ([AnzahlWochen] < 5)),\r\n    \t[ÜbernimmtFeiertage] bit \r\n    );\r\n\r\n    CREATE TABLE [dbo].[AzmWochen]\r\n    (\r\n    \t[AzmWochenID] uniqueidentifier ROWGUIDCOL NOT NULL \r\n    \t\tCONSTRAINT [DF_AzmWochen_AzmWochenID] DEFAULT (newid()) \r\n    \t\tCONSTRAINT [PK_AzmWochen_AzmWochenID] PRIMARY KEY CLUSTERED,\r\n    \t[Folgenummer] tinyint NOT NULL\r\n    \t\tCONSTRAINT [CK_AzmWochen_Folgenummer] CHECK (([Folgenummer] > 0) AND ([Folgenummer] < 10)),\t\t\r\n        \t[Wochennummer] tinyint NOT NULL\r\n    \t\tCONSTRAINT [CK_AzmWochen_Wochennummer] CHECK (([Wochennummer] > 0) AND ([Wochennummer] < 3)),\r\n        \t[ArbeitszeitmodellID] uniqueidentifier NOT NULL \r\n    \t\tCONSTRAINT [FK_AzmWochen_ArbeitszeitmodellID] FOREIGN KEY ([ArbeitszeitmodellID]) REFERENCES [dbo].[Arbeitszeitmodelle] ([ArbeitszeitmodellID]) ON UPDATE CASCADE ON DELETE CASCADE,\t\r\n    \t[AzmZeitMo] varchar(5) NOT NULL,\r\n    \t[AzmZeitDi] varchar(5) NOT NULL,\r\n    \t[AzmZeitMi] varchar(5) NOT NULL,\r\n    \t[AzmZeitDo] varchar(5) NOT NULL,\r\n    \t[AzmZeitFr] varchar(5) NOT NULL,\r\n    \t[AzmZeitSa] varchar(5) NOT NULL,\r\n    \t[AzmZeitSo] varchar(5) NOT NULL\r\n    );\r\n    ALTER TABLE AzmWochen ADD CONSTRAINT [UQ_AzmWochen_FolgeWochen] UNIQUE ([ArbeitszeitmodellID] ASC, [Folgenummer] ASC, [Wochennummer] ASC);\r\n\r\nI tried the hints:\r\n\r\n* `OPTION(USE HINT('FORCE_LEGACY_CARDINALITY_ESTIMATION'))`\r\n* `OPTION(USE HINT('QUERY_OPTIMIZER_COMPATIBILITY_LEVEL_140'))`\r\n\r\n... but they did not prevent the error.\r\n\r\nI inserted the two tables, test data, and the functions (`GetWtmTime` depends on two other scalar functions) into an empty test database and was able to execute the function twice. Then I again got the memory error.	724	2020-02-14 04:19:38.889408+00	0	4	1	784	2180	4165	253499	0	0	0	2020-02-14 04:19:38.889408+00	f	f	1	1
2	67	103	2014-12-17 18:30:53+00	In SQL Server, can I guarantee an order without an explicit ORDER BY clause when an index seek is forced on a table with only a clustered index?	**Update 2014-12-18** \r\n\r\nWith the overwhelming response to the main question being "No", the more interesting responses have focused on part 2, how to solve the performance puzzle with an explicit `ORDER BY`.  Although I've marked an answer already, I wouldn't be surprised if there were an even better performing solution.\r\n\r\n**Original**\r\n\r\nThis question arose because the only extremely fast solution I could find to a particular problem only works without an `ORDER BY` clause. Below is the full T-SQL needed to produce the problem, along with my proposed solution (I am using SQL Server 2008 R2, if that matters.)\r\n\r\n    --Create Orders table\r\n    IF OBJECT_ID('tempdb..#Orders') IS NOT NULL DROP TABLE #Orders\r\n    CREATE TABLE #Orders\r\n    (  \r\n           OrderID    INT NOT NULL IDENTITY(1,1)\r\n         , CustID     INT NOT NULL\r\n         , StoreID    INT NOT NULL       \r\n         , Amount     FLOAT NOT NULL\r\n    )\r\n    CREATE CLUSTERED INDEX IX ON #Orders (StoreID, Amount DESC, CustID)\r\n    \r\n    --Add 1 million rows w/ 100K Customers each of whom had 10 orders\r\n    ;WITH  \r\n        Cte0 AS (SELECT 1 AS C UNION ALL SELECT 1), --2 rows  \r\n        Cte1 AS (SELECT 1 AS C FROM Cte0 AS A, Cte0 AS B),--4 rows  \r\n        Cte2 AS (SELECT 1 AS C FROM Cte1 AS A ,Cte1 AS B),--16 rows \r\n        Cte3 AS (SELECT 1 AS C FROM Cte2 AS A ,Cte2 AS B),--256 rows \r\n        Cte4 AS (SELECT 1 AS C FROM Cte3 AS A ,Cte3 AS B),--65536 rows \r\n        Cte5 AS (SELECT 1 AS C FROM Cte4 AS A ,Cte2 AS B),--1048576 rows \r\n        FinalCte AS (SELECT  ROW_NUMBER() OVER (ORDER BY C) AS Number FROM   Cte5)\r\n    INSERT INTO #Orders (CustID, StoreID, Amount)\r\n    SELECT CustID = Number / 10\r\n         , StoreID    = Number % 4\r\n         , Amount     = 1000 * RAND(Number)\r\n    FROM  FinalCte\r\n    WHERE Number <= 1000000\r\n    \r\n    SET STATISTICS IO ON\r\n    SET STATISTICS TIME ON\r\n    \r\n    --For StoreID = 1, find the top 500 customers ordered by their most expensive purchase (Amount)\r\n    \r\n    --Solution A: Without ORDER BY\r\n    DECLARE @Top INT = 500\r\n    SELECT DISTINCT TOP (@Top) CustID\r\n    FROM #Orders WITH(FORCESEEK)\r\n    WHERE StoreID = 1\r\n    OPTION(OPTIMIZE FOR (@Top = 1), FAST 1);\r\n    --9 logical reads, CPU Time = 0 ms, elapsed time = 1 ms\r\n    GO\r\n    --Solution B: With ORDER BY\r\n    DECLARE @Top INT = 500\r\n    SELECT TOP (@Top) CustID\r\n    FROM #Orders\r\n    WHERE StoreID = 1\r\n    GROUP BY CustID\r\n    ORDER BY MAX(Amount) DESC\r\n    OPTION(MAXDOP 1)\r\n    --745 logical reads, CPU Time = 141 ms, elapsed time = 145 ms\r\n    --Uses Sort operator\r\n    \r\n    GO\r\n\r\nHere are the execution plans for Solution A and B, respectively:\r\n\r\n![Sol A][1]\r\n\r\n![Sol B][2]\r\n\r\nSolution A gives the performance I need, but I couldn't get it to work with the same performance when adding any kind ORDER BY clause (e.g., see Solution B).  And it certainly seems like Solution A would have to deliver its results in order, since 1) the table has only one index on it, 2) a seek is forced, thus eliminating the possibility of its using an allocation order scan based on IAM pages.  \r\n\r\nSo my questions are:\r\n\r\n1. Am I right that it will guarantee the order in this case without an order by clause?\r\n\r\n2. If not, is there another method to force a plan that is as fast as Solution A, preferably one that avoids sorts? Note that it would have to solve the exact same problem (for `StoreID = 1`, find the top 500 customers ordered by their most expensive purchase amount). It would also have to still use the `#Orders` table, but different indexing schemes would be OK.\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/PhZLn.jpg\r\n  [2]: http://i.stack.imgur.com/K3kZJ.jpg	110	2019-11-25 13:54:52.003297+00	0	4	1	178	262	245	86596	0	0	0	2019-11-25 13:54:52.003297+00	f	f	1	1
2	191	296	2017-08-10 19:23:37+00	Best way to shrink a DB after nulling data from varbinary(max)?	We have a database with a large amount of data stored in a field of type **varbinary(max)**. At some point we can purge that data for most rows, but not all. Our plan is to make that field nullable, and simply null out the data when it is no longer needed. Once we do that we would like to reduce the size of the DB. What is the best way to accomplish this? \r\n\r\nIf there isn't a good way to reclaim space with the current setup, one idea I have is to move that data field to a separate table with just two columns: the key to the main table and the data field. Then we could simply delete the rows when they are no longer needed. (And then do some sort of shrink.) However, this would be a much more difficult change to make than simply making the existing field nullable.\r\n\r\nNote: I actually don't care so much about making the database file smaller, but I do care about the newly freed up space becoming re-usable.\n\nOver 90% of the DB size is this one field. I'm at 3TB already.	304	2019-12-04 22:46:01.46469+00	0	4	1	371	653	1535	183215	0	0	0	2019-12-04 22:46:01.46469+00	f	f	1	1
2	33	293	2019-12-04 11:06:25+00	Insert multiple rows into a table with only an IDENTITY column	I have a table called `dbo.Groups` defined like this:\r\n\r\n```\r\nCREATE TABLE dbo.Groups\r\n(\r\n  GroupID int NOT NULL IDENTITY (1,1) PRIMARY KEY\r\n);\r\n```\r\n\r\nThe table really consists of just the one `IDENTITY` column.\r\n\r\nSometimes I want to insert into it multiple rows at once and get the generated IDs. (I already have a pre-defined table variable called `@output` with a single `ID` column to be used in the `OUTPUT` clause.)\r\n\r\nNow I know how I would proceed if it was a *single* row:\r\n\r\n```\r\nINSERT INTO\r\n  dbo.GroupID\r\nOUTPUT\r\n  inserted.GroupID INTO @output (ID)\r\nDEFAULT VALUES\r\n;\r\n```\r\n\r\nBut I want to be able to insert two or more in one go. The actual number is determined by the number of rows returned by this query:\r\n\r\n```\r\nSELECT\r\n  *\r\nFROM\r\n  dbo.MySource\r\n;\r\n```\r\n\r\nSo if the query returns one row, I want to insert one row into `dbo.Groups` and return the generated `GroupID`. If it is a hundred rows, then I would expect a hundred rows inserted and a hundred IDs generated and returned at once.\r\n\r\nOne obvious method is to insert one row at a time in a loop. I would like to avoid that and use a set-based approach instead, something along the lines of\r\n\r\n```\r\nINSERT INTO\r\n  dbo.GroupID\r\nOUTPUT\r\n  inserted.GroupID INTO @output (ID)\r\nSELECT\r\n  ...  -- what?\r\nFROM\r\n  dbo.MySource\r\n;\r\n```\r\n\r\nIs there a way to insert multiple rows into a table with just an `IDENTITY` column in (preferably) a single statement?	301	2019-12-04 17:45:52.717911+00	0	4	1	368	636	930	254771	0	0	0	2019-12-04 17:41:36.959582+00	f	f	1	1
4	202	343	2018-08-22 06:06:13+00	Aligning a text in 3d diagram	> **This question led to a new library in TikZ:**  \r\n> [`perspective` (PGF manual, chapter 63)](https://ctan.org/pkg/pgf?lang=en)\r\n\r\nI have a 3d object created with `inkscape` as shown below:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nNow I am trying to place a text in the same orientation as the light-blue colored area. However, I want to insert the text with `latex` instead of directly embedding it in the `pdf` (hence, I save it with `pdf+Latex` option in `inkscape`). \r\n\r\nBut when I try to insert the text in the 3d area, I get:\r\n\r\n[![!\\[enter image description here][2]][2]\r\n\r\nHow can I get `OMEAG` aligned parallely to the light-blue region.\r\n\r\nI could not attach the pdf of it, however the `MWE` is below:\r\n\r\n    %&lualatex\r\n    % !TeX program = lualatex\r\n    \\documentclass[11pt,a4paper]{article}\r\n    %\\usepackage[latin1]{inputenc}\r\n    \\usepackage{amsmath}\r\n    \\usepackage{amsfonts}\r\n    \\usepackage{amssymb}\r\n    \\usepackage{graphicx}\r\n    \\usepackage{xcolor}\r\n    \\usepackage{pgfplots}\r\n    \\usepackage{pstricks}    %for embedding pspicture.\r\n    \\pgfplotsset{compat=newest}\r\n    \\usepackage{tikz}\r\n    \\begin{document}\r\n    \t\\begin{figure}[h]\r\n    \t\\centering{\r\n    \t\t\\input{drawing4.pdf_tex}\r\n    \t\t\\caption{Top view.}\r\n    \t\t\\label{fig:aktomnpView}\r\n    \t}\r\n    \\end{figure}\r\n    \\end{document}\r\n\r\nand `\\input{drawing4.pdf_tex}` is below:\r\n\r\n    \\begingroup%\r\n      \\makeatletter%\r\n      \\providecommand\\color[2][]{%\r\n        \\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}%\r\n        \\renewcommand\\color[2][]{}%\r\n      }%\r\n      \\providecommand\\transparent[1]{%\r\n        \\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}%\r\n        \\renewcommand\\transparent[1]{}%\r\n      }%\r\n      \\providecommand\\rotatebox[2]{#2}%\r\n      \\newcommand*\\fsize{\\dimexpr\\f@size pt\\relax}%\r\n      \\newcommand*\\lineheight[1]{\\fontsize{\\fsize}{#1\\fsize}\\selectfont}%\r\n      \\ifx\\svgwidth\\undefined%\r\n        \\setlength{\\unitlength}{303.69978591bp}%\r\n        \\ifx\\svgscale\\undefined%\r\n          \\relax%\r\n        \\else%\r\n          \\setlength{\\unitlength}{\\unitlength * \\real{\\svgscale}}%\r\n        \\fi%\r\n      \\else%\r\n        \\setlength{\\unitlength}{\\svgwidth}%\r\n      \\fi%\r\n      \\global\\let\\svgwidth\\undefined%\r\n      \\global\\let\\svgscale\\undefined%\r\n      \\makeatother%\r\n      \\begin{picture}(1,1.35089637)%\r\n        \\lineheight{1}%\r\n        \\setlength\\tabcolsep{0pt}%\r\n        \\put(0,0){\\includegraphics[width=\\unitlength,page=1]{drawing4.pdf}}%\r\n        \\put(0.54334545,0.71256022){\\color[rgb]{0,0,0}\\makebox(0,0)[lt]{\\lineheight{1.25}\\smash{\\begin{tabular}[t]{l}OMEAG\\end{tabular}}}}%\r\n      \\end{picture}%\r\n    \\endgroup%\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/jV1OM.png\r\n  [2]: https://i.stack.imgur.com/pPIj7.png	352	2019-12-06 06:25:54.979379+00	0	4	1	418	740	1099	447114	0	0	0	2019-12-06 06:25:54.979379+00	f	f	1	2
1	811	650	2020-01-30 12:55:31.971006+00	Answer summaries' markdown rendering fail on separated links	It seems that the summary is only based on the first line of post source, so when a link uses a "named" link, it fails.\r\n\r\nGo back to the post listing and look at the summary to see this bug in action.	665	2020-01-30 12:57:58.843335+00	8	1	1	725	1871	3559	\N	0	0	0	\N	f	f	2	\N
1	885	710	2020-02-14 06:55:11.40425+00	First impressions	Jack Douglas reached out to me via email to get my impression of TopAnswers. I replied to him via email, but it occurs to me this might be useful for a broader audience.\r\n\r\nI know TopAnswers is a work in progress and I haven't looked at your roadmap yet. So I might tell you something that is going to be fixed or has a reason that is not yet clear to the casual observer. \r\n\r\nLogin feels odd. I like the idea of having a key that you can save in a password manager rather than email/password or third party credentials. But it's something I've never seen before and feels tenuous. If I lose my key, I lose my account with no way to recover. I created an account and didn't save my key the first time. Thankfully I didn't do anything with the account, so it wasn't a big deal. If this is the longterm design, make sure people save their key before doing anything they might want to recover. (As in block they from doing anything until they acknowledge they have saved the key.)\r\n\r\nI love the chat window rather than comments. It's something we'd talked about forever, but never been able to get any traction. One idea was to make it something like Wikipedia's talk pages. The permanent sidebar chat room actually emphasizes comments/chat compared to comments after the posts. Maybe hiding the chat page until the user purposely reveals it? It's a balance between discoverability and distraction.\r\n\r\nChat feels cramped in the sidebar too. With a narrow window, chat needs a lot of scrolling to read. (Jack pointed out it is possible to resize, but the line you need to grab is pretty easy to miss.) It's also hard to get an idea of when chat items occurred in relation to the questions and answers. There's no anchor for "2 hours later". The transcript view is easier to use in those respects. It also isn't clear to me when a reply is to another chat message and when it's to a post.\r\n\r\nThe overall design seems, um, tabular. I find it hard to read the posts and I think it's got something to do with the harsh horizontal lines. I'm not sure why the author of the question is flush left and the author of answers is flush right. It would probably help to have someone with a strong design sense go over the layout. Not urgent, but this is the overwhelming impression and will make it harder to engage with visitors who don't know the project.\r\n\r\nThe three-column editor is confusing. It doesn't help that the question is jammed into a narrow column. Having the question on the page is a great idea, but it would be better for it to have the space it needs at the top of the page. We tried the side-by-side preview on Documentation and it was not well-liked. For a better implementation, consider https://stackedit.io.\r\n\r\nIs there a way to scroll code boxes when the are too narrow? All I could do was make my window larger.\r\n\r\nI like the lack of votes on questions. The lack of downvotes is interesting and I'm curious if it will work. No closing (that I can see) is a bold choice too. I assume the idea is to not add this sort of thing in immediately in hopes of solving the inevitable problems in a different way. Seems to be the same idea with the lack of reputation and privileges. I'm not sure how well the site will stick with people, but it doesn't seem too hard to add that sort of thing in later.\r\n\r\nI like the ability to import content from SE. I was looking for a way to import all my content at once, however. The licensing options are understandable given history, but they sure will be confusing to someone who just wants a database answer or post a question.\r\n\r\nTags are underdeveloped at the moment. I was expecting a way to find all the questions in a tag when I clicked on one. I suppose there is value in collecting the data for now. \r\n\r\nAnyway, those are my impressions right now. As a proof of concept, I like what I see. It's encouraging that Google is already picking up some of your answers and that there is activity on the meta site. \r\n	725	2020-02-14 16:22:53.890014+00	13	4	1	785	2189	4045	\N	0	0	0	\N	f	f	2	\N
2	82	99	2014-09-25 13:48:49+00	Are RANK() and DENSE_RANK() deterministic or non-deterministic?	According to [official Microsoft BOL][1] DENSE_RANK is nondeterministic ([RANK()][2]). But according to [Ranking Functions by Itzik Ben-Gan][3]  "... the RANK() and DENSE_RANK() functions are always deterministic". Who is right?\r\n\r\nWhat I have found so far:\r\n[Microsoft's Definition][4] "Deterministic functions always return the same result any time they are called with a specific set of input values and given the same state of the database."\r\n\r\nSo in Set theory tables\r\nEmployees\r\n\r\n    Employee\t        Salary\r\n    Sue Right\t         1.00\r\n    Robin Page\t         1.00\r\n    Phil Factor\t         1.00\r\n\r\nand\r\nEmployees2\r\n\r\n    Employee\t        Salary\r\n    Phil Factor\t         1.00\r\n    Sue Right\t         1.00\r\n    Robin Page\t         1.00\r\n\r\nare the same. But Ranking functions return different values:\r\n\r\n    CREATE TABLE [dbo].[Employees](\r\n    \t--[ID] [int] IDENTITY(1,1) NOT NULL,\r\n    \t[Employee] [varchar](150) NOT NULL,\r\n    \t[Salary] [smallmoney] NULL,\r\n    ) ON [PRIMARY]\r\n    \r\n    GO\r\n    CREATE TABLE [dbo].[Employees2](\r\n    \t--[ID] [int] IDENTITY(1,1) NOT NULL,\r\n    \t[Employee] [varchar](150) NOT NULL,\r\n    \t[Salary] [smallmoney] NULL,\r\n    ) ON [PRIMARY]\r\n    \r\n    INSERT INTO [dbo].[Employees]\r\n    ([Employee] ,[Salary])\r\n    VALUES\r\n    ('Sue Right', 1)\r\n    , ('Robin Page', 1)\r\n    ,('Phil Factor', 1 )\r\n    GO\r\n    INSERT INTO [dbo].[Employees2]\r\n    ([Employee] ,[Salary])\r\n    VALUES\r\n    ('Phil Factor', 1 )\r\n    ,('Sue Right', 1)\r\n    ,('Robin Page', 1)\r\n    GO\r\n    SELECT RANK() OVER ( ORDER BY Salary) AS [Rank]\r\n    , DENSE_RANK() OVER (ORDER BY Salary ) AS [Dense_rank]\r\n    , [Employee]\r\n    FROM\r\n    dbo.Employees\r\n    \r\n    SELECT RANK() OVER ( ORDER BY Salary) AS [Rank]\r\n    , DENSE_RANK() OVER (ORDER BY Salary ) AS [Dense_rank]\r\n    , [Employee]\r\n    FROM\r\n    dbo.Employees2\r\n    \r\n    SELECT NTILE(3) OVER ( ORDER BY SALARY )\r\n    , [Employee]\r\n    FROM\r\n    dbo.Employees\r\n    \r\n    SELECT NTILE(3) OVER ( ORDER BY SALARY )\r\n    , [Employee]\r\n    FROM\r\n    dbo.Employees2\r\n\r\n  [1]: http://msdn.microsoft.com/en-GB/library/ms173825.aspx\r\n  [2]: http://msdn.microsoft.com/en-GB/library/ms176102.aspx\r\n  [3]: http://sqlmag.com/t-sql/ranking-functions\r\n  [4]: http://msdn.microsoft.com/en-GB/library/ms178091.aspx\r\n\r\n\r\n	106	2019-11-25 13:07:37.639975+00	0	4	1	174	251	1457	77639	0	0	0	2019-11-25 10:12:07.164107+00	f	f	1	1
2	208	332	2018-06-06 04:16:45+00	How Selective Should An Index Be?	Is there a general selectivity rule for when to apply a nonclustered index? \r\n\r\nWe know not to create an index on a 50/50 `bit` column:\r\n\r\n>[Should I index a bit field in SQL Server?][1]:\r\n>\r\n>Rows with 50/50 distribution, it might buy you very little performance gain\r\n\r\nHow selective should a SQL Server query be before an index should be applied?\r\n\r\nIs there a general rule or guideline? At 25% average selectivity distribution in a column? 10% selectivity? \r\n\r\nThis article by Tibor Karaszi is stating around 31%: [How selective do we need to be for an index to be used?][2]\r\n\r\n  [1]: https://stackoverflow.com/questions/231125/should-i-index-a-bit-field-in-sql-server\r\n  [2]: http://sqlblog.karaszi.com/how-selective-do-we-need-to-be-for-an-index-to-be-used/	340	2019-12-05 21:11:41.058557+00	0	4	1	407	715	2284	208852	0	0	0	2019-12-05 18:20:34.168473+00	f	f	1	1
2	45	187	2017-04-21 15:33:41+00	Why does a subquery reduce the row estimate to 1?	Consider the following contrived but simple query:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID <> 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM X_OTHER_TABLE) \r\n    \tELSE (SELECT TOP 1 ID FROM X_OTHER_TABLE_2) \r\n      END AS ID2\r\n    FROM X_HEAP;\r\n\r\nI would expect the final row estimate for this query to be equal to the number of rows in the `X_HEAP` table. Whatever I'm doing in the subquery shouldn't matter for the row estimate because it cannot filter out any rows. However, on SQL Server 2016 I see the row estimate reduced to 1 because of the subquery:\r\n\r\n[![bad query][1]][1]\r\n\r\nWhy does this happen? What can I do about it?\r\n\r\n\r\nIt's very easy to reproduce this issue with the right syntax. Here is one set of table definitions that will do it:\r\n\r\n    CREATE TABLE dbo.X_HEAP (ID INT NOT NULL)\r\n    CREATE TABLE dbo.X_OTHER_TABLE (ID INT NOT NULL);\r\n    CREATE TABLE dbo.X_OTHER_TABLE_2 (ID INT NOT NULL);\r\n    \r\n    INSERT INTO dbo.X_HEAP WITH (TABLOCK)\r\n    SELECT TOP (1000) ROW_NUMBER() OVER (ORDER BY (SELECT NULL))\r\n    FROM master..spt_values;\r\n    \r\n    CREATE STATISTICS X_HEAP__ID ON X_HEAP (ID) WITH FULLSCAN;\r\n\r\n\r\ndb fiddle [link][2].\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/IU6jI.png\r\n  [2]: http://dbfiddle.uk/?rdbms=sqlserver_2016&fiddle=7c1a04d76f8fec057545f72125f0ac90	194	2019-11-28 14:42:15.642634+00	0	4	1	262	391	1475	171654	0	0	0	2019-11-28 14:42:15.642634+00	f	f	1	1
2	120	208	2018-05-04 00:53:15+00	In SQL Server, is parallelism per operator, or something else?	I work with a really old DBA who says a lot of weird stuff. Dude has an O'Reilly book that only has an amoeba on the cover.\r\n\r\nAt lunch we were talking about parallelism, because our new server has 24 cores. He says that in a parallel plan, every operator gets DOP threads. So if you have MAXDOP 8 and your query has 4 parallel operators it'll use 32 threads at once.\r\n\r\nThat doesn't seem right because you'd run out of threads really fast. \r\n\r\nI also read that it might just be 8 for the whole query, which seems like too few.\r\n\r\n[Why Do I see more threads per SPID in sysprocesses than MAXDOP?][1]\r\n\r\nAre either of them right? \r\n\r\n\r\n  [1]: https://blogs.msdn.microsoft.com/sqlserverfaq/2009/01/05/why-do-i-see-more-threads-per-spid-in-sysprocesses-than-maxdop/	215	2019-11-30 12:58:32.99406+00	0	4	1	283	441	1486	205768	0	0	0	2019-11-30 12:58:32.99406+00	f	f	1	1
2	857	651	2020-01-30 17:44:09.025627+00	Trying to understand an isnull expression	I have a question about this fiddle:\r\n\r\n<>https://dbfiddle.uk?rdbms=sqlserver_2019&fiddle=5fb4c089af3d52e645aade8970793767\r\n\r\nIs it ever possible to get -1 as the result of this query? \r\nI think no, right because potato can be either null or not null and the case covers both scenario? \r\n\r\nsql server 11 \r\n\r\nSincerely, \r\n	666	2020-01-30 17:44:09.025627+00	0	4	6	726	1927	3928	\N	0	0	0	\N	f	f	1	\N
2	759	678	2020-02-03 17:20:52.044818+00	Using a Table as a Queue without sp_getapplock/sp_releaseapplock	I have a list of commands I need to execute, all of which are contained within a table I've named `myQueue`.  This table is a little unique in that some commands should be *grouped together* such that their execution is performed sequentially, rather than concurrently, as executing them concurrently causes unwanted data artifacts and errors.  Because of this, the queue cannot be classified in a typical **FIFO**/**LIFO** fashion as the dequeue order is determined at run-time.\r\n\r\nTo summarize:\r\n1) A Table named `myQueue` will act as a command queue (where dequeue order is determined at run-time)\r\n2) Commands are added to the table in a random way\r\n3) Commands may fall into *groups*, and if so, must be executed by a single worker thread in an ordered, sequential manner\r\n4) Any number of worker threads can be running when commands are being dequeued\r\n5) Dequeuing is performed via an `UPDATE` rather than a `DELETE` as this table is used for historical performance reporting for said commands\r\n\r\nMy current approach is to iterate over this table using explicit mutex logic via [`sp_getapplock`](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-getapplock-transact-sql?view=sql-server-ver15)/[`sp_releaseapplock`](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-releaseapplock-transact-sql?view=sql-server-ver15) calls.  While this works as expected, I was hoping to optimize the approach (in order to reduce `xp_userlock` waits), and after reading through Remus Rusanu's [excellent blog post on the topic](https://rusanu.com/2010/03/26/using-tables-as-queues/), I decided to go down the path of utilizing some table hints in hopes I could further optimize my process.\r\n\r\nI'll include the test code below, but to summarize my results, using table hints and eliminating calls to `sp_getapplock`/`sp_releaseapplock` results in up to three undesirable behaviors as follows:\r\n1) Deadlocking\r\n2) Multiple threads execute commands that are contained within a single *group*\r\n3) Thread Assignments are missing within a *group* of commands\r\n\r\n**What I'm hoping for is that someone will point out how I'm  not structuring my dequeing statements correctly so I can still move forward with using table hints exclusively.**  If that doesn't work, so be it, but I wanted to see if it could be done just the same.\r\n\r\nWhat I can say is if the code accommodates the deadlocking (e.g. retrying the offending operation as is currently included), the methods not using `sp_getapplock`/`sp_releaseapplock` (which don't exhibit undesirable behaviors 2 & 3) perform at least twice as fast, if not faster.\r\n\r\nThe tests can be setup with the following code.\r\n\r\nThe `myQueue` table creation and population with *commands* that are similar enough to my workload:\r\n\r\n```\r\nCREATE TABLE myQueue\r\n(\r\n\tID INT\tIDENTITY (1,1) PRIMARY KEY CLUSTERED,\r\n\tMain\tINT,\r\n\tSub\t\tINT,\r\n\tDetail\tINT,\r\n\tCommand\tVARCHAR(MAX),\r\n\tThread\tINT,\r\n\tStartDT\tDATETIME2,\r\n\tEndDT\tDATETIME2\r\n)\r\nGO\r\nINSERT INTO myQueue WITH (TABLOCKX) (Main, Sub, Detail, Command)\r\nSELECT\tABS(CHECKSUM(NEWID()) % 200),\r\n\t\tABS(CHECKSUM(NEWID()) % 1280),\r\n\t\tABS(CHECKSUM(NEWID())),\r\n\t\t'WAITFOR DELAY ''00:00:00.01'''\r\nFROM sys.types t1 CROSS JOIN \r\n\t sys.types t2 CROSS JOIN\r\n\t sys.types t3 CROSS JOIN\r\n\t (VALUES (1), (2), (3), (4), (5)) t4(x)\r\nGO\r\n\r\nCREATE NONCLUSTERED INDEX [IX_myQueue_Update]\r\nON [dbo].[myQueue] ([Main],[Sub])\r\nINCLUDE (Thread, EndDT)\r\nGO\r\n```\r\n\r\n\r\nThe Worker Threads all follow the same logic.  I recommend that if you run this locally, you just copy this code into separate query windows and execute accordingly, making sure all Worker Threads adhere to the same locking method:\r\n\r\n```\r\nSET NOCOUNT ON\r\nDECLARE @updOUT TABLE\r\n(\r\n\tMain\tINT,\r\n\tSub\t\tINT\r\n)\r\n-- Update @CurrentThread as a unique ID, I tend to\r\nSET NOCOUNT ON\r\nDECLARE @updOUT TABLE\r\n(\r\n\tMain\tINT,\r\n\tSub\t\tINT\r\n)\r\n-- Update @CurrentThread as a unique ID, I tend to\r\n-- number them 1 - N, with N being the number of threads I'm running\r\nDECLARE @CurrentThread INT = @@SPID, \r\n\t\t@main INT, @sub INT,\r\n\t\t@id INT, @command VARCHAR(MAX), \r\n\t\t@ErrorMessage NVARCHAR(4000)\r\nWHILE\tEXISTS(SELECT TOP 1 ID FROM myQueue WHERE EndDT IS NULL)\r\nBEGIN\r\n\tBEGIN TRY\r\n\r\n\t\t--/*\r\n\t\t-- Method 1: Top 1 WITH TIES within CTE, direct update against CTE, Contained with sp_getapplock/sp_releaseapplock\r\n\t\t-- works\r\n\t\t-- high volume of xp_userlock waits\r\n\t\tBEGIN TRY\r\n\t\t\tBEGIN TRAN\r\n\r\n\t\t\t\tEXEC sp_getapplock @Resource = 'myQueue', @LockMode = 'Update'\r\n\r\n\t\t\t\t;WITH dequeue AS\r\n\t\t\t\t(\r\n\t\t\t\t\tSELECT TOP 1 WITH TIES\r\n\t\t\t\t\t\tMain, Sub, Thread\r\n\t\t\t\t\tFROM\tmyQueue\r\n\t\t\t\t\tWHERE\tEndDT IS NULL\r\n\t\t\t\t\t\tAND\t(Thread IS NULL OR Thread = @CurrentThread)\r\n\t\t\t\t\tORDER BY Main, Sub\r\n\t\t\t\t)\r\n\t\t\t\tUPDATE\tdequeue\r\n\t\t\t\tSET\tThread = @CurrentThread\r\n\t\t\t\tOUTPUT\tDELETED.Main,\r\n\t\t\t\t\t\tDELETED.Sub\r\n\t\t\t\tINTO @updOUT\r\n\r\n\t\t\t\tEXEC sp_releaseapplock @Resource = 'myQueue'\r\n\t\t\tCOMMIT\r\n\t\tEND TRY\r\n\t\tBEGIN CATCH\r\n\t\t\tEXEC sp_releaseapplock @Resource = 'myQueue'\r\n\t\t\tROLLBACK TRAN\r\n\t\tEND CATCH\r\n\t\t--*/\r\n\r\n\t\t/*\r\n\t\t-- Method 2: Top 1 WITH TIES within CTE, direct update against CTE\r\n\t\t-- does not work\r\n\t\t-- some groupings contain multiple worker threads \r\n\t\t-- missing thread assignments (e.g. NULL value in Thread Column)\r\n\t\t-- deadlocking experienced\r\n\t\t;WITH dequeue AS\r\n\t\t(\r\n\t\t\tSELECT TOP 1 WITH TIES\r\n\t\t\t\tMain, Sub, Thread\r\n\t\t\tFROM\tmyQueue WITH (ROWLOCK, UPDLOCK, READPAST)\r\n\t\t\tWHERE\tEndDT IS NULL\r\n\t\t\t\tAND\t(Thread IS NULL OR Thread = @CurrentThread)\r\n\t\t\tORDER BY Main, Sub\r\n\t\t)\r\n\t\tUPDATE\tdequeue\r\n\t\tSET\tThread = @CurrentThread\r\n\t\tOUTPUT\tDELETED.Main,\r\n\t\t\t\tDELETED.Sub\r\n\t\tINTO @updOUT\r\n\t\t*/\r\n\r\n\t\t/*\r\n\t\t-- Method 3: Top 1 WITH TIES within CTE, join to myQueue table\r\n\t\t-- does not work\r\n\t\t-- some groupings contain multiple worker threads \r\n\t\t-- missing thread assignments (e.g. NULL value in Thread Column)\r\n\t\t-- deadlocking experienced\r\n\t\t;WITH dequeue AS\r\n\t\t(\r\n\t\t\tSELECT TOP 1 WITH TIES\r\n\t\t\t\tMain, Sub, Thread\r\n\t\t\tFROM\tmyQueue WITH (ROWLOCK, UPDLOCK, READPAST)\r\n\t\t\tWHERE\tEndDT IS NULL\r\n\t\t\t\tAND\t(Thread IS NULL OR Thread = @CurrentThread)\r\n\t\t\tORDER BY Main, Sub\r\n\t\t)\r\n\t\tUPDATE\tmyQ\r\n\t\tSET\tThread = @CurrentThread\r\n\t\tOUTPUT\tDELETED.Main,\r\n\t\t\t\tDELETED.Sub\r\n\t\tINTO @updOUT\r\n\t\tFROM\tmyQueue myQ WITH (ROWLOCK, UPDLOCK, READPAST)\r\n\t\t\t\t\tINNER JOIN dequeue\r\n\t\t\t\t\t\tON myQ.Main = dequeue.Main\r\n\t\t\t\t\t\tAND myQ.Sub = dequeue.Sub \r\n\t\t*/\r\n\r\n\t\t/*\r\n\t\t-- Method 4: Top 1 within CTE, join to myQueue table\r\n\t\t-- does not work\r\n\t\t-- some groupings contain multiple worker threads\r\n\t\t;WITH dequeue AS\r\n\t\t(\r\n\t\t\tSELECT TOP 1\r\n\t\t\t\tMain, Sub, Thread\r\n\t\t\tFROM\tmyQueue WITH (ROWLOCK, UPDLOCK, READPAST)\r\n\t\t\tWHERE\tEndDT IS NULL\r\n\t\t\t\tAND\t(Thread IS NULL OR Thread = @CurrentThread)\r\n\t\t\tORDER BY Main, Sub\r\n\t\t)\r\n\t\tUPDATE\tmyQ\r\n\t\tSET\tThread = @CurrentThread\r\n\t\tOUTPUT\tDELETED.Main,\r\n\t\t\t\tDELETED.Sub\r\n\t\tINTO @updOUT\r\n\t\tFROM\tmyQueue myQ WITH (ROWLOCK, UPDLOCK, READPAST)\r\n\t\t\t\t\tINNER JOIN dequeue\r\n\t\t\t\t\t\tON myQ.Main = dequeue.Main\r\n\t\t\t\t\t\tAND myQ.Sub = dequeue.Sub \r\n\t\t*/\r\n\r\n\t\t/*\r\n\t\t-- Method 5: Top 1 WITH TIES within CTE, join to myQueue table, PAGLOCK hint instead of ROWLOCK\r\n\t\t-- works*\r\n\t\t-- deadlocking experienced\r\n\t\t;WITH dequeue AS\r\n\t\t(\r\n\t\t\tSELECT TOP 1 WITH TIES\r\n\t\t\t\tMain, Sub, Thread\r\n\t\t\tFROM\tmyQueue WITH (PAGLOCK, UPDLOCK, READPAST)\r\n\t\t\tWHERE\tEndDT IS NULL\r\n\t\t\t\tAND\t(Thread IS NULL OR Thread = @CurrentThread)\r\n\t\t\tORDER BY Main, Sub\r\n\t\t)\r\n\t\tUPDATE\tmyQ\r\n\t\tSET\tThread = @CurrentThread\r\n\t\tOUTPUT\tDELETED.Main,\r\n\t\t\t\tDELETED.Sub\r\n\t\tINTO @updOUT\r\n\t\tFROM\tmyQueue myQ WITH (PAGLOCK, UPDLOCK, READPAST)\r\n\t\t\t\t\tINNER JOIN dequeue\r\n\t\t\t\t\t\tON myQ.Main = dequeue.Main\r\n\t\t\t\t\t\tAND myQ.Sub = dequeue.Sub \r\n\t\t*/\r\n\r\n\t\t/*\r\n\t\t-- Method 6: Top 1 WITH TIES within CTE, direct update against CTE, PAGLOCK hint instead of ROWLOCK\r\n\t\t-- works*\r\n\t\t-- deadlocking experienced\r\n\t\t;WITH dequeue AS\r\n\t\t(\r\n\t\t\tSELECT TOP 1 WITH TIES\r\n\t\t\t\tMain, Sub, Thread\r\n\t\t\tFROM\tmyQueue WITH (PAGLOCK, UPDLOCK, READPAST)\r\n\t\t\tWHERE\tEndDT IS NULL\r\n\t\t\t\tAND\t(Thread IS NULL OR Thread = @CurrentThread)\r\n\t\t\tORDER BY Main, Sub\r\n\t\t)\r\n\t\tUPDATE\tdequeue\r\n\t\tSET\tThread = @CurrentThread\r\n\t\tOUTPUT\tDELETED.Main,\r\n\t\t\t\tDELETED.Sub\r\n\t\tINTO @updOUT\r\n\r\n\t\t*/\r\n\r\n\t\t/*\r\n\t\t-- Method 7: Top 1 within CTE, join to myQueue table, PAGLOCK hint instead of ROWLOCK\r\n\t\t-- works*\r\n\t\t-- deadlocking experienced\r\n\t\t;WITH dequeue AS\r\n\t\t(\r\n\t\t\tSELECT TOP 1\r\n\t\t\t\tMain, Sub, Thread\r\n\t\t\tFROM\tmyQueue WITH (PAGLOCK, UPDLOCK, READPAST)\r\n\t\t\tWHERE\tEndDT IS NULL\r\n\t\t\t\tAND\t(Thread IS NULL OR Thread = @CurrentThread)\r\n\t\t\tORDER BY Main, Sub\r\n\t\t)\r\n\t\tUPDATE\tmyQ\r\n\t\tSET\tThread = @CurrentThread\r\n\t\tOUTPUT\tDELETED.Main,\r\n\t\t\t\tDELETED.Sub\r\n\t\tINTO @updOUT\r\n\t\tFROM\tmyQueue myQ WITH (PAGLOCK, UPDLOCK, READPAST)\r\n\t\t\t\t\tINNER JOIN dequeue\r\n\t\t\t\t\t\tON myQ.Main = dequeue.Main\r\n\t\t\t\t\t\tAND myQ.Sub = dequeue.Sub \r\n\t\t*/\r\n\r\n\t\tSELECT\tTOP 1 \r\n\t\t\t  @main = Main\r\n\t\t\t, @sub = Sub\r\n\t\tFROM @updOUT\r\n\r\n\t\tEND TRY\r\n\t\tBEGIN CATCH\r\n\t\t\tSELECT @ErrorMessage = 'Msg ' + CAST(ERROR_NUMBER() AS VARCHAR(10)) + ', Level ' + CAST(ERROR_SEVERITY() AS VARCHAR(10)) \r\n\t\t\t+ ', State ' + CAST(ERROR_STATE() AS VARCHAR(10)) + ', Line ' + CAST(ERROR_LINE() AS VARCHAR(10))\r\n\t\t\t+ CHAR(13) + CHAR(10) + ERROR_MESSAGE()\r\n\r\n\t\t\tRAISERROR(@ErrorMessage, 1, 1) WITH NOWAIT\r\n\r\n\t\t\t-- Set to Uselss values so cursor doesn't fire\r\n\t\t\tSELECT @main = -1, @sub = -1\r\n\t\tEND CATCH\r\n\r\n\t\tDELETE FROM @updOUT\r\n\r\n\t\tDECLARE WorkQueueCur INSENSITIVE CURSOR\r\n\t\tFOR\r\n\t\t\tSELECT\tID, Command\r\n\t\t\tFROM\tmyQueue\r\n\t\t\tWHERE\tMain = @main\r\n\t\t\t\tAND Sub = @sub\r\n\t\t\tORDER BY Detail\r\n\r\n\t\tOPEN WorkQueueCur\r\n\r\n\t\tFETCH NEXT FROM WorkQueueCur\r\n\t\tINTO @id, @command\r\n\r\n\t\tWHILE @@FETCH_STATUS = 0\r\n\t\tBEGIN\r\n\r\n\t\t\tRETRY1:\r\n\r\n\t\t\tBEGIN TRY\r\n\t\t\t\tUPDATE\tmyQueue\r\n\t\t\t\tSET StartDT = GETDATE()\r\n\t\t\t\tWHERE ID = @id\r\n\t\t\tEND TRY\r\n\t\t\tBEGIN CATCH\r\n\t\t\t\tSELECT @ErrorMessage = 'Retry1: Msg ' + CAST(ERROR_NUMBER() AS VARCHAR(10)) + ', Level ' + CAST(ERROR_SEVERITY() AS VARCHAR(10)) \r\n\t\t\t\t+ ', State ' + CAST(ERROR_STATE() AS VARCHAR(10)) + ', Line ' + CAST(ERROR_LINE() AS VARCHAR(10))\r\n\t\t\t\t+ CHAR(13) + CHAR(10) + ERROR_MESSAGE()\r\n\r\n\t\t\t\tRAISERROR(@ErrorMessage, 1, 1) WITH NOWAIT\r\n\r\n\t\t\t\tGOTO RETRY1\r\n\t\t\tEND CATCH\r\n\r\n\t\t\tEXEC(@command)\r\n\r\n\t\t\tRETRY2:\r\n\t\t\t\r\n\t\t\tBEGIN TRY\r\n\t\t\t\tUPDATE\tmyQueue\r\n\t\t\t\tSet\tEndDT = GETDATE()\r\n\t\t\t\tWHERE ID = @id\r\n\t\t\tEND TRY\r\n\t\t\tBEGIN CATCH\r\n\t\t\t\tSELECT @ErrorMessage = 'Retry2: Msg ' + CAST(ERROR_NUMBER() AS VARCHAR(10)) + ', Level ' + CAST(ERROR_SEVERITY() AS VARCHAR(10)) \r\n\t\t\t\t+ ', State ' + CAST(ERROR_STATE() AS VARCHAR(10)) + ', Line ' + CAST(ERROR_LINE() AS VARCHAR(10))\r\n\t\t\t\t+ CHAR(13) + CHAR(10) + ERROR_MESSAGE()\r\n\r\n\t\t\t\tRAISERROR(@ErrorMessage, 1, 1) WITH NOWAIT\r\n\r\n\t\t\t\tGOTO RETRY2\r\n\t\t\tEND CATCH\r\n\r\n\t\t\tFETCH NEXT FROM WorkQueueCur\r\n\t\t\tINTO @id, @command\r\n\t\tEND\r\n\r\n\t\tCLOSE WorkQueueCur\r\n\t\tDEALLOCATE WorkQueueCur\r\n\r\n\r\nEND\r\n```\r\n\r\nConfirmation of undesirable behaviors 2 and 3 (or lack thereof), above can be determined by running the following statement:\r\n\r\n```\r\n;WITH invalidMThread AS (\r\n\tSELECT\t*, DENSE_RANK() OVER (PARTITION BY Main, Sub ORDER BY Thread) AS ThreadCount\r\n\tFROM\tdbo.myQueue WITH (NOLOCK)\r\n\tWHERE\tStartDT IS NOT NULL\r\n), invalidNThread AS (\r\n\tSELECT\t*\r\n\tFROM\tdbo.myQueue WITH (NOLOCK)\r\n\tWHERE\tThread IS NULL\r\n\t\t\tAND StartDT IS NOT NULL\r\n)\r\nSELECT\tt1.*, 'Multiple Threads' AS Issue\r\nFROM\tdbo.myQueue t1 WITH (NOLOCK) \r\n\t\tINNER JOIN invalidMThread i1\r\n\t\t\tON i1.Main = t1.Main\r\n\t\t\tAND i1.Sub = t1.Sub\r\nWHERE\ti1.ThreadCount > 1\r\n\r\nUNION\r\n\r\nSELECT\tt1.*, 'Unassigned Thread(s)' AS Issue\r\nFROM\tdbo.myQueue t1 WITH (NOLOCK) \r\n\t\tINNER JOIN invalidNThread i2\r\n\t\t\tON i2.Main = t1.Main\r\n\t\t\tAND i2.Sub = t1.Sub\r\n\r\nORDER BY t1.Main, t1.Sub\r\n```\r\n\r\nAs always, I fully anticipate I missed some critical point Remus made in his article, so any help in pointing that out would be very much appreciated.	693	2020-02-05 21:04:01.506141+00	0	4	1	753	2040	3838	\N	0	0	0	\N	f	f	1	\N
1	138	237	2019-12-01 19:44:16.718995+00	Can we implement passwords?	As an incognito user (for security reasons), every time I exit out of my incognito window, the session cookies kept by this site for logins will be cast into the void. It's probably not the best thing to use a recovery token every time I want to log in, either.\r\n\r\nCan there be some way to log in rather than just the session cookies?	245	2019-12-01 19:44:16.718995+00	6	4	1	312	1184	2484	\N	0	0	0	\N	f	f	2	\N
1	96	360	2019-12-06 11:37:40.943735+00	Import pulls answers as questions	The SE import tool does not know when it has things crosswired. Accidentally give it IDs in the wrong order or only answer IDs and the first answer ends up being the question.	369	2019-12-06 11:37:40.943735+00	4	4	2	435	782	2381	\N	0	0	0	\N	f	f	2	\N
2	166	261	2019-07-04 15:02:15+00	Implicit conversion between decimals with different precisions	I know that when SQL Server handles an implicit conversion between types it converts the lowest priority one to the highest one.\r\n\r\nBut what is the result datatype when I perform an operation between decimals with different precisions?	269	2019-12-04 03:31:46.115643+00	0	4	1	336	574	954	242097	0	0	0	2019-12-04 03:31:46.115643+00	f	f	1	1
2	174	281	2017-08-03 14:23:53+00	Better approach for “LIKE OR LIKE, OR LIKE, OR LIKE, OR LIKE”	[In this question][1] he's having the same problem as I am. I need something like:\r\n\r\n    select * from blablabla \r\n    where product \r\n    like '%rock%' or\r\n    like '%paper%' or\r\n    like '%scisor%' or\r\n    like '%car%' or\r\n    like '%pasta%' \r\n\r\nThis is ugly and it's not using indexes.. In this case, this is really the only way to do this ( to select multiple words inside a string ), or should I use FULLTEXT?\r\n\r\nAs I understand, with fulltext, I can select multiple words inside a string.\r\n\r\n[This question talks about Full Text as well][2]\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/1865353/combining-like-and-in-for-sql-server\r\n  [2]: https://stackoverflow.com/questions/6506418/sql-searching-multiple-words-in-a-string	289	2019-12-04 14:25:06.589929+00	0	4	1	356	609	1519	182603	0	0	0	2019-12-04 14:25:06.589929+00	f	f	1	1
2	45	252	2019-03-22 04:21:55+00	Why does a simple loop result in ASYNC_NETWORK_IO waits?	The following T-SQL takes about 25 seconds on my machine with SSMS v17.9:\r\n\r\n    DECLARE @outer_loop INT = 0,\r\n    @big_string_for_u VARCHAR(8000);\r\n    \r\n    SET NOCOUNT ON;\r\n    \r\n    WHILE @outer_loop < 50000000\r\n    BEGIN\r\n    \tSET @big_string_for_u = 'ZZZZZZZZZZ';\r\n    \tSET @outer_loop = @outer_loop + 1;\r\n    END;\r\n\r\nIt accumulates 532 ms of `ASYNC_NETWORK_IO` waits according to both `sys.dm_exec_session_wait_stats` and `sys.dm_os_wait_stats`. The total wait time increases as the number of loop iterations increases. Using the `wait_completed` extended event I can see that the wait happens roughly every 43 ms with a few exceptions: \r\n\r\n[![wait table][1]][1]\r\n\r\nIn addition, I can get the call stacks that occur right before the `ASYNC_NETWORK_IO` wait:\r\n\r\n``` none\r\nsqldk.dll!SOS_DispatcherBase::GetTrack+0x7f6c\r\nsqldk.dll!SOS_Scheduler::PromotePendingTask+0x204\r\nsqldk.dll!SOS_Task::PostWait+0x5f\r\nsqldk.dll!SOS_Scheduler::Suspend+0xb15\r\nsqllang.dll!CSECCNGProvider::GetBCryptHandleFromAlgID+0xf6af\r\nsqllang.dll!CSECCNGProvider::GetBCryptHandleFromAlgID+0xf44c\r\nsqllang.dll!SNIPacketRelease+0xd63\r\nsqllang.dll!SNIPacketRelease+0x2097\r\nsqllang.dll!SNIPacketRelease+0x1f99\r\nsqllang.dll!SNIPacketRelease+0x18fe\r\nsqllang.dll!CAutoExecuteAsContext::Restore+0x52d\r\nsqllang.dll!CSQLSource::Execute+0x151b\r\nsqllang.dll!CSQLSource::Execute+0xe13\r\nsqllang.dll!CSQLSource::Execute+0x474\r\nsqllang.dll!SNIPacketRelease+0x165d\r\nsqllang.dll!CValOdsRow::CValOdsRow+0xa92\r\nsqllang.dll!CValOdsRow::CValOdsRow+0x883\r\nsqldk.dll!ClockHand::Statistic::RecordClockHandStats+0x15d\r\nsqldk.dll!ClockHand::Statistic::RecordClockHandStats+0x638\r\nsqldk.dll!ClockHand::Statistic::RecordClockHandStats+0x2ad\r\nsqldk.dll!SystemThread::MakeMiniSOSThread+0xdf8\r\nsqldk.dll!SystemThread::MakeMiniSOSThread+0xf00\r\nsqldk.dll!SystemThread::MakeMiniSOSThread+0x667\r\nsqldk.dll!SystemThread::MakeMiniSOSThread+0xbb9\r\n```\r\n\r\nFinally, I noticed that SSMS uses a surprising amount of CPU during the loop (about half a core on average). I'm unable to figure out what SSMS is doing during that time.\r\n\r\nWhy does a simple loop cause `ASYNC_NETWORK_IO `waits when executed through SSMS? The only output that I appear to get from the client from this query execution is the "Commands completed successfully." message.\r\n\r\n  [1]: https://i.stack.imgur.com/lUCzg.png	260	2019-12-03 12:40:05.677631+00	0	4	1	327	550	1496	232816	0	0	0	2019-12-03 12:38:27.864955+00	f	f	1	1
2	2	25	2019-11-13 08:09:54.789041+00	The Heap™ – Consultancy ©® 2.0	This post is meant for you, if you are or have been a regular in "The Heap" [chatroom on SE](https://chat.stackexchange.com/rooms/179/the-heap--consultancy-), and have already read our post expaining our [reason for building TopAnswers]((https://topanswers.xyz/meta?q=1)).\r\n\r\nI'd like to say a few things to you in particular about my intentions when inviting you here:\r\n\r\n* First of all, thank you for dropping in!\r\n* If you are happy on SE, I don't want to try and persuade you to come here to chat and/or contribute to the Q&A. We want to build this site for primarily for people who are *already looking* for the kind of things we are trying to provide (things like open source, a non-profit model, a pure focus on Q&A etc).\r\n* It doesn't have to be either/or anyway, at least as far as we are concerned — you are welcome to contribute in both places in any ways you chose.\r\n* One of my main reasons for posting here is because I don't want anyone from The SE Heap thinking we've been deliberately excluding you from TopAnswers. Even if you are not looking for what we are offering, you may like to know that it is offered, and that I would value your participation because you have been [such a great community](https://chat.stackexchange.com/transcript/message/52459260#52459260) on SE. I just wanted to have something concrete to show you first 😊.\r\n* I think SE underestimated how much influence the community in The SE Heap have had on shaping [dba.se](https://dba.stackexchange.com). I suspect this is also true of other sites on the network. These groups, and the platform that enables them, shouldn't be second-class citizens on the site.\r\n\r\nIn short, if TopAnswers is something you are looking for, or looks like it has the potential to be, please join in, especially by asking and answering interesting questions (and starring good answers), to 'seed' the site.\r\n\r\nIf things look like they are heading in the right direction, our next step will be to add a prompt on [db<>fiddle](https://dbfiddle.uk) along these lines:\r\n\r\n> Need help? Ask [a question about this fiddle](https://topanswers.xyz/question?community=databases&fiddlerdbms=postgres_11&fiddlehash=71940aadda50f55bfede87606cd1dc2c) on TopAnswers.xyz — the focussed Q&A site for expert answers that [is building](https://topanswers.xyz/meta?q=1) a library of knowledge.\r\n\r\nI'm currently looking into adding a markdown extension for fiddles, which the link above will use when it is ready.\r\n\r\nI can't say for sure how many questions we'll get from db<>fiddle, but usage has steadily increased to the point where we have reached about 1,000 unique visits per day, so there is at least the chance it will be effective for bringing them in. If a question comes from db<>fiddle there is also a good chance that the OP has already tried *something* to solve their problem, which is a great start!\r\n\r\nThanks again for dropping by and for reading this post. Please feel to use all the tools here (chat rooms like [The Heap 2.0](/databases), meta etc), to let us know what you think.	32	2019-11-14 17:33:25.666282+00	1	1	1	16	16	2433	\N	0	0	0	\N	f	f	3	\N
2	12	222	2018-07-03 09:00:56+00	Index Seek Operator Cost	For the [*AdventureWorks*][1] sample database query below:\r\n\r\n    SELECT \r\n        P.ProductID, \r\n        CA.TransactionID\r\n    FROM Production.Product AS P\r\n    CROSS APPLY\r\n    (\r\n        SELECT TOP (1)\r\n            TH.TransactionID\r\n        FROM Production.TransactionHistory AS TH\r\n        WHERE\r\n            TH.ProductID = P.ProductID\r\n        ORDER BY \r\n            TH.TransactionID DESC\r\n    ) AS CA;\r\n\r\nThe execution plan shows an *Estimated Operator Cost* of **0.0850383** (93%) for the *Index Seek*:\r\n\r\n[![plan][2]][2]\r\n\r\nThe cost is independent of the cardinality estimation model in use.\r\n\r\nIt is not a simple addition of the *Estimated CPU Cost* and *Estimated I/O Cost*. Neither is it the cost for one execution of the *Index Seek* multiplied by the *Estimated Number of Executions*. \r\n\r\nHow is this cost number arrived at?\r\n\r\n  [1]: https://github.com/Microsoft/sql-server-samples/releases/tag/adventureworks\r\n  [2]: https://i.stack.imgur.com/6FDuw.png	230	2019-12-01 14:58:58.291116+00	0	4	1	297	469	1487	211213	0	0	0	2019-12-01 14:58:58.291116+00	f	f	1	1
2	106	188	2017-05-09 13:46:32+00	Why am I getting “Snapshot isolation transaction aborted due to update conflict”?	We have two tables \r\n\r\n 1. Parent (Id int identity, Date datetime, Name nvarchar)  \r\n 2. Child  (Id int identity, ParentId int, Date datetime, Name nvarchar)\r\n\r\nThe Child having a foreign key relationship to the Parent.\r\n\r\nWe have enabled database level read committed snapshot isolation.\r\n\r\nWe only ever insert and delete rows for Parent and Child (no updates)\r\n\r\nWe have one process (transaction) which deletes old data from Child (and then Parent)\r\n\r\nWe have multiple other processes (transactions) which insert new data into Parent (and then Child)\r\n\r\nThe delete process regularly (but not all of the time) gets rolled back, *even though the insert process does not insert new Child rows which refer to the Parent rows which the delete wants to delete - it simply creates new Parent rows and one ore more new Child rows which refer to the new Parent*\r\n\r\nThe error when deleting the Parent rows is:\r\n\r\n> Snapshot isolation transaction aborted due to update conflict. You cannot use snapshot isolation to access table 'dbo.Child' directly or\r\n> indirectly in database 'Test' to update, delete, or insert the row\r\n> that has been modified or deleted by another transaction. Retry the\r\n> transaction or change the isolation level for the update/delete\r\n> statement.\r\n\r\nI am aware the people suggest having an index on the foreign key column - we'd prefer not to have to do this ideally (for space/performance reasons) - unless this is the only reliably way to get this to work.\r\n\r\nNoted [Snapshot isolation transaction aborted due to update conflict](https://stackoverflow.com/q/10718668) and [a pretty good article](https://sqlperformance.com/2014/06/sql-performance/the-snapshot-isolation-level) but neither of these gives me the understanding I would like to have.\r\n	195	2019-11-28 16:35:25.888393+00	0	4	1	263	396	1477	173091	0	0	0	2019-11-28 16:32:38.788403+00	f	f	1	1
2	16	668	2017-10-04 00:09:06+00	How do Batch Mode Adaptive Joins work?	Batch Mode Adaptive Joins are part of a [family of features][1] in the 2017 query processor that consists of:\r\n\r\n - Batch Mode Adaptive Joins \r\n - [Interleaved Execution][2]\r\n - [Batch Mode Memory Grant Feedback][3]\r\n\r\nSo how do Adaptive Joins work?\r\n\r\n\r\n  [1]: https://blogs.msdn.microsoft.com/sqlserverstorageengine/2017/04/19/introducing-batch-mode-adaptive-joins/\r\n  [2]: https://topanswers.xyz/databases?q=672\r\n  [3]: https://topanswers.xyz/databases?q=669	683	2020-02-03 03:15:07.372639+00	0	4	1	743	1947	3694	187576	0	0	0	2020-02-02 15:57:05.030909+00	f	f	1	1
2	719	528	2017-07-12 07:26:51+00	Only one administrator can connect at this time (MS SQL Server error 18461)	I was experimenting the effect of giving SQL Server a small amount of memory I thought it was going to recover.\r\n\r\nI configured SQL Server to use 200MB of memory now it does not want to start, I did some searches on the internet and I was advised to start SQL Server in single-user mode. however, I get the error \r\n\r\n> Login Failed for user A. Reason: Server is in single user mode. Only\r\n> one administrator can connect at this time (Microsoft SQL Server,\r\n> Error: 18461)\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\t\r\nI have stopped the SQL Server Agent.\r\n\r\n  [1]: https://i.stack.imgur.com/MDETI.png	541	2020-01-06 07:39:57.978193+00	0	4	1	603	1329	2856	179623	0	0	0	2020-01-06 07:39:57.978193+00	f	f	1	1
2	45	238	2018-10-03 03:26:33+00	Why is a scan faster than seek for this predicate?	I was able to reproduce a query performance issue that I would describe as unexpected. I'm looking for an answer that's focused on internals.\r\n\r\nOn my machine, the following query does a clustered index scan and takes about 6.8 seconds of CPU time:\r\n\r\n    SELECT ID1, ID2\r\n    FROM two_col_key_test WITH (FORCESCAN)\r\n    WHERE ID1 NOT IN\r\n    (\r\n    N'1', N'2',N'3', N'4', N'5',\r\n    N'6', N'7', N'8', N'9', N'10',\r\n    N'11', N'12',N'13', N'14', N'15',\r\n    N'16', N'17', N'18', N'19', N'20'\r\n    )\r\n    AND (ID1 = N'FILLER TEXT' AND ID2 >= N'' OR (ID1 > N'FILLER TEXT'))\r\n    ORDER BY ID1, ID2 OFFSET 12000000 ROWS FETCH FIRST 1 ROW ONLY\r\n    OPTION (MAXDOP 1);\r\n\r\nThe following query does a clustered index seek (only difference is removing the `FORCESCAN` hint) but takes about 18.2 seconds of CPU time:\r\n\r\n    SELECT ID1, ID2\r\n    FROM two_col_key_test\r\n    WHERE ID1 NOT IN\r\n    (\r\n    N'1', N'2',N'3', N'4', N'5',\r\n    N'6', N'7', N'8', N'9', N'10',\r\n    N'11', N'12',N'13', N'14', N'15',\r\n    N'16', N'17', N'18', N'19', N'20'\r\n    )\r\n    AND (ID1 = N'FILLER TEXT' AND ID2 >= N'' OR (ID1 > N'FILLER TEXT'))\r\n    ORDER BY ID1, ID2 OFFSET 12000000 ROWS FETCH FIRST 1 ROW ONLY\r\n    OPTION (MAXDOP 1);\r\n\r\nThe query plans are pretty similar. For both queries there are 120000001 rows read from the clustered index:\r\n\r\n[![query plans][1]][1]\r\n\r\nI am on SQL Server 2017 CU 10. Here is code to create and populate the `two_col_key_test` table:\r\n\r\n    drop table if exists dbo.two_col_key_test;\r\n    \r\n    CREATE TABLE dbo.two_col_key_test (\r\n    \tID1 NVARCHAR(50) NOT NULL,\r\n    \tID2 NVARCHAR(50) NOT NULL,\r\n    \tFILLER NVARCHAR(50),\r\n    \tPRIMARY KEY (ID1, ID2)\r\n    );\r\n    \r\n    DROP TABLE IF EXISTS #t;\r\n    \r\n    SELECT TOP (4000) 0 ID INTO #t\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2\r\n    OPTION (MAXDOP 1);\r\n    \r\n    \r\n    INSERT INTO dbo.two_col_key_test WITH (TABLOCK)\r\n    SELECT N'FILLER TEXT' + CASE WHEN ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) > 8000000 THEN N' 2' ELSE N'' END\r\n    , ROW_NUMBER() OVER (ORDER BY (SELECT NULL))\r\n    , NULL\r\n    FROM #t t1\r\n    CROSS JOIN #t t2;\r\n\r\nI am hoping for an answer that does more than call stack reporting. For example, I can see that `sqlmin!TCValSSInRowExprFilter<231,0,0>::GetDataX` takes significantly more CPU cycles in the slow query compared to the fast one:\r\n\r\n[![perview][2]][2]\r\n\r\nInstead of stopping there, I'd like to understand what that is and why there's such a large difference between the two queries.\r\n\r\nWhy is there a large difference in CPU time for these two queries?\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/sJNM2.png\r\n  [2]: https://i.stack.imgur.com/s99dW.png	246	2019-12-01 20:29:55.736059+00	0	4	1	313	499	1489	219137	0	0	0	2019-12-01 20:29:55.736059+00	f	f	1	1
2	109	193	2017-07-29 00:21:21+00	Why does performing a delete on my partitioned view result in a Clustered Index Insert?	I have a partitioned view that has the below insert trigger (poor mans partition).\r\nWhen I perform a DELETE, I get the below query plan:\r\n\r\n    delete from factproductprice where pricedate = '20170725'\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/WMiMZ.png\r\n\r\nTrigger on the view:\r\n\r\n    ALTER TRIGGER [dbo].[factProductPriceDelete] ON [dbo].[FactProductPrice]\r\n    INSTEAD OF DELETE AS\r\n    BEGIN\r\n      IF @@ROWCOUNT = 0 RETURN;\r\n    \r\n      DECLARE @PriceDate DATE\r\n      SELECT @PriceDate = CAST(PriceDate AS DATE) FROM DELETED\r\n      IF @PriceDate BETWEEN '20140101' AND '20141231'\r\n      BEGIN\r\n        DELETE FROM dbo.FactProductPrice2014\r\n    \tWHERE ProductId IN (SELECT ProductId FROM DELETED) AND SalesPriceSchemeId IN (SELECT SalesPriceSchemeId FROM DELETED) AND PriceDate IN (SELECT PriceDate FROM DELETED) \r\n      END\r\n      IF @PriceDate BETWEEN '20150101' AND '20151231'\r\n      BEGIN\r\n        DELETE FROM dbo.FactProductPrice2015\r\n    \tWHERE ProductId IN (SELECT ProductId FROM DELETED) AND SalesPriceSchemeId IN (SELECT SalesPriceSchemeId FROM DELETED) AND PriceDate IN (SELECT PriceDate FROM DELETED) \r\n      END\r\n      IF @PriceDate BETWEEN '20160101' AND '20161231'\r\n.... etc	200	2019-11-29 08:55:04.572966+00	0	4	1	268	406	1480	182154	0	0	0	2019-11-29 08:55:04.572966+00	f	f	1	1
2	173	271	2018-05-15 21:45:56+00	Why does SQL Server use a better execution plan when I inline the variable?	I have a SQL query that I am trying to optimize:\r\n\r\n    DECLARE @Id UNIQUEIDENTIFIER = 'cec094e5-b312-4b13-997a-c91a8c662962'\r\n\t\r\n    SELECT \r\n      Id,\r\n      MIN(SomeTimestamp),\r\n      MAX(SomeInt)\r\n    FROM dbo.MyTable\r\n    WHERE Id = @Id\r\n      AND SomeBit = 1\r\n    GROUP BY Id\r\n\r\n`MyTable` has two indexes:\r\n\r\n    CREATE NONCLUSTERED INDEX IX_MyTable_SomeTimestamp_Includes\r\n    ON dbo.MyTable (SomeTimestamp ASC)\r\n    INCLUDE(Id, SomeInt)\r\n\r\n    CREATE NONCLUSTERED INDEX IX_MyTable_Id_SomeBit_Includes\r\n    ON dbo.MyTable (Id, SomeBit)\r\n    INCLUDE (TotallyUnrelatedTimestamp)\r\n\r\nWhen I execute the query exactly as written above, SQL Server scans the first index, resulting in 189,703 logical reads and a 2-3 second duration. \r\n\r\nWhen I inline the `@Id` variable and execute the query again, SQL Server seeks the second index, resulting in only 104 logical reads and a 0.001 second duration (basically instant).\r\n\r\nI need the variable, but I want SQL to use the good plan. As a temporary solution I put an index hint on the query, and the query is basically instant. However, I try to stay away from index hints when possible. I usually assume that if the query optimizer is unable to do its job, then there is something I can do (or stop doing) to help it without explicitly telling it what to do.\r\n\r\nSo, why does SQL Server come up with a better plan when I inline the variable?	279	2019-12-04 14:16:14.598746+00	0	4	1	346	599	1506	206815	0	0	0	2019-12-04 14:16:14.598746+00	f	f	1	1
1	811	634	2020-01-28 22:54:33.917357+00	Rep requirements for editing content?	Are there any rep requirements for editing content and adding/removing tags from existing posts? 	649	2020-01-28 22:54:33.917357+00	0	1	1	709	1796	3448	\N	0	0	0	\N	f	f	2	\N
4	233	358	2017-01-02 13:23:44+00	How can we draw a duck (in order to create a tikzducks package and store it in CTAN)?	> **This question led to a new package:**  \r\n> [`tikzducks`](http://www.ctan.org/pkg/tikzducks)\r\n\r\nWe have a `scsnowman` package but we don't have a `ducks` package, that's quite inexplicable since ducks are [frequently used on TeX.SE][1] (e.g., samcarter had to fall back on `\\PHdove` of `phaistos` package [here](https://tex.stackexchange.com/a/344842/101651)).\r\n\r\nI'm not an expert, nevertheless, I'd like to create a `tikzducks` package, similar to `tikzpeople`, but I need your help in order to create a customizable duck!\r\n\r\nRequirements: \r\n\r\n - I'd like to have a `duck`, not something a bit like it (ducks with a human body are NOT accepted)\r\n - wings are mandatory\r\n - from the look of the duck, it should appear cleary that it was\r\n   created with LaTeX, not with other commercial tools.\r\n\r\nPossible color-keys for the duck:\r\n\r\n - body \r\n - head \r\n - wings \r\n - eyes \r\n - beak.\r\n\r\nPossible optional add-ons:\r\n\r\n - hat\r\n - sport tools (e.g., cricket bat)\r\n - male/female versions\r\n - a seasonal version\r\n - a Chinese version\r\n - a cook version (not cooked version)\r\n - a mathematician version.\r\n\r\nAs shown in Ulrike Fisher's answer, `tikzpeople` package has a beak feature but it doesn't allow to change the body of the image, so we get a person with a beak (quite ugly).\r\n\r\nLoopspace pointed out that there is already a [`ducks` package](https://tex.stackexchange.com/a/63759/101651) but it doesn't allow to change the style of the duck (like `tikzpeople` does for people) and it is not yet present in CTAN. \r\n\r\nMoreover, I'd like a style similar to this example of the seasonal version:\r\n\r\n[![enter image description here][3]][3]\r\n\r\nGo wild!\r\n\r\n> P.S. = consider this question as if it has always a bounty open!\r\n\r\n\r\n\r\n  [1]: https://tex.stackexchange.com/search?q=duck\r\n  [3]: https://i.stack.imgur.com/JAuZr.jpg	367	2019-12-06 10:39:44.434116+00	0	4	1	433	773	2067	346695	0	0	0	2019-12-06 10:39:44.434116+00	f	f	1	2
1	31	27	2019-11-13 11:27:29.660361+00	Is the application source code open/accessible?	Is the source code of TopAnswers open?  If so, where is it accessible?	34	2019-11-13 13:09:22.178135+00	6	4	1	8	1342	3894	\N	0	0	0	\N	f	f	2	\N
1	96	342	2019-12-06 06:05:43.580517+00	Set prefered license on a per-community basis	Much like the default users font preferences can now be set per-community (thanks!) I think the prefered license should probably be in that scope. I would like to default my TeX contributions to LPPL rather than CC0, but would like CC0 to continue to be my default additional license for code for most sites.	351	2019-12-06 06:21:11.460503+00	3	4	1	417	2083	2383	\N	0	0	0	\N	f	f	2	\N
2	19	264	2019-07-29 16:20:41+00	Why does this RX-X lock not appear in Extended Events?	## The Problem\r\nI have a pair of queries that, under serializable isolation, cause an RX-X lock. However, when I use Extended Events to watch lock acquisition, the RX-X lock acquisition never appears, it is only released. Where does it come from?\r\n\r\n## The Repro\r\nHere's my table:\r\n```\r\nCREATE TABLE dbo.LockTest (\r\nID int identity,\r\nJunk char(4)\r\n)\r\n\r\nCREATE CLUSTERED INDEX CX_LockTest --not unique!\r\nON dbo.LockTest(ID)\r\n\r\n--preload some rows\r\nINSERT dbo.LockTest\r\nVALUES ('data'),('data'),('data')\r\n```\r\n\r\nHere's my problem batch:\r\n```\r\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE\r\n\r\nBEGIN TRAN\r\n\r\nINSERT dbo.LockTest\r\nVALUES ('bleh')\r\n\r\nSELECT *\r\nFROM dbo.LockTest\r\nWHERE ID = SCOPE_IDENTITY()\r\n\r\n--ROLLBACK\r\n```\r\n\r\nI check locks held by this session, and see RX-X:\r\n```\r\nSELECT resource_type, request_mode, request_status, resource_description\r\nFROM sys.dm_tran_locks\r\nWHERE request_session_id = 72 --change SPID!\r\n```\r\n[![dm_tran_locks][1]][1]\r\n\r\nBut I also have an Extended Event on `lock_acquired` and `lock_released`. I filter it on the appropriate associated_object_id...there's no RX-X.\r\n\r\n[![Extended Event output][2]][2]\r\n\r\nAfter executing the rollback, I see RX-X (LAST_MODE) released, even though it was never acquired.\r\n\r\n[![LAST_MODE][3]][3]\r\n\r\n## What I've Tried\r\n\r\n - I looked at *all* locks in Extended Events - no filtering. No RX-X locks acquired.\r\n\r\n - I also tried Profiler: same results (except of course it gets the name right...no "LAST_MODE").\r\n\r\n - I ran the XE for lock escalations - it's not there. \r\n\r\n - There's no XE specifically for conversions, but I was able to confirm that at least the U to X lock conversion is captured by `lock_acquired`\r\n\r\nAlso of note is the RI-N that gets acquired but never released. My current hypothesis is that the RX-X is a conversion lock, as described [here][4]. There are overlapping key-range locks in my batch that look like they should qualify for conversion, but the RX-X lock isn't in the conversion table.\r\n\r\n**Where is this lock coming from, and why isn't it picked up by Extended Events?**\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/pewku.png\r\n  [2]: https://i.stack.imgur.com/q1EpO.png\r\n  [3]: https://i.stack.imgur.com/LDp97.png\r\n  [4]: https://docs.microsoft.com/en-us/sql/2014-toc/sql-server-transaction-locking-and-row-versioning-guide?view=sql-server-2014#conversion-locks	272	2019-12-04 12:25:03.298108+00	0	4	1	339	580	1500	244054	0	0	0	2019-12-04 12:21:12.913315+00	f	f	1	1
2	16	201	2017-09-30 16:40:04+00	Is there a way to prevent Scalar UDFs in computed columns from inhibiting parallelism?	Much has been written about the [perils of Scalar UDFs][1] in SQL Server. A casual search will return oodles of results. \r\n\r\nThere are some places where a Scalar UDF is the only option, though. \r\n\r\nAs an example: when dealing with XML: XQuery can't be used as a computed column definition. One option documented by Microsoft is to use a [Scalar UDF][2] to encapsulate your XQuery in a Scalar UDF, and then use it in a computed column.\r\n\r\nThis has various effects, and some workarounds.\r\n\r\n - Executes row by row when the table is queried\r\n - Forces all queries against the table to run serially\r\n\r\nYou can get around the row-by-row execution by schemabinding the function, and either persisting the computed column, or indexing it. Neither of those methods can prevent the forced serialization of queries hitting the table, even when the scalar UDF isn't referenced.\r\n\r\nIs there a known way to do that?\r\n\r\n  [1]: https://dba.stackexchange.com/questions/72330/is-support-for-parallel-scalar-udf-a-reasonable-feature-request\r\n  [2]: https://docs.microsoft.com/en-us/sql/relational-databases/xml/promote-frequently-used-xml-values-with-computed-columns	208	2019-11-30 07:47:55.55107+00	0	4	1	276	421	1482	187342	0	0	0	2019-11-30 07:47:55.55107+00	f	f	1	1
4	840	625	2019-10-30 15:55:39+00	How do I leave a 4:3 rectangle space on my beamer slide for screen recording with video	I intend to record video lectures. I am using Beamer for making the slides. How can I keep a rectangle space with aspect ratio of 4:3 (actual resolution is 640x480) on any corner so that it remains blank and all the other content simply wraps around it on all slides. In this rectangle area, I will have my video feed (640x480) on and the screen recorder will record my video+audio and the slides on an 1920x1080 resolution. The rectangular area should remain blank (without any border) since the same slides will be distributed.\r\n\r\nI have added the rectangle space in the image below as I would like to have it. I have no idea where to start searching for a solution. Can you give me some pointers or suggestions? [![rectangle area should remain untouched.][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/PTYwm.jpg\r\n	640	2020-01-27 23:59:25.183366+00	0	4	1	700	1746	3388	514247	0	0	0	2020-01-27 15:17:53.307535+00	f	f	1	2
2	12	196	2017-09-10 20:12:10+00	Unique index updates and statistics row modification counters	Given the following table, unique clustered index, and statistics:\r\n\r\n    CREATE TABLE dbo.Banana\r\n    (\r\n        pk integer NOT NULL, \r\n        c1 char(1) NOT NULL, \r\n        c2 char(1) NOT NULL\r\n    );\r\n    \r\n    CREATE UNIQUE CLUSTERED INDEX pk ON dbo.Banana (pk);\r\n    \r\n    CREATE STATISTICS c1 ON dbo.Banana (c1);\r\n    CREATE STATISTICS c2 ON dbo.Banana (c2);\r\n    \r\n    INSERT dbo.Banana \r\n        (pk, c1, c2) \r\n    VALUES \r\n        (1, 'A', 'W'), \r\n        (2, 'B', 'X'), \r\n        (3, 'C', 'Y'), \r\n        (4, 'D', 'Z');\r\n    \r\n    -- Populate statistics\r\n    UPDATE STATISTICS dbo.Banana;\r\n\r\n[![Example data][1]][1]\r\n\r\nThe statistics row modification counters obviously show zero before any updates:\r\n\r\n    -- Show statistics modification counters\r\n    SELECT\r\n        stats_name = S.[name], \r\n        DDSP.stats_id,\r\n        DDSP.[rows],\r\n        DDSP.modification_counter\r\n    FROM sys.stats AS S\r\n    CROSS APPLY sys.dm_db_stats_properties(S.object_id, S.stats_id) AS DDSP\r\n    WHERE\r\n        S.[object_id] = OBJECT_ID(N'dbo.Banana', N'U');\r\n\r\n[![Zero modification counters][2]][2]\r\n\r\nIncrementing each `pk` column value by one for every row:\r\n\r\n    -- Increment pk in every row\r\n    UPDATE dbo.Banana \r\n    SET pk += 1;\r\n\r\nUses the execution plan:\r\n\r\n[![Split Sort Collapse execution plan][3]][3]\r\n\r\nIt produces the following statistics modification counters:\r\n\r\n[![Post-update modification counters][4]][4]\r\n\r\n### Questions\r\n\r\n1. What do the Split, Sort, and Collapse operators do?\r\n2. Why do the `pk` stats show 2 modifications, but `c1` and `c2` show 5?\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/Cf0mu.png\r\n  [2]: https://i.stack.imgur.com/jWStU.png\r\n  [3]: https://i.stack.imgur.com/npF0U.png\r\n  [4]: https://i.stack.imgur.com/4vIJB.png	203	2019-11-29 15:31:56.549885+00	0	4	1	271	413	1481	185551	0	0	0	2019-11-29 15:31:56.549885+00	f	f	1	1
1	2	624	2020-01-26 22:07:22.653566+00	We have a new Code Golf community in private beta	Code Golf have had a few issues on Stack Exchange recently, and some participants have been asking "[is it finally time that we jump ship and look further ashore](https://codegolf.meta.stackexchange.com/q/18477/91666)?"\r\n\r\nWe've offered to host a community here and are currently trialling that in private beta with a handful of people contributing fairly actively so far. I was only dimly aware of this, but Code Golf never exactly fitted the SE model, and they got by with a few workarounds such as:\r\n\r\n* sandboxing on meta\r\n* using 'tags' for different question types\r\n\r\nFortunately, we have been able to accommodate both of those needs here in a less hacky way — we've added new question types particular to Code Golf, with unique voting rules, and enabled sandboxing by setting a question star threshold for allowing answers to be posted on Code Golf challenges.\r\n\r\nWe'll probably be doing more of this over the coming weeks, and some of the features may well end up being used on other communities. We don't want to negatively impact the simplicity of our code, but insofar as we can meet the needs of the Code Golf community without doing so, we'd like to help as much as we can.\r\n\r\nIf you'd like to join the private beta, please let us know in chat.	639	2020-01-26 22:07:22.653566+00	15	1	1	699	1739	3625	\N	0	0	0	\N	f	f	3	\N
1	702	632	2020-01-28 20:19:03.479865+00	Etiquette: Starting an answer with `#` or `##`?	Some questions are suited to answers which begin with a heading, and sometimes that heading might be hyperlinked. There's already [one good example on TeX.TA][1], but there are bound to be others.\r\n\r\nMy question is: should this heading be `H2` (which is my assumption), or `H1` (depending on how certain "html5" practices are understood), or even `H3` (although I doubt it)?\r\n\r\nObviously, this is reasonably trivial. But I am pretty sure that some people (`*cough*`Caleb`*cough*`) might care about a matter like this.\r\n\r\n\r\n[1]: https://topanswers.xyz/tex?q=350	647	2020-01-29 14:10:46.595104+00	8	1	1	707	1867	3533	\N	0	0	0	\N	f	f	2	\N
2	16	672	2017-10-04 00:57:42+00	How does Interleaved Execution work?	Interleaved Execution is part of a [family of features][1] in the 2017 query processor that consists of:\r\n\r\n - [Batch Mode Adaptive Joins][2] \r\n - Interleaved Execution\r\n - [Batch Mode Memory Grant Feedback][3]\r\n\r\nSo how does Interleaved Execution work?\r\n\r\n\r\n  [1]: https://blogs.technet.microsoft.com/dataplatforminsider/2017/09/28/enhancing-query-performance-with-adaptive-query-processing-in-sql-server-2017/\r\n  [2]: https://topanswers.xyz/databases?q=668\r\n  [3]: https://topanswers.xyz/databases?q=669	687	2020-02-03 03:15:46.968572+00	0	4	1	747	1949	3902	187580	0	0	0	2020-02-03 03:09:34.310841+00	f	f	1	1
1	811	633	2020-01-28 21:59:45.928189+00	Distinguish sites on browser tab (<title>)	I rely on my browser tabs' content (rather than position) to hit the right tab. However, TA makes this difficult:\r\n\r\n* all sites use the same icon\r\n* there's no mention of the site, only current post/room title\r\n\r\nI suggest:\r\n* Making the icon be the icon of the site\r\n* making the text `{post/room} - TopAnswers {site name}`\r\n\r\nFurthermore, and to match this, and to improve the UI:\r\n* change the top bar text to always be that of the text shown when editing a post, i.e. `TopAnswers {site name}` and only after that have a drop-down box which by has "switch site" shown.	648	2020-01-28 21:59:45.928189+00	5	1	1	708	1795	3457	\N	0	0	0	\N	f	f	2	\N
1	759	641	2020-01-30 01:14:57.41748+00	No Logout Button?	Hi everyone,\r\n\r\nGlad to see a good contingent of the dba.se community working on a new home.  I hope to see this site grow into a great community that continually finds itself on the first search result page for a good number of things I tend to dig for.\r\n\r\nI suspect much of the same posting etiquette exists here as over on dba.se, so I'll keep this question limited to one topic, which the post subject pretty well addresses.  \r\n\r\n**Why is there no logout button?  If there is, where is it?**\r\n\r\nThe only way I currently know how to *log out* is to delete the cookie for the site.  I don't know if this *feature* is on a worklog or not, but if not, can it be?\r\n\r\nWhy do you need a logout button?  Well, if you're like me and logged in on different machines under two different PINs, at some point you need to log out to log back in under a consolidated PIN.  Yes, it may not be the biggest deal, but I think in the long term the ability to log out will be necessary as I'm sure there are a plethora of good reasons why you'd want to log out of a site.\r\n\r\nThanks!	656	2020-02-03 14:24:20.018336+00	10	4	1	716	1972	3659	\N	0	0	0	\N	f	f	2	\N
4	96	340	2014-04-11 10:13:15+00	Variable page break penalty for odd/even pages?	I am working on a document that has a large number of external Lilypond files included. These render quite well and (while a bit of a pain to use) `lilypond-book` is smart enough to render the music systems at the available width of the host document and add line breaks (and potentially page breaks) between systems.\r\n\r\nI can even override the function Lilypond uses between systems to tell latex that I really-really don't want page breaks inside individual scores if it can help it:\r\n\r\n    \\newcommand{\\betweenLilyPondSystem}[1]{\\linebreak\\nopagebreak[3]}\r\n\r\nThis works great for keeping short scores together on a page, but there are a few cases where this is simply not possible. Some scores are simply too long and must span two or more pages. In these cases it much easier on musicians if the score starts on an even (left) page and runs over onto an odd (right) page.\r\n\r\nI would like to be able to change the weight of the page-break penalty so that scores tried to stay on a page but that scores starting on even pages shouldn't care much (say `\\nopagebreak[1]`), while ones on odd pages make a big deal out of it (`\\nopagebreak[3]` or even `4`). Additionally, it would be nice to have some sort  of page-clear inserted before scores that were going to run over such that longer scores always start on even pages.\r\n\r\nHow could this be accomplished?	349	2019-12-06 05:43:43.046181+00	0	4	1	415	731	1097	170919	0	0	0	2019-12-06 05:43:43.046181+00	f	f	1	2
2	37	591	2020-01-21 17:48:51.270613+00	Should TopAnswers\\Databases be only for "good" questions, or should it be inclusive of "questionable relevance" questions	I'm thinking canonical questions are obviously the best questions, but should we allow the type of question where the answer is only likely to help a small number of people in future?\r\n\r\nFor example, the so-called "help me write this query" type question.\r\n	605	2020-01-21 17:48:51.270613+00	4	4	1	666	1587	3177	\N	0	0	0	\N	f	f	2	\N
4	202	687	2020-02-07 09:56:53.01472+00	Drawing straight lines between sampled curves	I have two curves sampled as shown below:\r\n\r\n```\r\n%&lualatex\r\n% !TeX TXS-program:compile = txs:///lualatex/[--shell-escape]\r\n\\documentclass[convert={density=600,outext=.tiff}]{standalone}\r\n\\usepackage{tikz, pgfplots}\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\begin{axis}[width=2in, height=2in,xmin = 0, xmax = 1050, ymin = 0, ymax = 10, ylabel = {[$\\sim$]}, xlabel = {[\\#]},legend style={at={(1,0)}, anchor=south east, font=\\tiny, draw=none, fill=none}]\t;\r\n\\addplot [mesh, domain = 0:1000,samples=300] {5*(1-exp(-0.003*x))};\r\n\\addplot [black, domain=0:1000, samples = 18, only marks, mark size = 1pt, mark = +] {5*(1-exp(-0.003*x))};\r\n\\addplot [black, domain=0:1100, samples = 18, only marks, mark size = 1pt, mark = +] {10 - 5*(1-exp(-0.003*x))};\r\n\\end{axis}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\nI want to draw lines between the sampled data-points shown below:\r\n\r\n![iwd.jpg](/image?hash=efae4baf3c987a997f0c4dcf9d7fb2e3685cd4c48cfd615a743133c95422f851)\r\n\r\nBut not sure how to achieve it.	702	2020-02-07 09:57:31.216375+00	0	4	3	762	2048	4039	\N	0	0	0	\N	f	f	1	\N
2	181	558	2019-11-18 15:59:24+00	How to get the parameter values of a stored procedure that's in the middle of execution?	Is there a way to get the values of the parameters that were passed in to a stored procedure while the stored procedure is in the middle of execution? (I'm trying to debug an actively running stored procedure.)\r\n\r\nI know I can see the queries of the stored procedure that are currently being executed using the sys.dm_exec_requests and sys.dm_exec_sql_text DMVs but they don't show the values of the parameters being used.	571	2020-01-12 14:11:25.781276+00	0	4	1	633	1431	2947	253555	0	0	0	2020-01-12 14:11:25.781276+00	f	f	1	1
4	262	628	2020-01-28 14:04:47.089931+00	How to implement command mapping?	Currently, I want to have a pair of command, say `\\mycommand{}` and `\\mapmycommand{}{}`, so that\r\n\r\n```tex\r\n% Initially\r\n\\mycommand{a} -> \r\n\\mapmycommand{a}{b}\r\n\\mycommand{a} -> b\r\n\\mapmycommand{c}{d}\r\n\\mycommand{c} -> d\r\n\\mycommand{a} -> b\r\n```\r\n\r\nI have tried the following. All of them fail.\r\n\r\n```tex\r\n\\documentclass{article}\r\n\r\n\\def\\mycommand#1{}\r\n\\def\\mapmycommand#1#2{%\r\n  \\edef\\myoldcommand{\\mycommand}%\r\n  \\edef\\mycommand#1{\\myoldcommand\\ifstrequal{#1}{##1}{#2}{}}%\r\n}%\r\n\r\n\\begin{document}\r\n\\mycommand{a}%\r\n\\mapmycommand{a}{b}%\r\n\\mycommand{a}%\r\n\\mapmycommand{c}{d}%\r\n\\mycommand{c}%\r\n\\mycommand{a}%\r\n\\end{document}\r\n```\r\n\r\n```tex\r\n\\documentclass{article}\r\n\r\n\\def\\mycommand#1{}\r\n\\def\\mapmycommand#1#2{%\r\n  \\let\\myoldcommand\\mycommand\r\n  \\def\\mycommand#1{\\myoldcommand\\ifstrequal{#1}{##1}{#2}{}}%\r\n}%\r\n\r\n\\begin{document}\r\n\\mycommand{a}%\r\n\\mapmycommand{a}{b}%\r\n\\mycommand{a}%\r\n\\mapmycommand{c}{d}%\r\n\\mycommand{c}%\r\n\\mycommand{a}%\r\n\\end{document}\r\n```\r\n\r\nI also know of `\\g@addto@macro`, but so far I fail to apply this command correctly, because `\\mycommand` takes one argument, and obviously the following never works:\r\n\r\n```tex\r\n\\documentclass{article}\r\n\r\n\\def\\mycommand#1{}\r\n\\def\\mapmycommand#1#2{%\r\n  \\makeatletter%\r\n  \\g@addto@macro\\mycommand{\\ifstrequal{#1}{##1}{#2}{}}%\r\n  \\makeatother%\r\n}%\r\n\r\n\\begin{document}\r\n\\mycommand{a}%\r\n\\mapmycommand{a}{b}%\r\n\\mycommand{a}%\r\n\\mapmycommand{c}{d}%\r\n\\mycommand{c}%\r\n\\mycommand{a}%\r\n\\end{document}\r\n```\r\n\r\nWhat should I do to get the desired behaviour?	643	2020-01-28 14:04:47.089931+00	0	4	3	703	1778	3610	\N	0	0	0	\N	f	f	1	\N
4	234	493	2019-12-29 08:38:20.150147+00	Getting the outline of glyphs into TikZ in the 2020ies	I am wondering if one can get the outlines of glyphs somehow into Ti*k*Z without too much hassle. \r\n\r\nIn asymptote and to some extent in PSTricks this is possible, see e.g. https://tex.stackexchange.com/a/21593. (IMHO in PSTricks this is sort of a cheat, but I do not want to discuss this here.)\r\n\r\nIn Ti*k*Z it seems to be possible only with tremendous effort, see e.g. https://tex.stackexchange.com/a/21594. However, this information is almost a decade old. So I am hoping that there are now more user-friendly methods available, ideally as easy to use as in asymptote. \r\n\r\nWhy am I interested in this? One reason is that there is the perspective library, another reason is that there are generic nonlinear transformations. It would be really gorgeous if one could apply them to glyphs in Ti*k*Z.\r\n\r\nTo be clear, what I am not interested in is something that can also be achieved with path fading. I really want to have an explicit path that can be transformed in an any way a path can be transformed, without limitations. And, of course, the more user-friendly, the better.\r\n\r\nWhat have I tried? Not much that I can present here. So no MWE, sorry. I could copy one from the links above, but do not see what this will add. (This is not because I am too lazy to forge an MWE, but I really do not see its purpose here.)	505	2019-12-29 08:38:20.150147+00	0	4	3	568	1215	2514	\N	0	0	0	\N	f	f	1	\N
1	850	636	2020-01-29 15:02:46.148677+00	Pings in chat	How do pings in chat work? How to know which users will be notified of a message?\r\n\r\n(Since this questions seems to come up fairly regularly, I thought it would be good to have a meta question to summarise the information)	651	2020-01-29 15:02:46.148677+00	0	4	1	711	1814	3470	\N	0	0	0	\N	f	f	2	\N
4	830	673	2020-02-03 06:35:47.577632+00	Debug `! Missing number, treated as zero.` error	I was getting one error (i.e. `! Missing number, treated as zero.`) which I found irrelevant. I checked the brackets many a times and I think I am right at them. My question is why is `\\\\` good for the first line, but not good for the second line? Also replacing `\\par` with a blank line gives the exact result that I want, but why `\\\\` doesn't work? The code is as follows -\r\n\r\n```\r\n\\documentclass{article}\r\n\\setlength{\\parindent}{0pt}\r\n\r\n\\begin{document}\r\n[abc]\\\\\r\ndef\\par%\\\\\r\n[ghi]\\\\\r\n\\end{document}\r\n```	688	2020-02-03 12:52:15.160601+00	0	4	5	748	1969	3743	\N	0	0	0	\N	f	f	1	\N
1	167	637	2020-01-29 15:21:15.530785+00	Background colour for partially transparent avatars	If one's avatar is partially transparent, for example like the topanswers icon https://avatars3.githubusercontent.com/u/59606420?s=400&v=4, then it currently will be displayed with black background\r\n\r\n![document2.png](/image?hash=afdaa0ee88e16fea2fbe6d79a8f285ce225f5fe56676d190342164e474712780)\r\n\r\nFor me this was quite surprising, because from other sites/programs I am used to substituting transparency with white or grey. \r\n\r\n![document1.png](/image?hash=0d4983f94e35ee323eb7e41b55896fa9da5da02e927fa09076899c5164d7121f) ![document.png](/image?hash=6f610483220b6273b8189e0130c3661da4ffe84f29b8ec681c75573db7d95277)\r\n\r\nWhat does the community think? Do you like the black background? Should it be changed (if technically possible)?\r\n	652	2020-01-29 15:22:21.046581+00	3	4	1	712	1823	3475	\N	0	0	0	\N	f	f	2	\N
1	759	679	2020-02-03 17:31:23.414864+00	Codeblock Button Bug/Unexpected Behavior	So while posting a [long, drawn out question about using tables as queues](https://topanswers.xyz/databases?q=678), I had to post a few blocks of code and noticed the following (unexpected??) behavior.\r\n\r\nTake, for instance, the following simple SQL statement:\r\n\r\n> SELECT\t*  \r\n> FROM\ttable\r\n\r\n(blockquote intentional)\r\n\r\nWhen I highlight just those two lines and click either `CTRL`+`K` or the *Code* button, the following formatting occurs:\r\n\r\n`SELECT\t*\r\nFROM\ttable`\r\n\r\n(one backtick is placed prior to and at the end of the statement)\r\n\r\nWhat I would expect to happen, as this is a multi-line statement, is as follows:\r\n\r\n```\r\nSELECT\t*\r\nFROM\ttable\r\n```\r\n(three backticks are placed prior to and at the end of the statement)\r\n\r\nUpon further digging the UI works as expected *if I also select an empty line after the statement* and then either click `CTRL`+`K` or click the *Code* button.\r\n\r\nI suspect when a multi-line statement is selected, the multi-line code format notation (e.g. ```) should be used by default instead of one backtick, as is currently occurring.	694	2020-02-03 17:31:23.414864+00	5	4	1	754	1974	3722	\N	0	0	0	\N	f	f	2	\N
4	168	393	2019-12-09 20:09:53.69163+00	Do we let OPs of imported questions know there is an answer?	If we import a question from TeX.SX which doesn't have an answer there and give an answer here, do we let the OPs of such questions know there is an answer, but just not on TeX.SX?\r\n\r\nPoint in question: https://topanswers.xyz/tex?q=362 and its original https://tex.stackexchange.com/questions/518333\r\n\r\n---\r\n\r\nOf course this is only applicable once we go public.	404	2019-12-09 20:10:05.952941+00	10	4	1	468	1133	2117	\N	0	0	0	\N	f	f	2	\N
1	96	346	2019-12-06 06:48:39.563355+00	Retain link to original posting location when importing from SE	I understand the requirement to link for licensing reasons on other people's posts and not on your own, but I would actually _like_ to show the back linkts to original  locations when I import content from SE.\r\n\r\nFor posts that were directly imported, please add or allow and option to add a link back to the source even when importing my own posts.	355	2019-12-06 06:48:39.563355+00	1	4	1	421	2092	2379	\N	0	0	0	\N	f	f	2	\N
2	87	225	2018-08-16 11:58:09+00	Why does changing the declared join column order introduce a sort?	I have two tables with identically named, typed, and indexed key columns. One of the them has a *unique* clustered index, the other one has a *non-unique*.\r\n\r\n## The test setup\r\n\r\nSetup script, including some realistic statistics:\r\n\r\n    DROP TABLE IF EXISTS #left;\r\n    DROP TABLE IF EXISTS #right;\r\n\r\n    CREATE TABLE #left (\r\n        a       char(4) NOT NULL,\r\n        b       char(2) NOT NULL,\r\n        c       varchar(13) NOT NULL,\r\n        d       bit NOT NULL,\r\n        e       char(4) NOT NULL,\r\n        f       char(25) NULL,\r\n        g       char(25) NOT NULL,\r\n        h       char(25) NULL\r\n        --- and a few other columns\r\n    );\r\n    \r\n    CREATE UNIQUE CLUSTERED INDEX IX ON #left (a, b, c, d, e, f, g, h)\r\n    \r\n    UPDATE STATISTICS #left WITH ROWCOUNT=63800000, PAGECOUNT=186000;\r\n    \r\n    CREATE TABLE #right (\r\n        a       char(4) NOT NULL,\r\n        b       char(2) NOT NULL,\r\n        c       varchar(13) NOT NULL,\r\n        d       bit NOT NULL,\r\n        e       char(4) NOT NULL,\r\n        f       char(25) NULL,\r\n        g       char(25) NOT NULL,\r\n        h       char(25) NULL\r\n        --- and a few other columns\r\n    );\r\n    \r\n    CREATE CLUSTERED INDEX IX ON #right (a, b, c, d, e, f, g, h)\r\n    \r\n    UPDATE STATISTICS #right WITH ROWCOUNT=55700000, PAGECOUNT=128000;\r\n\r\n## The repro\r\n\r\nWhen I join these two tables on their clustering keys, I expect a one-to-many MERGE join, like so:\r\n\r\n    SELECT *\r\n    FROM #left AS l\r\n    LEFT JOIN #right AS r ON\r\n        l.a=r.a AND\r\n        l.b=r.b AND\r\n        l.c=r.c AND\r\n        l.d=r.d AND\r\n        l.e=r.e AND\r\n        l.f=r.f AND\r\n        l.g=r.g AND\r\n        l.h=r.h\r\n    WHERE l.a='2018';\r\n\r\nThis is the query plan I want:\r\n\r\n[![This is what I want.][1]][1]\r\n\r\n(Never mind the warnings, they have to do with the fake statistics.)\r\n\r\nHowever, if I change the order of the columns around in the join, like so:\r\n\r\n    SELECT *\r\n    FROM #left AS l\r\n    LEFT JOIN #right AS r ON\r\n        l.c=r.c AND     -- used to be third\r\n        l.a=r.a AND     -- used to be first\r\n        l.b=r.b AND     -- used to be second\r\n        l.d=r.d AND\r\n        l.e=r.e AND\r\n        l.f=r.f AND\r\n        l.g=r.g AND\r\n        l.h=r.h\r\n    WHERE l.a='2018';\r\n\r\n... this happens:\r\n\r\n[![The query plan after changing the declared column order in the join.][2]][2]\r\n\r\nThe Sort operator seems to order the streams according to the declared order of the join, i.e. `c, a, b, d, e, f, g, h`, which adds a blocking operation to my query plan.\r\n\r\n## Things I've looked at\r\n\r\n* I've tried changing the columns to `NOT NULL`, same results.\r\n* The original table was created with `ANSI_PADDING OFF`, but creating it with `ANSI_PADDING ON` does not affect this plan.\r\n* I tried an `INNER JOIN` instead of `LEFT JOIN`, no change.\r\n* I discovered it on a 2014 SP2 Enterprise, created a repro on a 2017 Developer (current CU).\r\n* Removing the WHERE clause on the leading index column does generate the good plan, but it kind of affects the results.. :)\r\n\r\n## Finally, we get to the question\r\n\r\n* Is this intentional?\r\n* Can I eliminate the sort without changing the query (which is vendor code, so I'd really rather not...). I can change the table and indexes.\r\n\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/HnXXA.png\r\n  [2]: https://i.stack.imgur.com/qw7bT.png	233	2019-12-01 15:21:59.459747+00	0	4	1	300	474	599	215078	0	0	0	2019-12-01 15:20:56.826434+00	f	f	1	1
2	37	548	2018-09-14 19:21:05+00	List ROW_OVERFLOW_DATA pages for a specific table	I'm attempting to get a list of pages for a table that has rows with ROW_OVERFLOW_DATA.  I can get the list of allocated pages from the undocumented DMV, `sys.db_db_database_page_allocations`, however, there appears to be no ROW_OVERFLOW_DATA pages listed in the output of that DMV.  Is there some other DMV that I simply cannot locate?\r\n\r\nMinimal, complete, and (hopefully!) verifiable example:\r\n\r\n    USE tempdb;\r\n    \r\n    IF OBJECT_ID(N'dbo.t', N'U') IS NOT NULL\r\n    DROP TABLE dbo.t;\r\n    GO\r\n    \r\n    CREATE TABLE dbo.t\r\n    (\r\n        rownum int NOT NULL IDENTITY(1,1)\r\n            PRIMARY KEY CLUSTERED\r\n        , on_row_data varchar(30) NOT NULL\r\n            DEFAULT ('on_row_data')\r\n        , off_row_data varchar(MAX) NOT NULL\r\n            DEFAULT REPLICATE('A', 20000) --PLENTY BIG ENOUGH!\r\n    ) WITH (DATA_COMPRESSION = NONE); --not compressing those pages!\r\n    \r\n    INSERT INTO dbo.t DEFAULT VALUES;\r\n    \r\n    DECLARE @ObjectID int = (SELECT o.object_id FROM sys.objects o WHERE o.name = 't');\r\n    DECLARE @PageID int;\r\n    DECLARE @PageTypeDesc varchar(100);\r\n    \r\n    SELECT FileID = dpa.allocated_page_file_id\r\n        , PageID = dpa.allocated_page_page_id\r\n        , PageTypeDesc = dpa.page_type_desc\r\n    FROM sys.dm_db_database_page_allocations(DB_ID(), @ObjectID, NULL, NULL, 'DETAILED') dpa\r\n\r\nOutput looks like:\r\n\r\n```\r\n╔════════╦════════╦══════════════╗\r\n║ FileID ║ PageID ║ PageTypeDesc ║\r\n╠════════╬════════╬══════════════╣\r\n║      1 ║   1598 ║ IAM_PAGE     ║\r\n║      3 ║ 105368 ║ DATA_PAGE    ║\r\n║      3 ║ 105369 ║ NULL         ║\r\n║      3 ║ 105370 ║ NULL         ║\r\n║      3 ║ 105371 ║ NULL         ║\r\n║      3 ║ 105372 ║ NULL         ║\r\n║      3 ║ 105373 ║ NULL         ║\r\n║      3 ║ 105374 ║ NULL         ║\r\n║      3 ║ 105375 ║ NULL         ║\r\n╚════════╩════════╩══════════════╝\r\n```\r\n\r\nWhich makes sense, other than the missing ROW_OVERFLOW_DATA page.  We have a single index allocation map page, and a full extents-worth of 8KB data pages, with only a single one of those pages actually allocated.\r\n\r\nSimilarly, if I use the undocumented `sys.fn_PhysLocCracker` function to show the page where each row exists, as in:\r\n\r\n    SELECT *\r\n    FROM dbo.t\r\n    CROSS APPLY sys.fn_PhysLocCracker(%%PHYSLOC%%)\r\n\r\nI only see the `DATA_PAGE` listed:\r\n\r\n```\r\n╔════════╦═════════════╦═════════════════════╦═════════╦═════════╦═════════╗\r\n║ rownum ║ on_row_data ║     off_row_data    ║ file_id ║ page_id ║ slot_id ║\r\n╠════════╬═════════════╬═════════════════════╬═════════╬═════════╬═════════╣\r\n║      1 ║ on_row_data ║ AAAAAAAAAAAAAAAAAAA ║       3 ║  105368 ║       0 ║\r\n╚════════╩═════════════╩═════════════════════╩═════════╩═════════╩═════════╝\r\n```\r\n\r\nLikewise, if I use `DBCC IND(database, table, index)` I only see the two pages listed:\r\n\r\n    DBCC IND (tempdb, t, 1);\r\n\r\nOutput:\r\n\r\n```\r\n╔═════════╦═════════╦════════╦════════╦════════════╦═════════╦═════════════════╦═════════════════════╦════════════════╦══════════╦════════════╦═════════════╦═════════════╦═════════════╦═════════════╦══╗\r\n║ PageFID ║ PagePID ║ IAMFID ║ IAMPID ║  ObjectID  ║ IndexID ║ PartitionNumber ║     PartitionID     ║ iam_chain_type ║ PageType ║ IndexLevel ║ NextPageFID ║ NextPagePID ║ PrevPageFID ║ PrevPagePID ║  ║\r\n╠═════════╬═════════╬════════╬════════╬════════════╬═════════╬═════════════════╬═════════════════════╬════════════════╬══════════╬════════════╬═════════════╬═════════════╬═════════════╬═════════════╬══╣\r\n║       1 ║    1598 ║ NULL   ║ NULL   ║ 2069582411 ║       1 ║               1 ║ 6989586877272752128 ║ In-row data    ║       10 ║ NULL       ║           0 ║           0 ║           0 ║           0 ║  ║\r\n║       3 ║  105368 ║ 1      ║ 1598   ║ 2069582411 ║       1 ║               1 ║ 6989586877272752128 ║ In-row data    ║        1 ║ 0          ║           0 ║           0 ║           0 ║           0 ║  ║\r\n╚═════════╩═════════╩════════╩════════╩════════════╩═════════╩═════════════════╩═════════════════════╩════════════════╩══════════╩════════════╩═════════════╩═════════════╩═════════════╩═════════════╩══╝\r\n```\r\n\r\n\r\nIf I look at the actual page contents, using `DBCC PAGE`, it looks like I *still* don't see anything about which page contains the ROW_OVERFLOW_DATA - I'm sure it must be there, I probably just don't know what to look at:\r\n\r\n    DBCC PAGE (tempdb, 3, 105368 , 3) WITH TABLERESULTS;\r\n\r\nThe results are too big to fit here, if I include the memory dump rows, but this is the header output:\r\n\r\n```\r\n╔══════════════╦════════════════════════════════╦═══════════════════════════════╦═══════════════════════════════╗\r\n║ ParentObject ║             Object             ║             Field             ║             VALUE             ║\r\n╠══════════════╬════════════════════════════════╬═══════════════════════════════╬═══════════════════════════════╣\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bpage                         ║ 0x000002431A8A2000            ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bhash                         ║ 0x0000000000000000            ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bpageno                       ║ (3:105368)                    ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bdbid                         ║ 2                             ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ breferences                   ║ 0                             ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bcputicks                     ║ 0                             ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bsampleCount                  ║ 0                             ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bUse1                         ║ 63172                         ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bstat                         ║ 0x10b                         ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ blog                          ║ 0x212121cc                    ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bnext                         ║ 0x0000000000000000            ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bDirtyContext                 ║ 0x000002435DA77160            ║\r\n║ BUFFER:      ║ BUF @0x000002437E86D5C0        ║ bstat2                        ║ 0x0                           ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_pageId                      ║ (3:105368)                    ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_headerVersion               ║ 1                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_type                        ║ 1                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_typeFlagBits                ║ 0x0                           ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_level                       ║ 0                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_flagBits                    ║ 0xc000                        ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_objId (AllocUnitId.idObj)   ║ 3920762                       ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_indexId (AllocUnitId.idInd) ║ 512                           ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ Metadata: AllocUnitId         ║ 144115445026914304            ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ Metadata: PartitionId         ║ 6989586877272752128           ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ Metadata: IndexId             ║ 1                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ Metadata: ObjectId            ║ 2069582411                    ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_prevPage                    ║ (0:0)                         ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_nextPage                    ║ (0:0)                         ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ pminlen                       ║ 8                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_slotCnt                     ║ 1                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_freeCnt                     ║ 66                            ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_freeData                    ║ 8124                          ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_reservedCnt                 ║ 0                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_lsn                         ║ (36:47578:1)                  ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_xactReserved                ║ 0                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_xdesId                      ║ (0:0)                         ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_ghostRecCnt                 ║ 0                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ m_tornBits                    ║ 0                             ║\r\n║ PAGE HEADER: ║ Page @0x000002431A8A2000       ║ DB Frag ID                    ║ 1                             ║\r\n║ PAGE HEADER: ║ Allocation Status              ║ GAM (3:2)                     ║ ALLOCATED                     ║\r\n║ PAGE HEADER: ║ Allocation Status              ║ SGAM (3:3)                    ║ NOT ALLOCATED                 ║\r\n║ PAGE HEADER: ║ Allocation Status              ║ PFS (3:105144)                ║ 0x40 ALLOCATED   0_PCT_FULL   ║\r\n║ PAGE HEADER: ║ Allocation Status              ║ DIFF (3:6)                    ║ NOT CHANGED                   ║\r\n║ PAGE HEADER: ║ Allocation Status              ║ ML (3:7)                      ║ NOT MIN_LOGGED                ║\r\n║ PAGE HEADER: ║ Slot 0 Offset 0x60 Length 8028 ║ Record Type                   ║ PRIMARY_RECORD                ║\r\n║ PAGE HEADER: ║ Slot 0 Offset 0x60 Length 8028 ║ Record Attributes             ║  NULL_BITMAP VARIABLE_COLUMNS ║\r\n║ PAGE HEADER: ║ Slot 0 Offset 0x60 Length 8028 ║ Record Size                   ║ 8028                          ║\r\n╚══════════════╩════════════════════════════════╩═══════════════════════════════╩═══════════════════════════════╝\r\n```\r\n	561	2020-01-10 21:28:18.462333+00	0	4	1	623	1397	2842	217660	0	0	0	2020-01-10 21:21:50.893152+00	f	f	1	1
8	895	713	2020-02-14 19:59:42.383978+00	How can I Conditionally use std::underlying_type?	I want to use [`std::underlying_type`](https://en.cppreference.com/w/cpp/types/underlying_type) with [`std::enable_if`](https://en.cppreference.com/w/cpp/types/enable_if) to conditionally cause [Substitution Failure Is Not An Error (SFINAE)](https://en.cppreference.com/w/cpp/language/sfinae). I'm using this function to conditionally allow either `unsigned int` parameters or enum parameters whose underlying type is `uint8_t`:\r\n\r\n    template <typename T>\r\n    std::enable_if_t<(std::is_enum_v<T> && std::is_same_v<std::underlying_type_t<T>, uint8_t>) || std::is_same_v<T, unsigned int>> Foo(const T param) {\r\n        std::cout << static_cast<int>(param) << std::endl;\r\n    }\r\n\r\nHowever I'm getting the error when calling `Foo(13U)`:\r\n\r\n> error: no matching function for call to `Foo(unsigned int)`\r\n\r\nHow can I change the `std::enable_if_t` parameter to make this work as intended?\r\n\r\n[**Live Example**](http://coliru.stacked-crooked.com/a/ef210af489da6d12)	729	2020-02-14 19:59:42.383978+00	0	4	1	788	2191	4038	\N	0	0	0	\N	f	f	1	\N
1	811	639	2020-01-29 23:59:03.9242+00	Make the search bar universally available	The search bar is only visible in the main site listing view, but not on post pages or during editing. But there's usually plenty of space in the top bar, so I see no reason to omit it.\r\n\r\nPlease make it universal.\r\n\r\n**Edit:** Now, the search bar on the main page is "live", filtering results as one types. On other pages, it should just be a "dumb" search bar, and only upon pressing [Enter] should one be brought to the main page and the search be performed.	654	2020-01-31 00:03:10.644851+00	5	1	1	714	1881	3567	\N	0	0	0	\N	f	f	2	\N
1	830	677	2020-02-03 12:58:52.861843+00	Notifications in 'newest first' order.	Currently the notifications are ordered according to time and in 'oldest first' manner. It's always better to have notifications in newest first chronology. Is there any strong motivation for having notifications in the 'oldest first' order?	692	2020-02-03 12:58:52.861843+00	8	4	5	752	1978	3776	\N	0	0	0	\N	f	f	2	\N
1	811	640	2020-01-30 00:44:34.507375+00	Special "About" post	* "What is this site about?"\r\n* "What are best practices here?"\r\n* "What kind of questions are on topic?"\r\n\r\nThese are some of the questions that should be easy to get an answer for.\r\n\r\nI suggest that the main site view top bar gets an [About]() link on the left, next to the site name. It should go to a special Wiki post where the community in question writes a brief intro to the site, and link to various relevant Wiki posts.	655	2020-02-12 22:16:55.458116+00	11	1	1	715	2200	4049	\N	0	0	0	\N	f	f	2	\N
1	702	638	2020-01-29 19:33:20.06599+00	Who is "Community"?	I have noticed [a post][1] [or two][2] by a user called "Community".\r\n\r\nWho is this user? The account appears to be a bit different from other "Community" users I've seen elsewhere.\r\n\r\n\r\n[1]: https://topanswers.xyz/meta?q=636\r\n[2]: https://topanswers.xyz/tex?q=606	653	2020-01-29 19:59:04.134831+00	6	1	1	713	1831	3582	\N	0	0	0	\N	f	f	2	\N
4	168	720	2020-02-17 16:00:18.64911+00	Voting on Contest Answers	Contests have a very different appeal to them compared to other question types. Also, there are usually no quality concerns for answers to contests.\r\n\r\nDo you think voting on contest *answers* should be different to voting on other objects. If so, which voting mechanics would you apply to contest answers?\r\n	736	2020-02-17 16:03:53.417629+00	8	6	3	795	2255	4176	\N	0	0	0	\N	f	f	2	\N
2	2	30	2019-11-14 11:41:45.717136+00	How can we make db<>fiddle (and integration here) better?	[db<>fiddle](https://dbfiddle.uk) was designed with markdown-based Q&A in mind, so it seems sensible to use a post here for comments/suggestions/bug reports.\r\n\r\nPlease feel free to comment in the chat room attached to this post.\r\n\r\n**Please post concrete feature requests and bug reports as answers below.** Please also post any feature requests and bug reports about db<>fiddle integration with TopAnswers.\r\n\r\nThe current list of supported databases is:\r\n\r\n* SQL Server 2012, 2014, 2016, 2017, 2017 Linux, 2019 and 2019 Linux RC1\r\n* Postgres 8.4, 9.4, 9.5, 9.6, 10, 11 and 12\r\n* Oracle 11.2 and 18\r\n* MySQL 5.6, 5.7 and 8.0\r\n* MariaDB 10.2, 10.3 and 10.4\r\n* SQLite 3.8, 3.16 and 3.27 (but 3.16 is currently offline for [security reasons](https://security-tracker.debian.org/tracker/CVE-2018-20346))\r\n* Db2 Developer-C 11.1 (with lots of thanks to [@mustaccio](https://dba.meta.stackexchange.com/users/23721/mustaccio))\r\n* Firebird 3.0\r\n\r\nHere's an example (the image links to the fiddle):\r\n\r\n[![SQL Server 2019 'help'](/image?hash=407870c3b82d22484cb884d64c05e7a70ca3f3e97cd776b4f82cf701a55e56a3)\r\n](https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=9bcd60e2bcd7966fc3be475addab8eb2)\r\n\r\nand here's a really interesting one just for fun:\r\n\r\n[![Naughts and Crosses](/image?hash=91efa2c2370e503e4158d295924693af7ce4f9eb25d8435126c2fcbb87e690fd)](https://dbfiddle.uk/?rdbms=postgres_11&fiddle=71940aadda50f55bfede87606cd1dc2c)\r\n\r\nfinally, just because we can, here is the same fiddle integrated into this post. You can edit it and run it live right here:\r\n\r\n<>https://dbfiddle.uk/?rdbms=postgres_11&fiddle=71940aadda50f55bfede87606cd1dc2c\r\n\r\n\r\n\r\n	37	2019-12-14 16:27:10.75195+00	5	1	1	31	2003	3729	\N	0	0	0	\N	f	f	2	\N
2	16	239	2019-01-02 18:16:12+00	When can SARGable predicates be pushed into a CTE or derived table?	## Sandbag\r\n\r\nWhile working on Top Quality Blog Posts®, I came across some optimizer behavior I found really ~~infuriating~~ interesting. I don't immediately have an explanation, at least not one I'm happy with, so I'm putting it here in case someone smart shows up.\r\n\r\nIf you want to follow along, you can grab the 2013 version of the [Stack Overflow data dump here][1]. I'm using the Comments table, with one additional index on it.\r\n\r\n    CREATE INDEX [ix_ennui] ON [dbo].[Comments] ( [UserId], [Score] DESC );\r\n\r\n## Query One\r\n\r\nWhen I query the table like so, I get an [odd query plan][2].\r\n\r\n    WITH x\r\n        AS\r\n         (\r\n             SELECT   TOP 101\r\n                      c.UserId, c.Text, c.Score\r\n             FROM     dbo.Comments AS c\r\n             ORDER BY c.Score DESC\r\n         )\r\n    SELECT *\r\n    FROM   x\r\n    WHERE  x.Score >= 500;\r\n\r\n[![NUTS][3]][3]\r\n\r\nThe SARGable predicate on Score isn't pushed inside the CTE. It's in a filter operator much later in the plan.\r\n\r\n[![NUTS][4]][4]\r\n\r\nWhich I find odd, since the `ORDER BY` is on the same column as the filter. \r\n\r\n## Query Two\r\n\r\nIf I change the query, it does get pushed.\r\n\r\n    WITH x\r\n        AS\r\n         (\r\n             SELECT   c.UserId, c.Text, c.Score\r\n             FROM     dbo.Comments AS c\r\n         )\r\n    SELECT TOP 101 *\r\n    FROM   x\r\n    WHERE  x.Score >= 500\r\n    ORDER BY x.Score DESC;\r\n\r\nThe [query plan changes][5], too, and runs much faster, with no spill to disk. They both produce the same results, with the predicate at the nonclustered index scan.\r\n\r\n[![NUTS][6]][6]\r\n\r\n[![NUTS][7]][7]\r\n\r\n## Query Three\r\n\r\nThis is the equivalent of writing the query like so:\r\n\r\n    SELECT   TOP 101\r\n             c.UserId, c.Text, c.Score\r\n    FROM     dbo.Comments AS c\r\n    WHERE c.Score >= 500\r\n    ORDER BY c.Score DESC;\r\n\r\n## Query Four\r\n\r\nUsing a derived table gets the same "bad" query plan as the initial CTE query\r\n\r\n    SELECT *\r\n    FROM   (   SELECT   TOP 101\r\n                        c.UserId, c.Text, c.Score\r\n               FROM     dbo.Comments AS c\r\n               ORDER BY c.Score DESC ) AS x\r\n    WHERE x.Score >= 500;\r\n\r\n\r\n### Things get even weirder when...\r\n\r\nI change the query to order the data ascending, and the filter to `<=`.\r\n\r\nTo keep from making this question overlong, I'm going to put everything together.\r\n\r\n## Queries\r\n\r\n    --Derived table\r\n    SELECT *\r\n    FROM   (   SELECT   TOP 101\r\n                        c.UserId, c.Text, c.Score\r\n               FROM     dbo.Comments AS c\r\n               ORDER BY c.Score ASC ) AS x\r\n    WHERE x.Score <= 500;\r\n    \r\n    \r\n    --TOP inside CTE\r\n    WITH x\r\n        AS\r\n         (\r\n             SELECT   TOP 101\r\n                      c.UserId, c.Text, c.Score\r\n             FROM     dbo.Comments AS c\r\n             ORDER BY c.Score ASC\r\n         )\r\n    SELECT *\r\n    FROM   x\r\n    WHERE  x.Score <= 500;\r\n    \r\n    \r\n    --Written normally\r\n    SELECT   TOP 101\r\n             c.UserId, c.Text, c.Score\r\n    FROM     dbo.Comments AS c\r\n    WHERE c.Score <= 500\r\n    ORDER BY c.Score ASC;\r\n    \r\n    --TOP outside CTE\r\n    WITH x\r\n        AS\r\n         (\r\n             SELECT   c.UserId, c.Text, c.Score\r\n             FROM     dbo.Comments AS c\r\n         )\r\n    SELECT TOP 101 *\r\n    FROM   x\r\n    WHERE  x.Score <= 500\r\n    ORDER BY x.Score ASC;\r\n\r\nPlans\r\n-----\r\n[Plan link][8].\r\n\r\n[![NUTS][9]][9]\r\n\r\nNote that none of these queries take advantage of the nonclustered index -- the only thing that changes here is the position of the filter operator. In no case is the predicate pushed to the index access.\r\n\r\n## A Question Appears!\r\n\r\nIs there a reason that a SARGable predicate can be pushed in some scenarios and not in others? The differences within the queries sorted in descending order are interesting, but the differences between those and the ones that are ascending bizarre.\r\n\r\nFor anyone interested, here are the plans with only an index on `Score`:\r\n\r\n - [DESC][10]\r\n - [ASC][11]\r\n\r\n\r\n  [1]: https://brentozar.com/go/querystack\r\n  [2]: https://www.brentozar.com/pastetheplan/?id=ByaivOq-E\r\n  [3]: https://i.stack.imgur.com/joCG8.jpg\r\n  [4]: https://i.stack.imgur.com/vGPWm.jpg\r\n  [5]: https://www.brentozar.com/pastetheplan/?id=SkxctOc-4\r\n  [6]: https://i.stack.imgur.com/gOh4o.jpg\r\n  [7]: https://i.stack.imgur.com/il4yj.jpg\r\n  [8]: https://www.brentozar.com/pastetheplan/?id=S1KsC_cbV\r\n  [9]: https://i.stack.imgur.com/eyw4w.jpg\r\n  [10]: https://www.brentozar.com/pastetheplan/?id=Hkvg2FqWE\r\n  [11]: https://www.brentozar.com/pastetheplan/?id=HkL42t5-N	247	2019-12-01 20:36:24.259498+00	0	4	1	314	501	1490	226185	0	0	0	2019-12-01 20:33:57.923175+00	f	f	1	1
1	2	211	2019-11-30 15:24:07.059118+00	I'd like a new community on TopAnswers. How do I go about making that happen?	TopAnswers has been designed to host multiple, independent[^1], communities of creators, curators, and beneficiaries of high quality Q&A.\r\n\r\nIf you'd like a particular community to be created here, please let us know in an answer below. We'll need:\r\n\r\n1. A topic that lends itself to the focused Q&A format.\r\n1. At least a few people who want to get involved, who are passionate about building that community here.\r\n\r\nIt would also be nice to know up front if you would hope to eventually get any particular customized tooling, things like:\r\n\r\n* specialist markdown plugins like MathML or MathJax\r\n* particular font support\r\n* integration with external tools or services/apis (for example ['databases'](/databases) has [db<>fiddle integration](/databases?q=30))\r\n\r\n\r\n\r\n\r\n\r\n\r\n[^1]: independent but perhaps sometimes overlapping, both in terms of topicality and participation.	218	2019-11-30 22:05:36.084634+00	9	1	1	286	2117	4037	\N	0	0	0	\N	f	f	2	\N
1	854	653	2020-01-31 03:18:30.107807+00	Strange chat scrolling behavior	If I scroll up in the chat to respond to an older message, when I start typing a message, the chat immediately starts scrolling down automatically and scrolls to the end.	668	2020-01-31 03:18:30.107807+00	7	4	1	728	1932	3693	\N	0	0	0	\N	f	f	2	\N
4	233	714	2020-02-15 09:44:00.145071+00	List of all the delimiters accepted by \\left	In Section *60.3 Delimiters* of the [TikZ-PGF manual](https://www.ctan.org/pkg/pgf) is written: \r\n\r\n> The <*delimiter*> can be any delimiter that is acceptable to TEX’s `\\left` command.\r\n\r\nWhich are all the delimiters acceptable to TEX’s `\\left` command? \r\n	730	2020-02-15 20:31:50.489874+00	0	4	1	789	2199	4044	\N	0	0	0	\N	f	f	1	\N
1	854	654	2020-01-31 03:39:34.313814+00	My questions?	Is there a way to list all the questions I asked? And similarly all the questions asked by another user?	669	2020-01-31 03:39:34.313814+00	2	4	1	729	1892	3961	\N	0	0	0	\N	f	f	2	\N
4	830	680	2020-02-04 06:08:06.993635+00	Draw an opaque circle (with some text) on a tikz line.	I have the following code right now -\r\n\r\n```\r\n\\documentclass{standalone}\r\n\\usepackage{tikz}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\draw (0,0) -- (3,0);\r\n\\draw (1.5,0) circle (5pt) node{a};\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\nIf we see the output, the circle has the line inside it which I don't want. Also I don't want to draw the line in two parts (i.e. \r\n\r\n```\r\n\\begin{tikzpicture}\r\n\\draw (0,0) -- (1.3,0);\r\n\\draw (1.7,0) -- (3,0);\r\n\\draw (1.5,0) circle (5pt) node{a};\r\n\\end{tikzpicture}\r\n```\r\n)\r\n\r\nThis code gives me the output exactly like I want, but I don't want to do that calculation manually and draw so many lines in my entire picture. How to have a circle which is opaque, but also with some text?	695	2020-02-04 06:09:00.559407+00	0	4	5	755	2053	3978	\N	0	0	0	\N	f	f	1	\N
2	164	307	2019-03-06 15:04:48+00	When do _WA_Sys_ statistics Get Updated?	I have some of the auto generated `_WA_Sys_` statistics in my database, which have not been updated in a while (in comparison to other statistics in the same table).\r\n\r\nThe rule of thumb appears to be in tables >500 rows, statistics are updated after (20% + 500 rows) changes.\r\n\r\nHowever, I can see using the following query:\r\n\r\n    SELECT\tt.name,\r\n    \t\ti.name,\r\n    \t\ti.rowcnt,\r\n    \t\ti.rowmodctr,\r\n    \t\tp.last_updated\r\n    FROM\tsys.sysindexes i\r\n    \t\tJOIN sys.tables t\r\n    \t\t\tON i.id = t.object_id\r\n    \t\tJOIN sys.stats s\r\n    \t\t\tON s.object_id = t.object_id AND i.name = s.name\r\n    \t\tCROSS APPLY sys.dm_db_stats_properties(s.object_id,s.stats_id) p\r\n    WHERE\trowmodctr > 0\r\n    ORDER BY i.rowmodctr DESC\r\n\r\n...that there is a table with a number of `_WA_Sys_` statistics that are way out of date (and have a `rowmodctr` that is higher than 20% + 500).\r\n\r\nIf a run a query against the table and add one of the columns associated with the out of date `_WA_Sys_` statistics in the `WHERE` clause, and check the *updated date* of the statistic, I can see it has updated.\r\n\r\nIf I run the same query with the `WHERE` clause again, the statistic doesn't update.\r\n\r\nIt seems like the `_WA_Sys` statistics update when a query is run that will use them and they are out of date?	315	2019-12-05 21:27:09.215324+00	0	4	1	382	721	1550	231466	0	0	0	2019-12-05 13:49:46.029444+00	f	f	1	1
1	12	689	2020-02-08 07:30:06.225398+00	Rewrite links for previously imported Q & A references	When importing a Q & A from Stack Exchange, it would be nice if any embedded links were automatically rewritten to TA references where available.\r\n\r\nFor example, I recently imported:\r\n\r\n* https://topanswers.xyz/databases?q=668\r\n* https://topanswers.xyz/databases?q=669\r\n* https://topanswers.xyz/databases?q=672\r\n\r\nThese were self-answered Q & A that formed a set, with each referring to the other two.\r\n\r\nAfter importing, I had to edit the TA posts to update the links from SE to TA.\r\n\r\nInitially, I am thinking of a one-off rewrite that happens automatically when any link referenced in the imported posts has a copy on TA.	704	2020-02-08 07:30:06.225398+00	7	4	2	764	2054	3845	\N	0	0	0	\N	f	f	2	\N
2	12	180	2016-12-28 13:20:49+00	Does SQL Server cache the result of a multi-statement table-valued function?	A multi-statement table-valued function returns its result in a table variable.\r\n\r\nAre these results ever reused, or is the function always fully evaluated every time it is called?	187	2019-11-27 23:42:04.508441+00	0	4	1	255	374	1471	159333	0	0	0	2019-11-27 23:42:04.508441+00	f	f	1	1
4	202	533	2020-01-08 07:18:51.807088+00	Mouse arrow symbol	Showcase your fun mouse arrow description.\r\n\r\nPossible (boring) option is   [this](http://i.stack.imgur.com/WMBAi.jpg).\r\n\r\n> **Only rule:** Be creative ;)	546	2020-01-08 07:24:01.882458+00	5	4	3	608	1358	3771	\N	0	0	0	\N	f	f	7	\N
4	202	534	2020-01-08 07:24:10.017091+00	Meta question: New tags and description	I am wondering how do we create new tags?\r\n\r\nAnd, is it possible to add description to each tags?	547	2020-01-08 07:24:10.017091+00	0	4	3	609	1352	2750	\N	0	0	0	\N	f	f	1	\N
1	2	541	2020-01-09 23:07:56.764475+00	What fonts should we allow on TopAnswers?	We already [allow](/meta?q=56) a limited choice of normal and monospace fonts which can be selected on [your profile](https://topanswers.xyz/profile)\r\n\r\nIf you want another font added, please answer below with your suggestion (one font per answer), so we can vote and figure out what is in demand.\r\n\r\nThe fonts must be available under a suitable license such as [OFL](https://en.wikipedia.org/wiki/SIL_Open_Font_License). A good place to look might be this [font library](https://fontlibrary.org/en/search?order=pop&license=OFL+%28SIL+Open+Font+License%29), and if you want sans-serif in particular (for example) you can [filter out the rest](https://fontlibrary.org/en/search?order=pop&license=OFL+%28SIL+Open+Font+License%29&category=sans-serif).	554	2020-01-09 23:07:56.764475+00	4	1	1	616	1898	3601	\N	0	0	0	\N	f	f	2	\N
2	161	302	2019-08-02 00:28:51+00	Constant Scan spooling	I have a table with a few dozens of rows. Simplified setup is following\r\n\r\n    CREATE TABLE #data ([Id] int, [Status] int);\r\n    \r\n    INSERT INTO #data\r\n    VALUES (100, 1), (101, 2), (102, 3), (103, 2);\r\n\r\nAnd I have a query that joins this table to a set of table value constructed rows (made of variables and constants), like\r\n\r\n    DECLARE @id1 int = 101, @id2 int = 105;\r\n    \r\n    SELECT\r\n    \tCOALESCE(p.[Code], 'X') AS [Code],\r\n    \tCOALESCE(d.[Status], 0) AS [Status]\r\n    FROM (VALUES\r\n    \t\t(@id1, 'A'),\r\n    \t\t(@id2, 'B')\r\n    \t) p([Id], [Code])\r\n    \tFULL JOIN #data d ON d.[Id] = p.[Id];\r\n\r\nQuery execution plan is showing that optimizer's decision is to use `FULL LOOP JOIN` strategy, which seems appropriate, since both inputs have very few rows. One thing I noticed (and cannot agree), though, is that TVC rows are being spooled (see area of the execution plan in the red box).\r\n\r\n[![Constant Scan spooling][1]][1]\r\n\r\nWhy optimizer introduces spool here, what is the reason to do it? There is nothing complex beyond the spool. Looks like it is not necessary. How to get rid of it in this case, what are the possible ways?\r\n\r\n___\r\n\r\n\r\nThe above plan was obtained on\r\n\r\n> Microsoft SQL Server 2014 (SP2-CU11) (KB4077063) - 12.0.5579.0 (X64)\r\n\r\n  [1]: https://i.stack.imgur.com/sJKdW.png	310	2019-12-05 02:33:10.408836+00	0	4	1	377	660	1543	244383	0	0	0	2019-12-05 02:33:10.408836+00	f	f	1	1
1	854	649	2020-01-30 11:13:05.422653+00	Login key popup issues	When I signed up on this site, the first thing that happened was it popped up a message giving me a login key and asking me to save it. The problem is the key was shown in an alert dialog (or something similar) which didn't allow me to copy and paste the key. I'm using Chromium 79.\r\n\r\nPlease display this information on a regular web page, and consider expanding the explanation, e.g. mentioning the fact that the key can also be retrieved from the user profile at a later time.\r\n\r\nOr perhaps it could go straight to the profile page (so the user can edit other details), with a one-time introduction message or something like that.	664	2020-01-30 11:44:12.490342+00	7	4	1	724	1872	3584	\N	0	0	0	\N	f	f	2	\N
1	811	683	2020-02-06 06:50:51.317988+00	Add custom classes for specific CSS	Pandoc's markdown allows:\r\n```markdown\r\n:::classname\r\ncontent\r\n:::\r\n```\r\nwhich renders as\r\n```html\r\n<div class="classname">\r\n<p>content</p>\r\n</div>\r\n```\r\nWe could selectively allow custom CSS by allowing the `:::` fence syntax and creating class names for selected white-listed CSS, e.g.\r\n```css\r\nfont-face\\3A monospace {\r\n  font-family: monospace;\r\n}\r\n```\r\nso we can write:\r\n```markdown\r\n:::font-family:monospace\r\ncontent\r\n:::\r\n```\r\nThen we can always add more CSS-rule classes later.\r\n\r\n**Note:** Do *not* allow arbitrary CSS, as that can be dangerous to users.	698	2020-02-06 06:50:51.317988+00	0	1	1	758	2055	3846	\N	0	0	0	\N	t	f	2	\N
4	830	693	2020-02-10 08:10:10.717891+00	Package linguex with multicols	This is my code.\r\n\r\n```\r\n\\documentclass{article}\r\n\\usepackage{multicol}\r\n\\usepackage{linguex}\r\n\\usepackage{tikz}\r\n\r\n\\begin{document}\r\n\t\\begin{multicols}{2}\r\n\t\\ex. Two ways of forming a triangle.\\\\\r\n\t\t\\a.\\begin{tikzpicture}\r\n\t\t\t\\draw (0,0) -- (-1.5,-3) -- (1.5,-3) -- cycle;\r\n\t\t\\end{tikzpicture}\\\\\r\n\t\t\\columnbreak\r\n\t\t\\b.\\begin{tikzpicture}\r\n\t\t\\draw (0,0) -- (3,0) -- (3,3) -- cycle;\r\n\t\t\\end{tikzpicture}\\\\\t\t\r\n\r\n\t\\end{multicols}\r\n\t\r\n\\end{document}\r\n```\r\n\r\nIt generates -\r\n\r\n![MWE.png](/image?hash=d751686a1b55d6b875d906ea342930a0cbea6d3975b0834d2f1a649f7ac83e8d)\r\n\r\nI want the `ex.` to be out of the `multicols` environment, because in the output the example `b.`, because I want both `a.` and `b.` to be on the same height. How to achieve it?\r\n\r\nPS - I tried `\\phantom{\\ex.}` but it doesn't work.	708	2020-02-10 08:10:10.717891+00	0	4	5	768	2116	3940	\N	0	0	0	\N	f	f	1	\N
4	234	692	2020-02-10 06:21:18.053613+00	2020 TikZling Academy Awards	Given that the Academy Awards (aka Oscar Awards) are not really fair (Ti*k*Zlings never even made it to the final), let us have our own\r\n\r\n![Screen Shot 2020-02-09 at 10.18.04 PM.png](/image?hash=c820ab5886e6cca4166c8bcba17c9a179669346b8d058d9b95b1507f3863b53e)\r\n\r\nThe best short movie will win. Due date for submission is July 24th, i.e. the same as for the TUG 2020 contest. Only submissions with LaTeX source code will be considered.	707	2020-02-10 06:25:14.712067+00	9	4	3	767	2249	4147	\N	0	0	0	\N	f	t	7	\N
4	872	675	2020-01-31 13:40:07+00	Adding dynamic information to the right upper edge of beamer template	I'm using the AnnArbor template in beamer as follows:\r\n\r\n    \\mode<presentation>\r\n    { \r\n    \\usetheme{AnnArbor}\r\n    \\usecolortheme[named=kugreen]{structure}\r\n    \\useinnertheme{circles}\r\n    \\usefonttheme[onlymath]{serif}\r\n    \\setbeamercovered{transparent}\r\n    \\setbeamertemplate{blocks}[rounded][shadow=true]\r\n    }\r\n\r\nAnd I would like to add a piece of information to some slides, which should be displayed on the top horizontal zone, right edge, as the arrow in the following screen capture shows:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nThis information should belong to each frame (it is not a global logo defined once and for all in the preamble), in fact I want to add bibliographical references for the contents of the frame, so it may be different for every frame or void.\r\n\r\nWhat do I need to redefine to obtain this?\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/BQXeS.png	690	2020-02-03 10:06:31.758912+00	0	4	1	750	2006	3767	526429	0	0	0	2020-02-03 10:06:31.758912+00	f	f	1	2
4	892	691	2020-02-09 17:41:40.086818+00	Lualatex and unicode-math	In this MWE\r\n\r\n```tex\r\n\\documentclass{scrartcl}  \r\n\r\n\\usepackage[spanish, activeacute]{babel}   \r\n\\usepackage{unicode-math}  \r\n\\usepackage{hyperref}  \r\n\r\n\\begin{document}  \r\n\\section{A section}  \r\n\\begin{equation}  \r\n  j =\\sqrt{-1}  \r\n\\end{equation}  \r\n\\end{document}\r\n```\r\n\r\nthere is a fatal error. However, if I comment `activeacute` or `\\usepackage{hyperref}` or `\\usepackage{unicode-math]` or I change the kind of document to article, for example, the error disappears. Any help? Thanks 	706	2020-02-09 23:32:24.009639+00	0	4	1	766	2067	3858	\N	0	0	0	\N	f	f	1	\N
2	155	253	2019-05-29 11:52:45+00	Self referencing scalar function nesting level exceeded when adding a select	**Purpose**\r\n\r\nWhen trying to create a test example of a self referencing function, one version fails while another one succeeds.\r\n\r\nThe only difference being an added `SELECT` to the function body resulting in a different execution plan for both.\r\n\r\n\r\n----------\r\n\r\n\r\n**The function that works** \r\n\r\n    CREATE FUNCTION dbo.test5(@i int)\r\n    RETURNS INT\r\n    AS \r\n    BEGIN\r\n    RETURN(\r\n    SELECT TOP 1\r\n    CASE \r\n    WHEN @i = 1 THEN 1\r\n    WHEN @i = 2 THEN 2\r\n    WHEN @i = 3 THEN  dbo.test5(1) + dbo.test5(2)\r\n    END\r\n    )\r\n    END;\r\n\r\n**Calling the function**\r\n\r\n    SELECT dbo.test5(3);\r\n    \r\n\r\n**Returns**\r\n\r\n    (No column name)\r\n    3\r\n\r\n\r\n----------\r\n\r\n\r\n**The function that does not work**\r\n\r\n    CREATE FUNCTION dbo.test6(@i int)\r\n    RETURNS INT\r\n    AS \r\n    BEGIN\r\n    RETURN(\r\n    SELECT TOP 1\r\n    CASE \r\n    WHEN @i = 1 THEN 1\r\n    WHEN @i = 2 THEN 2\r\n    WHEN @i = 3 THEN (SELECT dbo.test6(1) + dbo.test6(2))\r\n    END\r\n    )END;\r\n    \r\n\r\n**Calling the function**\r\n\r\n    SELECT dbo.test6(3);\r\n\r\nor \r\n\r\n    SELECT dbo.test6(2);\r\n\r\n**Results in the error**\r\n\r\n> Maximum stored procedure, function, trigger, or view nesting level\r\n> exceeded (limit 32).\r\n\r\n**Guessing the cause**\r\n\r\nThere is an additional compute scalar on the estimated plan of the failed function, calling \r\n\r\n    <ColumnReference Column="Expr1002" />\r\n    <ScalarOperator ScalarString="CASE WHEN [@i]=(1) THEN (1) ELSE CASE WHEN [@i]=(2) THEN (2) ELSE CASE WHEN [@i]=(3) THEN [Expr1000] ELSE NULL END END END">\r\n\r\n**And expr1000 being**\r\n\r\n    <ColumnReference Column="Expr1000" />\r\n    <ScalarOperator ScalarString="[dbo].[test6]((1))+[dbo].[test6]((2))">\r\n\r\n*Which could explain the recursive references exceeding 32.*\r\n\r\n**The actual question**\r\n\r\nThe added `SELECT` makes the function call itself over and over, resulting in an endless loop, but why is adding a `SELECT` giving this result?\r\n\r\n\r\n----------\r\n\r\n**Additional info**\r\n\r\n[Estimated execution plans][1]\r\n\r\n[DB<>Fiddle][2]\r\n\r\n    Build version:\r\n    14.0.3045.24\r\n\r\n*Tested on compatibility_levels 100 and 140*\r\n\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=rJVbfl36E\r\n  [2]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=99511f8276f8e74b225deaf3fb8fd344	261	2019-12-03 14:04:28.557655+00	0	4	1	328	551	1497	239324	0	0	0	2019-12-03 14:04:28.557655+00	f	f	1	1
1	811	695	2020-02-10 11:19:05.412045+00	Add "return-to-question" link	Sometimes, I regret pushing the "answer" button, and rather want to go back to the question. Since the original question is shown on the left, its title could conveniently be a link back to it.\r\n\r\n	710	2020-02-10 11:19:05.412045+00	1	1	1	770	2131	4004	\N	0	0	0	\N	t	f	2	\N
4	167	685	2020-02-06 14:58:31.11388+00	TUG2020 @ Rochester	This year's TeX User Group meeting will take place in Rochester (see https://tug.org/tug2020/ for details).\r\n\r\nTo commemorate this event, this contest asks you to use TeX, LaTeX and friends to produce something related to Rochester. \r\n\r\nSome suggestions:\r\n\r\n- typeset the city name in some creative way\r\n- let you inspired by some of the cities nicknames "The Flour City", "The Flower City" and "The World's Image Center"\r\n- Write a text whose paragraphs are shaped like the city skyline \r\n- a complete map of the marmot burrows below the city drawn in TikZ\r\n- ...\r\n\r\nPlease add your answer before July 24, 2020	700	2020-02-06 15:25:06.767458+00	12	4	1	760	2064	4146	\N	0	0	0	\N	f	f	7	\N
4	901	716	2020-02-16 12:49:27.225588+00	About TopTeX	*Place holder for our About page. This is still in the making, please read this as an early draft.*\r\n\r\n# What is TopTeX\r\n\r\nTopTeX is a friendly community for TeX questions and answers. It is part of topanswers.xyz, an open source platform, that is developed with the community in mind and not for-profit.\r\n\r\n# TopTeX's scope\r\n\r\nThe TopTeX community is all about the TeX typesetting engines and their friends.\r\n\r\nIn one sentence, we consider anything on topic that is included in a TeX installation (like TeX Live, MikTeX, etc.), plus editors dedicated to editing files of any TeX format.\r\n\r\nFurthermore questions about how to configure an extensible editor for (La)TeX or about a plugin for such an editor which is TeX related are considered on topic (an example question would be "How to configure editor XY to compile LaTeX files?").\r\n\r\nTo give you a few ideas, we consider on topic (non-exhaustive):\r\n\r\n- TeX, the program\r\n- TeX, the language\r\n- TeX formats, such as plain TeX, LaTeX, ConTeXt, ...\r\n- packages such as Ti*k*Z, pgfplots, asymptote, metapost, pstricks, ...\r\n- tools to organize references, glossaries and indices such as BibTeX, biblatex, makeindex...\r\n- tools to automate (La)TeX, such as `arara`, `latexmk`, ...\r\n- TeX distributions and their management\r\n- ...\r\n\r\nIf you're in doubt just ask in our [main chat](https://topanswers.xyz/tex) whether something is considered on topic. We're glad about any benevolent contributor and you'll get a friendly and timely reply!\r\n\r\n# TopTeX's not-scope\r\n\r\nThere are a few software solutions out there which were inspired by (La)TeX, but don't use TeX, those are not considered on topic here. This category includes tools such as:\r\n\r\n- MathJax\r\n- KaTeX\r\n- Patoline\r\n- SILE\r\n\r\nQuestions about extensible editors which don't directly deal with TeX are *not* considered on topic. An example for such a question would be "How to install plugins for editor XY?".\r\n\r\nFor explicit bug reports and feature requests, please follow the communication channels indicated in the documentation and user manuals of the individual packages or programs. This will ensure that your request will reach the author or maintainer.	732	2020-02-17 17:34:26.176311+00	15	4	1	791	2252	4144	\N	0	0	0	\N	f	f	8	\N
4	167	717	2020-02-16 14:55:59.463099+00	Icon for community wiki user	[We have the opportunity](https://topanswers.xyz/transcript?room=347&id=19710#c19710) to choose our own icon for community wiki posts at TopTeX. It will be used for post like [About TopTeX](https://topanswers.xyz/tex?q=716).\r\n\r\nI think this calls for a little contest :) \r\n\r\nPlease add your answers until Feb, 29th\r\n\r\nP.S. We just need an image, so LaTeX sources are not necessary (but I don't think anybody will complain, if they are included in an answer :) )\r\n	733	2020-02-17 14:15:14.765555+00	11	4	1	792	2243	4177	\N	0	0	0	\N	f	f	7	\N
1	811	694	2020-02-10 11:14:56.710605+00	Better support for tags	# Descriptions\r\n\r\nTags are necessarily brief, so non-insiders need access to more info about them. Having a title-attribute with a short description like on SE would help.\r\n\r\n# Searches\r\n\r\nClicking on a tag should change the search field to say "`[`tag name`]`" and this syntax should filter search results to only include posts tagged thusly. Would it be possible to make sure that on a tag-only search, any blog posts with that tag is always listed first? This would allow using blog posts to give full descriptions for tags, similar to the tag wikis on SE.	709	2020-02-10 14:22:30.091544+00	3	1	1	769	2219	4114	\N	0	0	0	\N	t	f	2	\N
2	77	111	2015-08-19 10:30:18+00	Cannot execute as the database principal because the principal “dbo” does not exist	I restored a backup of a database from SQL Server 2008 R2 to SQL Server 2012.\r\n\r\nWhen I try to access a particular page of my application, I get this error:\r\n\r\n> Cannot execute as the database principal because the principal "dbo" does not exist\r\n\r\nIt works with my 2008 R2 project perfectly. How can I resolve this issue?	118	2019-11-27 09:23:15.02115+00	0	4	1	186	324	348	111541	0	0	0	2019-11-26 09:14:16.267328+00	f	f	1	1
1	811	696	2020-02-10 11:26:34.518648+00	Better support for post types	# Descriptions\r\n\r\nPost type names are necessarily brief, so non-insiders need access to more info about them. It would help to have a short descriptive title-attribute on the shaded box that says the type.\r\n\r\n# Searches\r\n\r\nClicking on the shaded type box currently doesn't do anything, but I think it should change the search field to say "`{`type`}`" or some similar syntax, and that syntax should be enabled to search for just that post type.\r\n\r\n# Information\r\n\r\nThe ["about" post](https://topanswers.xyz/meta?q=640) should include or link to descriptions of the site's available post types.	711	2020-02-10 11:26:34.518648+00	0	1	1	771	2220	3865	\N	0	0	0	\N	t	f	2	\N
2	45	249	2019-02-03 16:57:07+00	What is a scalable way to simulate HASHBYTES using a SQL CLR scalar function?	As part of our ETL process, we compare rows from staging against the reporting database to figure out if any of the columns have actually changed since the data was last loaded.\r\n\r\nThe comparison is based on the unique key of the table and some kind of hash of all of the other columns. We currently use [`HASHBYTES`][1] with the `SHA2_256` algorithm and have found that it does not scale on large servers if many concurrent worker threads are all calling `HASHBYTES`.\r\n\r\nThroughput measured in hashes per second does not increase past 16 concurrent threads when testing on a 96 core server. I test by changing the number of concurrent `MAXDOP 8` queries from 1 - 12. Testing with `MAXDOP 1` showed the same scalability bottleneck.\r\n\r\nAs a workaround I want to try a SQL CLR solution. Here is my attempt to state the requirements:\r\n\r\n - The function must be able to participate in parallel queries\r\n - The function must be deterministic\r\n - The function must take an input of an `NVARCHAR` or `VARBINARY` string (all relevant columns are concatenated together)\r\n - The typical input size of the string will be 100 - 20000 characters in length. 20000 is not a max\r\n - The chance of a hash collision should be roughly equal to or better than the MD5 algorithm. `CHECKSUM` does not work for us because there are too many collisions.\r\n - The function must scale well on large servers (throughput per thread should not significantly decrease as the number of threads increases)\r\n\r\nFor Application Reasons™, assume that I cannot save off the value of the hash for the reporting table. It's a CCI which doesn't support triggers or computed columns (there are other problems as well that I don't want to get into).\r\n\r\nWhat is a scalable way to simulate `HASHBYTES` using a SQL CLR function? My goal can be expressed as getting as many hashes per second as I can on a large server, so performance matters as well. I am terrible with CLR so I don't know how to accomplish this. If it motivates anyone to answer, I plan on adding a bounty to this question as soon as I am able. Below is an example query which very roughly illustrates the use case:\r\n\r\n    DROP TABLE IF EXISTS #CHANGED_IDS;\r\n    \r\n    SELECT stg.ID INTO #CHANGED_IDS\r\n    FROM (\r\n    \tSELECT ID,\r\n    \tCAST( HASHBYTES ('SHA2_256', \r\n    \t\tCAST(FK1 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK2 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK3 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK4 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK5 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK6 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK7 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK8 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK9 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK10 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK11 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK12 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK13 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK14 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK15 AS NVARCHAR(19)) + \r\n    \t\tCAST(STR1 AS NVARCHAR(500)) +\r\n    \t\tCAST(STR2 AS NVARCHAR(500)) +\r\n    \t\tCAST(STR3 AS NVARCHAR(500)) +\r\n    \t\tCAST(STR4 AS NVARCHAR(500)) +\r\n    \t\tCAST(STR5 AS NVARCHAR(500)) +\r\n    \t\tCAST(COMP1 AS NVARCHAR(1)) + \r\n    \t\tCAST(COMP2 AS NVARCHAR(1)) + \r\n    \t\tCAST(COMP3 AS NVARCHAR(1)) + \r\n    \t\tCAST(COMP4 AS NVARCHAR(1)) + \r\n    \t\tCAST(COMP5 AS NVARCHAR(1)))\r\n    \t AS BINARY(32)) HASH1\r\n    \tFROM HB_TBL WITH (TABLOCK)\r\n    ) stg\r\n    INNER JOIN (\r\n    \tSELECT ID,\r\n    \tCAST(HASHBYTES ('SHA2_256', \r\n    \t\tCAST(FK1 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK2 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK3 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK4 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK5 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK6 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK7 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK8 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK9 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK10 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK11 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK12 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK13 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK14 AS NVARCHAR(19)) + \r\n    \t\tCAST(FK15 AS NVARCHAR(19)) + \r\n    \t\tCAST(STR1 AS NVARCHAR(500)) +\r\n    \t\tCAST(STR2 AS NVARCHAR(500)) +\r\n    \t\tCAST(STR3 AS NVARCHAR(500)) +\r\n    \t\tCAST(STR4 AS NVARCHAR(500)) +\r\n    \t\tCAST(STR5 AS NVARCHAR(500)) +\r\n    \t\tCAST(COMP1 AS NVARCHAR(1)) + \r\n    \t\tCAST(COMP2 AS NVARCHAR(1)) + \r\n    \t\tCAST(COMP3 AS NVARCHAR(1)) + \r\n    \t\tCAST(COMP4 AS NVARCHAR(1)) + \r\n    \t\tCAST(COMP5 AS NVARCHAR(1)) )\r\n     AS BINARY(32)) HASH1\r\n    \tFROM HB_TBL_2 WITH (TABLOCK)\r\n    ) rpt ON rpt.ID = stg.ID\r\n    WHERE rpt.HASH1 <> stg.HASH1\r\n    OPTION (MAXDOP 8);\r\n\r\nTo simplify things a bit, I'll probably use something like the following for benchmarking. I'll post results with `HASHBYTES` on Monday:\r\n\r\n    CREATE TABLE dbo.HASH_ME (\r\n        ID BIGINT NOT NULL,\r\n        FK1 BIGINT NOT NULL,\r\n        FK2 BIGINT NOT NULL,\r\n        FK3 BIGINT NOT NULL,\r\n        FK4 BIGINT NOT NULL,\r\n        FK5 BIGINT NOT NULL,\r\n        FK6 BIGINT NOT NULL,\r\n        FK7 BIGINT NOT NULL,\r\n        FK8 BIGINT NOT NULL,\r\n        FK9 BIGINT NOT NULL,\r\n        FK10 BIGINT NOT NULL,\r\n        FK11 BIGINT NOT NULL,\r\n        FK12 BIGINT NOT NULL,\r\n        FK13 BIGINT NOT NULL,\r\n        FK14 BIGINT NOT NULL,\r\n        FK15 BIGINT NOT NULL,\r\n        STR1 NVARCHAR(500) NOT NULL,\r\n        STR2 NVARCHAR(500) NOT NULL,\r\n        STR3 NVARCHAR(500) NOT NULL,\r\n        STR4 NVARCHAR(500) NOT NULL,\r\n        STR5 NVARCHAR(2000) NOT NULL,\r\n        COMP1 TINYINT NOT NULL,\r\n        COMP2 TINYINT NOT NULL,\r\n        COMP3 TINYINT NOT NULL,\r\n        COMP4 TINYINT NOT NULL,\r\n        COMP5 TINYINT NOT NULL\r\n    );\r\n    \r\n    INSERT INTO dbo.HASH_ME WITH (TABLOCK)\r\n    SELECT RN,\r\n    RN % 1000000, RN % 1000000, RN % 1000000, RN % 1000000, RN % 1000000,\r\n    RN % 1000000, RN % 1000000, RN % 1000000, RN % 1000000, RN % 1000000,\r\n    RN % 1000000, RN % 1000000, RN % 1000000, RN % 1000000, RN % 1000000,\r\n    REPLICATE(CHAR(65 + RN % 10 ), 30)\r\n    ,REPLICATE(CHAR(65 + RN % 10 ), 30)\r\n    ,REPLICATE(CHAR(65 + RN % 10 ), 30)\r\n    ,REPLICATE(CHAR(65 + RN % 10 ), 30)\r\n    ,REPLICATE(CHAR(65 + RN % 10 ), 1000),\r\n    0,1,0,1,0\r\n    FROM (\r\n        SELECT TOP (100000) ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) RN\r\n        FROM master..spt_values t1\r\n        CROSS JOIN master..spt_values t2\r\n    ) q\r\n    OPTION (MAXDOP 1);\r\n\r\n    SELECT MAX(HASHBYTES('SHA2_256',\r\n    CAST(N'' AS NVARCHAR(MAX)) + N'|' +\r\n    CAST(FK1 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK2 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK3 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK4 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK5 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK6 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK7 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK8 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK9 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK10 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK11 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK12 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK13 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK14 AS NVARCHAR(19)) + N'|' +\r\n    CAST(FK15 AS NVARCHAR(19)) + N'|' +\r\n    CAST(STR1 AS NVARCHAR(500)) + N'|' +\r\n    CAST(STR2 AS NVARCHAR(500)) + N'|' +\r\n    CAST(STR3 AS NVARCHAR(500)) + N'|' +\r\n    CAST(STR4 AS NVARCHAR(500)) + N'|' +\r\n    CAST(STR5 AS NVARCHAR(2000)) + N'|' +\r\n    CAST(COMP1 AS NVARCHAR(1)) + N'|' +\r\n    CAST(COMP2 AS NVARCHAR(1)) + N'|' +\r\n    CAST(COMP3 AS NVARCHAR(1)) + N'|' +\r\n    CAST(COMP4 AS NVARCHAR(1)) + N'|' +\r\n    CAST(COMP5 AS NVARCHAR(1)) )\r\n    )\r\n    FROM dbo.HASH_ME\r\n    OPTION (MAXDOP 1);\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/functions/hashbytes-transact-sql?view=sql-server-2017\r\n	257	2019-12-03 11:59:43.982858+00	0	4	1	324	543	4093	228789	0	0	0	2019-12-03 11:59:43.982858+00	f	f	1	1
1	38	566	2020-01-14 13:04:39.615329+00	Display roomicons on main list view	As a followup to ["Custom question room avatars"](https://topanswers.xyz/meta?q=432), I would like to see roomicons on the main list view. Seeing the icons in the nav bar is great and all, but I don't immediately know what they mean. Adding roomicons to the list view would help me visually associate recent posts with the corresponding nav bar indicator. The experience of clicking on the orange numbered circle would go from seeing which room you land in by surprise to a more intentional (and thus satisfying) click.\r\n\r\nIt's worth mentioning that the nav bar icons do currently contain the title of the parent post in hover-over alt text. However the delay in rendering this results in a less satisfying experience. Why wait for the on-hover action to see what I'd see anyway if I just clicked on it?\r\n\r\nBy way of _very rough_ mockup - please see below (although I expect putting the icons on the left would make more sense)\r\n\r\n![](https://topanswers.xyz/image?hash=d666554d0dc5210dc8497e7a5e2a522cd375806ebc31ce0ee1509debba3bb02e)\r\n[~HT_jspaint~](https://jspaint.app/#local:c6c6b948491ab)	579	2020-01-14 13:46:11.133832+00	2	4	1	641	2113	3915	\N	0	0	0	\N	f	f	2	\N
1	167	394	2019-12-09 22:00:41.941725+00	Add links to answers	It would be nice if there would be a way to link directly to answers. \r\n\r\nUse case: for example when giving credit to some information taken from an answer, it would be very convientient to have a direct link to the answer instead of having to link to the question and giving the name of the author to distigish between multiple answers (which will also fail if one user posts multiple answers ...).	405	2019-12-09 22:00:41.941725+00	11	4	3	469	2104	3937	\N	0	0	0	\N	f	f	2	\N
1	811	721	2020-02-17 22:43:10.569763+00	Audible ping	It would be really nice to have the option of *hearing* every time I get a notification, possibly with various options like being silent if the TA has focus, etc. like on SE chat.	737	2020-02-18 06:17:12.201778+00	6	1	1	796	2256	4193	\N	0	0	0	\N	t	f	2	\N
1	2	595	2020-01-22 11:01:52.953438+00	How can we promote TopAnswers?	There are a lot of disaffected people on Stack Exchange, and some of them may be glad to know of an alternative. Word gradually gets around of course, but perhaps we should be thinking about nudging people over here and suggesting they plant some new communities.\r\n\r\nWhat are our options for doing that?	609	2020-01-22 11:01:52.953438+00	6	1	1	670	2128	3946	\N	0	0	0	\N	f	f	2	\N
1	811	697	2020-02-10 14:21:08.076888+00	How does one indicate dislike of a Meta proposal?	Let's say a site has plenty of users. Someone asks a Meta question and someone answers it with a proposal. Maybe 90% of the users dislike this proposal, but without down-votes, the proposal can only gather up-votes over time, until it appears to be so popular that it is considered community consensus.	712	2020-02-10 14:21:08.076888+00	4	1	1	772	2120	3972	\N	0	0	0	\N	t	f	2	\N
1	811	722	2020-02-17 22:55:56.986197+00	Ctrl+Enter to submit	Would be nice if the quite standard Ctrl+Enter could submit or finalise an edit.	738	2020-02-17 22:55:56.986197+00	3	1	1	797	2254	4169	\N	0	0	0	\N	t	f	2	\N
1	96	425	2019-12-12 08:16:50.908007+00	Use TA bot user to post timeline changes in chat	Notifications about post changes are useful, as is the edit history, but neither of these things is well cordinated with the other activity on posts.\r\n\r\nI would suggest adding a TA bot user that posts in the room chat when activities happen on a post.\r\n\r\n> User C posted new answer.\r\n\r\n> User M flagged answer.\r\n\r\n> User A edited answer.\r\n\r\n> Unel N counter-flagged answer.\r\n\r\nHaving these interspersed with other chat messages (that may or may not give further clues as to what inspired changes) would much more useful that the flat display currently available (especially for flaggers) that reveals nothing about when or why they took the action of flagging.\r\n\r\nI would suggest this completely replace the current system of showing avatars of flaggers under the question where –to my eyes– it still looks like that is some sort of endorsement.	436	2019-12-12 08:16:50.908007+00	1	4	2	500	2097	3881	\N	0	0	0	\N	f	f	2	\N
4	262	715	2020-02-16 09:21:23.542679+00	How to implement a "switch" that "comment out" a list environment?	Currently, I want to have a pair of command, say `\\commentfoo` and `\\uncommentfoo` which work like this\r\n\r\n```tex\r\n\\documentclass{article}\r\n\\usepackage{enumitem}\r\n\\newlist{foo}{enumerate}{1}\r\n% Do something with \\commentfoo and \\uncommentfoo\r\n\\begin{document}\r\n% This should be shown\r\n\\begin{foo}\r\n  \\item a\r\n  \\item b\r\n\\end{foo}\r\n\\commentfoo\r\n% This should be hidden, i.e. comment out\r\n\\begin{foo}\r\n  \\item a\r\n  \\item b\r\n\\end{foo}\r\n\\uncommentfoo\r\n% This should be shown again\r\n\\begin{foo}\r\n  \\item a\r\n  \\item b\r\n\\end{foo}\r\n\\end{document}\r\n```\r\n\r\nSo far I have used `comment`'s commands, and it works quite well for "normal" environments:\r\n\r\n```tex\r\n\\documentclass{article}\r\n\\usepackage{comment}\r\n\\newenvironment{foo}{}{}\r\n\\def\\commentfoo{\\excludecomment{foo}}\r\n\\def\\uncommentfoo{\\includecomment{foo}}\r\n\\begin{document}\r\n\\begin{foo}\r\n  This should be shown 1\r\n\\end{foo}\r\n\\commentfoo\r\n\\begin{foo}\r\n  This should be hidden\r\n\\end{foo}\r\n\\uncommentfoo\r\n\\begin{foo}\r\n  This should be shown 2\r\n\\end{foo}\r\n\\end{document}\r\n```\r\n\r\nBut for list environments, it fails\r\n\r\n```tex\r\n\\documentclass{article}\r\n\\usepackage{enumitem}\r\n\\usepackage{comment}\r\n\\newlist{foo}{enumerate}{2}\r\n\\setlist[foo]{label=\\arabic*.}\r\n\\def\\commentfoo{\\excludecomment{foo}}\r\n\\def\\uncommentfoo{\\includecomment{foo}}\r\n\\begin{document}\r\n\\begin{foo}\r\n  \\item This should be shown 1\r\n\\end{foo}\r\n% works until this point\r\n\\commentfoo\r\n\\begin{foo}\r\n  \\item This should be hidden\r\n\\end{foo}\r\n\\uncommentfoo\r\n\\begin{foo}\r\n  \\item This should be shown 2\r\n\\end{foo}\r\n\\end{document}\r\n```\r\n\r\nHow to implement `\\commentfoo` and `\\uncommentfoo` that works for list environments?	731	2020-02-16 09:21:23.542679+00	0	4	3	790	2211	4121	\N	0	0	0	\N	f	f	1	\N
1	167	700	2020-02-11 16:02:46.551344+00	Link to post in edit notifications	What do you think about making the edit notification link to the post itself in addition or instead of linking to the post history page?\r\n\r\n![topanswers.png](/image?hash=0ab4466752f4cef290884a4305e49c51157299e7133a1bfc10c1e69d25ad43d1)\r\n\r\nPersonally I would prefer a link to the post itself because:\r\n\r\n- Especially for answer edits, I often need to have a look at the question first to remember what the topic was about. \r\n\r\n- Many changes are easy to see in the post itself because nice user prefix them with "edit:" or even with nice date information like in the change log. Viewing the busy history page which shows 3 versions at once feels unnecessary complicate to get the information I want\r\n\r\n- Even if changes are not immediately visible, I find it easier to skim through the post to compare to what I have read previously than to look at the history page\r\n\r\n- the post history page is like a cul-de-sac. Notifications or room icons are not visible there, so if I want to move on to other posts I must either go to the parent post itself, the main site or use my browser to go back to where I came from. Especially the route via escaping through the parent post feels like a waste of time, because in most cases this would have already given me the information I wanted without visiting the history page\r\n\r\n- Not your fault, but of my slow laptop: especially post history pages of very long posts or many edits either take a long time to render or make my browser freeze completely\r\n\r\n(don't get me wrong, the history page is very useful, I would just prefer to not go to it by default. If I want to see it, I can still access it from the post itself)\r\n\r\n	715	2020-02-11 16:02:46.551344+00	3	4	1	775	2130	3981	\N	0	0	0	\N	f	f	2	\N
4	233	503	2020-01-01 21:53:12.55587+00	Bug in new PGF-TikZ version?	This code:\r\n\r\n```\r\n\\documentclass[11pt]{standalone}\r\n\\usepackage{tikz}\r\n\\usepackage{pgfplots}\r\n\\pgfplotsset{compat=newest}\r\n\\usetikzlibrary{calc, intersections, fit}\r\n\\usetikzlibrary{arrows.meta}\r\n\\usetikzlibrary{positioning}\r\n\\usetikzlibrary{backgrounds}\r\n\\tikzset{\r\n\tdeclare function={\r\n\t\tiperbole(\\x)=\r\n\t\t\t(sqrt((.4)+(.04*(\\x-2)^2)))\r\n\t\t\t;\r\n\t\t}\r\n\t}\r\n\t\\pgfplotsset{\r\n\t\tassi/.style={\r\n\t\t\taxis lines=middle,\r\n\t\t\taxis line style={-Stealth},\r\n\t\t\tticks=none,\r\n\t\t\tsamples=300,\r\n\t\t\tclip=false,\r\n\t\t\twidth=10cm\r\n\t\t\t},\r\n\t}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n  \\begin{axis}[\r\n    assi,\r\n\t\tymin=-3,\r\n\t\txmin=-0.05,\r\n\t\tenlargelimits={value=.05,upper},\r\n    xlabel = {$\\sigma$},\r\n    ylabel = {$\\mu$},\r\n    every axis x label/.style={at=(current axis.right of origin),\t\tanchor=north east},\r\n    every axis y label/.style={at=(current axis.above origin),\t\tanchor=north east},\r\n\t] \r\n\t\\addplot[\r\n\t\tthick,\r\n\t\tdomain={-3:11},\r\n        restrict y to domain={2:11},\r\n        smooth, name path=frontier\r\n\t\t]\r\n\t\t({iperbole(\\x)},{x});\r\n    \\addplot[\r\n\t\tthick, dashed,\r\n\t\tdomain={-3:7},\r\n        restrict y to domain={-2:2},\r\n        smooth, name path=boundary\r\n\t\t]\r\n\t\t({iperbole(\\x)},{x});\r\n    \\pgfmathsetmacro{\\MVPx}{iperbole(2)}\r\n    \\pgfmathsetmacro{\\Sx}{iperbole(2.5)}\r\n    \\pgfmathsetmacro{\\Lx}{iperbole(5)}\r\n    \\pgfmathsetmacro{\\MAXx}{iperbole(11)}\r\n    \\pgfmathsetmacro{\\INTER}{7.5*\\MVPx-2}\r\n    \\addplot[thick,dotted,domain={0:\\MAXx}, smooth, name path=varbasso] {7.5*x-\\INTER};\r\n    \\addplot[thick,domain={0:\\MAXx}, smooth, name path=varalto] {7.5*x-2};\r\n    \\draw[fill=black,name intersections={of=frontier and varalto, by={i1,i2}}] \r\n        (i1) circle (1pt) node [above left] {$I$}\r\n        (i2) circle (1pt) node [above left] {$J$};\r\n  \\end{axis}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\nworked one month ago (and works on Overleaf, which is not up-to-date), but now it gives this error:\r\n\r\n\r\n```\r\n! Argument of \\pgfmathfloatparse@@ has an extra }.\r\n<inserted text> \r\n                \\par \r\nl.66   \\end{axis}\r\n                 \r\n? \r\n! Emergency stop.\r\n<inserted text> \r\n                \\par \r\nl.66   \\end{axis}\r\n                 \r\nEnd of file on the terminal!\r\n\r\n```\r\nIs it a bug?\r\n\r\n\r\n	515	2020-01-01 22:04:12.437089+00	0	4	1	578	1261	3025	\N	0	0	0	\N	f	f	1	\N
2	164	259	2019-06-21 12:24:58+00	Index Seek scanning whole table dependent on parameter value	I have a query:\r\n\r\n    SELECT\tId, \r\n    \t    ColumnA,\r\n    \t    ColumnB\r\n    FROM\tMyTable\r\n    WHERE\tColumnA = @varA OR\r\n    \t    ColumnB = @varB  \r\n\r\nThe table is defined as\r\n\r\n    CREATE TABLE MyTable\r\n    (\r\n    \tId INT IDENTITY(-2147483648,1) PRIMARY KEY,\r\n    \tColumnA\tVARCHAR(22)\r\n    \tColumnB VARCAHR(22)\r\n    )\r\n\r\nand there is a non clustered index on the table\r\n\r\n    CREATE INDEX IX_MyIndex ON MyTable\r\n    (\r\n    \tColumnA\r\n    )\r\n\r\n\r\nWhen I run the query with the parameters below:\r\n\r\n    DECLARE @varA nvarchar(4000) = ''\r\n    DECLARE @varB  nvarchar(8) = '10140730'\r\n\r\nThe execution plan shows an index seek on `IX_MyIndex`, however it shows number of rows read as 17million rows but actual number of rows as 0 (There are 0 rows in MyTable.ColumnA with the value '')\r\nIf I turn `SET STATISTICS IO ON` I can see the full table is being read\r\n\r\nThis makes sense as per: [this article in the section "Here’s a “bad” index seek"][1]\r\n\r\n\r\nHowever, when I run the same query with the parameters:\r\n\r\n    DECLARE @varA nvarchar(8) = 'a'\r\n    DECLARE @varB  nvarchar(8) = '10140730'\r\n\r\nThe seek operator doesn't have a "number of rows read" property (there are 0 rows MyTable.ColumnA in with value 'a') and `SET STATISTICS IO` reports single figure logical reads\r\n\r\nIncidentally, the plan has an implicit convert warning and the issue goes away when I change the query like so:\r\n\r\n    SELECT\tId, \r\n    \t    ColumnA,\r\n    \t    ColumnB\r\n    FROM\tMyTable\r\n    WHERE\tColumnA = CONVERT(VARCHAR(22),@varA) OR\r\n    \t    ColumnB = CONVERT(VARCHAR(22),@varB)\r\n\r\nOr change the underlying column to `NVARCHAR`\r\n\r\nHowever, I am curious as to why the behaviour of the index seek with the two different values for `@varA` is different even though both of them return the same number of records in the table (0)\r\n\r\n\r\n  [1]: https://www.brentozar.com/archive/2019/04/index-scans-arent-always-bad-and-index-seeks-arent-always-great/	267	2019-12-04 01:26:57.367457+00	0	4	1	334	565	1587	241112	0	0	0	2019-12-04 01:26:57.367457+00	f	f	1	1
1	96	426	2019-12-12 08:22:51.498009+00	Add access to notification history	A transcript like page for accessing past dismissed notification history would be useful. I accidentally dismissed a couple notifications (from mobile, the UI is nearly unusable) and now have no idea what they were and no way to find out.	437	2019-12-12 08:22:51.498009+00	6	4	2	501	2098	3882	\N	0	0	0	\N	f	f	2	\N
1	96	427	2019-12-12 08:28:29.106531+00	Changes to tags should show up in post edit history	Currently the post "history" page only shows edits to the body content. I suggest it be expanded to a more complete timeline of events related to the question, including tag edits. It could also show key events like flag actions (in addition to the chat bot messages [suggested here](https://topanswers.xyz/meta?q=425#question)). This would provide a clearer picture of how the post evolved and who did what. Right now tag edits are virtually anonymous and the changes un-auditable.	438	2019-12-12 08:28:29.106531+00	9	4	2	502	2100	3938	\N	0	0	0	\N	f	f	2	\N
1	168	390	2019-12-09 19:43:38.393393+00	Race conditions on edits.	Just now it happened that both of us, @marmot and I, wanted to make an edit to the same question (https://topanswers.xyz/meta?q=389), he saved his edit prior to me (at 2019-12-09 19:32:55), but I clicked on "edit" before he did, so I got the state the question was in prior to his edit, and after I did my edit and saved it (2019-12-09 19:33:23) I deleted everything he added in the meantime.\r\n\r\nThere should be some measures to prevent/resolve such race conditions.	401	2019-12-09 19:43:38.393393+00	8	4	1	465	2086	3952	\N	0	0	0	\N	f	f	2	\N
1	167	504	2020-01-02 11:01:01.485147+00	How are messages on the starboard ordered?	Just out of curiosity, how is the order of messages on the starboard determined?\r\n\r\n- it does not seem to be chronologically because I see \r\n   - 20th Dec\r\n   - yesterday\r\n   - last Sunday\r\n   \r\n- it does not seem to be the number of stars because I see\r\n   - 2 stars\r\n   - 1 star\r\n   - 2 stars\r\n   \r\n![Screen Shot 2020-01-02 at 11.55.01.png](/image?hash=f23d089a9692bc9ad4fa4753e2049f076ba419191f781e0bdc6e403be4c8e154)	516	2020-01-02 11:01:01.485147+00	3	4	1	579	1494	3095	\N	0	0	0	\N	f	f	2	\N
1	811	719	2020-02-16 22:24:39.266759+00	Unused [fun] and [mix-up] tags	What are these two tags for?	735	2020-02-16 22:24:39.266759+00	2	1	1	794	2227	4111	\N	0	0	0	\N	t	f	2	\N
1	202	538	2020-01-09 11:34:16.00531+00	Could we add an option to add tag descriptions	Could you add possibility to add descriptions for tags.	551	2020-01-09 11:34:16.00531+00	9	4	3	613	2218	4116	\N	0	0	0	\N	f	f	2	\N
2	111	194	2017-08-01 19:45:34+00	Forcing Flow Distinct	I have a table like this:\r\n\r\n\tCREATE TABLE Updates\r\n\t(\r\n\t\tUpdateId INT NOT NULL IDENTITY(1,1) PRIMARY KEY,\r\n\t\tObjectId INT NOT NULL\r\n\t)\r\n\r\nEssentially tracking updates to objects with an increasing ID.\r\n\r\nThe consumer of this table will select a chunk of 100 distinct object IDs, ordered by `UpdateId` and starting from a specific `UpdateId`. Essentially, keeping track of where it left off and then querying for any updates.\r\n\r\nI've found this to be an interesting optimization problem because I've only been able to generate a maximally optimal query plan by writing queries that *happen* to do what I want due to indexes, but do not *guarantee* what I want:\r\n\r\n\tSELECT DISTINCT TOP 100 ObjectId\r\n\tFROM Updates\r\n\tWHERE UpdateId > @fromUpdateId\r\n\r\nWhere `@fromUpdateId` is a stored procedure parameter.\r\n\r\nWith a plan of:\r\n\r\n\tSELECT <- TOP <- Hash match (flow distinct, 100 rows touched) <- Index seek\r\n\r\nDue to the seek on the `UpdateId` index being used, the results are already nice and ordered from lowest to highest update ID like I want. And this generates a *flow distinct* plan, which is what I want. But the ordering obviously isn't guaranteed behavior, so I don't want to use it.\r\n\r\nThis trick also results in the same query plan (though with a redundant TOP):\r\n\r\n\tWITH ids AS\r\n\t(\r\n\t\tSELECT ObjectId\r\n\t\tFROM Updates\r\n\t\tWHERE UpdateId > @fromUpdateId\r\n\t\tORDER BY UpdateId OFFSET 0 ROWS\r\n\t)\r\n\tSELECT DISTINCT TOP 100 ObjectId FROM ids\r\n\r\nThough, I'm not sure (and suspect not) if this truly guarantees ordering.\r\n\r\nOne query I hoped SQL Server would be smart enough to simplify was this, but it ends up generating a very bad query plan:\r\n\r\n\tSELECT TOP 100 ObjectId\r\n\tFROM Updates\r\n\tWHERE UpdateId > @fromUpdateId\r\n\tGROUP BY ObjectId\r\n\tORDER BY MIN(UpdateId)\r\n\r\nWith a plan of:\r\n\r\n\tSELECT <- Top N Sort <- Hash Match aggregate (50,000+ rows touched) <- Index Seek\r\n\r\nI'm trying to find a way to generate an optimal plan with an index seek on `UpdateId` and a *flow distinct* to remove duplicate `ObjectId`s. Any ideas?\r\n\r\n[Sample data](https://gist.github.com/anonymous/a05400310a127188b949aa25990d1c17) if you want it. Objects will rarely have more than one update, and should almost never have more than one within a set of 100 rows, which is why I'm after a *flow distinct*, unless there's something better I don't know of? However,  there is no guarantee that a single `ObjectId` won't have more than 100 rows in the table. The table has over 1,000,000 rows and is expected to grow rapidly.\r\n\r\nAssume the user of this has another way to find the appropriate next `@fromUpdateId`. No need to return it in this query.	201	2019-11-29 12:34:42.881242+00	0	4	1	269	407	464	182410	0	0	0	2019-11-29 12:34:42.881242+00	f	f	1	1
1	1	498	2019-12-30 11:48:41.578006+00	TopAnswers Changelog	| date | change | related meta posts/comments\r\n| :--: | ------ | -------------\r\n| 2020-02-19 | adjust header navigation and rollout to all pages | [Make the site selector universally available](/meta?q=645), [Browsing questions + using browser "back" button is suboptimal](/meta?q=652)\r\n| 2020-02-17 | new search features | [Is there a way to browse a list of tags?](/meta?q=485), [View/filter questions by tag](/meta?q=369), [Better support for tags](/meta?q=694)\r\n| 2020-02-16 | new header navigation control (experimental) | [Special "About" post](/meta?q=640)\r\n| 2020-02-12 | make long code blocks expandable | [A proposal for less crowding in chat pane (particularly on long comments)](/meta?q=513#a563)\r\n| 2020-02-10 | tweak voting thresholds to be exact powers of 10 | [How is reputation earned?](/meta?q=232#a179)\r\n| 2020-02-09 | system notifications | [New system notifications](/meta?q=699)\r\n| 2020-02-09 | limit the number of answer summaries visible\r\n| 2020-02-04 | switch notification order | [Notifications in 'newest first' order.](/meta?q=677)\r\n| 2020-01-31 | 'or later' license selection | [To-do list before going public](/tex?q=379#a703), […I always specify the version as an inequality (e.g., GPL ≥3.0)…](/transcript?room=543&id=15058#c15058)\r\n| 2020-01-22 | SE account linking | [Link SE identity without manual admin intervention](/meta?q=409#a647)\r\n| 2020-01-18 | improve look of quoted comments | \r\n| 2020-01-16 | stable sort order for room icons (experimental) | [Pin a chatroom permanently to list of rooms](/meta?q=505)\r\n| 2020-01-15 | room icons now use community colour only | [Custom question room avatars](/meta?q=432#a437)\r\n| 2020-01-12 | room icon 'unread' counter revamp | "[…the new message counts on chat room icons have stopped appearing…](/transcript?room=1&id=11228&year=2019&month=12&day=27#c11228)", [Allow opening chat rooms in new tabs](/meta?q=536#a586)\r\n| 2020-01-11 | star board (take 2)\r\n| 2020-01-06 | show extra hover info on question list | "[…I can't tell which answer was edited…](https://topanswers.xyz/transcript?room=1&id=12209#c12209)"\r\n| 2020-01-05 | add landing page | [Feature Request: Landing Page](/meta?q=514), [Please add an "about" or "imprint" page](/meta?q=263)\r\n| 2020-01-04 | improve metadata for imported posts | [Keep original date of imported posts](/meta?q=457), [Show who imported a post from SE](/meta?q=495)\r\n| 2020-01-03 | fail2ban with some default apache filters \r\n| 2020-01-01 | ~~star board~~\r\n| 2019-12-30 | rounded corners on user identicons	510	2020-02-19 15:25:12.736876+00	17	1	1	573	2265	3197	\N	0	0	0	\N	f	f	3	\N
2	84	98	2014-09-10 16:13:07+00	Switching Data In Fails with “allows values that are not allowed by check constraints or partition function on target table”	Given the following \r\n\r\n    -- table ddl\r\n    create table dbo.f_word(\r\n    \tsentence_id int NULL,\r\n    \tsentence_word_id int NULL,\r\n    \tword_id int NULL,\r\n    \tlemma_id int NULL,\r\n    \tsource_id int NULL,\r\n    \tpart_of_speech_id int NULL,\r\n    \tperson_id int NULL,\r\n    \tgender_id int NULL,\r\n    \tnumber_id int NULL,\r\n    \ttense_id int NULL,\r\n    \tvoice_id int NULL,\r\n    \tmood_id int NULL,\r\n    \tcase_id int NULL,\r\n    \tdegree_id int NULL,\r\n    \tcitation nvarchar(100) NULL\r\n    );\r\n    -- create partition function\r\n    create partition function pf_f_word_source_id (int)\r\n    as range left for values \r\n    (\r\n    \t1,2,3,4,5,6,7,8,9,10,11,12,13,14,\r\n    \t15,16,17,18,19,20,21,22,23\r\n    );\r\n    \r\n    -- create the partition scheme\r\n    create partition scheme ps_f_word as partition pf_f_word_source_id to \r\n    (\r\n    \t[primary],[primary],[primary],[primary],[primary],[primary],[primary],[primary],[primary],\r\n    \t[primary],[primary],[primary],[primary],[primary],[primary],[primary],[primary],[primary],\r\n    \t[primary],[primary],[primary],[primary],[primary],[primary]\r\n    );\r\n    \r\n    -- partition the index\r\n    create unique clustered index cix_fword on dbo.f_word \r\n    (\r\n    \tsource_id,\r\n    \tsentence_id,\r\n    \tsentence_word_id,\r\n    \tword_id,\r\n    \tlemma_id,\r\n    \tpart_of_speech_id,\r\n    \tperson_id,\r\n    \tgender_id,\r\n    \tnumber_id,\r\n    \ttense_id,\r\n    \tvoice_id,\r\n    \tmood_id,\r\n    \tcase_id,\r\n    \tdegree_id \r\n    )\r\n    on ps_f_word (source_id);\r\n    \r\n    -- swapin table ddl\r\n    \r\n    create table dbo.f_word_swapin(\r\n    \tsentence_id int NULL,\r\n    \tsentence_word_id int NULL,\r\n    \tword_id int NULL,\r\n    \tlemma_id int NULL,\r\n    \tsource_id int NULL,\r\n    \tpart_of_speech_id int NULL,\r\n    \tperson_id int NULL,\r\n    \tgender_id int NULL,\r\n    \tnumber_id int NULL,\r\n    \ttense_id int NULL,\r\n    \tvoice_id int NULL,\r\n    \tmood_id int NULL,\r\n    \tcase_id int NULL,\r\n    \tdegree_id int NULL,\r\n    \tcitation nvarchar(100) NULL\r\n    ) on [primary];\r\n    \r\n    -- create the same index on the swapin table\r\n    create unique clustered index cix_fword_swapin on dbo.f_word_swapin \r\n    (\r\n    \tsource_id,\r\n    \tsentence_id,\r\n    \tsentence_word_id,\r\n    \tword_id,\r\n    \tlemma_id,\r\n    \tpart_of_speech_id,\r\n    \tperson_id,\r\n    \tgender_id,\r\n    \tnumber_id,\r\n    \ttense_id,\r\n    \tvoice_id,\r\n    \tmood_id,\r\n    \tcase_id,\r\n    \tdegree_id \r\n    );\r\n    \r\n    -- add check constraints WITH CHECK\r\n    ALTER TABLE dbo.f_word_swapin\r\n    WITH CHECK\r\n    ADD CONSTRAINT ck_f_word_swapin_lb\r\n    CHECK ( source_id > 12);\r\n    \r\n    ALTER TABLE dbo.f_word_swapin\r\n    WITH CHECK\r\n    ADD CONSTRAINT ck_f_word_swapin_ub\r\n    CHECK ( source_id <= 13);\r\n\r\n\r\nThen, move the data around:\r\n\r\n    -- switch data OUT of the partitioned table\r\n    ALTER TABLE dbo.f_word\r\n    SWITCH PARTITION 13 TO dbo.f_word_swapin;\r\n    \r\n    -- attempt to switch data back IN \r\n    ALTER TABLE dbo.f_word_swapin\r\n    SWITCH TO dbo.f_word PARTITION 13;\r\n\r\nBelow is the "Script Table As ... CREATE" DDL just to verify the same table structures.\r\n\r\n    /****** Object:  Table [dbo].[f_word_swapin]    Script Date: 9/10/2014 10:01:01 AM ******/\r\n    SET ANSI_NULLS ON\r\n    GO\r\n    \r\n    SET QUOTED_IDENTIFIER ON\r\n    GO\r\n    \r\n    CREATE TABLE [dbo].[f_word_swapin](\r\n    \t[sentence_id] [int] NULL,\r\n    \t[sentence_word_id] [int] NULL,\r\n    \t[word_id] [int] NULL,\r\n    \t[lemma_id] [int] NULL,\r\n    \t[source_id] [int] NULL,\r\n    \t[part_of_speech_id] [int] NULL,\r\n    \t[person_id] [int] NULL,\r\n    \t[gender_id] [int] NULL,\r\n    \t[number_id] [int] NULL,\r\n    \t[tense_id] [int] NULL,\r\n    \t[voice_id] [int] NULL,\r\n    \t[mood_id] [int] NULL,\r\n    \t[case_id] [int] NULL,\r\n    \t[degree_id] [int] NULL,\r\n    \t[citation] [nvarchar](100) NULL\r\n    ) ON [PRIMARY]\r\n    \r\n    /****** Object:  Table [dbo].[f_word]    Script Date: 9/10/2014 10:09:43 AM ******/\r\n    SET ANSI_NULLS ON\r\n    GO\r\n    \r\n    SET QUOTED_IDENTIFIER ON\r\n    GO\r\n    \r\n    CREATE TABLE [dbo].[f_word](\r\n    \t[sentence_id] [int] NULL,\r\n    \t[sentence_word_id] [int] NULL,\r\n    \t[word_id] [int] NULL,\r\n    \t[lemma_id] [int] NULL,\r\n    \t[source_id] [int] NULL,\r\n    \t[part_of_speech_id] [int] NULL,\r\n    \t[person_id] [int] NULL,\r\n    \t[gender_id] [int] NULL,\r\n    \t[number_id] [int] NULL,\r\n    \t[tense_id] [int] NULL,\r\n    \t[voice_id] [int] NULL,\r\n    \t[mood_id] [int] NULL,\r\n    \t[case_id] [int] NULL,\r\n    \t[degree_id] [int] NULL,\r\n    \t[citation] [nvarchar](100) NULL\r\n    )\r\n    \r\n    GO\r\n\r\nSWITCHing OUT works just fine. SWITCHing IN produces the following error:\r\n\r\n> Msg 4972, Level 16, State 1, Line 1 ALTER TABLE SWITCH statement\r\n> failed. Check constraints or partition function of source table\r\n> 'greek.dbo.f_word_swapin' allows values that are not allowed by check\r\n> constraints or partition function on target table 'greek.dbo.f_word'.\r\n\r\nRunning: \r\n\r\n    select target_partition_id = $PARTITION.pf_f_word_source_id(source_id), \r\n    \t*\r\n    from dbo.f_word_swapin;\r\n\r\nverifies that all data should go back into partition 13.\r\n\r\nI'm really pretty new to partitioning so I'm sure that I'm doing things incorrectly, I just don't know what it is.	105	2019-11-25 10:07:00.69029+00	0	4	1	173	241	241	76232	0	0	0	2019-11-25 10:07:00.69029+00	f	f	1	1
1	115	232	2019-12-01 18:33:57.896137+00	How is reputation earned?	I infer that reputation (how much?) is earned for upvotes on posts (anywhere or just main?).  I also know that there are no downvotes.  What else affects reputation?\r\n	240	2019-12-01 18:33:57.896137+00	10	4	1	307	2123	3921	\N	0	0	0	\N	f	f	2	\N
2	188	291	2017-10-18 00:49:39+00	GETUTCDATE()-2 vs DATEADD(d,-2,GETUTCDATE())	I was wondering what the difference between the following two methods is:\r\n\r\n     GETUTCDATE()-2  \r\nand\r\n\r\n      DATEADD(d,-2,GETUTCDATE())\r\n\r\nI guess using `DATEADD` is the correct way, but was wondering why?	299	2019-12-04 14:32:19.715357+00	0	4	1	366	623	866	188712	0	0	0	2019-12-04 14:32:19.715357+00	f	f	1	1
1	96	370	2019-12-06 14:06:03.607205+00	Strip Markdown from snippets	The answer snippets that show beneath questions in the question list should do more than just extract the first line of Markdown, they should strip it of any markup.	379	2019-12-06 14:06:03.607205+00	3	4	2	445	1880	3704	\N	0	0	0	\N	f	f	2	\N
1	96	399	2019-12-10 07:41:55.908681+00	Posting a question should ask for tags up front	The current workflow for tags requires posting, then adding tags. This is both inconvenient for the poster (more steps) and will make later tools for post review more difficult.\r\n\r\nThe line asking for tags should be something included in the question post so one post action can mean the end of the process.	410	2019-12-10 07:41:55.908681+00	10	4	2	474	2091	3712	\N	0	0	0	\N	f	f	2	\N
4	167	387	2019-12-09 16:55:08.658458+00	Rotate image used in tikz path decoration	I would like to place an image along a tikz path in such a way that it rotates with the loops of the path, e.g. in the example below the duck should be head down when flying though the looping.\r\n\r\nMWE without rotation:\r\n\r\n```\r\n\\documentclass{beamer}\r\n\r\n\\usepackage{tikz}\r\n\\usetikzlibrary{calc,decorations.markings}\r\n\r\n\\setbeamertemplate{background}{%\r\n  \\begin{tikzpicture}[\r\n    remember picture,\r\n    overlay,\r\n    decoration={\r\n      markings, \r\n      mark=at position \\thepage/10 with {\r\n        \\node{\\includegraphics[width=2cm]{example-image-duck}};\r\n      }\r\n    }\r\n  ]\r\n    \\path[postaction=decorate,draw] (-0.0024, -0.846) .. controls (2.9731, -2.564) and (4.9397, -0.8115) .. (4.9397, -4.2473) .. controls (4.9397, -8.2498) and (-2.9933, -2.7915) .. (0.9993, -3.0728) .. controls (10.1424, -3.7168) and (9.4455, -9.435) .. (6.1477, -7.1566) .. controls (3.4609, -5.3004) and (10.1189, 2.4134) .. (13.8065, -10.4522);\r\n  \\end{tikzpicture}%\r\n}\r\n\r\n\\begin{document}\r\n\r\n\\begin{frame}\r\n  \\pause[10]\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n\r\n![document.gif](/image?hash=d01d5d4b1d22a5a09778078d947b40623bbd369dec6e18166da806f590ee9944)	398	2019-12-09 16:55:08.658458+00	0	4	3	462	913	1588	\N	0	0	0	\N	f	f	1	\N
8	895	724	2020-02-18 15:38:48.949713+00	Consider Entire string When Converting to an int	Given `std::string str`, what's the best way to ensure that the entirity of `str` *is* an integer?\r\n\r\n 1. [`std::strtol`](http://en.cppreference.com/w/cpp/string/byte/strtol): The entire string has been consumed if after the following block, `std::distance(c_str, const_cast<const char*>(length)) == std::size(str)`\r\n\r\n```\r\nchar* length;\r\nconst auto c_str = std::data(str)\r\nconst int num = std::strtol(c_str, &length, 10);\r\n```\r\n\r\n 2. [`std::sscanf`](http://en.cppreference.com/w/cpp/io/c/fscanf): The entire string has been consumed if after the following block, `length == std::size(str)`\r\n\r\n```\r\nstd::size_t length = {};\r\nint num;\r\n\r\nstd::sscanf(std::data(str), "%d%zn", &num, &length);\r\n```\r\n\r\n 3. [`std::stoi`](http://en.cppreference.com/w/cpp/string/basic_string/stol): The entire string has been consumed if the following block did not throw.\r\n\r\n```\r\nstd::size_t length;\r\nconst auto num = std::stoi(str, &length)\r\n\r\nif(length != std::size(str))  \r\n{\r\n    throw invalid_argument("invalid stoi argument");\r\n}\r\n```  \r\n\r\n[**Live Example**](https://ideone.com/LO2Qnq)	740	2020-02-18 15:38:48.949713+00	0	4	1	799	2260	4171	\N	0	0	0	\N	f	f	1	\N
1	2	699	2020-02-10 23:44:57.997935+00	New system notifications	We've added a new kind of notification for system messages.\r\n\r\nConcern has been raised several times[^1] that it is very easy for a new user to lose access to their account. Not that it is *easier* here, but that there is no *automatic* recovery mechanism because we don't (and won't) insist users supply an email address.\r\n\r\nWe'd been mulling over adding a banner notification for new users, (just for new users because you have already saved your [login key](https://topanswers.xyz/profile?highlight-recovery) somewhere safe, haven't you?) that can be dismissed, just like you see on other popular Q&A sites — but then we asked ourselves "why not just use the notification UI you already have?".\r\n\r\nOf course that makes it simple to develop, and easier to maintain, but it also:\r\n\r\n* introduces new users to the notification area the moment they join\r\n* doesn't interfere with any other part of the UI\r\n* looks nice enough:  \r\n  ![Screenshot 2020-02-10 at 23.21.15.png](/image?hash=c5398905c2f17d761f6d9e547c6b5c36b6390e072a9a79f255a1d0ba1ab239e3)\r\n\r\nWe thought about sending out a notification to everyone using TopAnswers to introduce the notifications, but we are sensitive about spamming you, so I'm writing this blog post instead. However you may see notifications from time to time as we are toying with using them for:\r\n\r\n* 'congratulations' messages when you gain the power to award [an extra star](/meta?q=232#a179)\r\n* warnings about any scheduled downtime\r\n\r\nNew users might also get:\r\n\r\n* 'thanks for voting, please keep doing it' on their first vote\r\n* 'thanks for posting' on their first answer\r\n\r\nBut we are always going to try to err on the side of caution. Any messages will be genuinely informative, and infrequent.\r\n\r\n[^1]: e.g. [here](/meta?q=234)	714	2020-02-10 23:44:57.997935+00	8	1	1	774	2124	3971	\N	0	0	0	\N	t	f	3	\N
4	830	702	2020-02-12 14:37:39.724613+00	Underline Devanagari characters of different heights	```\r\n\\documentclass{article}\r\n\\usepackage{fontspec}\r\n\\setmainfont[Script=Devanagari,Mapping=devanagarinumerals]{Shobhika}\r\n\\usepackage{soulutf8}\r\n\\setul{0.26cm}{0.5pt}\r\n\r\n\\begin{document}\r\n\t\\underline{अबकड} \\underline{कू} \\underline{क्लू} \\underline{ट्टू}\r\n\t\\ul{abcd} \\ul{अबकड} \\ul{कू} \\ul{क्लू} \\ul{ट्टू}\r\n\\end{document}\r\n```\r\n\r\nThis produces -\r\n\r\n![Screenshot from 2020-02-12 20-31-56.png](/image?hash=3fcc53fdab9e1fb3c98c38e7af78f49e44360db4b1691d5b845ba4215fc20e2b)\r\n\r\nHow to resolve this?	717	2020-02-12 15:02:21.577063+00	0	4	5	777	2174	4008	\N	0	0	0	\N	f	f	1	\N
4	785	585	2019-11-29 11:40:05+00	Show only last overlay in beamer for a single frame	Is there an overlay specification for `\\frame<...>{` that yields the last version of the frame if the frame is built with relative overlays?\r\n\r\nThe handout version achieves this, but sets it globally. I'm looking for a frame-specific solution. I think it works (not elegantly) with `\\frame<N>{` for some sufficiently large `N`.	599	2020-01-20 14:47:38.378762+00	0	4	1	660	1553	3152	518536	0	0	0	2020-01-20 14:47:38.378762+00	f	f	1	2
4	96	350	2019-12-06 07:36:10.922823+00	What should the site icon be?	What should we use for this site's identicon (also used for the top level site chat)?	359	2019-12-06 07:36:10.922823+00	6	4	3	425	1058	1999	\N	0	0	0	\N	f	f	2	\N
8	895	726	2020-02-19 13:43:36.818188+00	Can I use a Regex on a Stream?	\r\nI want to call [`regex_match`](https://en.cppreference.com/w/cpp/regex/regex_match) or similar on every line in a file conditionally operating on whether there was a match. Is there a way to do this without slurping the entire file?	743	2020-02-19 13:43:36.818188+00	0	4	1	801	2263	4186	\N	0	0	0	\N	f	f	1	\N
1	811	645	2020-01-30 09:40:40.599043+00	Make the site selector universally available	The site selector is only visible in the main site listing view, but not on post pages or during editing. But there's usually plenty of space in the top bar, so I see no reason to omit it.\r\n\r\nPlease make it universal.	660	2020-01-30 09:40:40.599043+00	6	1	1	720	2269	4187	\N	0	0	0	\N	f	f	2	\N
1	96	244	2019-12-02 11:44:54.573246+00	How granular should site scopes be?	I'm interested in asking and answering question about several topics. I'd like to [propose them as sites](https://topanswers.xyz/meta?q=211) but am unsure what level of granularity to propose:\r\n\r\n1. Computing (Superuser + ...)\r\n1. Programming (Software Engineering + ...)\r\n1. Coding (Stack Overflow + ...)\r\n1. Language X (tighter scope than anything on SE)\r\n\r\nMy inital question(s) might be Lua (for example). Should I consider getting some Lua folks involved and propose per-language sites (this would quickly run into scope overlap and other issues) or just go ahead and propose a general programming site and use tags to scope to a language?\r\n\r\nAnother case study might be my interest in a site to ask questions about [The SILE Typesetter](https://sile-typesetter.org/). I've actually considered proposing that the SE LaTeX site expand their scope to cover it because conceptually it is similar, but the expertise is also different so its kind of a grey area.\r\n\r\nShould I propose:\r\n\r\n1. A site about publishing in general?\r\n1. A site about typesetting?\r\n1. A site about LaTeX and derivaties (including conceptual ones that share no code)\r\n1. A site about SILE in specific?	252	2019-12-02 11:44:54.573246+00	4	4	2	319	2196	4046	\N	0	0	0	\N	f	f	2	\N
1	168	361	2019-12-06 11:39:05.471723+00	Support language selection for code blocks	Perhaps this is already supported, but I screwed up. On most sites you can select the language for which the syntax highlighting should be applied by giving the language name after the three forward ticks like this:\r\n\r\n```\r\n ```latex\r\n```\r\n\r\nThis broke syntax highlighting completely for me when I tried:\r\n\r\n```latex\r\n\\documentclass{standalone}\r\n\\begin{document}\r\n\\Huge test\r\n\\end{document}\r\n```	370	2019-12-06 14:06:50.011292+00	0	4	1	436	814	1159	\N	0	0	0	\N	f	f	2	\N
1	115	652	2020-01-31 01:51:19.919733+00	Browsing questions + using browser "back" button is suboptimal	For a while I've noticed that if I view a question and then use browser "back" to go back to the question list whence I came, it sends me to the top of the page.  That's a little annoying if I was most of the way down the page of questions; I have to page down to find where I left off.  But I didn't raise it because it seemed minor in the grand scheme of things -- something I would have brought up eventually, but it didn't press me to write the meta question.\r\n\r\nJust now, however, I was paging through questions (because I was away for a bit and missed a bunch of stuff), and ran into this variant of what I've described: I was on page 2 of the question list, viewed a question, used "back" -- and was back on the top of *page 1* again.\r\n\r\nIs there anything we can do about this?  By the way, I don't tend to use browser "back" if there's a convenient in-page affordance; I usually use the link in the page header to go back to the main page.  But these are cases where I use "back" because there's some context, and I'd rather not lose that context.\r\n	667	2020-01-31 01:51:19.919733+00	16	4	1	727	2234	4181	\N	0	0	0	\N	f	f	2	\N
4	233	352	2019-12-06 07:46:37.269331+00	How to filter the unanswered questions?	It is possible to make only the unanswered questions appear in the list?	361	2019-12-06 07:46:37.269331+00	2	4	1	427	805	1560	\N	0	0	0	\N	f	f	2	\N
8	2	723	2020-02-18 15:05:40.744908+00	Can anyone help fix this Firefox scrolling bug?	Flexbox is used in lot's of places on TopAnswers, and has great cross-browser support. One edge case is using `flex-direction: column-reverse` which is useful for the chat/comments pane. Unfortunately Firefox does not allow scrolling any overflowed content in this case (though Chrome and Safari appear to work perfectly).\r\n\r\nThere is a 6-year old bug [on BugZilla](https://bugzilla.mozilla.org/show_bug.cgi?id=1042151#c46) reporting this, with some recent activity suggesting it's getting more interest, but no guarantee of a fix in any particular timeframe.\r\n\r\nA [recent comment](https://bugzilla.mozilla.org/show_bug.cgi?id=1042151#c46) suggests the issue is in [this C++ file](https://bugzilla.mozilla.org/show_bug.cgi?id=1042151#c46):\r\n\r\n>The relevant code for the flex layout algorithm is [nsFlexContainerFrame.cpp](https://searchfox.org/mozilla-central/source/layout/generic/nsFlexContainerFrame.cpp). I'd probably start looking at [users of mFlexDirection in layout/](https://searchfox.org/mozilla-central/search?q=mFlexDirection&case=false&regexp=false&path=), then their callers, etc.\r\n>\r\n>The scrollability of a scroll container is determined in [nsHTMLScrollFrame::ReflowScrolledFrame](https://searchfox.org/mozilla-central/rev/690e903ef689a4eca335b96bd903580394864a1c/layout/generic/nsGfxScrollFrame.cpp#546).\r\n>\r\n>Sorry I can't point to exact steps to fix this (as I don't know where the bug is myself), but the above would be the first places where I'd look at.\r\n>\r\n>…\r\n\r\nNo idea if this is a huge job or something simple, or even whether it's going to require more than just general C++ knowledge to investigate. Anyone care to take a look and see if they can get a feel for how big the job is?	739	2020-02-18 15:05:40.744908+00	1	1	1	798	2258	4183	\N	0	0	0	\N	t	f	2	\N
2	37	728	2020-02-20 14:27:57.001101+00	Is Page Life Expectancy a decent surrogate for measuring I/O churn?	Page Life Expectancy, or PLE, is defined on [Microsoft Docs](https://docs.microsoft.com/en-us/sql/relational-databases/performance-monitor/sql-server-buffer-manager-object?view=sql-server-ver15#buffer-manager-performance-objects) as:\r\n\r\n> Indicates the number of seconds a page will stay in the buffer pool without references.\r\n\r\nDisregarding the complexities of "without references", I think of PLE in terms of how many pages are being transferred into the buffer pool from storage.  In other words, if you have a buffer pool size of 1 GB, and PLE is 100 seconds, then the server is transferring 1 GB of data from storage every 100 seconds, or 1/100 GB per second.  \r\n\r\nIs that assumption relatively safe?  Is quoting the number of bytes per second derived from calculating the Buffer Pool Size divided by the PLE a reliable enough number when pushing for more memory to be added to a server?\r\n	745	2020-02-20 14:27:57.001101+00	0	4	1	803	2273	4190	\N	0	0	0	\N	f	f	1	\N
1	38	433	2019-12-12 16:09:01.641565+00	Chat timeline events - question edits	Since questions and answers are sometimes editted in response to dialogue in chat, does it make sense to add a visual indicator in the chat transcript that an edit occured? \r\n\r\nLooking back at an older exchange, this UI element might provide clarity on context of the chat transcript and evolution of the Q/A exchange that would otherwise be lost.\r\n\r\nIf this is something that is desirable, perhaps a few elements to this feature might also be considered\r\n\r\n* hotlink to the edit/diff from the transcript event\r\n* replying to the event directly (as one would reply to a chat message)\r\n* "commit messages" for each edit to help the chat event provide summary context\r\n* others?\r\n	444	2019-12-12 16:09:01.641565+00	10	4	1	508	2101	3888	\N	0	0	0	\N	f	f	2	\N
8	895	729	2020-02-20 20:00:10.303171+00	How can I Obtain all the Match Positions of a Regex over a String?	I've written a toy example to try to help me understand this. Given the following string:\r\n\r\n    const char input[] = "if (KnR)\\n"\r\n                         "\\tfoo();\\n"\r\n                         "if (spaces) {\\n"\r\n                         "    foo();\\n"\r\n                         "}\\n"\r\n                         "if (allman)\\n"\r\n                         "{\\n"\r\n                         "\\tfoo();\\n"\r\n                         "}\\n"\r\n                         "if (horstmann)\\n"\r\n                         "{\\tfoo();\\n"\r\n                         "}\\n"\r\n                         "if (pico)\\n"\r\n                         "{\\tfoo(); }\\n"\r\n                         "if (whitesmiths)\\n"\r\n                         "\\t{\\n"\r\n                         "\\tfoo();\\n"\r\n                         "\\t}\\n";\r\n\r\nIf I'm using the following: `const regex r("(.+?)\\\\s*\\\\{?\\\\s*(.+?;)\\\\s*\\\\}?\\\\s*")` How can I find the begin and end position of all of the first capture in `input` and of all the second capture in `input`?\r\n\r\nSo for example I expect capture 1 to have the following ranges:\r\n\r\n 1. 0 to 8\r\n 1. 17 to 28\r\n 1. 44 to 55\r\n 1. 68 to 82\r\n 1. 94 to 103\r\n 1. 115 to 131\r\n\r\nAnd I expect capture 2 to have the following ranges:\r\n\r\n 1. 10 to 16\r\n 1. 35 to 41\r\n 1. 59 to 65\r\n 1. 85 to 91\r\n 1. 106 to 112\r\n 1. 136 to 142	746	2020-02-20 20:00:10.303171+00	0	4	1	804	2276	4192	\N	0	0	0	\N	f	f	1	\N
2	177	276	2019-06-13 09:47:29+00	SQL Server has encountered occurences of I/O requests taking longer than 15 seconds	On Production SQL Server, we have following config:\r\n\r\n3 Dell PowerEdge R630 servers, combined into Availability Group\r\nAll 3 are connected to single Dell SAN storage unit which is a RAID array\r\n\r\nFrom time to time, on PRIMARY we are seeing messages similar to below:\r\n\r\n>    SQL Server has encountered 11 occurrence(s) of I/O requests taking longer \r\n    than 15 seconds to complete on file [F:\\Data\\MyDatabase.mdf] in database id 8.  \r\n    The OS file handle is 0x0000000000001FBC.  \r\n    The offset of the latest long I/O is: 0x000004295d0000.  \r\n    The duration of the long I/O is: 37397 ms.\r\n\r\nWe are novice in performance troubleshooting\r\n\r\nWhat are the most common ways or best practices in troubleshooting this particular issue related to storage ?\r\nWhat performance counters, tools, monitors, apps, etc. must be used to narrow down to the root cause of such messages ?\r\nMight be there is a Extended Events that can help, or some kind of audit / logging ?\r\n\r\n\r\n	284	2019-12-04 14:22:11.186333+00	0	4	1	351	604	1512	240469	0	0	0	2019-12-04 14:22:11.186333+00	f	f	1	1
1	96	397	2019-12-10 07:08:27.97183+00	Edit history page should link back to the post	There is not an obvious way to navigate from a post's edit history page back to that post in context. There should be.\r\n\r\nIn the event I arrived at the history from the post itself, the back button does function (although a visible link would also be good), but in the case of following it from an external link or from notifications or whatever, there is actually no way to get there withouth going to the home page, hunting for the question, and opening it manually.	408	2019-12-10 07:10:05.800172+00	5	4	2	472	1002	2646	\N	0	0	0	\N	f	f	2	\N
2	748	557	2019-05-02 14:13:46+00	Why does this derived table improve performance?	I have a query which takes a json string as a parameter. The json is an array of latitude,longitude pairs.\r\nAn example input might be the following.\r\n```sql\r\ndeclare @json nvarchar(max)= N'[[40.7592024,-73.9771259],[40.7126492,-74.0120867]\r\n,[41.8662374,-87.6908788],[37.784873,-122.4056546]]';\r\n```\r\nIt calls a TVF that calculates the number of POIs around a geographical point, at 1,3,5,10 mile distances.\r\n```sql\r\ncreate or alter function [dbo].[fn_poi_in_dist](@geo geography)\r\nreturns table\r\nwith schemabinding as\r\nreturn \r\nselect count_1  = sum(iif(LatLong.STDistance(@geo) <= 1609.344e * 1,1,0e))\r\n      ,count_3  = sum(iif(LatLong.STDistance(@geo) <= 1609.344e * 3,1,0e))\r\n      ,count_5  = sum(iif(LatLong.STDistance(@geo) <= 1609.344e * 5,1,0e))\r\n      ,count_10 = count(*)\r\nfrom dbo.point_of_interest\r\nwhere LatLong.STDistance(@geo) <= 1609.344e * 10\r\n```\r\nThe intent of the json query is to bulk call this function. If I call it like this the performance is very poor taking nearly 10 seconds for just 4 points:\r\n```sql\r\nselect row=[key]\r\n      ,count_1\r\n      ,count_3\r\n      ,count_5\r\n      ,count_10\r\nfrom openjson(@json)\r\ncross apply dbo.fn_poi_in_dist(\r\n            geography::Point(\r\n                convert(float,json_value(value,'$[0]'))\r\n               ,convert(float,json_value(value,'$[1]'))\r\n               ,4326))\r\n```\r\nplan = https://www.brentozar.com/pastetheplan/?id=HJDCYd_o4\r\n\r\nHowever, moving the construction of the geography inside a derived table causes the performance to improve dramatically, completing the query in about 1 second.\r\n\r\n```\r\nselect row=[key]\r\n      ,count_1\r\n      ,count_3\r\n      ,count_5\r\n      ,count_10\r\nfrom (\r\nselect [key]\r\n      ,geo = geography::Point(\r\n                convert(float,json_value(value,'$[0]'))\r\n               ,convert(float,json_value(value,'$[1]'))\r\n               ,4326)\r\nfrom openjson(@json)\r\n) a\r\ncross apply dbo.fn_poi_in_dist(geo)\r\n```\r\nplan = https://www.brentozar.com/pastetheplan/?id=HkSS5_OoE\r\n\r\n\r\nThe plans look virtually identical. Neither uses parallelism and both use the spatial index. There is an additional lazy spool on the slow plan that I can eliminate with the hint `option(no_performance_spool)`. But the query performance does not change. It still remains much slower.\r\n\r\nRunning both with the added hint in a batch will weigh both queries equally.\r\n\r\nSql server version =\r\nMicrosoft SQL Server 2016 (SP1-CU7-GDR) (KB4057119) - 13.0.4466.4 (X64)\r\n\r\nSo my question is why does this matter? How can I know when I should calculate values inside a derived table or not?	570	2020-01-12 14:06:46.551624+00	0	4	1	632	1429	3207	237217	0	0	0	2020-01-12 14:06:46.551624+00	f	f	1	1
1	96	243	2019-12-02 10:14:10.805715+00	Add downvotes, because 0 sends the wrong message in some cases	I realize this will be controversial, but I strongly believe you will not have a sucessful site with "top" answers without having answers that have a special reason to be sent to the "bottom", and also that the "bottom" needs to be below zero.\r\n\r\n* On most sites (and hence in general user perception) 0 votes signals a lack of engagement, and hence this is generally viewed as neutral. An **outright dangerous** answer with a score of zero will still look like something people might try.\r\n\r\n* On all sites, you will have some small amount of "bad" signal — people will upvote stupid answers.\r\n\r\nIn both scenarios experts _NEED_ to be able to send a negative signal. **Upvotes vs. more of upvotes helps sort good from bad answers, but sending only positive signals is not enough.**\r\n\r\nCase in point:\r\n\r\n> Q. How do I enable and start apache?\r\n>\r\n> A(+6). `systemd enable --now httpd`\r\n>\r\n> A(+1). `rm -rf /etc/httpd`	251	2019-12-04 01:44:20.168232+00	9	4	2	318	1523	3757	\N	0	0	0	\N	f	f	2	\N
1	21	14	2019-11-08 20:36:11.148997+00	How should "New Chat ™" handle comment edits (if at all)?	I posted a comment in chat that had an obvious spelling error, and realized that I couldn't edit it. No big deal, just post the correction in another comment...but then I saw that I had yet another error (I know, I can barely speak English), and had to post another comment.\r\n\r\nSE chat gives a time limit to edit your comments (I think either 2  or 5 minutes, and mods might me able to edit them whenever). How should we handle comment edits? A time limit? No limit at all? Possibility of reading the original comment to avoid misunderstandings and/or arguments? An edit history, in other words.	21	2019-11-11 12:16:49.744691+00	2	4	1	17	179	181	\N	0	0	0	\N	f	f	2	\N
2	114	198	2011-08-31 09:10:35+00	Is there a way to access temporary tables of other sessions in postgres?	I'm working with a Windows application that uses a (local) postgres Database and stores some information in a temporary table. I'd like to have a look at the temporary table, but pgadmin and dbVis tell me: `ERROR: cannot access temporary tables of other sessions` when trying to query the data. I tried changing the permissions of the schema and table, but this didn't seem to help, even though I'm accessing the database with the same user as the program itself (at least in dbVis). Is there a setting I can change in my database that allows me to have "root" access to all session in my database?	205	2019-11-29 17:39:30.622012+00	0	4	1	273	415	490	5236	0	0	0	2019-11-29 17:39:30.622012+00	f	f	1	1
4	167	400	2019-12-10 13:14:49.877857+00	How to summarise what we are building here	When this site goes public it would be nice to have a couple of short summaries of what we are building that we can use when spreading the word to potential new users\r\n	411	2019-12-10 13:14:49.877857+00	5	4	3	475	1648	3770	\N	0	0	0	\N	f	f	2	\N
4	262	442	2019-12-14 06:44:04.627687+00	updmap error "Did you run mktexlsr" but I have already run it	Today I tried to update my TeX Live distribution with `tlmgr update --all` (with admin right of course). During the run, I see the following lines:\r\n\r\n```none\r\nrunning mktexlsr ...\r\ndone running mktexlsr.\r\nrunning mtxrun --generate ...\r\ndone running mtxrun --generate.\r\nrunning updmap-sys ...\r\n\r\ntlmgr.pl: updmap-sys failed (status 1), output:\r\nupdmap will read the following updmap.cfg files (in precedence order):\r\n  c:/texlive/2019/texmf-config/web2c/updmap.cfg\r\n  c:/texlive/2019/texmf-dist/web2c/updmap.cfg\r\nupdmap may write changes to the following updmap.cfg file:\r\n  c:/texlive/2019/texmf-config/web2c/updmap.cfg\r\ndvips output dir: "c:/texlive/2019/texmf-var/fonts/map/dvips/updmap"\r\npdftex output dir: "c:/texlive/2019/texmf-var/fonts/map/pdftex/updmap"\r\ndvipdfmx output dir: "c:/texlive/2019/texmf-var/fonts/map/dvipdfmx/updmap"\r\nupdmap [ERROR]: The following map file(s) couldn't be found:\r\nupdmap [ERROR]:         classico.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         classicovn.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         dayroman.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         gandhi.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         garamondvn.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         tli.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         ua1.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         ugm.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         ul9.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         ulg.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         webo.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]:         zgm.map (in c:/texlive/2019/texmf-config/web2c/updmap.cfg)\r\nupdmap [ERROR]: Did you run mktexlsr?\r\n\r\n        You can disable non-existent map entries using the option\r\n          --syncwithtrees.\r\n\r\nC:\\texlive\\2019\\bin\\win32\\runscript.tlu:911: command failed with exit code 1:\r\nperl.exe c:\\texlive\\2019\\texmf-dist\\scripts\\texlive\\updmap.pl  --sys\r\n```\r\nIn the first lines, it told me that `mktexlsr` was already run. However, later when `updmap`, it ended with exit code `1` and a message saying "*Did you run mktexlsr?*"\r\n\r\nExpectedly, the last line is\r\n```none\r\ntlmgr.pl: An error has occurred. See above messages. Exiting.\r\n```\r\nEverything else goes as normal.\r\n\r\nWhat could have gone wrong and what can I do about it?	453	2019-12-14 06:44:04.627687+00	0	4	1	517	1065	2028	\N	0	0	0	\N	f	f	1	\N
2	602	435	2018-11-21 12:33:45+00	Don't RESEED tables on Query Store Clean	We are using Query Store. And now we have solved one problem with it by cleaning the Query Store every night. And when you clean the Query Store, the Query Store tables are being `RESEED`ed :-( \r\n\r\nDo you know how clean the Query Store without `RESEED`ing the tables? 	446	2019-12-13 09:47:49.278664+00	0	4	1	510	1046	1941	223090	0	0	0	2019-12-13 09:47:49.278664+00	f	f	1	1
1	96	398	2019-12-10 07:34:19.856078+00	Post rate limit is really agressive	I've been having constant problems with the post rate limiting. I think it's too agressive for normal use. I would suggest a more variable scale where a few things in relatively quick sucession don't lock me out, but if there is a small flood that continues it kicks in.\r\n\r\nIt seems like right nowe there is a flat rate of some kind. 1 answer post per 60 second block or 1 question per 120 second block or something like that.\r\n\r\nThese numbers are out of a hat, but:\r\n\r\n* 5 posts in 5 minutes should be fine. It shouldn't batter if they are all in 5 seconds or spread across the time block.\r\n* 10 posts in 10 minutes probably shouldn't be fine.\r\n* 30 posts in 30 minutes definitely shouldn't be fine.	409	2019-12-10 07:34:19.856078+00	4	4	2	473	2090	3875	\N	0	0	0	\N	f	f	2	\N
1	96	417	2019-12-11 16:23:49.021173+00	Add a max-height style to fenced code blocks	We have some really _really_ long code blocks showing up on the TeX site sometimes. I think it actually makes it harder to understand what the gist of the answer is. I would suggest adding a `max-height` style rule to code blocks across all sites that shows maybe 25 or 30 lines at most, after which it would scroll internaly. I think this arrangement would be useful more often than not.\r\n\r\nFor those "not" cases where the long code really needs to be read through (for example with lots of comments where the code comments are the substance of the answer) a couple ways to mitigate negative effects.\r\n\r\n\r\n1. Offer an optional class on the code block (not on by default) that  would disable this. I don't know about the [markdown-it used here](https://topanswers.xyz/meta?q=72#question) but Pandoc's Markdown flavor would let you set a class on a code block something like this:\r\n\r\n    ~~~markdown\r\n    ``` {.no-scroll}\r\n    code here...\r\n    ```\r\n    ~~~\r\n\r\n1. Offer a "light-box" style maximizer to view code blocks in the full frame.\r\n\r\n1. Add a [copy / download](https://topanswers.xyz/meta?q=309#question) tools.	428	2019-12-11 16:23:49.021173+00	2	4	2	492	2107	1737	\N	0	0	0	\N	f	f	2	\N
2	751	89	2012-04-11 22:22:32+00	What's the difference between a temp table and table variable in SQL Server?	This seems to be an area with quite a few myths and conflicting views.\r\n\r\nSo what is the difference between a table variable and a local temporary table in SQL Server?	96	2019-11-24 09:12:50.630254+00	0	4	1	164	393	1476	16385	0	0	0	2019-11-24 09:12:50.630254+00	f	f	1	1
2	103	185	2017-03-14 01:17:59+00	Union implemented with Hash Match operator	I was reviewing the SQL Server physical operators listed on TechNet (don't judge, you know you've done it) and [read that the Hash Match physical operator is sometimes used to implement the `UNION` logical operator](https://technet.microsoft.com/en-us/library/ms189582(v=sql.105).aspx).\r\n\r\nI have never seen that done and would like to learn more.  An example query would be great.  When is it used and when is it better than the alternatives?  (Those are usually the same, but not always.)	192	2019-11-28 13:52:37.938934+00	0	4	1	260	385	1473	167086	0	0	0	2019-11-28 13:52:37.938934+00	f	f	1	1
1	262	373	2019-12-07 15:55:49.144127+00	What should the icon of TopAnswers be?	TopAnswers needs an icon to represent itself on the Internet. It is necessary to have more than some word "TopAnswers" in the header, in my opinion.\r\n\r\n![auxiliary.png](/image?hash=f7d3930c592bc0e0ac8dde874f92e589903084b90830974f0165d68e4735463e)\r\n\r\nSimilarly, each community needs one such icon.\r\n\r\nThe current icon, which is used throughout the site across every community is the [favicon](https://topanswers.xyz/favicon.ico) of the site, which looks like this\r\n\r\n![auxiliary.ico](/image?hash=71e10e45ba6c3f43b333bfab3a8eb6cf855eaf51b795f22028b2288f85720411) (16 &times; 16)\r\n\r\nI can't find a larger version.\r\n\r\nDo you think this icon will be a good representative of TopAnswers, or do you think a different one will do better? What is your opinion on this?	384	2019-12-07 15:55:49.144127+00	8	4	1	448	1712	3400	\N	0	0	0	\N	f	f	2	\N
1	43	80	2019-11-22 10:36:04.370419+00	How should bugs and feature requests to topanswers.xyz be reported?	As a question here?	87	2019-11-24 12:13:35.338661+00	3	4	1	155	2081	3873	\N	0	0	0	\N	f	f	2	\N
2	284	403	2015-09-23 17:15:28+00	UPDATE statement processing records that should be eliminated by WHERE clause	I am getting a very odd error. Consider the table below:\r\n\r\n\tCREATE TABLE #MyTable (\r\n\t\tKey1 INT , Key2 INT ,\r\n\t\tx SMALLINT , y INT , z INT ,\r\n\t\ta FLOAT , b FLOAT , c SMALLINT , s FLOAT\r\n\t)\r\n\r\n\t-- insert many records\r\n\r\n\tCREATE UNIQUE CLUSTERED INDEX CI ON #MyTable ( Key1 , Key2 )\r\n\r\nFor some reason, the following update statement tries to divide by zero. This can only happen if `c=0` or `c=1`. The `WHERE` clause clearly specifies `c>1`.\r\n\r\n\t-- this fails with divide-by-zero error\r\n\tUPDATE\t#MyTable\r\n\tSET\ts = CASE\r\n\t\t\tWHEN a - SQUARE ( b ) / c <= 0 THEN 0\r\n\t\t\tELSE ( a - SQUARE ( b ) / c ) / ( c - 1 )\r\n\t\tEND\r\n\tWHERE   x <= 622 AND c > 1 AND ( y > 0 OR z > 0 )\r\n\r\nThe problem is completely eliminated if I redundantly check for `c<=1` in my `CASE` expression:\r\n\r\n\t-- this completes without an error\r\n\tUPDATE\t#MyTable\r\n\tSET\ts = CASE\r\n\t\t\tWHEN ( c <= 1 ) OR ( a - SQUARE ( b ) / c <= 0 ) THEN 0\r\n\t\t\tELSE ( a - SQUARE ( b ) / c ) / ( c - 1 )\r\n\t\tEND\r\n\tWHERE   x <= 622 AND c > 1 AND ( y > 0 OR z > 0 )\r\n\r\nHas anyone encountered this before? Why would SQL Server touch the records with `c>1`?\r\n\r\nThe problem is also avoided if there is no index on the table (the index is useful in steps later in the procedure). Why would the existence of an index cause a condition in the `WHERE` clause to be ignored?	414	2019-12-11 04:20:50.592983+00	0	4	1	478	948	1681	115948	0	0	0	2019-12-11 04:20:50.592983+00	f	f	1	1
1	21	72	2019-11-20 01:55:36.314389+00	What markdown options are available?	Is there a list or a list of markdown options that work in TopAnswers?. I've seen in some comments that the options in this link work here https://markdown-it.github.io/, but I'm not sure if all of them work or just some of them. \r\n\r\nAlso, I saw that @Jack added the option to use a markdown for dbfiddle (which is great!), but I know this because I read the comment, so I think it should be good to have an updated list somewhere.	79	2019-11-20 01:55:36.314389+00	4	4	1	147	1868	3434	\N	0	0	0	\N	f	f	2	\N
1	167	411	2019-12-11 14:17:55.281707+00	Dismiss button for notifications sometimes not visible	It can happen that the dismiss button for notifications goes off the screen. \r\n\r\n![Screen Shot 2019-12-11 at 10.33.23.png](/image?hash=644ce205f28959152b83222a56c7bbe440f1dddbf510988175fcd4ffea8fd165)\r\n\r\nFor shorter texts this problem obviously does not occure, but it also works for longer usernames/question titles because in this case a line break is added.  	422	2019-12-11 14:17:55.281707+00	4	4	1	486	2112	3962	\N	0	0	0	\N	f	f	2	\N
1	17	412	2019-12-11 14:22:05.095888+00	What order do/should questions appear in the list?	Many sites have mechanisms for "bumping" old posts so they rise to the top of the list.\r\n\r\nRight now I'm struggling to read through the list to find the topic I want without search - I think an ordering to ensure active topics are kept near the top of the list is important.\r\n\r\nSo to the questions:\r\n\r\nWhat's the current state of play in regards to order of questions?\r\nWhat are some ideas of how it could work?	423	2019-12-11 14:22:05.095888+00	5	4	1	487	987	1740	\N	0	0	0	\N	f	f	2	\N
1	115	395	2019-12-10 00:17:44.985529+00	Do we need the "shadow" on question titles on question pages?	Question titles on the main page are nice and clear:\r\n\r\n![Screen Shot 2019-12-09 at 7.12.26 PM.png](/image?hash=efaee81920b4f8f1876050463e1dda2ec6b56d2cffaf722cb84aca7e41f23a45)\r\n\r\nOn the question pages themselves, though, they've gotten fuzzy.  I think there's a faint shadow there:\r\n\r\n![Screen Shot 2019-12-09 at 7.12.35 PM.png](/image?hash=5809547598fe2c1c57021db612cf06f82c1f4b32a965e4a98d85acf276fc99a6)\r\n\r\nIs this by design or is it a regression?  (I don't think it's been that way all along; it jumped out at me just now.)\r\n\r\nI turned off userscripts and styles to confirm it's not something I'm doing.  Doesn't seem to be.\r\n\r\nBecause it's large I can still read it, and most of the time I'm coming from the question list so I've already seen it, but just now I followed a link and thus my first view of the question title was of a fuzzy one.\r\n	406	2019-12-10 00:17:44.985529+00	6	4	1	470	943	1711	\N	0	0	0	\N	f	f	2	\N
2	2	69	2019-11-18 10:41:27.751025+00	What default fonts should we use on TopAnswers/databases?	We may [allow everyone to select their own font](/meta?q=56), but whether we do or not, we need to choose default fonts for TopAnswers/databases (both regular and monospace for `code`).\r\n\r\nPlease answer below with suggestions, one suggestion per answer, so we can vote and figure out the most popular.\r\n\r\nThe fonts must be available under a suitable license such as [OFL](https://en.wikipedia.org/wiki/SIL_Open_Font_License). A good place to look might be this [font library](https://fontlibrary.org/en/search?order=pop&license=OFL+%28SIL+Open+Font+License%29), and if you want sans-serif in particular you can [filter out the rest](https://fontlibrary.org/en/search?order=pop&license=OFL+%28SIL+Open+Font+License%29&category=sans-serif).	76	2019-11-30 11:08:42.986682+00	2	1	1	144	425	660	\N	0	0	0	\N	f	f	2	\N
2	608	444	2013-04-02 18:40:49+00	Concatenation Physical Operation: Does it guarantee order of execution?	In standard SQL, the result of a `union all` is not guaranteed to be in any order.  So, something like:\r\n\r\n    select 'A' as c union all select 'B'\r\n\r\nCould return two rows in any order (although, in practice on any database I know of, 'A' will come before 'B').\r\n\r\nIn SQL Server, this turns into an execution plan using a "concatenation" physical operation.\r\n\r\nI could easily imagine that the concatenation operation would scan its inputs, returning whatever input has records available.  However, I found the following statement on the web ([here][1]):\r\n\r\n> The Query Processor will execute this plan in the order that the\r\n> operators appear in the plan, the first is the top one and the last is\r\n> the end one.\r\n\r\nQuestion:  Is this true in practice?  Is this guaranteed to be true?\r\n\r\nI haven't found any reference in Microsoft documentation that the inputs are scanned in order, from the first to the last.  On the other hand, whenever I try running it, the results suggest that the inputs are, indeed, processed in order.\r\n\r\nIs there a way to have the engine process more than one input at a time?  My tests (using much more complicated expressions than constants) are on a parallel-enabled 8-core machine, and most queries do take advantage of the parallelism.\r\n\r\n\r\n\r\n  [1]: https://www.simple-talk.com/sql/learn-sql-server/showplan-operator-of-the-week---concatenation/#commentform	455	2019-12-14 07:01:12.986376+00	0	4	1	519	1082	2265	39081	0	0	0	2019-12-14 07:01:12.986376+00	f	f	1	1
1	96	409	2019-12-11 07:25:16.732224+00	Link SE identity without manual admin intervention	We need a tool of some kind such that user accounts here can authenticate their SE identity and link their accounts so that their important posts are parented correctly.\r\n\r\nPinging @Jack and getting him to hand verify and link SE identities does not scale.	420	2020-01-04 01:19:44.808403+00	7	4	2	484	1595	3297	\N	0	0	0	\N	f	f	2	\N
2	285	404	2016-02-16 18:47:54+00	Intermittent “Conversion failed…” error	I have several stored procedures. These procedures when called have been fully operational for over 18 months without error.  Over the last 3-5 months, occasionally I will receive the error:\r\n\r\n> Conversion failed when converting date and/or time from character string\r\n\r\nAttempts to run the query immediately after initial failure are unsuccessful.  If I wait 2-5 minutes, I can run the queries without problem or error.\r\n\r\nAt first I thought there was an error in the code (CAST or SUBSTRING statements) but these were ruled out.\r\n\r\n> I pulled each piece of code and ran it individually in the MS SQL\r\n> Server Console.  Each piece of the code ran successfully in the\r\n> console without error.  After this exercise, I was able to run the\r\n> stored procedure (unaltered) successfully.\r\n\r\nI have verified that no one is adding or modifying any data in the database at the time the stored procedure is executed when the error occurs.  \r\n\r\n> Due to the infrequency of the error occurring it is difficult to\r\n> troubleshoot.  After the most recent occurrence my thought was that\r\n> someone was editing data and that one or more fields were not\r\n> populated resulting in a divide by zero error however this was not the\r\n> case.  No one was connected to or editing the database at the time.\r\n\r\nAny ideas what could cause this issue?\r\n\r\n### Stored procedure details\r\n\r\nNote: \r\n\r\n - Unable to post entire content as exceeds char limit\r\n - Selected items in SP with comment tags are omitted, due to duplicate of selected item above.\r\n - Again, just to re-iterate, SP query works most of the time, when the script fails you have to wait 2-5 min to successfully run.\r\n\r\nSQL calling SP:\r\n\r\n    dbo.SP_Report_1 \r\n    @YEAR=2016,\r\n    @MTH=1,\r\n    @CL1_CONTRACT1= '%PROG1%',\r\n    @CL2_TELSUP_Contract1='%CL6 TELE%',\r\n    @CL2_HW_Contract1='%CL6 PRODTYPE1%',\r\n    @CL2_PROD1_Contract1='%PROD1 LCS%',\r\n    @CL3_PROD1_Contract1='%CL3 PROD1%'\r\n\r\nTbl Structures:\r\n\r\n    Tbl Issues\r\n      IS_ISSUE_NO\r\n      IS_ISSUE_TYPE_ID\r\n      IS_RECEIVED_DATETIME   (14 char number as YYYYMMDDhhmmss) not my sw, unable to chg\r\n      IS_RESOLVED_DATETIME   (14 char number as YYYYMMDDhhmmss) not my sw, unable to chg\r\n      IS_COMPANY_ID\r\n      IS_CONTRACT_INSTANCE_ID\r\n      IS_CATEGORY\r\n      REPORTING_METHOD\r\n         \r\n    Tbl Issue_Types\r\n      ISTY_ISSUE_TYPE_ID\r\n      ISTY_ISSUE_TYPE_NAME\r\n         \r\n    Tbl COMPANIES\r\n      CO_COMPANY_ID\r\n      PARENT_ORG0\r\n         \r\n    Tbl CDTBL_COMPANIES_PARENT_ORG0\r\n      CT_CODE\r\n      CT_DATA\r\n         \r\n    Tbl CUSTOMER_CONTRACTS\r\n      CC_CONTRACT_INSTANCE_ID\r\n      CC_CONTRACT_ID\r\n         \r\n    Tbl CONTRACT_MASTER\r\n      CTR_CONTRACT_ID\r\n      CTR_CONTRACT_DESC\r\n         \r\n    Tbl ISSUE_CATEGORIES\r\n      ICG_CATEGORY_ID\r\n      ICG_CATEGORY_NAME\r\n\r\n \r\nStored Procedure:\r\n \r\n\r\n     @YEAR varchar(200),\r\n     @MTH varchar(200),\r\n     @CL1_CONTRACT1 VARCHAR(200),\r\n     @CL2_TELSUP_Contract1 VARCHAR(200),\r\n     @CL2_HW_Contract1 VARCHAR(200),\r\n     @CL2_PROD1_Contract1 VARCHAR(200),\r\n     @CL3_PROD1_Contract1 VARCHAR(200)\r\n    \r\n     AS\r\n     BEGIN\r\n    \t-- SET NOCOUNT ON added to prevent extra result sets from\r\n    \t-- interfering with SELECT statements.\r\n    SET NOCOUNT ON;\r\n    \r\n    -- THIS PROCEDURE RETURNS COUNT OF ISSUES DURING GIVEN PERIOD\r\n    \r\n    \r\n    SELECT\r\n    TOTAL_OPENED_ISSUES,\r\n    TOTAL_CLOSED_ISSUES,\r\n    CL2_ISSUES,\r\n     --CL1_ISSUES,\r\n     --CL4_ISSUES,\r\n     --CLCAT1_ISSUES,\r\n     --CLCAT2_ISSUES,\r\n     --OTHER_ISSUES,\r\n    PRODTYPE2_ISSUES,\r\n     --PRODTYPE1_ISSUES,\r\n     --UNSUPPORTED_PRODTYPE1_ISSUES,\r\n     --CL1_UNSUPPORTED_PRODTYPE1_ISSUES,\r\n     --OTHER_UNSUPPORTED_PRODTYPE1_ISSUES,\r\n     --SYSTEM_ISSUES,\r\n     --OTHER_TYPE_ISSUES,\r\n    CL5_CTR_ISSUES, \r\n     --CL3_PROD1_CTR_ISSUES, \r\n     --CL2_TELSUP_CTR_ISSUES, \r\n     --CL2_HW_CTR_ISSUES, \r\n     --CL2_PROD1_CTR_ISSUES, \r\n     --CL4_CTR_ISSUES, \r\n     --UNSUPPORTED_PROD1_ISSUES,\r\n     --OTHER_CTR_ISSUES,\r\n    CSRS_BY_EMAIL,\r\n     --CSRS_BY_PHONE,\r\n     --CSRS_BY_VM_CL1_HR,\r\n     --CSRS_BY_VM_D_HR,\r\n     --CSRS_BY_WEB_FRM,\r\n     --CSRS_BY_OTHR,\r\n     --CSRS_BY_SAI,\r\n    CL2_HLTHCHK_ISSUES,\r\n     --CL1_HLTHCHK_ISSUES,\r\n     --CL4_HLTHCHK_ISSUES,\r\n     --CLCAT1_HLTHCHK_ISSUES,\r\n     --CLCAT2_HLTHCHK_ISSUES,\r\n     --OTHER_HLTHCHK_ISSUES,\r\n    CL5_PRODTYPE2_ISSUES,\r\n     --CL5_PRODTYPE1_ISSUES,\r\n     --CL5_UNSUPPORTED_PRODTYPE1_ISSUES,\r\n     --CL5_SYSTEM_ISSUES,\r\n     --CL5_OTHER_TYPE_ISSUES,\r\n    CL6_PRODTYPE2_ISSUES,\r\n     --CL6_PRODTYPE1_ISSUES,\r\n     --CL6_UNSUPPORTED_PRODTYPE1_ISSUES,\r\n     --CL6_SYSTEM_ISSUES,\r\n     --CL6_OTHER_TYPE_ISSUES\r\n    \r\n    \r\n    FROM \r\n    \r\n    --ALL TOTAL CSRS OPENED\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS TOTAL_OPENED_ISSUES\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID \r\n    \r\n    WHERE\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    ) AS TOTAL_OPENED_CSRs,\r\n    \r\n    --ALL TOTAL CSRS CLOSEDED\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS TOTAL_CLOSED_ISSUES\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID  \r\n    \r\n    WHERE\r\n    IS_RESOLVED_DATETIME <> 0 AND\r\n    (\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR  AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    ) AND\r\n    (\r\n    MONTH(CAST(SUBSTRING (str(IS_RESOLVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RESOLVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RESOLVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RESOLVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RESOLVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RESOLVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    )\r\n    ) AS TOTAL_CLOSED_CSRs,\r\n    \r\n    -- ORG RELATED CSRS\r\n    --ALL CL2 CSRs\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS CL2_ISSUES\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN\r\n    COMPANIES ON IS_COMPANY_ID = CO_COMPANY_ID LEFT JOIN\r\n    CDTBL_COMPANIES_PARENT_ORG0 ON COMPANIES.PARENT_ORG0 = CDTBL_COMPANIES_PARENT_ORG0.CT_CODE LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID \r\n    \r\n    WHERE\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    CDTBL_COMPANIES_PARENT_ORG0.CT_DATA LIKE '%CL2%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    ) AS CL2_CSRs, \r\n    \r\n    \r\n    \r\n    -- TYPE RELATED CSRS\r\n    --PRODTYPE2 CSRs\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS PRODTYPE2_ISSUES\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN\r\n    ISSUE_CATEGORIES ON IS_CATEGORY = ICG_CATEGORY_ID LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID \r\n    \r\n    WHERE\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    ICG_CATEGORY_NAME = 'PRODTYPE2' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    ) AS PRODTYPE2_CSRs,\r\n    \r\n    \r\n    \r\n    -- CONTRACT RELATED CSRS\r\n    --CL5 CSRs\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS CL5_CTR_ISSUES\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN\r\n    CUSTOMER_CONTRACTS ON IS_CONTRACT_INSTANCE_ID = CC_CONTRACT_INSTANCE_ID LEFT JOIN \r\n    CONTRACT_MASTER ON  CC_CONTRACT_ID = CTR_CONTRACT_ID LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID \r\n    \r\n    WHERE\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    (CTR_CONTRACT_DESC Like @CL5_CONTRACT1\r\n     --OR CTR_CONTRACT_DESC Like @CL1_CONTRACT2\r\n     ) AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    ) AS CL5_CTR_CSRs,\r\n    \r\n    \r\n    \r\n    -- CSRS BY ORIGINATION\r\n    --CSRS REPORTED BY EMAIL\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS CSRS_BY_EMAIL\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID \r\n    \r\n    WHERE\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' AND\r\n    ISSUES.REPORTING_METHOD = 2\r\n    ) AS CSRs_REPORTED_BY_EMAIL,\r\n    \r\n    \r\n    \r\n    -- HEALTH CHK CSRS\r\n    --ALL CL2 HLTHCHK CSRs\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS CL2_HLTHCHK_ISSUES\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN\r\n    COMPANIES ON IS_COMPANY_ID = CO_COMPANY_ID LEFT JOIN\r\n    CDTBL_COMPANIES_PARENT_ORG0 ON COMPANIES.PARENT_ORG0 = CDTBL_COMPANIES_PARENT_ORG0.CT_CODE LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID \r\n    \r\n    WHERE\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    CDTBL_COMPANIES_PARENT_ORG0.CT_DATA LIKE '%CL2%' AND\r\n    ISTY_ISSUE_TYPE_NAME LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    ) AS CL2_HLTHCHK_CSRs, \r\n    \r\n    \r\n    \r\n    \r\n    -- CSR BY CONTRACT (SUB BY TYPE)\r\n    -- CL5 RELATED TYPE CSRS\r\n    --PRODTYPE2 CSRs\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS CL5_PRODTYPE2_ISSUES\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN\r\n    CUSTOMER_CONTRACTS ON IS_CONTRACT_INSTANCE_ID = CC_CONTRACT_INSTANCE_ID LEFT JOIN \r\n    CONTRACT_MASTER ON  CC_CONTRACT_ID = CTR_CONTRACT_ID LEFT JOIN\r\n    ISSUE_CATEGORIES ON IS_CATEGORY = ICG_CATEGORY_ID LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID \r\n    \r\n    WHERE\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    (CTR_CONTRACT_DESC Like @CL5_CONTRACT1\r\n     --OR CTR_CONTRACT_DESC Like @CL1_CONTRACT2\r\n     ) AND\r\n    ICG_CATEGORY_NAME = 'PRODTYPE2' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    ) AS CL5_PRODTYPE2_CSRs,\r\n    \r\n    \r\n    \r\n    --PRODTYPE2 CSRs\r\n    -- CL6 RELATED TYPE CSRS\r\n    (SELECT\r\n    COUNT(IS_ISSUE_NO) AS CL6_PRODTYPE2_ISSUES\r\n    \r\n    FROM \r\n    ISSUES LEFT JOIN\r\n    COMPANIES ON IS_COMPANY_ID = CO_COMPANY_ID LEFT JOIN\r\n    CDTBL_COMPANIES_PARENT_ORG0 ON COMPANIES.PARENT_ORG0 = CDTBL_COMPANIES_PARENT_ORG0.CT_CODE LEFT JOIN\r\n    ISSUE_CATEGORIES ON IS_CATEGORY = ICG_CATEGORY_ID LEFT JOIN \r\n    ISSUE_TYPES ON IS_ISSUE_TYPE_ID = ISTY_ISSUE_TYPE_ID \r\n    \r\n    WHERE\r\n    MONTH(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @MTH AND \r\n    YEAR(CAST(SUBSTRING (str(IS_RECEIVED_DATETIME,16),7,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),9,2) + CHAR(45) + SUBSTRING (str(IS_RECEIVED_DATETIME,16),3,4) AS DATE)) = @YEAR AND\r\n    CDTBL_COMPANIES_PARENT_ORG0.CT_DATA LIKE '%CL2%' AND\r\n    ICG_CATEGORY_NAME = 'PRODTYPE2' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%HEALTH%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%COURTESY%' AND\r\n    ISTY_ISSUE_TYPE_NAME NOT LIKE '%INTERNAL%' \r\n    ) AS CL6_PRODTYPE2_CSRs,\r\n    \r\n     END\r\n\r\n	415	2019-12-11 04:24:49.891139+00	0	4	1	479	950	1682	129442	0	0	0	2019-12-11 04:22:12.812547+00	f	f	1	1
1	234	389	2019-12-09 19:30:12.720786+00	It is impossible to write comments for me at the moment 	... which is why this became a question (rather than a comment). Whenever I try to add a comment, I get\r\n\r\n```\r\n00 Internal Server Error\r\n42P01: ERROR: relation &quot;account_community&quot; does not exist\r\nLINE 2: insert into account_community(account_id,community_id,acco...\r\n^\r\nQUERY:\r\ninsert into account_community(account_id,community_id,account_community_regular_font_id,account_community_monospace_font_id)\r\nselect aid,cid,community_regular_font_id,community_monospace_font_id from community where community_id=cid\r\non conflict on constraint account_community_pkey do nothing;\r\n\r\nCONTEXT: SQL function &quot;_ensure_account_community&quot; during startup\r\nSQL function &quot;new_chat&quot; statement 4\r\n```\r\n\r\nUntil very recently I could add comments. Of course I tried to reload the page.\r\n\r\n**UPDATE**: Just now it worked again. Please feel free to delete the "question" if it is inappropriate (yet at the time of writing this was the only way I could contact anyone using tools on this site). Perhaps there will be an answer that tells others what to do in such situations.	400	2019-12-09 19:37:28.134263+00	3	4	3	464	918	2139	\N	0	0	0	\N	f	f	2	\N
1	96	396	2019-12-10 07:02:35.320474+00	Post (+edit) notifications should also link directly to the post	The notification for new answers to posts and for edits to existing answers both link to the edit history page. In the case of edits this is a useful place to link if I want to see what changed, but _always_ on new posts and quite often on edits the thing I want to see is actually the post in context. The notification should be reworded to to link to both the post itself in context and the edit history.\r\n\r\nFor new posts:\r\n\r\n> [New answer] on [<question title>] by [<author>].\r\n\r\nFor edits:\r\n\r\n> [Answer post] on <question title> [edited] by [<author>].	407	2019-12-10 07:02:35.320474+00	4	4	2	471	2089	3954	\N	0	0	0	\N	f	f	2	\N
1	96	385	2019-12-09 12:44:12.55261+00	Add short link codes for topanswers sites	The short link formats for SE sites (e.g. dba.se) are really convenient, but as this ecosystem grows it will be even more useful to link these sites. I suggest adding a similar auto-linking format for top answers sites.	396	2019-12-09 12:44:12.55261+00	4	4	2	460	2094	3322	\N	0	0	0	\N	f	f	2	\N
1	96	408	2019-12-11 07:18:57.425488+00	Change post status from 'regular' to 'meta'	What a "meta" question is is  going to be a regular point of confusion for users, and questions *will* get asked under the wrong post type. Being able to fix this with a simple status flag change would be much less messy and disruptive than requiring deletion an reposting.	419	2019-12-11 07:18:57.425488+00	9	4	2	483	2010	3861	\N	0	0	0	\N	f	f	2	\N
2	286	405	2014-05-09 17:12:31+00	What is the correct way to ensure unique entries in a temporal database design?	I'm having trouble with the design of a temporal database. I need to know how to make sure I have only one active record for any given timeframe for a store. I have read [this answer](https://dba.stackexchange.com/a/56605/37401), but I'm afraid I can't wrap my head around how the trigger would work. Particularly, how I would work that trigger into my existing one that prevents updates to records, and inserts a new record instead. My real problem is that I do not know how to prevent a Store from having more than one effective date when the finished date is null. (i.e. prevent 2 active records for a store).\r\n\r\nThis is what I have, but it allows me to insert a new record for a store with a different effective date.\r\n\r\nTable Definition:\r\n\r\n    /****** Object:  Table [PCR].[Z_STORE_TEAM]    Script Date: 05/09/2014 13:05:57 ******/\r\n    IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[Z_STORE_TEAM]') AND type in (N'U'))\r\n    DROP TABLE [Z_STORE_TEAM]\r\n    GO\r\n    \r\n    IF NOT EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[Z_STORE_TEAM]') AND type in (N'U'))\r\n    BEGIN\r\n    CREATE TABLE [Z_STORE_TEAM](\r\n    \t[STORENUM] [int] NOT NULL,\r\n    \t[TEAM] [varchar](10) NULL,\r\n    \t[EFFECTIVE] [date] NOT NULL,\r\n    \t[FINISHED] [date] NULL,\r\n    PRIMARY KEY CLUSTERED \r\n    (\r\n    \t[STORENUM] ASC,\r\n    \t[EFFECTIVE] ASC\r\n    )WITH (PAD_INDEX  = OFF, STATISTICS_NORECOMPUTE  = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS  = ON, ALLOW_PAGE_LOCKS  = ON) ON [PRIMARY]\r\n    ) ON [PRIMARY]\r\n    END\r\n    GO\r\n\r\nSample Data:\r\n\r\n    INSERT [Z_STORE_TEAM] ([STORENUM], [TEAM], [EFFECTIVE], [FINISHED]) VALUES (1, N'1', CAST(0x01380B00 AS Date), CAST(0x81380B00 AS Date))\r\n    INSERT [Z_STORE_TEAM] ([STORENUM], [TEAM], [EFFECTIVE], [FINISHED]) VALUES (1, N'2', CAST(0x81380B00 AS Date), NULL)\r\n    INSERT [Z_STORE_TEAM] ([STORENUM], [TEAM], [EFFECTIVE], [FINISHED]) VALUES (2, N'1', CAST(0x01380B00 AS Date), NULL)\r\n    INSERT [Z_STORE_TEAM] ([STORENUM], [TEAM], [EFFECTIVE], [FINISHED]) VALUES (2, N'2', CAST(0x20380B00 AS Date), NULL)\r\n\r\nInstead of Update Trigger:\r\n\r\n    CREATE TRIGGER [tr_ZStoreTeam_update] \r\n       ON  [Z_STORE_TEAM]\r\n       INSTEAD OF UPDATE\r\n    AS \r\n    BEGIN\r\n    \t-- SET NOCOUNT ON added to prevent extra result sets from\r\n    \t-- interfering with SELECT statements.\r\n    \tSET NOCOUNT ON;\r\n    \r\n        -- Insert statements for trigger here\r\n    \tINSERT INTO PCR.Z_STORE_TEAM(STORENUM,TEAM,EFFECTIVE)\r\n    \tSELECT I.STORENUM,I.TEAM,GETDATE() AS EFFECTIVE\r\n    \tFROM inserted I\r\n    \tINNER JOIN PCR.Z_STORE_TEAM ST\r\n    \t\tON I.STORENUM = ST.STORENUM\r\n    \t\t\r\n    \tUPDATE ST\r\n    \tSET FINISHED = GETDATE()\r\n    \tFROM PCR.Z_STORE_TEAM ST\r\n    \tINNER JOIN inserted I\r\n    \t\tON ST.STORENUM = I.STORENUM\r\n    \t\tAND ST.EFFECTIVE = I.EFFECTIVE\r\n    END\r\n    \r\n    GO	416	2019-12-11 04:26:55.896766+00	0	4	1	480	951	1683	64862	0	0	0	2019-12-11 04:26:55.896766+00	f	f	1	1
1	691	486	2019-12-27 14:58:00.267721+00	What is the goal of TopAnswers?	I was directed to explore TopAnswers.xyz in the wake of events on SE. Looking around, I see a site that is functional but still feels like a work in progress, as I would expect.\r\n\r\nMy question pertains to the meta conversations I'm scanning about how to implement the site. Many of the questions ask for variations to do things differently from SE. Functionally, I've generally liked how SE works. My only issues have been with the management (and with the existence of a "management" that can override the local communities). But it seems a lot of other people want some changes in how questions are asked and answered. \r\n\r\nThat makes me curious: What is the goal of TopAnswers? Is it simply to replicate SE functionality away from the corporate structure of SE? Or is there some statement of principles that could be used as a guide to decide whether and how to vary from the (you gotta admit: successful) SE formula? 	498	2019-12-27 14:58:00.267721+00	5	4	1	561	1201	3891	\N	0	0	0	\N	f	f	2	\N
4	167	501	2020-01-01 19:00:25.27062+00	Resume footnote numbering from one document in another	Assume I have an article with a couple of footnotes\r\n\r\n```\r\n\\documentclass{article}\r\n\\begin{document}\r\ntest\\footnote{text}\r\n\\newpage\r\ntest\\footnote{text}\r\n\\end{document}\r\n```\r\n\r\nHow to resume the footnote counter from this document in another one, e.g. the next document should continue with footnote 3	513	2020-01-01 19:17:17.523462+00	0	4	3	576	1249	2523	\N	0	0	0	\N	f	f	1	\N
1	96	353	2019-12-06 07:48:56.883789+00	There needs to be a delete tool	Letting people vandalize the site by deleting their own or others posts is not okay...\r\n\r\nBut there are legitimate needs for deleting something. Especially when posted in error, such as our case of [an answer](https://topanswers.xyz/tex?q=351) getting posted as a question instead of under the [question it belongs under](https://topanswers.xyz/tex?q=343). This kind of mess needs to be fixed, not left around to confuse and clutter every future visitor.	362	2019-12-06 14:08:02.610262+00	8	4	2	428	1178	2251	\N	0	0	0	\N	f	f	2	\N
4	233	372	2019-12-07 07:33:54.576049+00	What about changing the chat title?	"TeX, LaTeX and friends" is the same title of TeX.SE, what about changing it? For example to "TopTeX"?	383	2019-12-07 08:11:17.251123+00	5	4	1	447	889	1334	\N	0	0	0	\N	f	f	2	\N
2	182	280	2019-06-04 14:29:32+00	SQL counting distinct over partition	I have a table with two columns, I want to count the distinct values on Col_B over (conditioned by) Col_A.\r\n\r\n### MyTable\r\n\r\n| Col_A | Col_B |\r\n|-------|-------|\r\nA  \t  | 1\r\nA\t  | 1\r\nA\t  | 2\r\nA\t  | 2\r\nA\t  | 2\r\nA\t  | 3\r\nb\t  | 4\r\nb\t  | 4\r\nb\t  | 5\r\n\r\n### Expected Result\r\n\r\n| Col_A | Col_B | Result |\r\n|-------|-------|--------|\r\nA\t    | 1\t    | 3\r\nA\t    | 1\t    | 3\r\nA\t    | 2\t    | 3\r\nA\t    | 2\t    | 3\r\nA\t    | 2\t    | 3\r\nA\t    | 3\t    | 3\r\nb\t    | 4\t    | 2\r\nb\t    | 4\t    | 2\r\nb\t    | 5\t    | 2\r\n\r\nI tried the following code\r\n\r\n~~~\r\nselect *, \r\ncount (distinct col_B) over (partition by col_A) as 'Result'\r\nfrom MyTable\r\n~~~\r\n\r\n`count (distinct col_B)` is not working.\r\nHow can I rewrite the count function to count distinct values?\r\n	288	2019-12-04 14:43:49.063419+00	0	4	1	355	625	1532	239788	0	0	0	2019-12-04 14:24:43.545257+00	f	f	1	1
2	752	589	2020-01-21 17:17:04.50773+00	How to prioritize rows with upper case?	I've noticed some weird behavior in SQL Server, a case insensitive column can group two strings together:\r\n\r\n    if object_id('Tempdb..#names') is not null begin drop table #names drop table #sheep end\r\n\r\n    create table #names (fct_name varchar(255))\r\n    insert into #names (fct_name)\r\n    values \r\n    ('Angus McFife'),\r\n    ('Angus Mcfife'),\r\n    ('Willie McNulty')\r\n\r\n    create table #sheep (farmer varchar(255), sheep int)\r\n    insert into #sheep (farmer, sheep)\r\n    values\r\n    ('Angus McFife', 35),\r\n    ('Angus Mcfife', 16),\r\n    ('Willie McNulty', 52)\r\n\r\n\r\n    select fct_name, sum(s.sheep) as herd\r\n    from #names n\r\n    inner join #sheep s on n.fct_name = s.farmer\r\n    group by n.fct_name\r\n      \r\nThis is causing a problem, because Angus will now get double counted:\r\n\r\n    fct_name\therd\r\n    Angus McFife\t102\r\n    Willie McNulty\t52\r\n    \r\nAngus only has 35 sheep, but our report will show he has 102! Or maybe he actually has 51. Maybe 16? Are there two farmers called Angus? Who can say? Basically, the entire agricultural industry of Scotland will grind to a halt.\r\n\r\nIn our source system (an oracle DB for a surgery application) there are no ID's for these columns - they join on names, but they are case sensitive. I'd prefer not to make the SQL Server data warehouse case sensitive if I can avoid it - especially making just one column case sensitive could cause a lot of confusion down the road.\r\n\r\n**QUESTION:** What is the best way to ensure that Angus doesn't get double counted? 	603	2020-01-21 18:52:43.276759+00	0	4	1	664	1634	3208	\N	0	0	0	\N	f	f	1	\N
1	167	464	2019-12-18 13:42:14.134379+00	Make field for login key saveable for browser/password manager	Would it be possible to add some attritbute to the "Enter PIN" field to enbale browsers/password managers to save the key?\r\n\r\n![Screen Shot 2019-12-18 at 14.37.28.png](/image?hash=6ec12046f925c28ef2e8faafee793e2fc0b38d02e6f7307c5dd98e781e79e999)\r\n\r\n\r\n	475	2019-12-18 22:23:48.015194+00	5	4	1	539	1140	2369	\N	0	0	0	\N	f	f	2	\N
2	37	255	2019-05-30 19:25:40+00	What is the purpose of this Uniq1002 column in this index scan?	Take the following repro:\r\n\r\n```\r\nUSE tempdb;\r\n\r\nIF OBJECT_ID(N'dbo.t', N'U') IS NOT NULL\r\nDROP TABLE dbo.t\r\nGO\r\nCREATE TABLE dbo.t\r\n(\r\n    id int NOT NULL \r\n        PRIMARY KEY \r\n        NONCLUSTERED \r\n        IDENTITY(1,1)\r\n    , col1 datetime NOT NULL\r\n    , col2 varchar(800) NOT NULL\r\n    , col3 tinyint NULL\r\n    , col4 sysname NULL\r\n);\r\n\r\nINSERT INTO dbo.t (\r\n      col1\r\n    , col2\r\n    , col3\r\n    , col4\r\n    ) \r\nSELECT TOP(100000) \r\n      CONVERT(datetime, \r\n         DATEADD(DAY, CONVERT(int, CRYPT_GEN_RANDOM(1)), '2000-01-01 00:00:00'))\r\n    , replicate('A', 800)\r\n    , sc2.bitpos\r\n    , CONVERT(sysname, CHAR(65 + CRYPT_GEN_RANDOM(1) % 26) \r\n        + CHAR(65 + CRYPT_GEN_RANDOM(1) % 26) \r\n        + CHAR(65 + CRYPT_GEN_RANDOM(1) % 26))\r\nFROM sys.syscolumns sc\r\n    CROSS JOIN sys.syscolumns sc2;\r\n```\r\n\r\nHere I'm adding a clustered index onto a set of columns that are not unique, and typical single-column non-clustered index:\r\n\r\n```\r\nCREATE CLUSTERED INDEX t_cx \r\nON dbo.t (col1, col2, col3);\r\n\r\nCREATE INDEX t_c1 ON dbo.t(col4); \r\n```\r\n\r\nThis query forces SQL Server to do a lookup into the clustered index.  Please forgive the use of the index hint, it was the quickest way to get the repro:\r\n\r\n```\r\nSELECT id\r\n    , col1\r\n    , col2\r\n    , col3\r\nFROM dbo.t aad WITH (INDEX = t_c1)\r\nWHERE col4 = N'JSB'\r\n    AND col1 > N'2019-05-30 00:00:00';\r\n```\r\n\r\nThe [actual query plan](https://www.brentozar.com/pastetheplan/?id=ryfqhopaV) shows a non-existent column in the Output List for the nonclustered index scan:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nOstensibly, this represents the uniqifier used in the non-unique clustered index.  Is that the case?  Is a column named like that *always* the clustered index uniqifier?\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/CKANG.png	263	2019-12-04 01:10:12.829009+00	0	4	1	330	1389	2846	239487	0	0	0	2019-12-04 01:10:12.829009+00	f	f	1	1
2	201	308	2018-08-04 18:30:57+00	How to parse query plans to get a count of nested loop operators with unordered prefetch?	My boss wants me to parse a set of query plans stored in a table and to determine how many nested loop operators with [unordered prefetching][1] are present in each plan. I only have around 100 query plans so performance isn't very important. I tried doing it myself but quickly got confused and couldn't make progress.\r\n\r\nThe structure of the table:\r\n\r\n    DROP TABLE IF EXISTS dbo.query_plans;\r\n    \r\n    CREATE TABLE dbo.query_plans (\r\n    plan_name VARCHAR(100),\r\n    query_xml XML\r\n    );\r\n\r\nI uploaded T-SQL to add three example query plans to the table on [pastebin][2]. This is the output that I'm looking for:\r\n\r\n    ╔═════════════╦════════════════╗\r\n    ║  plan_name  ║ OPERATOR_COUNT ║\r\n    ╠═════════════╬════════════════╣\r\n    ║ NO_PREFETCH ║              0 ║\r\n    ║ 1_PREFETCH  ║              1 ║\r\n    ║ 2_PREFETCH  ║              2 ║\r\n    ╚═════════════╩════════════════╝\r\n\r\nI can't answer any questions as to why I need to do this. Thanks!\r\n\r\n  [1]: http://sqlblog.com/blogs/paul_white/archive/2013/08/31/sql-server-internals-nested-loops-prefetching.aspx\r\n  [2]: https://pastebin.com/4e3BhHTC	316	2019-12-05 13:50:25.340544+00	0	4	1	383	676	1146	214073	0	0	0	2019-12-05 13:50:25.340544+00	f	f	1	1
2	121	287	2017-05-26 13:20:49+00	Why is it recommended to store BLOBs in separate SQL Server tables?	[This highly-upvoted SO answer](https://stackoverflow.com/a/5613926/87698) recommends to put images in separate tables, even if there is only a 1:1 relationship with another table:\r\n\r\n> If you decide to put your pictures into a SQL Server table, I would strongly recommend using a separate table for storing those pictures - do not store the employee photo in the employee table - keep them in a separate table. That way, the Employee table can stay lean and mean and very efficient, assuming you don't always need to select the employee photo, too, as part of your queries.\r\n\r\nWhy? I was under the impression that [SQL Server only stores a pointer to some dedicated BLOB data structure](http://sqlmag.com/t-sql/varbinarymax-tames-blob) in the table, so why bother to manually create another layer of indirection? Does it really improve performance significantly? If yes, why?\r\n	295	2019-12-04 14:30:16.693922+00	0	4	1	362	618	1530	174678	0	0	0	2019-12-04 14:30:16.693922+00	f	f	1	1
2	604	437	2012-03-12 02:29:32+00	Can you explain this execution plan?	I was researching something else when I came across this thing. I was generating test tables with some data in it and running different queries to find out how different ways to write queries affects execution plan. Here is the script that I used to generate random test data:\r\n\r\n\tIF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID('t') AND type in (N'U'))\r\n\tDROP TABLE t\r\n\tGO\r\n\r\n\tCREATE TABLE t \r\n\t(\r\n\t c1 int IDENTITY(1,1) NOT NULL \r\n\t,c2 int NULL\r\n\t) \r\n\tGO\r\n\r\n\tinsert into t\r\n\tselect top 1000000 a from\r\n\t(select t1.number*2048 + t2.number a, newid() b\r\n\tfrom [master]..spt_values t1 \r\n\tcross join  [master]..spt_values t2\r\n\twhere t1.[type] = 'P' and t2.[type] = 'P') a\r\n\torder by b\r\n\tGO\r\n\r\n\tupdate t set c2 = null\r\n\twhere c2 < 2048 * 2048 / 10\r\n\tGO\r\n\r\n\r\n\tCREATE CLUSTERED INDEX pk ON [t] (c1)\r\n\tGO\r\n\r\n\tCREATE NONCLUSTERED INDEX i ON t (c2)\r\n\tGO\r\n\r\n\r\nNow, given this data, I invoked the following query:\r\n\r\n    select * \r\n    from t \r\n    where \r\n          c2 < 1048576 \r\n       or c2 is null\r\n    ;\r\n\r\nTo my great surprise, the execution plan that was generated for this query, was [this][1]. (Sorry for the external link, it's too large to fit here). \r\n\r\n\r\nCan someone explain to me what's up with all these "[Constant Scans][2]" and "[Compute Scalars][3]"? What's happening?\r\n\r\n![Plan][4]\r\n\r\n      |--Nested Loops(Inner Join, OUTER REFERENCES:([Expr1010], [Expr1011], [Expr1012]))\r\n           |--Merge Interval\r\n           |    |--Sort(TOP 2, ORDER BY:([Expr1013] DESC, [Expr1014] ASC, [Expr1010] ASC, [Expr1015] DESC))\r\n           |         |--Compute Scalar(DEFINE:([Expr1013]=((4)&[Expr1012]) = (4) AND NULL = [Expr1010], [Expr1014]=(4)&[Expr1012], [Expr1015]=(16)&[Expr1012]))\r\n           |              |--Concatenation\r\n           |                   |--Compute Scalar(DEFINE:([Expr1005]=NULL, [Expr1006]=NULL, [Expr1004]=(60)))\r\n           |                   |    |--Constant Scan\r\n           |                   |--Compute Scalar(DEFINE:([Expr1008]=NULL, [Expr1009]=(1048576), [Expr1007]=(10)))\r\n           |                        |--Constant Scan\r\n           |--Index Seek(OBJECT:([t].[i]), SEEK:([t].[c2] > [Expr1010] AND [t].[c2] < [Expr1011]) ORDERED FORWARD)\r\n\r\n  [1]: https://docs.google.com/spreadsheet/pub?key=0At0rnZg7VspEdFpuc29qWmYwbnlLdVdsZkVRamxJQ0E&single=true&gid=0&output=html\r\n  [2]: http://msdn.microsoft.com/en-us/library/ms188318.aspx\r\n  [3]: http://msdn.microsoft.com/en-us/library/ms178082.aspx\r\n  [4]: https://i.stack.imgur.com/QTNL2.jpg	448	2019-12-13 09:53:53.291545+00	0	4	1	512	1051	2266	14789	0	0	0	2019-12-13 09:53:53.291545+00	f	f	1	1
1	702	599	2020-01-23 05:28:56.739934+00	Comments on own posts?	It looks like the wee `comment` link that appears at the bottom of any "post" entry (question *or* answer) does **not** get generated for "own" posts. That is, you cannot attach a comment to your own questions and answers.\r\n\r\nMy assumption is that this is "by design". However, on SE I would from time to time want to add a brief comment about one of my contributions, and miss the option of being able to do this on TA — especially with the (welcome!) way that TA handles comment/chat along side Q&As.\r\n\r\nI would be interested to know what others think about self commentary, and whether it would be desirable to implement the `comment` link for "own posts".	613	2020-01-23 05:28:56.739934+00	5	1	1	674	1632	3294	\N	0	0	0	\N	f	f	2	\N
2	669	469	2019-12-20 15:08:53+00	ALTER fixed-length COLUMN on compressed table	I wonder if compression of a table actually changes `ALTER TABLE` rules. I found no information on this topic. Let's consider a simple example. I'll create a table and alter one of the columns from `INT` to `BIGINT`.\r\n\r\n```sql\r\nCREATE TABLE dbo.test1\r\n(\r\n  c1 int not null,\r\n  c2 int not null,\r\n  primary key (c1)\r\n) \r\n```\r\n\r\nPhysical row looks just like we can suggest, two fields, 4 bytes each.\r\n\r\n```sql\r\nSELECT pc.leaf_offset, pc.max_length\r\nFROM sys.system_internals_partitions p\r\n  join sys.system_internals_partition_columns pc \r\n    on p.partition_id = pc.partition_id\r\nWHERE p.object_id = object_id('dbo.test1')\r\n```\r\n\r\n```none\r\nleaf_offset\tmax_length\r\n4\t        4\r\n8\t        4\r\n```\r\n\r\nLet's change c2 from `INT` to `BIGINT`\r\n\r\n```sql\r\nALTER TABLE dbo.test1 ALTER COLUMN c2 bigint not null\r\n```\r\n\r\nWe increased the length of a fixed-length column so SQL Server adds one more column at the end of the row. The same query shows \r\n\r\n```none\r\nleaf_offset\tmax_length\r\n4\t        4\r\n8\t        4\r\n12\t        8\r\n```\r\n\r\nSo far so good, everything works just like books and numerous articles describe. Let's try to do the same thing with a compressed table.\r\n\r\n```sql\r\nCREATE TABLE dbo.test2\r\n(\r\n  c1 int not null,\r\n  c2 int not null,\r\n  primary key (c1)\r\n) with (data_compression = page)\r\n```\r\n\r\nThe physical structure looks like this:\r\n\r\n```none\r\nleaf_offset\tmax_length\r\n-1\t        4\r\n-2\t        4\r\n```\r\n\r\nAll 'offsets' are negative. Only variable-length columns suppose to have negative offsets. It surprised me at first, I never seen this documented. It makes sense though, the compressed page can't have fixed offsets from the beginning of the row. Now let's try to alter the column.\r\n\r\n```sql\r\nALTER TABLE dbo.test2 ALTER COLUMN c2 bigint not null\r\n```\r\n\r\nThe second surprise, the 'new' column has not been added.\r\n\r\n```none\r\nleaf_offset\tmax_length\r\n-1\t        4\r\n-2\t        8\r\n```\r\n\r\nOne more surprise, it looks like on SQL Server 2017 this operation is 'metadata-only', still about to confirm it. SQL Server 2014 handles it in the old way.\r\n\r\nMy questions\r\n\r\n 1. Do we have any document available on how table compression affects `ALTER TABLE`? Who can shed some light on it?\r\n 2. Does SQL Server 2017 improve this? \r\n\r\nI have read [this sample chapter](https://www.microsoftpressstore.com/articles/article.aspx?p=2225060&seqNum=4) from Microsoft SQL Server 2012  Internals. It covers the column descriptor row format for compressed data, but covers only physical structures involved and doesn't touch schema changes at all.	480	2019-12-21 09:07:43.142055+00	0	4	1	544	1755	2286	256046	0	0	0	2019-12-21 09:07:43.142055+00	f	f	1	1
1	1	29	2019-11-13 14:13:43.959672+00	The (not very) Many Memes of Meta	One meme per answer please. 	36	2019-12-02 15:33:36.606619+00	6	1	1	29	536	3256	\N	0	0	0	\N	f	f	2	\N
1	17	428	2019-12-12 10:25:32.731573+00	Error details on the front end	As shown in https://topanswers.xyz/meta?q=389#question full error stacks seem to be passed to the front end if anything fails.\r\n\r\nAre we at risk of any of the following from this?\r\n\r\n1. Helping potential attacks gain further vectors\r\n2. Confusing the user\r\n3. Lack of "What am I meant to do?" from the perspective of the user?\r\n\r\nMy take on this is that we should try and pass "friendly" error messages where possible to mitigate the above effects.	439	2019-12-12 10:25:32.731573+00	6	4	1	503	1092	2141	\N	0	0	0	\N	f	f	2	\N
1	96	430	2019-12-12 14:26:32.086841+00	Votes on (confirmed) flagged posts should not count towards star count	Right now even stars against [flagged and "deleted" posts](https://topanswers.xyz/meta?q=418#question) count towards the user's total score. This has no particular effect right now because blanket priviledges are granted to everybody out of the gate (even drive-by spammers), but it will be evident why this is a problem as soon as [voting is an earned priviledge](https://topanswers.xyz/meta?q=418#question). In that scenario even people who post obvious junk will be enabled to continue abusing the system because the community flagging and cleaning up their mess won't automatically stop them from continuing.\r\n\r\nStar counts on flagged posts should be immediately revoked from the user's total. This will cause some consternation when flagging is abused, but conterflagged posts would restore that.	441	2019-12-12 14:26:32.086841+00	0	4	2	505	1037	1880	\N	0	0	0	\N	f	f	2	\N
1	96	431	2019-12-12 14:36:41.000079+00	The flag tool should not be accessable to zero-star users	The flag tool has been made visible to _all_ users, including drive-bys. Similar to the [issue with allowing votes from such accounts](https://topanswers.xyz/meta?q=429#question) but even more serious ... **the flag tool should be off limits until earned**. Sure, the user who flagged is shown (unlike for votes), but given the low barier for creating accounts it is very easy to abuse right now. Use another browser, sign-up for a temporary account, and (at least temporarily) you can delete anybodys' content from public sight.\r\n\r\nFlagging functionality should only be offered to people with _some_ track record of posititve participation.	442	2019-12-12 14:36:41.000079+00	1	4	2	506	1029	1893	\N	0	0	0	\N	f	f	2	\N
2	654	454	2014-12-11 18:06:05+00	Are actual stored procedures the only mechanism that implements temp table caching?	The meat of the question: are actual stored procedures the only mechanism that implements temp table caching or do system stored procedures such as `sp_executeSQL` / `sp_execute` also take advantage of them?\r\n\r\nI am not a DBA, so please use little words. Our application sends over prepared statements that, from the profiler, I see run all SQL through `sp_prepexec` which is a system procedure for both running `sp_prepare` and `sp_execute`. What I'm trying to do is figure out if I am benefiting from temp table caching. \r\n\r\nI've been using this guide with object_id() to examine behavior\r\n\r\nhttps://sqlkiwi.blogspot.com/2012/08/temporary-tables-in-stored-procedures.html\r\n\r\nThen point #3 on this blog post suggests that EXEC cannot use temp table caching, but leaves out whether sp_executeSQL can:\r\nhttp://blogs.msdn.com/b/turgays/archive/2013/09/18/exec-vs-sp-executesql.aspx\r\n\r\nIn my query sent over via the client I have created a simple temp table.\r\n\r\n    DECLARE @foo int; -- set by JDBC, unused but required to force a prepared statement\r\n\r\n    SELECT 1 AS id\r\n    \tINTO #tmp\r\n    \r\n    SELECT OBJECT_ID('tempdb..#tmp');\r\n\r\nIn profiler, I can see:\r\n\r\n    declare @p1 int\r\n    set @p1=NULL\r\n    exec sp_prepexec @p1 output,N'@P1 int',N'declare @foo INT = @P1 \r\n    \r\n    SELECT 1 as id\r\n    \tinto #tmp\r\n    \r\n    select Object_id(''tempdb..#tmp'');\r\n    DROP TABLE #tmp;',1\r\n    select @p1\r\n\r\nI also get a cachehit from this. However, the object_id of the temp table appears to be changing on me, which is not the behavior I would see if this temp table were created in a real stored procedure. However, when I run this same code through `sp_executeSQL`, I'm also seeing that the object_id of the temp table has changed. This leads me to believe that only "real" user created stored procedures take advantage of temp table caching.\r\n	465	2019-12-17 13:11:13.129996+00	0	4	1	529	1106	2169	85949	0	0	0	2019-12-17 10:06:05.721841+00	f	f	1	1
2	180	277	2017-11-27 18:31:43+00	Why would a SELECT query cause writes?	I've noticed that on a server running SQL Server 2016 SP1 CU6 sometimes an Extended Events session shows a SELECT query causing writes.\r\nFor example:\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\nThe execution plan shows no obvious cause for the writes, such as a hash table, spool, or sort that could spill to TempDB:\r\n\r\n[![enter image description here][2]][2]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/4E3Mf.png\r\n  [2]: https://i.stack.imgur.com/5ObPc.png\r\n\r\nVariable assignment to a MAX type or an automatic statistics update could also cause this, but neither was the cause of the writes in this case.\r\n\r\nWhat else could the writes be from?	285	2019-12-04 14:23:01.133435+00	0	4	1	352	605	1516	191825	0	0	0	2019-12-04 14:23:01.133435+00	f	f	1	1
2	198	304	2019-09-02 06:29:54+00	Why doesn't this recursive CTE with a parameter use an index when it does with a literal?	I am using a recursive CTE on a tree structure to list all the descendants of a particular node in the tree. If I write a literal node value in my `WHERE` clause, SQL Server seems to actually apply the CTE just to that value, giving a query plan with [low actual rows counts, et cetera](https://www.brentozar.com/pastetheplan/?id=HJ0ImE5Hr):\r\n\r\n[![query plan with literal value][1]][1]\r\n\r\n However, if I pass the value as a parameter, it seems to [realize (spool) the CTE and then filter it after the fact](https://www.brentozar.com/pastetheplan/?id=B1h7m4qBr):\r\n\r\n[![query plan with parameter value][2]][2]\r\n\r\nI could be reading the plans wrong. I haven’t noticed a performance issue, but I am worried that realization of the CTE could cause issues with larger data sets, especially in a busier system. Also, I normally compound this traversal on itself: I traverse up to ancestors and back down to descendants (to ensure that I gather all related nodes). Due to how my data is, each set of “related” nodes is rather small, so realization of the CTE doesn’t make sense. And when SQL Server seems to realize the CTE, it is giving me some quite large numbers in its “actual” counts.\r\n\r\nIs there a way to get the parameterized version of the query to act like the literal version? I want to put the CTE in a reusable view.\r\n\r\nQuery with literal:\r\n\r\n```sql\r\nCREATE PROCEDURE #c AS BEGIN;\r\n\tWITH descendants AS (SELECT\r\n\t\t t.ParentId Id\r\n\t\t,t.Id DescendantId\r\n\tFROM #tree t\r\n\tWHERE t.ParentId IS NOT NULL\r\n\tUNION ALL SELECT\r\n\t\t d.Id\r\n\t\t,t.Id DescendantId\r\n\tFROM descendants d\r\n\tJOIN #tree t ON d.DescendantId = t.ParentId)\r\n\tSELECT d.*\r\n\tFROM descendants d\r\n\tWHERE d.Id = 24\r\n\tORDER BY d.Id, d.DescendantId;\r\nEND;\r\nGO\r\nEXEC #c;\r\n```\r\n\r\nQuery with parameter:\r\n\r\n```sql\r\nCREATE PROCEDURE #c (@Id BIGINT) AS BEGIN;\r\n\tWITH descendants AS (SELECT\r\n\t\t t.ParentId Id\r\n\t\t,t.Id DescendantId\r\n\tFROM #tree t\r\n\tWHERE t.ParentId IS NOT NULL\r\n\tUNION ALL SELECT\r\n\t\t d.Id\r\n\t\t,t.Id DescendantId\r\n\tFROM descendants d\r\n\tJOIN #tree t ON d.DescendantId = t.ParentId)\r\n\tSELECT d.*\r\n\tFROM descendants d\r\n\tWHERE d.Id = @Id\r\n\tORDER BY d.Id, d.DescendantId;\r\nEND;\r\nGO\r\nEXEC #c 24;\r\n```\r\n\r\nSetup code:\r\n\r\n```sql\r\nDECLARE @count BIGINT = 100000;\r\nCREATE TABLE #tree (\r\n\t Id BIGINT NOT NULL PRIMARY KEY\r\n\t,ParentId BIGINT\r\n);\r\nCREATE INDEX tree_23lk4j23lk4j ON #tree (ParentId);\r\nWITH number AS (SELECT\r\n\t\t CAST(1 AS BIGINT) Value\r\n\tUNION ALL SELECT\r\n\t\t n.Value * 2 + 1\r\n\tFROM number n\r\n\tWHERE n.Value * 2 + 1 <= @count\r\n\tUNION ALL SELECT\r\n\t\t n.Value * 2\r\n\tFROM number n\r\n\tWHERE n.Value * 2 <= @count)\r\nINSERT #tree (Id, ParentId)\r\nSELECT n.Value, CASE WHEN n.Value % 3 = 0 THEN n.Value / 4 END\r\nFROM number n;\r\n```\r\n\r\n  [1]: https://i.stack.imgur.com/uHkeY.png\r\n  [2]: https://i.stack.imgur.com/04yqe.png	312	2019-12-05 02:42:36.217851+00	0	4	1	379	665	1272	246734	0	0	0	2019-12-05 02:41:28.805477+00	f	f	1	1
2	116	202	2017-10-03 14:47:30+00	How do you create a view with SNAPSHOT_MATERIALIZATION in SQL Server 2017?	SQL Server 2017 has a couple new stored procedures:\r\n\r\n - sp_refresh_single_snapshot_view – input param for @view_name nvarchar(261), @rgCode int\r\n - sp_refresh_snapshot_views – input param for @rgCode int\r\n\r\nAnd new entries in sys.messages:\r\n\r\n - 10149 – Index that has SNAPSHOT_MATERIALIZATION cannot be created on view ‘%.*ls’ because view definition contains memory-optimized table(s).\r\n - 10642 – SNAPSHOT_MATERIALIZATION cannot be set for index ‘%.*ls’ on ‘%.*ls’ because it is only applicable to indexes on views.\r\n - 10643 – SNAPSHOT_MATERIALIZATION cannot be set for ‘%.*ls’ on ‘%.*ls’ because it is only applicable to clustered indexes on views.\r\n - 10648 – SNAPSHOT_MATERIALIZATION cannot be set for partitioned index ‘%.*ls’ on ‘%.*ls’.\r\n - 10649 – Nonclustered index ‘%.*ls’ cannot be created on ‘%.*ls’ that has clustered index ‘%.*ls’ with SNAPSHOT_MATERIALIZATION.\r\n - 10650 – Refresh of snapshot view(s) requires snapshot isolation to be enabled on the database.\r\n - 3760 – Cannot drop index ‘%.*ls’ on view ‘%.*ls’ that has SNAPSHOT_MATERIALIZATION.\r\n - 4524 – Cannot alter view ‘%.*ls’ because it has snapshot materialization.\r\n - 4525 – Cannot use hint ‘%ls’ on view ‘%.*ls’ that has snapshot materialization before the view is refreshed.\r\n\r\nAnd new Extended Events:\r\n\r\n[![Snapshot view Extended Events][1]][1]\r\n\r\nSo how can we create a snapshot-materialized view? (Microsoft hasn't documented it yet, obviously.) Here's a [gist with things I've tried so far][2] that haven't worked.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/qxTlM.png\r\n  [2]: https://gist.github.com/BrentOzar/fa611ac88bda7151536a5ac5043baa34	209	2019-11-30 07:50:28.655389+00	0	4	1	277	423	1483	187533	0	0	0	2019-11-30 07:50:28.655389+00	f	f	1	1
1	2	598	2020-01-22 23:35:11.139846+00	How should we style flagged/hidden posts (for those that can see them)?	We have recently added a diagonal stripe effect to the background colour of flagged/hidden posts, mainly because the red tint wasn't very visible on the TeX community.\r\n\r\nThere has been some discussion in comments about whether we need to supplement this with some text that explicitly describes what the background colour means, is this necessary? Or do we need to do something else to indicate the state of the post, like add a symbol or something?\r\n\r\nHere is an example of how these posts currently look:\r\n\r\n![Screenshot 2020-01-22 at 23.30.33.png](/image?hash=31c863d8568141386de93bd0d739a0cd8255eb25766766859a6f184ac385d6ba)	612	2020-01-22 23:35:11.139846+00	5	1	1	673	1631	3381	\N	0	0	0	\N	f	f	2	\N
2	75	109	2015-04-02 16:46:18+00	How does SQL Server's optimizer estimate the number of rows in a joined table?	I am running this query in the [AdventureWorks2012][1] database: \r\n\r\n    SELECT \r\n    \ts.SalesOrderID,\r\n    \td.CarrierTrackingNumber,\r\n    \td.ProductID,\r\n    \td.OrderQty\r\n    FROM Sales.SalesOrderHeader s \r\n    JOIN Sales.SalesOrderDetail d \r\n    \tON s.SalesOrderID = d.SalesOrderID\r\n    WHERE s.CustomerID = 11077\r\n\r\nIf I look at the estimated execution plan, I see the following: \r\n\r\n![enter image description here][2]\r\n\r\nThe initial index seek (top right) is using the IX_SalesOrderHeader_CustomerID index and searching on the literal 11077. It has an estimate of 2.6192 rows. \r\n\r\n![enter image description here][3]\r\n\r\nIf I use `DBCC SHOW_STATISTICS ('Sales.SalesOrderHeader', 'IX_SalesOrderHeader_CustomerID') WITH HISTOGRAM`, it shows that the value 11077 is between the two sampled keys 11019 and 11091. \r\n\r\n![enter image description here][4]\r\n\r\nThe average number of distinct rows between 11019 and 11091 is 2.619718, or rounded to 2.61972 which is the value of estimated rows shown for the index seek.\r\n\r\nThe part I don't understand is the estimated number of rows for the clustered index seek against the SalesOrderDetail table. \r\n\r\n![enter image description here][5]\r\n\r\nIf I run `DBCC SHOW_STATISTICS ('Sales.SalesOrderDetail', 'PK_SalesOrderDetail_SalesOrderID_SalesOrderDetailID')`:\r\n\r\n![enter image description here][6]\r\n\r\nSo the density of the SalesOrderID (which I am joining on) is 3.178134E-05. That means that 1/3.178134E-05 (31465) equals the number of unique SalesOrderID values in the SalesOrderDetail table.\r\n\r\nIf there are 31465 unique SalesOrderID's in the SalesOrderDetail, then with an even distribution, the average number of rows per SalesOrderID is 121317 (total number of rows) divided by 31465. The average is 3.85561\r\n\r\nSo if the estimated number of rows to be loop through is 2.61972, and the average to be returned in 3.85561, the I would think the estimated number of rows would be 2.61972 * 3.85561 = 10.10062.\r\n\r\nBut the estimated number of rows is 11.4867.\r\n\r\nI think my understanding of the second estimate is incorrect and the differing numbers seems to indicate that. What am I missing? \r\n\r\n  [1]: https://msftdbprodsamples.codeplex.com/releases/view/93587\r\n  [2]: http://i.stack.imgur.com/sagIZ.jpg\r\n  [3]: http://i.stack.imgur.com/U0YHf.jpg\r\n  [4]: http://i.stack.imgur.com/PlnIE.jpg\r\n  [5]: http://i.stack.imgur.com/BWiRG.jpg\r\n  [6]: http://i.stack.imgur.com/YkX6J.jpg	116	2019-11-25 22:55:37.822689+00	0	4	1	184	268	1461	96913	0	0	0	2019-11-25 22:55:37.822689+00	f	f	1	1
1	12	18	2019-11-09 22:39:42.344595+00	What should the site content licence be?	What should the licence for content contributed here be?\r\n\r\nThe main options are:\r\n\r\n1. [![CC0][1]][3] [CC0 "No Rights Reserved"][3]\r\n2. [![CC BY][2]][4] [Attribution 4.0 International (CC BY 4.0)][4]\r\n3. [![CC BY-SA][5]][6] [Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)][6]\r\n\r\nOption 3 is the same as the Stack Exchange licence, the others are more permissive.\r\n\r\n---\r\n\r\nThis question is about what license this site redistributes your contributions under. Everyone is, of course, free to license their own content in whatever way(s) they choose.\r\n\r\nAll of the above options are permissive enough for the site — and the 'attribution' requirement of your CC-BY and CC-BY-SA grant would appear to be satisfied by showing your name on the post and in the edit histories.\r\n\r\n[1]: https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/CC0_button.svg/88px-CC0_button.svg.png\r\n[2]: https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/CC-BY_icon.svg/88px-CC-BY_icon.svg.png\r\n[3]: https://creativecommons.org/share-your-work/public-domain/cc0/\r\n[4]: https://creativecommons.org/licenses/by/4.0/\r\n[5]: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/CC-BY-SA_icon.svg/88px-CC-BY-SA_icon.svg.png\r\n[6]: https://creativecommons.org/licenses/by-sa/4.0/	25	2019-11-16 15:07:03.346111+00	4	4	1	24	437	504	\N	0	0	0	\N	f	f	2	\N
1	2	439	2019-12-13 17:18:24.502014+00	Consistent font sizes	We just made an attempt to be more consistent with things like font sizes across the site. You shouldn't notice any big difference but some font sizes might look slightly larger.\r\n\r\nWe also made several other style changes as we were going along. There are bound to be at least one or two regressions.\r\n\r\nPlease ping me in this room if something suddenly doesn't look right. 	450	2019-12-13 17:18:24.502014+00	4	1	1	514	1056	2024	\N	0	0	0	\N	f	f	3	\N
2	2	199	2015-07-28 07:27:52+00	Simultaneous calls to the same function: how are deadlocks occurring?	My function `new_customer` is called several times per second (but only once per session) by a web application. The very first thing it does is lock the `customer` table (to do an 'insert if not exists'&mdash;a simple variant of an `upsert`).\r\n\r\nMy understanding of [the docs](http://www.postgresql.org/docs/9.4/static/sql-lock.html) is that other calls to `new_customer` should simply queue until all previous calls have finished:\r\n\r\n> LOCK TABLE obtains a table-level lock, waiting if necessary for any conflicting locks to be released.\r\n\r\nWhy is it sometimes deadlocking instead?\r\n\r\ndefinition:\r\n\r\n    create function new_customer(secret bytea) returns integer language sql \r\n                    security definer set search_path = postgres,pg_temp as $$\r\n      lock customer in exclusive mode;\r\n      --\r\n      with w as ( insert into customer(customer_secret,customer_read_secret)\r\n                  select secret,decode(md5(encode(secret, 'hex')),'hex') \r\n                  where not exists(select * from customer where customer_secret=secret)\r\n                  returning customer_id )\r\n      insert into collection(customer_id) select customer_id from w;\r\n      --\r\n      select customer_id from customer where customer_secret=secret;\r\n    $$;\r\n\r\nerror from log:\r\n\r\n```none\r\n2015-07-28 08:02:58 BST DETAIL:  Process 12380 waits for ExclusiveLock on relation 16438 of database 12141; blocked by process 12379.\r\n        Process 12379 waits for ExclusiveLock on relation 16438 of database 12141; blocked by process 12380.\r\n        Process 12380: select new_customer(decode($1::text, 'hex'))\r\n        Process 12379: select new_customer(decode($1::text, 'hex'))\r\n2015-07-28 08:02:58 BST HINT:  See server log for query details.\r\n2015-07-28 08:02:58 BST CONTEXT:  SQL function "new_customer" statement 1\r\n2015-07-28 08:02:58 BST STATEMENT:  select new_customer(decode($1::text, 'hex'))\r\n```\r\n\r\nrelation:\r\n\r\n    postgres=# select relname from pg_class where oid=16438;\r\n    ┌──────────┐\r\n    │ relname  │\r\n    ├──────────┤\r\n    │ customer │\r\n    └──────────┘\r\n\r\n\r\n---\r\n### edit:\r\n\r\nI've managed to get a simple-ish reproducible test case. To me this looks like a bug due to some sort of race condition.\r\n\r\nschema:\r\n\r\n    create table test( id serial primary key, val text );\r\n    \r\n    create function f_test(v text) returns integer language sql security definer set search_path = postgres,pg_temp as $$\r\n      lock test in exclusive mode;\r\n      insert into test(val) select v where not exists(select * from test where val=v);\r\n      select id from test where val=v;\r\n    $$;\r\n\r\nbash script run simultaneously in two bash sessions:\r\n\r\n    for i in {1..1000}; do psql postgres postgres -c "select f_test('blah')"; done\r\n\r\nerror log (usually a handful of deadlocks over the 1000 calls):\r\n\r\n    2015-07-28 16:46:19 BST ERROR:  deadlock detected\r\n    2015-07-28 16:46:19 BST DETAIL:  Process 9394 waits for ExclusiveLock on relation 65605 of database 12141; blocked by process 9393.\r\n            Process 9393 waits for ExclusiveLock on relation 65605 of database 12141; blocked by process 9394.\r\n            Process 9394: select f_test('blah')\r\n            Process 9393: select f_test('blah')\r\n    2015-07-28 16:46:19 BST HINT:  See server log for query details.\r\n    2015-07-28 16:46:19 BST CONTEXT:  SQL function "f_test" statement 1\r\n    2015-07-28 16:46:19 BST STATEMENT:  select f_test('blah')\r\n\r\n---\r\n\r\n### edit 2:\r\n\r\n@ypercube [suggested a variant](http://chat.stackexchange.com/transcript/message/23065138#23065138) with the `lock table` outside the function:\r\n\r\n    for i in {1..1000}; do psql postgres postgres -c "begin; lock test in exclusive mode; select f_test('blah'); end"; done\r\n\r\ninterestingly this eliminates the deadlocks.	206	2019-11-29 17:45:48.598172+00	0	4	1	274	418	473	108290	0	0	0	2019-11-29 17:43:57.739426+00	f	f	1	1
2	684	484	2015-05-19 11:56:07+00	Why 0 is equal to empty string?	I need some help in finding why the following `T-SQL` statement returns `1` (true):\r\n\r\n    SELECT IIF( 0 = '', 1, 0)\r\n\r\nI guess someone has change an `ANSI` option like `SET ANSI_NULLS` or something else that is causing the behavior.\r\n\r\nMy issue is that I am joining some values and in the final row set I have values which are joined by `0` and `''` values, which is not correct.	496	2019-12-25 09:35:26.548311+00	0	4	1	559	1187	2485	101884	0	0	0	2019-12-25 09:35:26.548311+00	f	f	1	1
1	167	455	2019-12-17 12:40:47.901391+00	Make question/answer boxes eligible for browser spell checking	Would it be possible to make the input boxes for questions and answers eligible for browser spell checking?\r\n\r\nI'm not on good terms with spelling, therefore I find the spell checking of my browser very helpful to catch at least some of my typos. On TopAnswers this works great in the box for composing chat messages, but not in the editor for questions and answers. I don't know what the technical difference between the two boxes are, but would it be possible to change the question/answer editor in such a way that browsers could spell check it?\r\n\r\n\r\n\r\n\r\n	466	2019-12-17 12:40:47.901391+00	12	4	3	530	1182	2371	\N	0	0	0	\N	f	f	2	\N
2	63	150	2016-10-25 17:57:12+00	Cardinality Estimate for LIKE operator (Local Variables)	I was under the impression that when using the `LIKE` operator in all optimise for unknown scenarios both the legacy and new CEs use a 9% estimate (assuming that relevant statistics are available and the query optimiser doesn't have to resort to selectivity guesses).\r\n\r\nWhen executing the below query against the credit database I get different estimates under the different CEs.  Under the new CE I receive an estimate of 900 rows which I was expecting, under the legacy CE I receive an estimate of 241.416 and I can't figure out how this estimate is derived.  Is anyone able to shed any light?\r\n\r\n    -- New CE (Estimate = 900)\r\n    DECLARE @LastName VARCHAR(15) = 'BA%'\r\n    SELECT * FROM [Credit].[dbo].[member]\r\n    WHERE [lastname] LIKE @LastName;\r\n    \r\n    -- Forcing Legacy CE (Estimate = 241.416)\r\n    DECLARE @LastName VARCHAR(15) = 'BA%'\r\n    SELECT * FROM [Credit].[dbo].[member]\r\n    WHERE [lastname] LIKE @LastName\r\n    OPTION (\r\n    QUERYTRACEON 9481,\r\n    QUERYTRACEON 9292,\r\n    QUERYTRACEON 9204,\r\n    QUERYTRACEON 3604\r\n    );\r\n\r\nIn my scenario, I already have the credit database set to compatibility level 120, hence why in the second query I'm using trace flags to force the legacy CE and to also provide information on what statistics are used/considered by the query optimiser.  I can see the column statistics on 'lastname' are being used but I still can't work out how the estimate of 241.416 is derived.\r\n\r\nI couldn't find anything online other than [this Itzik Ben-Gan article][1], which states "When using the LIKE predicate in all optimize for unknown scenarios both the legacy and new CEs use a 9 percent estimate.". The information in that post would appear to be incorrect.\r\n\r\n  [1]: http://sqlmag.com/sql-server/sql-server-query-optimization-no-unknown-unknowns	157	2019-11-27 10:40:19.86722+00	0	4	1	225	331	1465	153295	0	0	0	2019-11-27 10:40:19.86722+00	f	f	1	1
2	175	274	2019-03-26 11:13:57+00	Hide Select Output from T-SQL	I'm trying to get query execution time, but I also want to hide the query output. I just want the elapsed time - no output.\r\n\r\n### Example\r\n\r\n    DECLARE @Start datetime\r\n    DECLARE @End datetime\r\n    \r\n    SELECT @StartTimeWA=GETDATE() \r\n    \r\n    SELECT \r\n    \t   [id] \r\n          ,[database_id]\r\n    \t  ,[proc_name]\r\n          ,[exec_t] from\r\n      [DB].[dbo].[STAT] \r\n    \r\n    SELECT @End=GETDATE()\r\n    \r\n    SELECT DATEDIFF(MS,@Start,@End) AS [Duration]\r\n\r\nAt the moment, I get query output *and* at the bottom my duration, which is the only thing I want as far as the output goes. I haven't been able to do this and wondering whether anyone else has come across similar problems? This is something I want to do in T-SQL not Management Studio or anything like that.\r\n\r\nI'm trying to monitor the time it takes to run the select statement and report back to a server. I have an external monitor server which will run that every minute and get the time back (duration it took) which I will use over time to trend / baseline. As the current query spits out the select results and my duration it skews it, and my monitor server gets confused. I just wanted the duration column. I will also be doing this for inserts, which will be straightforward as it won't need to perform a select.\r\n\r\nI'm trying to do this *purely in T-SQL*. I don't want to use DMVs as I want to get the time it takes (snapshot) when I run a query and check whether this changes over time when the server goes through the various levels of load as this will give me a good idea as to whether query execution time changes.	282	2019-12-04 14:18:11.836776+00	0	4	1	349	602	1509	233124	0	0	0	2019-12-04 14:18:11.836776+00	f	f	1	1
2	16	283	2018-09-26 16:42:29+00	Why doesn't SQL Server have any missing index requests in the DMVs or Query Plans?	I have a SQL Server database where the queries are pretty slow, and there's a lot of locking and blocking. \r\n\r\nWhen I look at the missing index DMVs and query plans, there aren't any suggestions. \r\n\r\nWhy is that?	291	2019-12-04 14:25:50.33905+00	0	4	1	358	612	1526	218641	0	0	0	2019-12-04 14:25:50.33905+00	f	f	1	1
1	262	380	2019-12-08 15:01:01.855511+00	"Paste picture" feature	In Stack Exchange, I insert images to posts like this:\r\n\r\n1. If the image is already saved in my computer: I just upload it normally.\r\n2. If the image is a screenshot or an online image: I copy the image to clipboard and paste (Ctrl + V in Windows) to the post.\r\n\r\nIn TopAnswers, (1) is already available. However (2) is missing. So I have to save images, even my screenshots, to a file named `auxiliary.png` in my pictures folder, and then upload the file. It is very time-consuming compared to the way I did in SE. Sometimes my computer goes laggy, it increased the time wasted even more.\r\n\r\nI don't know whether older browsers (IE, ...) support such a feature, but I hope the feature will be implemented soon :).	391	2019-12-08 15:01:01.855511+00	7	4	1	455	2085	3982	\N	0	0	0	\N	f	f	2	\N
2	661	463	2013-02-14 11:37:12+00	Why are non-digits LIKE [0-9]?	My server's default collation is `Latin1_General_CI_AS`, as determined by this query:\r\n\r\n    SELECT SERVERPROPERTY('Collation') AS Collation;\r\n\r\nI was surprised to discover that with this collation I can match non-digit characters in strings using the predicate `LIKE '[0-9]'`.\r\n\r\nWhy in the default collation does this happen? I can't think of a case where this would be useful. I know I can work around the behavior using a binary collation, but it seems like a strange way to implement the default collation.\r\n\r\n## Filtering digits produces non-digit caracters\r\n\r\nI can demonstrate the behavior by creating a column that contains all possible single-byte character values and filtering the values with the digit-matching predicate.\r\n\r\nThe following statement creates a temporary table with 256 rows, one for each code point in the current code page:\r\n\r\n    WITH P0(_) AS (SELECT 0 UNION ALL SELECT 0),\r\n    P1(_) AS (SELECT 0 FROM P0 AS L CROSS JOIN P0 AS R),\r\n    P2(_) AS (SELECT 0 FROM P1 AS L CROSS JOIN P1 AS R),\r\n    P3(_) AS (SELECT 0 FROM P2 AS L CROSS JOIN P2 AS R),\r\n    Tally(Number) AS (\r\n      SELECT -1 + ROW_NUMBER() OVER (ORDER BY (SELECT 0))\r\n      FROM P3\r\n    )\r\n    SELECT Number AS CodePoint, CHAR(Number) AS Symbol\r\n    INTO #CodePage\r\n    FROM Tally\r\n    WHERE Number >= 0 AND Number <= 255;\r\n\r\nEach row contains the integer value of the code point, and the character value of the code point. Not all of the character values are displayable - some of the code points are strictly control characters. Here is a selective sample of the output of `SELECT CodePoint, Symbol FROM #CodePage`:\r\n\r\n    0\t\r\n    1\t\r\n    2\t\r\n    ...\r\n    32\t \r\n    33\t!\r\n    34\t"\r\n    35\t#\r\n    ...\r\n    48\t0\r\n    49\t1\r\n    50\t2\r\n    ...\r\n    65\tA\r\n    66\tB\r\n    67\tC\r\n    ...\r\n    253\tý\r\n    254\tþ\r\n    255\tÿ\r\n\r\nI would expect to be able to filter on the Symbol column to find digit characters using a `LIKE` predicate and specifying the range of characters '0' thru '9':\r\n\r\n    SELECT CodePoint, Symbol\r\n    FROM #CodePage\r\n    WHERE Symbol LIKE '[0-9]';\r\n\r\nIt produces a surprising output:\r\n\r\n    CodePoint\tSymbol\r\n    48\t0\r\n    49\t1\r\n    50\t2\r\n    51\t3\r\n    52\t4\r\n    53\t5\r\n    54\t6\r\n    55\t7\r\n    56\t8\r\n    57\t9\r\n    178\t²\r\n    179\t³\r\n    185\t¹\r\n    188\t¼\r\n    189\t½\r\n    190\t¾\r\n\r\nThe set of code points 48 thru 57 are the ones I expect. What surprises me is that the symbols for superscripts and fractions are also included in the result set!\r\n\r\nThere might be a mathematical reason to think of exponents and fractions as numbers, but it seems wrong to call them digits.\r\n\r\n## Using binary collation as a workaround\r\n\r\nI understand that to get the result I expect, I can force the corresponding binary collation `Latin1_General_BIN`:\r\n\r\n    SELECT CodePoint, Symbol\r\n    FROM #CodePage\r\n    WHERE Symbol LIKE '[0-9]' COLLATE Latin1_General_BIN;\r\n\r\nThe result set includes only the code points 48 thru 57:\r\n\r\n    CodePoint\tSymbol\r\n    48\t0\r\n    49\t1\r\n    50\t2\r\n    51\t3\r\n    52\t4\r\n    53\t5\r\n    54\t6\r\n    55\t7\r\n    56\t8\r\n    57\t9	474	2019-12-18 07:45:44.753289+00	0	4	1	538	1121	2227	34730	0	0	0	2019-12-18 07:44:12.221668+00	f	f	1	1
1	8	73	2019-11-20 07:58:17.872391+00	What's the logic for being able to vote on a question	For some reason I can star some questions but not others.\r\n\r\nIt's not only questions I have posted an answer to myself because I can not vote on [this one](https://topanswers.xyz/meta?q=26).\r\n\r\nIt's also not only votes on meta because I can't vote on [this one](https://topanswers.xyz/databases?q=15) either. \r\n\r\nHowever I can star [this question](https://topanswers.xyz/databases?q=69)\r\n\r\nWhat's the logic behind this?	80	2019-11-20 07:58:17.872391+00	4	4	1	148	145	2389	\N	0	0	0	\N	f	f	2	\N
4	234	441	2019-12-14 03:59:03.084539+00	L3 basics	I am a bit mystified by all the `\\ExplSyntaxOn ... \\ExplSyntaxOff` codes that appear all over the place, and have a couple of questions related to this.\r\n\r\n1. Where can one get a basic introduction in the concepts, syntax and purpose?\r\n2. What is the ultimate purpose of this? If this is to make the usual syntax obsolete, why?\r\n3. How is the performance of this compared to more basic commands?\r\n4. Are some of the things settled by now? What I mean is are there some parts that did not change in the last couple of years, and are guaranteed not to change in the future. In other words, are there packages and/or methods that can be used for publications that go to the arXiv with the guarantee that this can get compiled there, and if someone downloads this in 10 years from now, it will then still run through on an up-to-date LaTeX installation?\r\n5. Are there any concerns that this development may eventually hurt LaTeX in the sense that users who use LaTeX but are not really into coding may no longer be able to fix their own codes, and that these users may switch to other typesetting systems?\r\n	452	2019-12-14 03:59:03.084539+00	0	4	3	516	1074	2097	\N	0	0	0	\N	f	f	1	\N
1	167	456	2019-12-17 13:03:28.982555+00	Please consider adding pagination to the question list	Did you consider to add pagiation to the list of questions (e.g. https://topanswers.xyz/meta)?\r\n\r\n- If searching for some specific post, using the `show more` button can be a bit cumbersome because it does not allow to skip to a specific location. For example if I remember that the post I'm looking for was one of the first posts, I have to press the `show more` button several times until I reach it\r\n\r\n- without pagination I feel kinda lost, because I have no idea how much more posts there are. I don't know if it is feasible to just skim all the titles and if I stop somewhere there is always this nagging feeling "what if there are only one or two more post, then you would have seem all instead of giving up mid way"	467	2019-12-17 13:03:28.982555+00	13	4	1	531	1170	2624	\N	0	0	0	\N	f	f	2	\N
1	168	495	2019-12-29 10:12:30.341383+00	Show who imported a post from SE	Since there is the possibility that new users who see many imported questions and answers might think that topanswers is just a copy of SE, I think it would be a good idea to also show *who* imported a question. That way visitors wouldn't only see that a question was imported, but also that it was imported by the one who asked it or answered it (of course provided that the one who imported the question is one of the users involved in asking or answering it).\r\n\r\nI think this would help topanswer to give a much better first impression to potential new users in that it is a viable source of new and good information, not only yet another web-archive.	507	2019-12-29 10:12:30.341383+00	1	6	3	570	1313	2630	\N	0	0	0	\N	f	f	2	\N
1	709	514	2020-01-04 02:11:31.54413+00	Feature Request: Landing Page	Greetings!\r\n\r\nI just discovered TopAnswers and I like where everything is going. Currently https://topanswers.xyz redirects to https://topanswers.xyz/meta.  I don't believe that this is the best first impression for new users.\r\n\r\nI think that some kind of landing page would be really nice to help explain the purpose of the site and help new users get familiar.  I understand that with the site being as young as it is, this would likely be a fast changing page, so keeping it succinct would be best initially.\r\n\r\nIs this too soon?  Is there already one and I just missed it?  What do you think?\r\n\r\n\r\n	527	2020-01-04 02:11:31.54413+00	9	4	1	589	1325	2664	\N	0	0	0	\N	f	f	2	\N
1	14	440	2019-12-13 18:25:49.907568+00	A request to not abuse the system 	I'd like to request that community members not intentionally abuse the Top Answers system, during what is essentially a public alpha / beta. \r\n\r\nI'm sure there are many seemingly valid reasons to do this, but I have so far found the practice to be both disruptive and de-motivating.\r\n\r\n- If there are bugs in the system, I'd recommend that we report those bugs with as much detail as necessary\r\n- If there are opportunities for abuse of the systems features, I'd recommend we explain how those features can be exploited and discuss solutions here on Meta\r\n- If there are security vulnerabilites, I'd recommend that we report those as well (either on Meta or privately to the development team, depending on the severity)\r\n\r\nI can see how demonstrating these issues, by exploiting them, may seem like it is more impactful.  But it is harmful to the communities we are trying to foster.\r\n\r\nThere is at least one post in the Databases community that has been used as a punching bag by posting an answer with harmful code and then artificially voting it up.  This really bummed me out personally.  I even tried to aid the situation by providing my two stars to the valid answer, only to see even more fake votes piled on to the harmful answer.\r\n\r\nCausing issues like this also creates an artificial sense of urgency to address certain changes or feature requests (because there are active problems related to them), rather than allowing development to be prioritized by what brings value to the community.\r\n\r\nThe development team here are fully volunteers, and this site is still in the very early stages of being developed.  I hope we can try to respect that by engaging in positive, constructive ways.	451	2019-12-13 18:25:49.907568+00	17	4	1	515	1057	2517	\N	0	0	0	\N	f	f	2	\N
1	709	529	2020-01-06 12:49:38.554192+00	User Activity	As this site grows, finding old posts will become infeasable.   I think that we need some way to track your activity so that you can "re-find" them.  A quick and easy solution would be something like a "User Activity" tab from your user profile.  When the tab is clicked it would reveal a list of all questions asked, answers, and comments.	542	2020-01-06 12:49:38.554192+00	7	4	1	604	1931	3709	\N	0	0	0	\N	f	f	2	\N
1	43	97	2019-11-24 12:00:08.31398+00	Can we use QR codes for linking accounts etc?	Then it should just be a matter of pointing your phone's camera at your laptop's screen to get mobile :-)	104	2019-11-24 12:00:08.31398+00	5	4	1	172	673	961	\N	0	0	0	\N	f	f	2	\N
1	167	536	2020-01-09 00:21:00.897468+00	Allow opening chat rooms in new tabs	The links in the list of chat rooms at the top right work well if opened in the same tab, but if I want to open them in a new tab^[If there are multiple rooms with new messages I would like to open them in new tabs where they can already load while I still read the current one. This way I don't have to wait for the pages to load and can just switch to them], e.g.\r\n\r\n![Screen Shot 2020-01-09 at 01.02.07.png](/image?hash=bab055c424cf0b0367579ed65f04402f07937a7d4d4f239bf2a0483435b87ef0)\r\n\r\nthen instead of the chat room the landing page of topanswers is opened.\r\n\r\nWould it be possible to enable these links also to be opened in a new tab? \r\n \r\n	549	2020-01-09 00:22:45.841548+00	9	4	1	611	1439	2930	\N	0	0	0	\N	f	f	2	\N
4	167	586	2020-01-20 15:53:48.979598+00	Remove padding before and after tikz lines	How to remove the padding before and after lines drawn in tikz? \r\n\r\nThe padding seems to depend on the `line width` (maybe `.5\\pgflinewidth`?). I already tried to modify all `sep` keys I could think of, but as you can see in the following picture, some padding remained at the start and end of lines. \r\n\r\n![Screen Shot 2020-01-20 at 16.40.42.png](/image?hash=e48c286a80d8ef575322dd8de0b96e8e921f96bcba13501dd3b0d679dfc4972b) \r\n\r\n(the red arrows mark the excess padding I would like to get rid of)\r\n\r\n```\r\n\\documentclass[tikz]{standalone}\r\n\r\n\\begin{document}\r\n\r\n\\begin{tikzpicture}[\r\n\t\tinner sep=0pt,\r\n\t\tinner xsep=0pt,\r\n\t\tinner ysep=0pt,\t\t\r\n\t\touter xsep=0pt,\r\n\t\touter ysep=0pt,\r\n\t\touter sep=0pt\t\r\n\t]\r\n\t\\draw[line width=5cm, green] (0,0) -- (\\paperwidth,0);\r\n\\end{tikzpicture}\r\n\r\n\\begin{tikzpicture}[\r\n\t\tinner sep=0pt,\r\n\t\tinner xsep=0pt,\r\n\t\tinner ysep=0pt,\t\t\r\n\t\touter xsep=0pt,\r\n\t\touter ysep=0pt,\r\n\t\touter sep=0pt\t\r\n\t]\r\n\t\\draw[line width=1cm, green] (0,0) -- (\\paperwidth,0);\r\n\\end{tikzpicture}\r\n\r\n\\end{document}\r\n```\r\n\r\n\r\n\r\n	600	2020-01-20 16:16:10.774202+00	0	4	3	661	1569	3158	\N	0	0	0	\N	f	f	1	\N
1	115	485	2019-12-26 04:09:58.720791+00	Is there a way to browse a list of tags?	I thought I'd see if I could try out a *non*-meta site here on TopAnswers, to get more of the "regular site" experience, so I did some looking around on Databases, the only site I have access to.  I'm not a DB expert by any means, but there are some niche areas where I might be able to answer questions, so I thought to browse tags to see if those areas are represented.  I don't see a way to do that; did I miss it, or is this something for the future?\r\n\r\n(I could also search, which I did and came up empty, which probably means my niche hasn't come up here yet.)\r\n	497	2019-12-26 04:09:58.720791+00	9	4	1	560	2222	4151	\N	0	0	0	\N	f	f	2	\N
1	709	579	2020-01-16 20:51:55.25803+00	Private Beta Access	Out of curiousity, I dropped [https://topanswers.xyz/tex](/tex) in my browser to see what's happening on the private beta over there.  Since I hadn't requested access, I unsurprisingly didn't get in.  \r\n\r\nThe thing that DID surprise me however, was that I was presented with the beautiful error string below.\r\n\r\n```\r\nERROR: invalid room CONTEXT: PL/pgSQL function _error(integer,text) line 1 at RAISE SQL function "login_room" statement 3 SQL function "login_community" statement 1 \r\n```\r\n\r\nPerhaps we could have something a bit nicer than an error string?\r\n\r\n**Here are a couple of ideas, ordered from "most reasonable to least reasonable":**\r\n- present the user with a "request access" page if they are logged in\r\n- point the user to a question on Meta where they can request access in chat\r\n- just present a static webpage saying that the page is not yet out of beta\r\n- leave it as is, people likely won't try to access beta sites, and it isn't a high priority	593	2020-01-16 20:53:57.872072+00	10	4	1	654	1526	3144	\N	0	0	0	\N	f	f	2	\N
1	96	182	2019-11-28 11:48:57.821647+00	Who will moderate and what tools will they have access to?	Who will moderate, and under what principles shall they do so?\r\n\r\nHow will that set of 'who' change over time? Appointments? Elections? Some privilege level mechanism?\r\n\r\nWhat tools will be made available for moderation purposes?	189	2019-12-01 16:53:33.078619+00	3	4	1	257	1177	3214	\N	0	0	0	\N	f	f	2	\N
4	233	494	2019-12-29 10:01:06.396634+00	How can I have the tick under the y axis equal to the tick of a label?	As you can see, the VaR tick is longer than the one under the y axis.\r\n\r\nHow can I have them equals without manually drawing the tick under the y axis? \r\n\r\nP.S. = The skewed Normal distribution is from https://tex.stackexchange.com/a/461839/101651.\r\n\r\n~~~\r\n\\documentclass[11pt]{standalone}\r\n% tikz\r\n\\usepackage{tikz}\r\n\\usepackage{pgfplots}\r\n\\pgfplotsset{compat=newest}\r\n\\usetikzlibrary{arrows.meta}\r\n\\usetikzlibrary{positioning}\r\n\\usetikzlibrary{backgrounds}\r\n\t\\pgfplotsset{\r\n\t\tassi/.style={\r\n\t\t\taxis lines=middle,\r\n\t\t\taxis line style={-Stealth},\r\n\t\t\tticks=none,\r\n\t\t\tsamples=300,\r\n\t\t\tclip=false,\r\n\t\t\t},\r\n\t}\r\n\\usetikzlibrary{math}\r\n\\tikzmath{%\r\n  function h1(\\x, \\lx) { return (9*\\lx + 3*((\\lx)^2) + ((\\lx)^3)/3 + 9); };\r\n  function h2(\\x, \\lx) { return (3*\\lx - ((\\lx)^3)/3 + 4); };\r\n  function h3(\\x, \\lx) { return (9*\\lx - 3*((\\lx)^2) + ((\\lx)^3)/3 + 7); };\r\n  function skewnorm(\\x, \\l) {\r\n    \\x = (\\l < 0) ? -\\x : \\x;\r\n    \\l = abs(\\l);\r\n    \\e = exp(-(\\x^2)/2);\r\n    return (\\l == 0) ? 1 / sqrt(2 * pi) * \\e: (\r\n      (\\x < -3/\\l) ? 0 : (\r\n      (\\x < -1/\\l) ? \\e / (8 * sqrt(2 * pi)) * h1(\\x, \\x*\\l) : (\r\n      (\\x <  1/\\l) ? \\e / (4 * sqrt(2 * pi)) * h2(\\x, \\x*\\l) : (\r\n      (\\x <  3/\\l) ? \\e / (8 * sqrt(2 * pi)) * h3(\\x, \\x*\\l) : (\r\n      sqrt(2/pi) * \\e)))));\r\n  };\r\n}\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\begin{axis}[\r\n\tassi,\r\n\tenlarge x limits={value=.04,upper},\r\n\tenlarge y limits={value=.02,lower},\r\n\tymax=.73,\r\n\txscale=1.2,\r\n\txlabel = {Loss},\r\n\tx label style={at=(current axis.right of origin),\r\n\t\tanchor=west},\r\n\tylabel = {Probability\\\\ density},\r\n\ty label style={%at=(current axis.above origin),\r\n\t\tanchor=north east, align=center, %inner sep=0pt\r\n\t\t},\r\n\t]\r\n\t\\pgfmathsetmacro{\\limiteinf}{-1.9}\r\n\t\\pgfmathsetmacro{\\limitesup}{2.8}\r\n\t\\pgfmathsetmacro{\\VaRx}{1.6}\r\n\t\\pgfmathsetmacro{\\VaR}{skewnorm(\\VaRx+.6,2)}\r\n\t\\begin{scope}[on background layer]\r\n\t\t\\addplot[fill=gray!30, draw=none, domain=\\VaRx:\\limitesup] \r\n\t\t\t{skewnorm((x+.6), 2)} \\closedcycle;\r\n\t\\end{scope}\r\n\t\\addplot[very thick, domain=\\limiteinf:\\limitesup] {skewnorm((x+.6), 2)};\r\n\t\\draw (axis cs:\\VaRx,0) -- (axis cs:\\VaRx,\\VaR);\r\n\t\\draw (axis cs:\\VaRx,0) -- (axis cs:\\VaRx,-.02) \r\n\t\tnode[below=4pt, inner sep=0pt] {$\\mathrm{VaR}_\\alpha$};\r\n\t\\node (unomenoalfa) at (axis cs:2.3,0.12) {$1-\\alpha$}; \r\n\t\\draw[-Stealth] (unomenoalfa) -- (axis cs:1.7,0.015);\r\n\\end{axis}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n~~~\r\n	506	2019-12-29 13:46:48.098815+00	0	4	1	569	1231	2493	\N	0	0	0	\N	f	f	1	\N
2	16	76	2019-11-21 17:07:59.982285+00	How Do Partial Aggregates Calculate Their Memory Grants?	From [Partial Aggregation](https://blogs.msdn.microsoft.com/craigfr/2008/01/18/partial-aggregation/):\r\n\r\n> First, a partial hash aggregate requests only a fixed minimal memory grant as it presumes that it will be computing a relatively small number of groups.\r\n\r\n - Is the memory grant _always_ the same? i.e. is it always 1024kb (making a number up, here).\r\n\r\n - Is the memory grant derived from the query's total memory grant?\r\n\r\n - Is the memory grant a runtime decision based on how much memory is left over from other memory consuming operators?\r\n\r\nI've noticed that, in repeated runs of the same query with no changes made, where it receives the same memory grant, different numbers of rows will emit from the Partial Aggregate, because:\r\n\r\n> Second, a partial hash aggregate never spills rows to tempdb.  If a partial hash aggregate runs out of memory, it simply stops aggregating and begins returning non-aggregated rows.\r\n\r\nWhich in turn can have effects on other operators, where spills may be larger because fewer aggregated rows were emitted from the Partial Aggregate.\r\n\r\nIn this example, equal rows leave the Hash Match, but the Partial Aggregate emits slightly more rows in the top example. In the Global Aggregate all the way to the left -- the Hash Match -- both spill.\r\n\r\n[![NUTS][1]][1]\r\n     \r\nBut they spill slightly different amounts. Not a significant difference, but enough to make me wonder.\r\n\r\n[![NUTS][2]][2]\r\n\r\nI'd spent some time using the MAX_GRANT_PERCENT hint to limit the amount of memory that the query gets overall, but that didn't seem to have a reliable impact on the Partial Aggregate.\r\n\r\nThe query looks like this:\r\n\r\n```\r\nSELECT TOP 1000\r\n\t   u.DisplayName,\r\n\t   MAX(u.Reputation) AS Reputation,\r\n\t   SUM(CASE WHEN p.PostTypeId = 1 THEN p.Score END ) AS QuestionsScore,\r\n\t   SUM(CASE WHEN p.PostTypeId = 2 THEN p.Score END ) AS AnswersScore \r\nFROM dbo.Users AS u\r\nJOIN dbo.Posts AS p\r\nON p.OwnerUserId = u.Id\r\nWHERE u.DisplayName LIKE 'user[1-4]%'\r\nAND EXISTS (SELECT 1 FROM dbo.Badges AS b WHERE b.UserId = p.OwnerUserId)\r\nGROUP BY u.DisplayName\r\nORDER BY Reputation DESC;\r\n```\r\n\r\n[1]: https://i.imgur.com/OTj5e2X.png\r\n[2]: https://i.imgur.com/jWG3SUr.jpg	83	2019-11-21 21:27:57.178315+00	0	4	1	151	147	150	\N	0	0	0	\N	f	f	1	\N
2	37	539	2020-01-09 18:23:53.930716+00	Is access to certain rooms restricted?	As in the title, is there a way to request access to rooms such as the "Stop the Merry Go Round" room?	552	2020-01-09 18:23:53.930716+00	4	4	1	614	1371	2790	\N	0	0	0	\N	f	f	2	\N
1	96	292	2019-12-04 14:52:15.73392+00	Add a way to hide or disable chat markdown previews	The preview window in chat is very distracating. For short messages there are two places things are changing as I type and my eye gets confused which to focus on. For  longer messages the height of everything jumping around just adds more confusion.\r\n\r\nI think previews make sense for posts where there is a lot of visual separation and potentially a lot of formatting. For chat I don't really find it useful at all. Markdown formatting typically used in chat is minimal enough and simple enough to "shoot from the hip" anyway.\r\n\r\nI would like an option in my profile (or wherever) to completely disable the feature and not show the pane at all. Less clutter, less jittery movement.	300	2019-12-04 14:52:15.73392+00	2	4	2	367	637	1660	\N	0	0	0	\N	f	f	2	\N
4	234	540	2020-01-09 20:41:28.599859+00	Overlapping steps in beamer	I am wondering if one can have a simpler code that achieves what the following does:\r\n\r\n\r\n```\r\n\\documentclass{beamer}\r\n\\begin{document}\r\n\\begin{frame}[t]\r\n\\frametitle{Some extensive slide (overlap 1)}\r\n\\only<1-2>{Statement A\r\n\\[A=B\\]}\r\n\\pause\r\n\\only<2-3>{Statement B\r\n\\[B=C\\]}\r\n\\pause\r\n\\only<3-4>{Statement C\r\n\\[C=D\\]}\r\n\\pause\r\n\\only<4-5>{Statement D\r\n\\[D=E\\]}\r\n\\pause\r\n\\only<5-6>{Statement E\r\n\\[E=F\\]}\r\n\\pause\r\n\\only<6-7>{Statement F\r\n\\[F=G\\]}\r\n\\end{frame}\r\n\r\n\\begin{frame}[t]\r\n\\frametitle{Another extensive slide (overlap 2)}\r\n\\only<1-3>{Statement A\r\n\\[A=B\\]}\r\n\\pause\r\n\\only<2-4>{Statement B\r\n\\[B=C\\]}\r\n\\pause\r\n\\only<3-5>{Statement C\r\n\\[C=D\\]}\r\n\\pause\r\n\\only<4-6>{Statement D\r\n\\[D=E\\]}\r\n\\pause\r\n\\only<5-7>{Statement E\r\n\\[E=F\\]}\r\n\\pause\r\n\\only<6-8>{Statement F\r\n\\[F=G\\]}\r\n\r\n\\end{frame}\r\n\r\n\\end{document}\r\n\r\n```\r\n\r\n![ani.gif](/image?hash=67ad62b30bb8358ca3d57acbe995deed54f702efd3a60c8ea2ba5462f20c0b3f)\r\n\r\nThat is the frame should keep each item (or however you want to call it) only for n steps (denoted overlap in the animation). In a way this looks like scrolling the slide (while keeping the title fixed). I am aware of tricks like `\\only<.(1)>`, which @samcarter shared with me in the chat but I could not use them here in a meaningful way. Ideally I just would have to add a command like `\\PauseOverlay` between the items to achieve the effect.\r\n	553	2020-01-09 20:41:28.599859+00	0	4	3	615	1384	2805	\N	0	0	0	\N	f	f	1	\N
1	17	74	2019-11-20 09:12:45.023282+00	How are questions to be closed?	How are questions to be closed?\r\n\r\nFlagging seems mandatory to me - it's important we can flag abusive or otherwise damaging content.\r\n\r\nWhat about content that simply isn't suitable, is lacking in thought or detail or for some other reason will not contribute to the overall goal of achieving a good Q&A resource?\r\n\r\nPlease post answers with your suggestions.	81	2019-11-20 11:36:06.898424+00	2	4	1	149	146	2626	\N	0	0	0	\N	f	f	2	\N
4	167	600	2020-01-23 17:26:49.067331+00	Automatically define missing hex colours for tikz	I occasionally convert svg images to tikz with the help of [svg2tikz](https://github.com/kjellmf/svg2tikz). The resulting tikz code uses hex colour names in the format of `crrggbb` in which `r`, `g` and `b` are hexadecimal figures, e.g. `cffa500`. \r\n\r\nTo use this code I normally either have to add suitable definitions in my document, e.g. `\\definecolour{cffa500}{HTML}{ffa500}` or I have to replace all the colours by defined colours. Both is a bit laborious if I just want to have a quick look at the image.\r\n\r\nThat's why I am wondering if one might hack xcolor so that instead of throwing an undefined colour error, it tests if the missing colour starts with `c` and if yes defines this missing colour on the fly. (This will cause strange problems if there are other undefined colours starting with `c`, but I'm willing to take this risk)\r\n\r\nThe following code seems to work for commands like `\\color` and `\\textcolor`, but if I try to use it with tikz, it fails with `! Package xcolor Error: Undefined color cffa500.`. \r\n\r\n\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\usepackage{xcolor}\r\n\\usepackage{xstring}\r\n\\usepackage{xpatch}\r\n\\usepackage{tikz}\r\n\r\n\\makeatletter\r\n\\xpatchcmd{\\XC@split@viii}{\\c@lor@error{`\\@@nam'}\\def\\@@nam{black}}{%\r\n\t\\def\\colorprefix{c}%\r\n\t\\StrSplit{#1}{1}{\\firstletter}{\\restletter}%\r\n\t\\if\\firstletter\\colorprefix\r\n\t\t\\definecolor{#1}{HTML}{\\restletter}%\r\n\t\t\\def\\@@nam{#1}%\r\n\t\\else\r\n\t\t\\c@lor@error{`\\@@nam'}\\def\\@@nam{black}%\r\n\t\\fi  \r\n}{\\typeout{patching xcolor sucess}}{\\typeout{patching xcolor failure}}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\r\n\\textcolor{cffa500}{\\rule{2cm}{2cm}}\r\n\\textcolor{c10a500}{\\rule{2cm}{2cm}}\r\n\r\n%\\tikz{\\path[fill=cffa500] (0,0) circle[radius=2];}\r\n\r\n\\end{document}\r\n```\r\n\r\nP.S. in case someone is wondering why not simply define all possible hex colours? An input file with a complete list of them is about 600 MB and I have yet to find out how long it takes to input this list into a document because the job is still running ... either it takes a really long time which is impractical or it hangs indefinitely	614	2020-01-23 19:47:15.854531+00	0	4	3	675	1655	3768	\N	0	0	0	\N	f	f	1	\N
4	167	583	2020-01-17 10:02:38.05491+00	Compile two versions of a document from the same latex source	I am making an exercise sheet and need two versions, one for the students with only the questions and one for me with both the questions and the solutions. \r\n\r\nHow can I automatically compile two versions of the same document without commenting/uncommenting things and renaming pdfs between the compilations?\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\usepackage[\r\n\tnoanswer % comment or uncomment here\r\n]{exercise}\r\n\r\n\\begin{document}\r\n\r\n    \\begin{Exercise}[title={Title},label=ex1]\r\n\r\n        question text\r\n\r\n    \\end{Exercise}\r\n\r\n    \\begin{Answer}[ref={ex1}]\r\n\r\n        solution\r\n\r\n    \\end{Answer}\r\n\r\n\\end{document}\r\n```	597	2020-01-17 10:02:38.05491+00	0	4	3	658	1537	3124	\N	0	0	0	\N	f	f	1	\N
4	96	344	2019-12-06 06:27:40.143161+00	How do we create new tags?	There is a notable lack of tags available here. None at all actually. Never mind TeX relevant ones, just none for anybody.\r\n\r\nHow do we go about adding relevant tags?	353	2019-12-06 06:27:40.143161+00	3	4	3	419	775	2751	\N	0	0	0	\N	f	f	2	\N
1	2	8	2019-11-05 13:56:02.164932+00	TopAnswers Code of Conduct	Be nice.	15	2019-11-05 13:56:02.164932+00	11	1	1	10	182	3449	\N	0	0	0	\N	f	f	3	\N
1	709	513	2020-01-04 02:03:15.723477+00	A proposal for less crowding in chat pane (particularly on long comments)	This is my first post on TA, and I must say that I'm quite impressed with what I've seen so far in the community. I have little knowledge of UI/UX and web development, please bear with me on this request.\r\n\r\nFrom what I can tell, currently the chat pane deals with long messages by limiting the messages to 9 lines long, then a scroll bar appears within the message box, and the user then has to move their cursor over the message to scroll through the message.\r\n\r\nI'm a fast reader and like to skim things very quickly, and this behavior hinders my ability to do so.  This requires me to 1. find my mouse 2. hover over the comment box 3. scroll not to slow and not too fast.\r\n\r\nScrolling in general is annoying to me personally because depending on where I am (home or work) I might be using Mac (touchpad), Windows (mouse), or a handful of different Linux distros (touchpad AND mouse), and I hate tinkering with things like scrolls speed / direction.  Maybe I make it hard on myself, but I digress.\r\n\r\nI'd like to propose a couple of alternative suggestions:\r\n\r\n1. **No maximum comment view**.  Comments can be arbitrary size without creating this "scroll panel within a scroll panel".  The logic is that users can quickly scroll past all but the most ridiculous posts.\r\n\r\n2. **Maximum comment view is set to approximately half page** and instead of the scroll panel, have some indication that the post has been truncated (such as three dots at the bottom of the post). When the user clicks on the post (or perhaps just the three dots), the post would expand to full size for the reader's viewing pleasure. \r\n\r\n3. **Large maximum comment size, but no limit on the view** - SE comments are limited to 600 characters long I think, perhaps something like that would provide a happy medium between the two above approaches, easy to implement and not too limiting on user experience.\r\n\r\n4. **Someone else here probably has better ideas**\r\n\r\n\r\nThoughts?  I don't know which solution is best. I also don't know how much of a priority UX/UI is at the moment either. This site is obviously quite young.  I'd love to see a better solution than the current one, though I love the direction that everything is going!	526	2020-01-04 02:03:15.723477+00	7	4	1	588	2152	2687	\N	0	0	0	\N	f	f	2	\N
4	262	575	2020-01-15 13:02:07.715442+00	How to have many TEXMF trees in my system	Currently I am managing my custom packages in `~/texmf`, and it is going very well.\r\n\r\nHowever, due to some reasons, I want to have many TEXMF trees in my system. For example, I want `/path/to/another/texmf` or `/path/to/another/different/texmf` to be treated by `kpsewhich` and TeX the same way `~/texmf` is being treated.\r\n\r\nIs it possible?\r\n\r\nI am using TeX Live 2019 in Windows 10 (but feel free to suggest Unix solutions :)).	589	2020-01-15 13:02:07.715442+00	0	4	1	650	1505	3068	\N	0	0	0	\N	f	f	1	\N
1	2	530	2020-01-07 15:43:00.023731+00	Can/should we start a Linux (or *nix) community on TopAnswers	### update 15 Jan: we've started a *nix private beta — please let us know if you'd like to participate!\r\n\r\n---\r\n\r\nTopAnswers is designed to host multiple communities, not just [databases](/databases) and TeX (currently in private beta).\r\n\r\nIf a group of people want to start a community here, they can post an answer on ["I'd like a new community on TopAnswers. How do I go about making that happen?"](/meta?q=211) requesting it, but we may also want to be proactive at this early stage, and actively seek out like-minded folk to start a few more early communities.\r\n\r\nI've been thinking for some time that some variation of [U&L on SE](https://unix.stackexchange.com) would be viable here. One or two people have made [comments like this](/transcript?room=528&id=12163#c12163):\r\n\r\n> In terms of SE equivalents, a Unix/Linux site would probably be most interesting to me initially…\r\n\r\nWe also have at least one high-rep (>50k) U&L user active here and interested in joining a private beta.\r\n\r\nLast but not least, the following may be common ground for some of those currently giving their time to U&L communities like SE:\r\n\r\n* [our principles](/meta?q=1) of being 'by the community, for the community', and not for profit\r\n* perhaps more than for any other group, being 'the product' does not appeal to U&L folk, and that will never happen here\r\n* our open source mindset, seen both in [releasing our own code](/meta?q=221#a580) and in our use of Linux/Apache/Postgres etc\r\n* our love for [KISS](https://en.wikipedia.org/wiki/KISS_principle) and 'do one thing and do it well' — two things Stack Exchange is leaving behind as it evolves\r\n\r\nIt might count against us with some that we chose to host on GitHub, but hey, we can still move to [Sourcehut](https://sourcehut.org/) when it comes out of beta ;)\r\n\r\nIf you are interested in joining a Linux or \\*nix community here, please join in the conversation in the comments. We need to work out questions about scope, target expertise and more.\r\n	543	2020-01-15 09:08:12.442789+00	9	1	1	605	1479	3231	\N	0	0	0	\N	f	f	2	\N
1	234	497	2019-12-29 15:09:27.81899+00	Can we have analogs of the "community wiki" option?	How about having means to make an answer a community wiki? That is, no one editing it, including the first one to write it, earns stars for their personal score. I feel that there are situations in which one individual user can only give a partial answer but the community may provide a (more or less) complete answer. For instance, if one asks about the list of deprecated LaTeX packages, there should be IMHO just one but complete answer rather than several which rise to the top depending on how often they got starred.\r\n\r\nSo wiki answers would have the benefit of having the answer better organized. To be more specific, consider the possible question\r\n\r\n> Which LaTeX packages are known to be mutually incompatible.\r\n\r\nIt is quite likely that not a single user can answer this, or will have the passion to do all the research to do that properly. In a wiki answer we could have *one* answer which sorts the packages (say alphabetically) and the information is much more accessible than having, say, 67 independent answers, which may also vary in their style. \r\n\r\nThere are many more examples in which the answers will be sort of encyclopedic, and the otherwise very useful way of ranking answers may not make too much sense.\r\n\r\nOne may say that the current settings do not prevent someone from editing an original answer to make it more complete. This is true, but I feel that users may not be as motivated doing that as for a wiki post, the more so since these answers will be conceivably getting a lot of stars, and those who are interested in competing for stars may just not be willing to spend time to make someone else get more stars.\r\n	509	2020-01-01 20:36:47.902017+00	3	4	3	572	1364	2781	\N	0	0	0	\N	f	f	2	\N
1	96	221	2019-12-01 06:46:00.407706+00	Where should the project source code be hosted	Because @Jack asked in chat and because evaluting the options and making a recomendation is going to take a bit of space...\r\n\r\nWhere should the project source be hosted? Github? Gitlab? Other?	229	2019-12-01 06:46:00.407706+00	3	4	2	296	1481	3416	\N	0	0	0	\N	f	f	2	\N
1	702	518	2020-01-04 10:59:33.032975+00	Models of collaborative online culture?	This isn't a question, and I'm far from sure that it materially contributes to the development of TA. But! :) This is a "community of collaborators", some of whom I "know" and trust....\r\n\r\nI just ran across [an article][1] (“Open Source is Broken”, by Don Goodman-Wilson, 9/17.10.2019) which questions deeply the morality of "open source" and explores the "what next?" question:\r\n\r\n> What does a people-centric collaborative software development model look like? I’d like to explore some basic properties that such a model ought to have, as a starting place for building new institutions to support productive, non-exploitative software development.\r\n\r\nThis felt to me like an important article, and I would value some help in thinking it through. I'm fairly confident that at least a few folks here will have some thoughts about it. If you're willing to share, it could be a constructive conversation, especially as new communities are coming into being here.\r\n\r\nP.s. I would have liked to tag this with `[tag:discussion]`, but that's not available and I couldn't create it. ;)\r\n\r\n\r\n[1]: https://dev.to/degoodmanwilson/open-source-is-broken-g60\r\n\r\n\r\n	531	2020-01-04 23:03:41.151619+00	7	1	1	593	1317	3645	\N	0	0	0	\N	f	f	2	\N
1	38	432	2019-12-12 14:57:32.79724+00	Custom question room avatars	I only just now figured out that the top right nav bar is... well... a nav bar. \r\n\r\nOf course now I've also noticed that it gets filled with "gravatar" style icons for question-rooms I'm participating in and I can fast-navigate. \r\n\r\nHere's the one for _this very question_:\r\n\r\n![roomicon.png](/image?hash=f23bad3ee56e94922e34e1f534235cd5550a8c18595fac53189628a49fa0a0f2)\r\n\r\nSadly though, only the "room"-rooms have friendly icons and the questions are auto-generated. Would it be pragmatic to add a "custom icon" option for questions to make them more distinctive in the nav bar? \r\n\r\nTo also head off the conversation around certain obvious compelling truths about why this might be a bad idea here's an ["LMGTFY" for free icon libs][1] (although I imagine the one with the best API would be the winner).\r\n\r\n[1]: https://www.google.com/search?q=free+thumbnail+icon+library:	443	2019-12-14 16:15:45.720357+00	4	4	1	507	1077	3055	\N	0	0	0	\N	f	f	2	\N
1	96	429	2019-12-12 12:38:00.585509+00	Allowing votes before any other activity makes votes meaningless 	I understand the desire to lower the barrier to participation so that people can jump right in, but there are some limits that serve a very pragmatic purpose.\r\n\r\n**Drive by voters water down the voting system.**\r\n\r\nProblem: Any activity that can be taken by any visitor to the site without leaving any community visible activity is a recipe for abuse of that activity. Most notably right now the vote tool can be used by first time visitors. Since this tool doesn't leave any crumbs that might tip off other site users that there was a bad actor, the resulting actions (in this case the star system) is rendered nearly useless.\r\n\r\nSolution: Require receiving at least 1 star to earn the privilege of giving stars. This will leave a trail of posts that the community can use to weed out bad actors. The resulting overall star scores across the site will be lower, but the actual number will be much more meaningful becasue it won't be diluted with votes from people who don't contribute.	440	2019-12-31 19:15:43.039926+00	3	4	2	504	1344	2711	\N	0	0	0	\N	f	f	2	\N
1	2	28	2019-11-13 13:07:53.805764+00	What open-source license do we want to release the TopAnswers platform source code under?	We have made a commitment that [as much as possible of the platform will be open-source and publically available](https://topanswers.xyz/meta?q=1). Before releasing the code I'd like to get feedback on what license(s) to release it under.\r\n\r\n---\r\n\r\nUpdate: we have settled on the [AGPL v3](https://github.com/topanswers/topanswers/blob/master/LICENSE). We are free to revisit this decision in the future if need. Worth noting that the [Codidact](https://codidact.org/) project (which has similar aims to TopAnswers but will probably end up looking very different) [has also chosen AGPL](https://forum.codidact.org/t/what-software-license-should-we-use/205/29?u=jackdouglas) which may be useful if we share any code.	35	2020-01-07 13:42:19.444017+00	4	1	1	22	1339	3638	\N	0	0	0	\N	f	f	2	\N
1	691	487	2019-12-27 15:21:59.560393+00	Without an e-mail addr, how do you send notifications that someone has responded to a question or comment?	I saw the commentary about not keeping too much personal information. But without any e-mail addresses, how do you send notifications of activity? Without that feature, I don't imagine any future where I would use this site. It pretty much limits the utility to only those who are power users who will be checking it every single day. That's not how I use SE, and it definitely is not how I use SE at work... when I need a Stack Overflow question answered, I post it, and then I go back to work. I do not want to have to click back regularly to poll for results. \r\n\r\nSo, question: What are the plans for sending notifications if you're not allowing us to submit e-mail addresses? 	499	2019-12-27 15:21:59.560393+00	4	4	1	562	2111	3242	\N	0	0	0	\N	f	f	2	\N
4	814	606	2020-01-24 14:27:50.80331+00	Minimal working example	I've just been asked to add an MWE, what is that?	621	2020-01-28 15:23:12.452799+00	8	4	1	681	1786	4124	\N	0	0	0	\N	f	f	2	\N
1	167	506	2020-01-02 11:47:22.083246+00	Highlight replied to message in chat instead of starboard	If one replies to a stared message, it is highlighted in the starboard but not in the chat window. Would it be possible to highlight the messages in the chat window instead? \r\n\r\nRight now it is a bit difficult to follow the chain of replies if there is a mixture of starred and non-starred messages.\r\n\r\n![Screen Shot 2020-01-02 at 12.39.08.png](/image?hash=451deb8e39d047e1413cbb3dbffd047ce0d8a62147cf86ea2b2c7c355008495c)	518	2020-01-02 11:47:22.083246+00	5	4	1	581	1264	2917	\N	0	0	0	\N	f	f	2	\N
4	96	339	2019-12-06 05:42:20.634583+00	Can I add a page break penalty that varries by context?	In a previous life I [asked a question](https://topanswers.xyz/tex?q=340) about page break penalties and weighting them differently depending on whether they landed on odd or even pages.\r\n\r\nThe basic answer to that using KOMA-Script classes (which I happened to be using) ran something like this:\r\n\r\n```tex\r\n\\ifthispageodd{\\nopagebreak[3]}{\\nopagebreak[4]}\r\n```\r\n\r\nThis worked okay, and it is something I use in producion every day. However it has one significant draw back: **it onnly works on the second pass**. This means it sometimes lets me down pretty hard because I have one project that does _many_ passes, basically fitting arbitrary content into a fixed page count by iterating over  the intput incrementally taking predefined steps to shrink things until it all fits.\r\n\r\nUnless I did three passes of every itteration in that process, I don't get accurate break weights.\r\n\r\nIs there any way to expidite this and know what page I'm on in the current rendering pass (not the previous one) and set a penalty based on that?	348	2019-12-06 05:44:22.783781+00	0	4	3	414	972	1664	\N	0	0	0	\N	f	f	1	\N
1	115	236	2019-12-01 19:36:41.459178+00	How, and at what granularity, can we import content?	(I know this is buried in chat somewhere, but rather than digging for it I think it'd be better to have a meta question.)\r\n\r\nMy existing SE community is interested in joining TopAnswers.  (I'll make a specific request on the question for that purpose.)  If we do that, how can we import content?  In particular:\r\n\r\n- If we import everything, then (a) *can* we and (b) how would imported content get wired up to owners who create accounts here later?\r\n\r\n- If we import only per-user, after the user is created here, does that include importing questions (asked by other people) so we can import a user's answers?\r\n\r\n- Is an import one-time or can it be updated?  For example, if a user continues to participate on SE and then wants to bring new or edited posts over (that were already brought over once), is that possible?  Or does importing create a fork and it's up to the user to choose where to participate after that?\r\n\r\n- How does import handle edit history and the attribution thereof?\r\n\r\n- I assume we're doing the right things with attribution.  What does that look like?\r\n\r\n- I don't have to attribute my *own* content to SE, right?  After all, it's my copyright and I can directly license it to TopAnswers.\r\n	244	2019-12-01 19:36:41.459178+00	5	4	1	311	1653	3834	\N	0	0	0	\N	f	f	2	\N
1	115	230	2019-12-01 18:12:38.363123+00	What privileges exist and how are they earned?	On Stack Exchange, various privileges are granted upon reaching certain reputation values.  On Codidact, we've been talking about having "privilege tracks": for example, a track record of accepted edits lets you edit without review, a track record of constructive comments lets you increase the number of comments you can leave, and so on.  (So having 10k rep but never having performed any moderation task wouldn't let you delete posts like it does on SE.)  I'm not advocating that, just explaining it for context.\r\n\r\nHow do (or will) privileges work on TopAnswers?  I've seen mention of one case in chat: the number of votes you get per day depends on your reputation.  What else?\r\n	238	2019-12-01 18:12:38.363123+00	1	4	1	305	484	595	\N	0	0	0	\N	f	f	2	\N
1	234	382	2019-12-08 19:39:28.179124+00	Should we install a refereeing system?	I am wondering if one should install a refereeing system. We will (presubably) have moderators, who will remove posts that are offensive. That is, they will make sure that how we say things is fine, but not so much what we say. But then we will have still posts that are polite (or at least not offensive), but wrong. \r\n\r\nTherefore we might think of having referees around who will mark (or even delete) posts that are just wrong. \r\n\r\nOr, if we want to avoid the controversies that may arise from deleting posts, we may think of having referees "validate" posts. This would be a bit similar to the refereeing system in academia (or at least the idea that underlies this system), where virtually everyone can post something on the arXiv, but it is a bit harder to get the paper published in a refereed journal.\r\n\r\nOf course, this would mean some extra work for the users here, but in my opinion it would pay off since this will improve the quality of answers quite a bit.	393	2019-12-08 19:39:40.144989+00	1	4	3	457	893	2238	\N	0	0	0	\N	f	f	2	\N
1	96	535	2020-01-08 08:29:50.474914+00	Add a user setting to prefer plain 'textarea' input method over CodeMirror	Don't get  me wrong. CodeMirror is awesome and I'm glad it is the default input editor. But it isn't for everybody. In particular I use [Firenvim](https://github.com/glacambre/firenvim) to automatically edit content fields across the web using in full-blown Neovim editor environment, complete with all my usual plugins. My setup includes Markdown and prose editing niceties that go far beyond even what is built in here.\r\n\r\nI would propose a user setting to be saved in my profile that says I prefer not to have the full CodeMirror editor but a simple textarea instead. This would allow be to use my own editor without a bunch of hacks trying to extricate CodeMirror from  the picture. I can and have actually gotten it to work, but it's kind of brittle.	548	2020-01-08 08:29:50.474914+00	0	4	2	610	1355	2727	\N	0	0	0	\N	f	f	2	\N
1	167	584	2020-01-17 10:53:39.074264+00	Return of the dead: unread chat counters are hunting me	I have a weird problem with the unread chat badges, I don't quite understand. Here a typical sequences of events:\r\n\r\n- the chat icon on the top right shows me that there are new messages in the tavern. \r\n\r\n- I click on it, go to the tavern and the message counter vanished. So far so go. \r\n\r\n- I continue browsing some questions for a bit and the counter stays away. \r\n\r\n- I leave\r\n\r\n- I come back 10 minutes later and the unread chat messages shows again the same number of unread messages as when I last visited the page, but upon clicking it there are no new messages in the chat room\r\n\r\nSome observations:\r\n\r\n- This seems not a one-time problem, I repeated this a couple of times. \r\n\r\n- This happened in two meta rooms I have in my chat list, it did not happen for rooms at toptex, which also had messages in the mean time. There the counter stayed away after visiting them once\r\n\r\n- When I post a message myself, this seems to permanently remove the "ghost notifications" for this chat room, not for the other rooms\r\n\r\n- like [@PeterVandivier](https://topanswers.xyz/transcript?room=1&id=14094&year=2020#c14094) I first thought this was my fault for messing with different tabs, but today I tried to do everything in only a single tab\r\n\r\nAny ideas what causes this or how to avoid?	598	2020-01-17 11:00:31.624541+00	5	4	1	659	1535	3867	\N	0	0	0	\N	f	f	2	\N
1	702	561	2020-01-12 16:11:31.753097+00	Please add real human-readable time/date information for posts	I'm not a fan of "3 days ago", "a month ago", i.e.  *relative* date/time information of the kind provided in the "meta" for posts in Q&A and chat (where there's also "an hour later", "7 days later", and so on). At least in chat there is a hover for precise date-time info, but not (yet) Q&A posts.\r\n\r\nIdeally, it would be great to have a tool-tip with human-readable date/time for those "relative" time-stamps. Hopefully this is a fairly trivial thing to add, and at least then if one did want (or, for some odd reason, need) to know the precise time of posting, that would be readily available.\r\n\r\n---\r\n\r\nP.s. I see in viewing the page source that the current date/time information has the format `data-seconds="4836210.182808"`. I'm not familiar with that format, and after searching the TA code I'm none the wiser. I see it is associated with `epoch`, but it doesn't seem to correlate to PHP's UNIX "epoch". So, as a bonus, if someone could kindly explain what that "data-seconds" number represents, I will have learned something. :)	574	2020-01-12 19:50:47.463739+00	14	1	1	636	1441	3058	\N	0	0	0	\N	f	f	2	\N
2	113	197	2012-03-30 20:04:17+00	Oracle Shutdown Method	Shutting down a database before doing an upgrade or a patch can be done several ways.\r\n\r\n    shutdown immediate;\r\n\r\nor \r\n\r\n    shutdown abort;\r\n    startup restrict;\r\n    shutdown immediate;\r\n\r\nor\r\n\r\n    shutdown abort;\r\n    startup restrict;\r\n    shutdown;\r\n\r\nor \r\n\r\n    alter system checkpoint;\r\n    shutdown abort;\r\n    startup restrict;\r\n    shutdown immediate;\r\n    \r\nOf course there are other options as well.  Which should be preferred and why?	204	2019-11-29 17:22:44.100576+00	0	4	1	272	414	489	15888	0	0	0	2019-11-29 17:22:44.100576+00	f	f	1	1
4	234	512	2020-01-03 21:37:59.10267+00	Proposal to have some kind of a form for non-meta questions	This question is to gauge the opinion of the community with regards to having the answer fields of non-meta question being initially nonempty. That is, the questioner may get to see the following contents:\r\n\r\n\r\n> <your question>  \r\n>   \r\n> Minimal code example:\r\n>\r\n>\r\n> ```\r\n> \\documentclass[<your necessary options>]{<your document class>}\r\n> <minimal preamble to define all macros in the code>\r\n> \\begin{document}\r\n> <code>\r\n> \\end{document}\r\n> ```\r\n>\r\n>\r\n> Compiler: `pdflatex`, `lualatex`, ... <indicate yours>\r\n\r\nThis may spare us with having to explain over and over that most of the questions require an MWE, and increase the threshold to just post an image, say, and asking for the Ti*k*Z code that produces this. In the few instances in which users ask plain TeX questions or about things like editors or related things, the user will have to erase a few lines, but this may be the lesser evil compared to zillions of questions in which users post a code fragment and expect others to guess the package in which a certain macro got defined. \r\n\r\nAn analogous form can be found on the [pgf github site](https://github.com/pgf-tikz/pgf/issues), (probably) for very similar reasons.\r\n\r\n	525	2020-01-04 17:20:48.800569+00	8	4	3	587	1309	2934	\N	0	0	0	\N	f	f	2	\N
2	795	592	2020-01-21 21:02:15.828275+00	Select from Final table performance implication	I have a question about this fiddle:\r\n\r\n<>https://dbfiddle.uk?rdbms=db2_11.1&fiddle=d5d2f86597ebdbda42e9b17a778b3e01\r\n\r\nWhat are the performance implications of using "select * from final table" over separate Update and Select statements if the table is large with (millions or hundreds of millions of records) and updates on individual records in the range of 100s of thousands or millions, assuming primary is and index are defined properly ? 	606	2020-01-21 21:02:15.828275+00	0	4	1	667	1588	3179	\N	0	0	0	\N	f	f	1	\N
4	234	502	2020-01-01 21:27:05.337722+00	Analogs of wiki posts	One may wonder whether one can have the analogs of wiki questions on this site. There are various arguments why this would be useful. For instance, if someone asks\r\n\r\n\r\n> Which LaTeX packages are known to be mutually incompatible.\r\n\r\n\r\nthen it will IMHO be advantageous to have on structure answer in which there is an alphabetically ordered, say, list rather than several partial answers. \r\n\r\nThe current state of affairs is summarized [here](https://topanswers.xyz/answer?id=554), and I am posting this also because [I was asked to do so in the comments](https://topanswers.xyz/transcript?room=509&id=11617#c11617) under [this question on the main meta site](https://topanswers.xyz/meta?q=497#question).\r\n	514	2020-01-04 17:56:46.462888+00	8	4	3	577	1311	2739	\N	0	0	0	\N	f	f	2	\N
1	96	183	2019-11-28 12:16:39.715704+00	A mechanism is needed to invalidate account recovery codes.	The account system here is a little bit wonky. I understand some of the technical benefits, but not working how people expect (how every other user account system works) will eventually have its own cost.\r\n\r\nIn the mean time at least one technical aspect should be dealt with asap. The short lifetime PIN for validating another sessions is okay if both devices are at hand, but often that is not the case. Hence the recovery code will get used a lot, and saved, and passed around between computers by who knows what mechanism. Mistakes happen. These will get compromised. If you don't believe me:\r\n\r\n    6a2271e8-2f4c-663b-43ac-2e245414fedd\r\n\r\nYes that's my actual account recovery code — before I fiddled with it and typed over some hex values.\r\n\r\nThe point is that "treating something like a password" doesn't mean it will stay private. Just a couple months ago a series of mistakes with my password manager and its autotype mechanism led me to post my real SE password into an SE chat session. Thankfully somebody pointed this out and I reset the password and all sessions in a hurry, but mistakes happen. Recovery codes should not be permanent any more than passwords. A mechanism to reset them is required.	190	2019-11-28 12:19:51.272369+00	6	4	1	258	388	1662	\N	0	0	0	\N	f	f	2	\N
1	167	505	2020-01-02 11:11:30.78189+00	Pin a chatroom permanantly to list of rooms	I would like to keep track of chat rooms like the Tavern or the main chat room of the tex site, but I don't always post messages in them, so they age away. \r\n\r\nWould it be possible to somehow pin them to my list of chat rooms at the top right?	517	2020-01-02 22:53:54.281011+00	8	4	1	580	1507	3901	\N	0	0	0	\N	f	f	2	\N
1	2	1	2019-11-03 20:12:20.736341+00	Why we are building TopAnswers	## The Problem with Stack Overflow\r\n\r\nA focused Q&A platform, that keeps the signal:noise ratio high, is a great benefit to people looking for answers on the internet. For a long time this need has been well served by [Stack Overflow](https://stackoverflow.com) and its sister sites, but:\r\n\r\n* Stack Overflow Inc is a for-profit company with shareholders, and won't ever be purely focused on the mission of building a library of knowledge on the internet. The profit motive is the powerful, and eventually it is going to push altruism into second place.\r\n\r\n* Because there is no obvious profit in free, simple, focused, Q&A, a profit-seeking company is going to explore other avenues for generating revenue (like [Teams](https://stackoverflow.com/teams), [Jobs](https://stackoverflow.com/jobs), etc). The Q&A then becomes the platform for promoting those other, profitable services. That is bound to mean a less pure focus on the Q&A itself.\r\n\r\n* There is a large and comitted community of volunteers working on the Stack Overflow network, answering questions, clearing up junk, and performing many other essential tasks. There are also employees of Stack Overflow. Relationships are generally good between the two groups, especially because the Community Managers (paid staff who liase with the community) are well respected and liked. However there is inevitably a discordance between the two groups because their priorities are inherently different.\r\n\r\n## How we think we can do better\r\n\r\nTopAnswers is intended to solve these issues for the long term. Here, the aim is not to operate for profit. We aren't operating a 'loss leader' strategy, where we take all the costs until the community is big, and then try and leverage that community to make a profit.\r\n\r\nInstead the aim is to **keep a pure focus on creating great content, as a community, for as long as possible.**\r\n\r\nTo that end, we promise that:\r\n\r\n1. We will not go down the for-profit route, and will apply for charitable status as soon as any income from donations exceeds the basic level needed to run a CIO in the UK as well as paying our hosting costs. This is not a high bar.\r\n\r\n1. We will not diversify into other profit-seeking areas and try to leverage the community here to promote them. If we branch out at all, those activities will:\r\n\r\n   * be compatible with focussed Q&A and building a library of knowledge for the good of everyone (for example, services like [db<>fiddle](https://dbfiddle.uk))\r\n\r\n   * also be not for profit\r\n\r\n1. If you join this community and contribute your time and energy towards the Q&A here, we will never treat you as 'the product'. We want a real community with genuinely shared priorities.\r\n\r\n1. As much as possible of the platform will always be open-source and publicly available, and most is currently [on GitHub](https://github.com/topanswers/topanswers) now.\r\n\r\n## Where to start\r\n\r\nSome of this will have to be taken on trust to start with. There are also some blanks that need filling in, for example:\r\n\r\n * ~~What license shall we use for contributions?~~ *We are allowing contributors [to choose their license](/meta?q=18#a8) from a range of options*.\r\n * ~~And for the platform itself?~~ *We have [have picked the AGPL v3](/meta?q=28#a116)*.\r\n * [What Q&A communities will we host](/meta?q=211) and how will we decide that?\r\n * What about all the millions of other details, including those that don't look important until you start to scale, when they suddenly blow up in your face?\r\n \r\nWe'd like to work together with you, to answer these question and others like them. This 'meta' community is where we'd like those discussions to take place, and we promise to always be responsive to, and to listen carefully to, the community here. No other channel, *even Twitter*, will ever take priority over this one.	7	2020-01-08 13:32:19.992398+00	21	1	1	7	1359	3782	\N	0	0	0	\N	f	f	3	\N
4	262	375	2019-12-08 05:27:29.098369+00	A way to type the LaTeX logo without Mathjax	I wholeheartedly agree with you that we should not use Mathjax on this site.\r\n\r\nHowever, I think it would be great if we are able to use the stylised LaTeX logo (and TeX and XeTeX ... as well). For example the title of [the LaTeX Wikibook](https://en.wikibooks.org/wiki/LaTeX) is a very nicely-stylised logo.\r\n\r\nOf course we won't need Mathjax for this. A quick Google search gives me [this](https://stackoverflow.com/q/8160514) or [this](http://nitens.org/taraborelli/texlogo), etc.	386	2019-12-08 05:27:29.098369+00	4	4	3	450	861	1263	\N	0	0	0	\N	f	f	2	\N
1	709	532	2020-01-08 04:28:03.60186+00	What is the difference between "blog post" and "question"	Previously I was under the impression that "Blog Posts" were just using the "Question" question with the intent to disseminate information rather than solicit information.  \r\n\r\nHowever, **there seem to be some differences**.\r\n\r\n**Differences Noticed:**\r\n1. Blog posts are not editable\r\n2. Blog posts are not answerable\r\n\r\nThe following "Blog Posts" are a couple of examples.\r\n\r\n**Blog Posts:**\r\n* [Why we are building TopAnswers](/meta?q=1)\r\n* [TopAnswer's Changelog](/meta?q=498)\r\n* [Additional license grant for original code](/meta?q=24)\r\n\r\n**Follow Up Questions:**\r\n\r\n1. Can I make one?  If so, how?\r\n2. Why not make them editable?  As content ages, user edits are an easy way to keep them up to date.  Currently this is not possible and [requesting in the comments is the only way](/transcript?room=7&id=12346#c12346), which seems a bit inefficient. Perhaps if you want to restrict edits more for Blog Posts, a different edit approval path than normal questions could be used?	545	2020-01-08 04:45:30.670345+00	6	4	1	607	1573	3160	\N	0	0	0	\N	f	f	2	\N
1	2	56	2019-11-17 10:51:50.177505+00	Should we allow everyone to select their own font?	dezso [asked](/meta?q=26):\r\n\r\n>could we please use a typeface that is a better fit for screens? Something sans serif would be very welcome.\r\n\r\nI'm thinking it's unlikely that everyone will be happy with any particular typeface we might choose, though of course some will be more generally popular than others.\r\n\r\nSo I had the idea that we could allow you to select your typeface from a predefined list. At first I instinctively thought it as a *very bad* idea, but I haven't actually managed to come up with any strong reason why we should not do it, can you think of any?\r\n\r\n---\r\n\r\nupdate: we've added a limited choice of normal and monospace fonts which can be selected on [your profile](/profile). It may not be immediately clear from there, but each community (ie 'databases' and 'meta' and errrr, 'private') already has it's own 'default' which new users get. ~~They just happen to currently be all the same: Quattrocento & DejaVu Sans Mono.~~ That ~~will change~~ has changed based on feedback on community-meta posts like [this one](/databases?q=69) for 'databases'.\r\n\r\n\r\nWe won't change your font once you have joined, even if the default changes — you'll have to change it yourself if you choose to do so — like the default license for contributions, changes apply to new users only.\r\n\r\nOf course if there is a particular reason why a font works best on a community, members of that community can (and we might be able to help) promote it there.	63	2020-01-09 23:09:49.633737+00	3	1	1	131	1370	2816	\N	0	0	0	\N	f	f	2	\N
1	38	567	2020-01-14 13:15:58.676355+00	Allow tagging of answers	Regarding ["YYYY-MM-DD for SQL Server"](https://topanswers.xyz/databases?q=30#a576) on the databases community, the "`status-completed`" and "`re-opened`" and subsequent "`fixed (again)`" states of the issue are tracked in markdown diff as a part of the post history (and bump the "line 1" description off the preview box) rather than in something more sensible (like a [chat timeline marker perhaps](https://topanswers.xyz/meta?q=427#question)). \r\n\r\nAnswers do not always conform to the same category(ies) as their prompting question(s). Additionally, the impetus to modify the tags of a base question to conform to the content of subsequent responses may flatten the call-and-response nature of the question-then-answer. Adding the ability to tag an Answer separately from the base Question obviates this potential.\r\n\r\n---\r\n\r\nSidebar: it might make sense to restrict answer tags to tags that are not already assigned to the parent question.	580	2020-01-14 13:15:58.676355+00	7	4	1	642	1462	3465	\N	0	0	0	\N	f	f	2	\N
1	96	384	2019-12-09 12:39:00.196286+00	SE site short links should be case insensitive.	The short link code tex.se seems to work but TeX.SE does not. Likewise dba.se works but dba.SE does not.\r\n\r\nThese short link codes should be completely case insensitive.	395	2019-12-09 19:20:56.7807+00	7	4	2	459	1574	3168	\N	0	0	0	\N	f	f	2	\N
2	12	207	2018-03-27 15:19:01+00	What is the StatementParameterizationType plan attribute?	I have noticed execution plans sometimes include a `StatementParameterizationType` attribute.\r\n\r\n[![Properties window][1]][1]\r\n\r\nWhat is this, what does it mean, and when does it appear?\r\n\r\n  [1]: https://i.stack.imgur.com/nQ7Xp.png	214	2019-11-30 12:54:32.708496+00	0	4	1	282	431	522	202424	0	0	0	2019-11-30 12:54:32.708496+00	f	f	1	1
1	691	511	2020-01-03 20:35:52.882087+00	Can we have a sanity-check window for question edits before answers are allowed? 	This is a problem very common on WorldBuilding SE: someone posts a question that can be interpreted in three different ways. Before senior community can rally to put the question on hold, other users start answering it. Now we have a question with answers. Sometimes the answers are consistent, so we can edit the question to match and then tell the person who asked the question, "If that wasn't what you meant, post the other interpretation as a new question." But sometimes the answers don't match, and then we have a mess. \r\n\r\nWe know that WorldBuilding is not a perfect match for the Q&A format website, but it is a good enough match, and has shown its value, if only for being essentially the only site within SE willing to answer hypothetical questions. \r\n\r\nI know the goal is to get answers fast, so no one likes things to stand in the way of answers being posted. On WB SE, I requested that questions from new users be automatically put "On Hold" until someone could look at them and clean them up. I got totally shot down for that suggestion -- with some comments that were definitely not in the spirit of encouraging discussion. \r\n\r\nIt has been years since I made that post, and since then, I've continued to see this problem on WB SE. **And so, I have a new proposal that maybe this site will be interested in.**\r\n\r\nWhen there are as many users as WB SE has, a senior community member with edit rights will come along within a couple minutes. If the site was smaller, that might be a longer gap.\r\n\r\nSo I propose this:\r\n1. All new questions are on hold for X minutes when first posted, where X gets smaller as the number of senior community members on the forum increases. X has a minimum threshold of 1 minute. \r\n2. During the hold period, any senior community member can hit the "Looks Fine" button to unlock it or the "Needs Edits" button, which puts the question into edit mode and keeps it on hold until the edits are complete. \r\n3. During this window, community members may also Vote To Close if the question has a fatal flaw. If there's even one VTC, the "Looks Fine" button gets disabled.\r\n4. After X minutes, if no one comes along to pick one of the two buttons and it doesn't have enough VTCs to close, then the question unlocks for answers. \r\n5. Bonus points if there's an easy "Move To Sandbox" button where the question can be edited without being answered, where the person asking the question can chat with community to get a good question. \r\n\r\nI really think this would **improve the quality of answers** to questions. I think it would also **reduce the number of snarky, combative frame-challenge answers** when someone asks a question based on false assumptions. 	524	2020-01-03 20:35:52.882087+00	1	4	1	586	1283	2953	\N	0	0	0	\N	f	f	2	\N
2	21	184	2018-07-04 16:38:21+00	Changing the use of GETDATE() in the entire database	I need to migrate an on-premises SQL Server 2017 database to an Azure SQL database, and I'm facing some challenges since there's quite a bit of limitations to go through.\r\n\r\nIn particular, since an Azure SQL database works only in UTC time (no time zones) and we need the local time, we have to change the use of `GETDATE()` **everywhere** in the database, which has proven to be more work than I anticipated.\r\n\r\nI created a user defined function to get the local time that works correctly for my time zone:\r\n\r\n    CREATE FUNCTION [dbo].[getlocaldate]()\r\n    RETURNS datetime\r\n    AS\r\n    BEGIN\r\n    \tDECLARE @D datetimeoffset;\r\n    \tSET @D = CONVERT(datetimeoffset, SYSDATETIMEOFFSET()) AT TIME ZONE 'Pacific SA Standard Time';\r\n    \tRETURN(CONVERT(datetime,@D));\r\n    END\r\nThe issue I'm having trouble with is to actually change `GETDATE()` with this function in every view, stored procedure, computed columns, default values, other constraints, etc.\r\n\r\nWhat would be the best way to implement this change?\r\n\r\nWe are in the public preview of [Managed Instances][1]. It still has the same issue with `GETDATE()`, so it doesn't help with this problem. Moving to Azure is a requirement. This database is used (and will be used) always in this time zone.\r\n\r\n  [1]: https://docs.microsoft.com/en-us/azure/sql-database/sql-database-managed-instance	191	2019-11-28 13:30:10.192303+00	0	4	1	259	389	445	211352	0	0	0	2019-11-28 13:30:10.192303+00	f	f	1	1
2	181	279	2018-06-14 20:25:44+00	If a CTE is defined in a query and is never used, does it make a sound?	Do unused CTEs in queries affect performance and / or alter the generated query plan?	287	2019-12-04 14:24:17.20888+00	0	4	1	354	607	1518	209695	0	0	0	2019-12-04 14:24:17.20888+00	f	f	1	1
1	38	568	2020-01-14 14:11:41.341642+00	Add post body preview to notifications	When another user answers your question or edits an answer on your question, you get a friendly pop-up, like this:\r\n\r\n![Screenshot 2020-01-14 at 14.07.19.png](/image?hash=ade588ffcafde3cab101c23238ecb0090697af6d37cf7243df43859370aad6fd)\r\n\r\nWhen you click through, you get more information, which might look like this:\r\n\r\n![Screenshot 2020-01-14 at 14.09.57.png](/image?hash=249743a517f1ef60e71b315f3a048b8295e16d6ef870d6a157b37b94131112d8)\r\n\r\nIt would be nice to have a preview of this diff in the pop up (subject to certain practical restrictions).	582	2020-01-14 14:11:41.341642+00	5	4	1	643	2114	3914	\N	0	0	0	\N	f	f	2	\N
4	168	359	2019-12-06 11:30:44.25677+00	Which questions do we import?	Is there any consensus on which questions we import? Do we only import questions we provided answers to (or we asked). Do we import outstanding questions which are neither? Do we just import questions based on our own gusto?	368	2019-12-06 11:31:05.41778+00	7	4	1	434	836	1336	\N	0	0	0	\N	f	f	2	\N
1	96	210	2019-11-30 13:35:22.155844+00	Profile page "saves" drop data	Each fieldset on the profile page has a separate save button, but each save button reloads the page. This causes anything changed in another fieldset other than the one being saved to be lost. If all the fields are presented at once either they should save automatically one at a time without a reload (ala AJAX) or a single save operation should process all fields at once.\r\n\r\nI understand technically how this happened and it's easy enough to work around by changing one thing at a time, but this _will_ trip people up.	217	2019-11-30 13:35:22.155844+00	2	4	2	285	462	552	\N	0	0	0	\N	f	f	2	\N
1	96	354	2019-12-06 07:54:23.413868+00	Add migration tool that changes the parent site	We're getting questions in our private beta community that are very general to the entire platform and should be asked in main meta. This brings up the idea that a "reparent" tool to migrate questions would be really dandy. For starters it wouldn't even have to leave a breadcrumb trail on the site it came  from, I'd say they could just be flat out moved.\r\n\r\nThe feature could be added one step at a time:\r\n\r\n1. Let the owner and/or site admins change the site parent ID on questions.\r\n\r\n1. Add a redirect so that URLs for question IDs where the question ID has changed sites won't break because the site name will update (benefit of having postings in a single table across sites, there will be downsides later!)\r\n\r\n1. Add an entry to the post history showing the reparent event took place.\r\n\r\n1. Add stub posts from the source site linking to the new home for the sake of late-comers.	363	2019-12-06 11:27:52.430803+00	1	4	2	429	828	1266	\N	0	0	0	\N	f	f	2	\N
2	752	564	2020-01-13 21:34:34.952203+00	What is the best way to transfer encrypted columns via flat file?	We have to send some PHI (personal health information) out to a vendor, and they will return something back. Currently, they are asking for only the PII (personally Identifiable Information) columns to be encrypted, then exported as a flat file.\r\n\r\nThe data will look something like this:\r\n\r\nfirst_name,last_name,Heart_rate\r\n\r\nIan C.,Weiner,80\r\n\r\nAnita,butterscratch,92\r\n\r\nQuestion: How would you encrypt first_name, last_name but leave Heart_rate as it is?\r\n\r\n"Do it differently" is an acceptable answer - it seems like a weird request to me.\r\n\r\nWe are currently on SQL Server 2014, SSIS 2015.\r\n\r\n	577	2020-01-13 21:37:12.923638+00	0	4	1	639	1449	2960	\N	0	0	0	\N	f	f	1	\N
1	702	563	2020-01-13 10:32:44.431167+00	What chance true italics for fonts that include it?	It seems that [the css files that give us our fonts of choice][1] only include regular and **bold** styles for each font. In the case of [Quattrocento][q], only those two styles are available. But for the other choices, there is also *italics*. In the absence of the italics font, we get a digitally skewed slant (= [oblique][o]) of the regular font. This is (to my way of thinking) unseemly.\r\n\r\nGiven that the markdown `*emphasis*` is displayed as `<em>emphasis</em>` and [this calls][em] italics by default, we get a fair bit of slant face. Would it be possible to get true italics for those font choices which support it?\r\n\r\nHere's one example of the difference this can make:\r\n\r\n![Perpetua_roman_and_italic.png](/image?hash=a403aeb3af4ef17e105341d7f247bc95d33f47bb2613e6c1f09c4505dc5d6683)\r\n\r\nThe "leaning" `f`, `a`, and `g` are the most striking in this example, but plenty of other Latin letter shapes are typically drawn differently for italics. I live in hope!\r\n\r\n[o]: https://en.wikipedia.org/wiki/Oblique_type\r\n[1]: https://github.com/topanswers/topanswers/tree/master/get/fonts\r\n[q]: https://fonts.google.com/specimen/Quattrocento\r\n[em]: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/em	576	2020-01-13 10:32:44.431167+00	8	1	1	638	1450	3000	\N	0	0	0	\N	f	f	2	\N
1	234	593	2020-01-21 23:13:51.595955+00	What is the expected voting behavior?	Here is some standard situation that often occurs on SE sites. Imagine there is a question.\r\n\r\n1. One standard situation is that user A answers it. In a way, A has done the "main work".\r\n2. Now user B copies A's answer and adds some detail that makes the answer a bit more elegant, the result more pretty, whatever. Importantly, B's amendments are not crucial for the answer to work.\r\n\r\nWhom are we going to upvote now. B's answer may well be the better answer, but would not have been possible with out A's work. One could now have (at least) two very different opinions:\r\n\r\n1. All we care about is that the best answer is on top. Even though B's contribution may only be incremental, theirs is still the better answer. Other users will appreciate it if they see the (slightly) more elegant answer on the top.\r\n2. We should not encourage the behavior of taking the main credit for something that one has not done. On the long run this will discourage expert users from contributing because someone else can always just add something minor and get the full credit.\r\n\r\nThis question is to ask whether or not there is something like a recommended behavior. In other words, I'd like to ask which of the two options (if any) looks more reasonable to the community. 	607	2020-01-21 23:14:03.662898+00	6	4	3	668	1590	3196	\N	0	0	0	\N	f	f	2	\N
2	45	572	2020-01-14 16:40:31.931005+00	Can I prevent users accessing data in global temporary tables?	By default, end users can query `tempdb.sys.tables` to get a full list of tables and they can read data from any of the global temporary tables. Is there a way to prevent end users from getting a list of all temporary tables?\r\n\r\nThe background is that The Application uses global temporary tables for certain ETL tasks. I would prefer for end users to not be able to look at the data in those tables. Please assume that changing to local temporary tables is not an option.	586	2020-01-14 17:17:47.709719+00	0	4	1	647	1473	3009	\N	0	0	0	\N	f	f	1	\N
4	233	379	2019-12-08 12:58:47.090524+00	To-do list before going public	Let's list what is needed before going public.\r\n	390	2019-12-08 15:29:21.997257+00	10	4	1	454	1939	3527	\N	0	0	0	\N	f	f	2	\N
1	96	305	2019-12-05 05:11:55.146957+00	Expose settings and actions via stable Javascript functions	I'm a die-hard keyboard user. My browser does not have any chrome. There is no address bar, no menu, no back buttons. My operating system and desktop environment does not have any chrome. There is no task bar and no start menu. The top left pixel on this monitor right now is from the canvas of this site, as is the bottom right. No my browser is not in "full screen" mode, that's just the way every I have the UI configured (Firefox + [Tridactyl](https://github.com/tridactyl/tridactyl) + disable all normal Firefox UI components).\r\n\r\nWhenever possible, I _don't_ use the mouse. I do have one, but some days I come to the office and discover about lunch time the battery is dead. It's possible to go for hours withouth touching it. In general I find it's much more convinient for me to stay on the keyboard when possible. Most browser interactions can be mapped to keybindings, and I can navigate most sites withouth the mouse.\r\n\r\nAs far as I'm concerned one of the coolest things SE ever did was finally integrating the keybindings userscript that floated around for years into the site by default and enabled a profile option to use it across all sites. Unfortunately the keybindings were not very comprehensive, but they did help with the basics.\r\n\r\n**I would propose** being careful during even this early phase of development to expose as many settings and user actions as possible through stable documented function calls so that users can map their own keybindings to them. A default set of keybindings might be included in the site out of the box, but it should be easy for a user to come along and rebind the actions to anything they wanat.\r\n\r\nI'll be looking into what exactly would be entailed in doing this well, but it would be nice to keep in mind as UI features are conceived.	313	2019-12-05 05:12:46.640593+00	5	4	2	380	667	1199	\N	0	0	0	\N	f	f	2	\N
1	96	537	2020-01-09 10:32:51.584915+00	Can we open the issue tracker on Github?	I think the Github issue tracker would be a very useful addition to the workflow. However I don't think there should be more than one venue for any one class of issue, and some types are probably better handled here on meta.\r\n\r\nHere's my idea:\r\n\r\n1. Enable the issue tracker, but disable blank issues and require picking from an issue template.\r\n1. [Configure the new issue](https://help.github.com/en/github/building-a-strong-community/configuring-issue-templates-for-your-repository#configuring-the-template-chooser) wizard with prominent links back here to meta for types of issues that should be redirected here.\r\n   - Anything site policy or community related (e.g. "Add downvotes" or "Add a site for Fuzzy Bears").\r\n   - All user experience feature requests. ("Add tag descriptions")\r\n1. Include explicit types and matching templates for the types of bugs that should be tracked there.\r\n   - Security bugs ("XSS in ...")\r\n   - Code style issues ("Change PHP code style to ...")\r\n   - Deployment / workflow issues ("Remove NPM module source from repo")\r\n   - Technical feature requests ("Add API method for...")\r\n   - Other feature requests _only having been vetted on meta_, as cross link to a met post ("Implement feature discussed in meta post X")\r\n\r\nA major thing that the issue tracker will do that meta posts here will not is collate commits with the issues they are related to. This makes it easier to collaborate as developers, track progress, discuss technical details, do code reviews, etc. It will also provide a home for things that would otherwise be noise on this Q&A site as they are simply not relevant or interesting to 95% of the intended audience of the actual site.\r\n\r\nNote I'm happy to actually set this up but I'd like to see consensus or approval here first...	550	2020-01-09 10:32:51.584915+00	10	4	2	612	1508	3128	\N	0	0	0	\N	f	f	2	\N
1	115	235	2019-12-01 19:14:56.80144+00	Could the mobile view favor Q&A over chat, please?	After creating an account here I tried it out on my phone (which, for SE, I use(d) to monitor activity, particularly as a moderator).  This is what I see when I visit meta on my Android phone using Chrome:\r\n\r\n![Screenshot_2019-12-01-13-07-26.jpg](/image?hash=bf7ef9f0961a813d3b6c57c843dc7ff0478ca59b049fd5006b7b9dc4eed823d4)\r\n\r\nIt took me a lot of poking around to (somehow; I'm not sure how to reproduce) get to a view that had a "questions" button at the bottom, at which point I could get to the questions list.  But if I view a question and then try to get back to the question list, I end up back at chat again instead and I'm back to figuring out how to get out.\r\n\r\nI don't know a lot about front-end stuff, but I'm guessing/hoping that each section on the site has something akin to a Z-order (a priority), and chat is currently priortized higher, and prioritizing the other part (left pane on desktop) would fix it.  But that's a lot of guessing and I don't know what it would break, so maybe that's not the answer.\r\n\r\nCould we do *something* to make a mobile view non-painful?  I'm not suggesting we invest heavily in making it great, just asking if we can make it primarily about Q&A instead of chat.\r\n	243	2019-12-01 19:14:56.80144+00	1	4	1	310	575	2677	\N	0	0	0	\N	f	f	2	\N
2	12	81	2019-11-22 11:32:20.353783+00	What does BmkToPage mean in an execution plan?	The following db<>fiddle:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=2cb25ee984d14a05757a85cf318e3b5b\r\n\r\n...produces an execution plan with an *RID Lookup* like:\r\n\r\n![Plan with RID Lookup](/image?hash=102934261ef688343e3d23039f340f296a830999da4f3cf0b54d2fa0529976e3 "Plan with RID Lookup")\r\n\r\nThe *RID Lookup* is expected, because the nonclustered index is not covering for the query. The execution engine has to fetch the `padding` column from the heap base table.\r\n\r\nMy question is about the *Compute Scalar*:\r\n\r\n![Compute Scalar](https://i.imgur.com/RWsigPa.png "Compute Scalar with BmkToPage")\r\n\r\n**What is the `BmkToPage` function, and what purpose does it serve in this plan?**	88	2019-11-22 11:32:20.353783+00	0	4	2	156	169	1451	\N	0	0	0	\N	f	f	1	\N
2	117	203	2017-10-17 13:19:11+00	Why does truncating a temp table at the end of the stored procedure that creates it free tempdb space faster?	SQL Server caches temp tables created within stored procedures and merely renames them when the procedure ends and is subsequently executed. My question has to do with when the tempdb space is released. I've read that the table is [truncated at the end of the procedure](http://web.archive.org/web/20180130201242/http://sqlblog.com/blogs/paul_white/archive/2012/08/15/temporary-tables-in-stored-procedures.aspx). I've read in comments that this is [handled on a per-session basis](http://web.archive.org/web/20180422150925/http://sqlblog.com:80/blogs/paul_white/archive/2012/08/17/temporary-object-caching-explained.aspx) and seen a question about whether or not cleanup is necessary [answered on MSDN](https://social.msdn.microsoft.com/Forums/sqlserver/en-US/522d302a-857f-4730-b49f-cca7fb236912/is-it-necessary-to-clean-up-drop-temporary-tables-in-stored-procedures?forum=transactsql). But what if it is never executed by the same session twice?\r\n\r\nI've also heard that there is a background garbage collection process that frees up that space once the table is out of scope. \r\n\r\nTruncating a temp table at the end of the stored procedure that creates it seems to cause the space the table uses in tempdb for the data to be released faster than if no truncate statement is used, despite expectations to the contrary. Why? \r\n\r\nWhat would be the relative performance implications of using or not using such a truncate statement? When using SNAPSHOT isolation, tempdb is often stressed and I would think that releasing space used in tempdb from a large temp table as soon as possible would prevent otherwise unnecessary growth of tempdb. Would this potential space savings come at the cost of performance?\r\n\r\nHere is some code to reproduce the issue (mostly from @TheGameiswar, with some changes):\r\n\r\n    SET NOCOUNT ON;\r\n    GO\r\n    ALTER PROC usp_test\r\n    AS\r\n    BEGIN\r\n    \tIF object_id('tempdb..#temp') IS NOT NULL\r\n    \t\tDROP TABLE #temp\r\n    \r\n    \tSELECT *\r\n    \tINTO #temp\r\n    \tFROM [dbo].[Event_28] -- This is a table with 15313 rows, using 35648 KB according to sp_spaceused\r\n    \r\n    \t--SELECT SUM(user_object_reserved_page_count) AS [user object pages used]\r\n    \t--\t,(SUM(user_object_reserved_page_count) * 1.0 / 128) AS [user object space in MB]\r\n    \t--\t,getdate() AS BeforeTruncate\r\n    \t--FROM tempdb.sys.dm_db_file_space_usage;\r\n     --   TRUNCATE TABLE #temp\r\n    \t--SELECT SUM(user_object_reserved_page_count) AS [user object pages used]\r\n    \t--\t,(SUM(user_object_reserved_page_count) * 1.0 / 128) AS [user object space in MB]\r\n    \t--\t,getdate() AS AfterTruncate\r\n    \t--FROM tempdb.sys.dm_db_file_space_usage;\r\n\r\n    END\r\n    GO\r\n    \r\n    SELECT SUM(user_object_reserved_page_count) AS [user object pages used]\r\n    \t,(SUM(user_object_reserved_page_count) * 1.0 / 128) AS [user object space in MB]\r\n    \t,getdate() AS 'before'\r\n    FROM tempdb.sys.dm_db_file_space_usage;\r\n    \r\n    EXEC usp_test\r\n    GO\r\n    \r\n    SELECT SUM(user_object_reserved_page_count) AS [user object pages used]\r\n    \t,(SUM(user_object_reserved_page_count) * 1.0 / 128) AS [user object space in MB]\r\n    \t,getdate() AS 'final'\r\n    FROM tempdb.sys.dm_db_file_space_usage;\r\n    GO 40\r\n\r\nThe commented lines were left commented out for some runs and un-commented for others. When the `TRUNCATE` was commented out, it took between 2.25 and 4.5 seconds before the results of the `tempdb.sys.dm_db_file_space_usage` query (4472 more pages and 34.9375 MB larger) matched the result before the procedure was executed. With the lines (including the `TRUNCATE`) un-commented, it only took about 0.11 - 0.9 seconds. These results are from a live system, with some minor data growth in the source table during this experiment.\r\n\r\nSample output with the code commented out (2.69 seconds from the first to last "final" entry):\r\n\r\n    user object pages used user object space in MB                 before\r\n    ---------------------- --------------------------------------- -----------------------\r\n    1536                   12.000000                               2017-10-04 21:03:42.197\r\n    \r\n    Beginning execution loop\r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:42.423\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:42.533\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:42.643\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:42.883\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:42.990\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:43.100\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:43.450\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:43.650\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:43.767\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:43.993\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:44.103\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:44.213\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:44.437\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:44.553\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:44.663\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:44.887\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6000                   46.875000                               2017-10-04 21:03:45.003\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    1536                   12.000000                               2017-10-04 21:03:45.113\r\n\r\nSample results with the code un-commented (0.11 seconds from the first to last "final" entry):\r\n\r\n    user object pages used user object space in MB                 before\r\n    ---------------------- --------------------------------------- -----------------------\r\n    1536                   12.000000                               2017-10-04 21:07:39.807\r\n    \r\n    user object pages used user object space in MB                 BeforeTruncate\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6016                   47.000000                               2017-10-04 21:07:39.923\r\n    \r\n    user object pages used user object space in MB                 AfterTruncate\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6016                   47.000000                               2017-10-04 21:07:39.923\r\n    \r\n    Beginning execution loop\r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    6016                   47.000000                               2017-10-04 21:07:40.160\r\n    \r\n    user object pages used user object space in MB                 final\r\n    ---------------------- --------------------------------------- -----------------------\r\n    1536                   12.000000                               2017-10-04 21:07:40.270\r\n	210	2019-11-30 07:57:43.056745+00	0	4	1	278	424	1484	188659	0	0	0	2019-11-30 07:57:43.056745+00	f	f	1	1
2	776	582	2020-01-17 01:40:36.953419+00	What is the best method to restrict certain entries from taking on certain values	First time reader.  First time poster to TopAnswers.\r\n\r\nThank you for reading this question, despite my poor title.  I could not think of a succinct and accurate title.\r\n\r\nI am building a small sqlite database for a personal project.  I am using Python and SqlAlchemy.  I'm doing this mostly for the challenge and for the self education.\r\n\r\nI have a table "components".  One of the attributes of this table is "type".  "type" is restricted to only three values, which I am going to enforce through SqlAlchemy's enum.\r\n\r\nEach of the three "types" have "rates" associated with them.\r\nType "A" is measured in "parts / second"\r\nTypes "B" and "C" are both measured by the amount of time it takes to make one part (seconds / part).\r\n\r\nI am not sure of the most appropriate way of handling the difference in rate measurements.\r\n\r\nOption 1:\r\nShould I simply invert A to be like B & C (or vice versa).  The reason I am a little leery about this option is rounding errors.\r\n\r\nOption 2:\r\nIf I decide to not simply change the rates measure (like in option 1), do I store the raw numbers in a "rate" attribute and then use my Python code to determine how to apply the numbers? or do I set up two tables which resemble something like a lookup table, one to store the rate values of A and the other to store the rate values of B and C (since they are both measured in seconds / part)\r\n\r\nIs there a better way than these two options?\r\n\r\n	596	2020-01-17 01:40:36.953419+00	0	4	1	657	1614	3180	\N	0	0	0	\N	f	f	1	\N
1	8	349	2019-12-06 07:09:51.967572+00	Provide an visual indication of changes in edit history	As it is now, the edit history of a post shows the versions of the post after one another without any clear indication what changed.\r\n\r\nFor example, I edited this question: [Re-importing questions fails ungracefully](https://topanswers.xyz/question-history?id=347) fixing 2 typos (I believe) but to someone reviewing the edit it's not clear at all what has been changed of if I have introduced typos or errors.\r\n\r\nEven for a short post like this you need to really concentrate hard to spot the changes, and in a longer post I think it's easy to miss changes which could lead to errors being introduced in edits that go unnoticed.\r\n\r\nNot extremely high on the priority list of course and it doesn't need to be a fancy side-by-side comparison either.	358	2019-12-06 07:09:51.967572+00	1	4	1	424	750	1436	\N	0	0	0	\N	f	f	2	\N
4	168	597	2020-01-22 18:38:04.257415+00	When do we formally elect moderators?	Currently we have @samcarter and @Caleb as part of the clean up crew along with @Jack and I'm glad that you three are willing to keep our small community nice and clean.\r\n\r\nNevertheless, I think we should find an early consensus on when to have a formal moderator election. What should be our goal as a new, hopefully thriving community to reach before we want to hold formal elections?\r\n\r\nShould we aim to get at least <*num*> users so that there is a "democratic basis", or should we agree on a specific time, e.g., say there should be elections <*num*> months after going public?	611	2020-01-22 23:11:09.898546+00	6	6	3	672	1629	3224	\N	0	0	0	\N	f	f	2	\N
2	2	10	2019-11-05 17:50:26.263456+00	How do I upgrade from the Express Edition of SQL Server 2019 Preview to RTM on RHEL?	The [docs say](https://docs.microsoft.com/en-gb/sql/linux/quickstart-install-connect-red-hat?view=sql-server-ver15):\r\n\r\n>Download the Microsoft SQL Server 2019 Red Hat repository configuration file:\r\n>\r\n>     sudo curl -o /etc/yum.repos.d/mssql-server.repo https://packages.microsoft.com/config/rhel/7/mssql-server-2019.repo\r\n\r\nBut the repo for the preview was at <https://packages.microsoft.com/config/rhel/7/mssql-server-preview.repo>\r\n\r\nDo I need to reinstall from scratch or can I just switch repo and `yum update`?	17	2019-11-05 23:34:34.429893+00	0	1	1	3	3	261	\N	0	0	0	\N	f	f	1	\N
1	168	263	2019-12-04 12:08:50.151814+00	Please add an "about" or "imprint" page	I think for the process of assessing the network and for the decision-making whether one wants to create an account on this site or not, a small "About" or "Imprint" page would be a vast improvement.\r\n\r\nInformation I'd like to see before I create an account is, where the servers are located, who owns the rights to the contents, whether it is a corporate or private person who maintains the server, etc, and perhaps most important, what's the scope of the site.	271	2019-12-04 12:08:50.151814+00	13	4	1	338	2110	3893	\N	0	0	0	\N	f	f	2	\N
1	115	233	2019-12-01 18:48:29.732792+00	Could we have easy access to all our contributions?	After posting a few things I went to my profile, expecting to see links to those posts.  But the profile page is currently only profile settings.  On SE, I found that set of links really important, both in finding my own stuff (e.g. to answer a related question where I want to link to prior work) and in looking for specific things I remember that so-and-so posted, or for that matter just to get a sense of another user's interests and work.  (Hey, so-and-so gave me a great answer to that thorny git problem; what else has this person written that I might benefit from?)\r\n\r\nIs this already planned but you just haven't gotten to it?  If so, do you have any sense of when it might be possilbe?  If you're not planning on it, are you open to persuasion?\r\n	241	2019-12-01 18:48:29.732792+00	15	4	1	308	1915	3726	\N	0	0	0	\N	f	f	2	\N
1	115	228	2019-12-01 18:00:05.585353+00	Please do not assume wide screens	I just answered a question (my first time posting here), and this is the edit window I was presented with:\r\n\r\n![Screen Shot 2019-12-01 at 12.52.40 PM.png](/image?hash=ed905782c62566de9030deda1f0c5555fc1462e0575ff6908e3794ce799e4e4f)\r\n\r\nThat is...not ideal.  The question is hard to read, and I didn't even use the whole column so the extra space for entry and preview didn't help me.  I get why this would be helpful for longer posts or for people who go full-screen with their browsers on huge monitors, but I can't do that. My current browser width is 1157px, which does not seem unreasonable.  (It's wider than what would allow me to have browser and editor side by side, a not-infrequent desire.  And it's almost as wide as my tablet can support full-screen.)  Also, zoom makes this layout worse, and some of us need a little help there.\r\n\r\nI don't have a concrete suggestion here.  From SE I'm used to the preview being below rather than next to the edit window, but that does lead to undesirable scrolling sometimes.  I don't know how well the side-by-side setup here would work with code, though.  Maybe what I (someone who is pretty fluent in Markdown) want is for the preview to be smaller but *expandable*, so I can mostly ignore it and then check my work if I'm doing something tricky (or before posting).\r\n	236	2019-12-01 18:00:05.585353+00	2	4	1	303	2082	3968	\N	0	0	0	\N	f	f	2	\N
1	2	24	2019-11-12 16:26:15.652441+00	Additional (dual) license grant for original code	When you post a question or answer, you choose the license you are granting for the entire post. For example, the license granted on this post is CC0 1.0 as indicated above.\r\n\r\nYou can also, optionally, choose to grant an additional (dual) license for the portions of the post that are your own original source code. You should make it clear in your post exactly which code this refers to if there is any ambiguity.\r\n\r\nThe additional license is currently limited to this list, but we may add other licenses to the list if there is demand to do so (e.g. [Apache 2.0](https://opensource.org/licenses/Apache-2.0)):\r\n\r\n* [CC0 1.0](https://creativecommons.org/publicdomain/zero/1.0): CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\r\n* [MIT](https://opensource.org/licenses/MIT): The MIT License\r\n* [GPLv2](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html): GNU General Public License, version 2\r\n* [GPLv3](https://www.gnu.org/licenses/gpl-3.0.html): GNU General Public License\r\n* [LPPL 1.3c](https://www.latex-project.org/lppl/lppl-1-3c): The LaTeX project public license (LPPL), version 1.3c	31	2020-01-15 14:50:34.785992+00	5	1	1	6	1488	3041	\N	0	0	0	\N	f	f	3	\N
2	98	290	2017-12-21 19:38:43+00	What function quotes an identifier in dynamic-sql with SQL Server?	What is the SQL Server method of safe-quoting identifiers for dynamic sql generation.\r\n\r\n* MySQL has [`quote_identifier`](https://dev.mysql.com/doc/refman/8.0/en/sys-quote-identifier.html)\r\n* PostgreSQL has [`quote_ident`](https://www.postgresql.org/docs/current/static/functions-string.html)\r\n\r\nHow do I ensure given a dynamically generated column name for a dynamically generated statement that the column itself isn't a SQL-injection attack.\r\n\r\nLet's say I have a SQL Statement,\r\n\r\n    SELECT [$col] FROM table;\r\n\r\nwhich is essentially the same as\r\n\r\n    'SELECT [' + $col + '] FROM table;'\r\n\r\nWhat stops an injection attack where\r\n\r\n    $col = "name] FROM sys.objects; \\r\\n DROP TABLE my.accounts; \\r\\n\\ --";\r\n\r\nResulting in\r\n\r\n\r\n\r\n    SELECT [name] FROM sys.objects;\r\n    DROP TABLE my.accounts;\r\n    -- ] FROM table;\r\n	298	2019-12-04 14:32:00.670958+00	0	4	1	365	621	865	193767	0	0	0	2019-12-04 14:32:00.670958+00	f	f	1	1
1	131	234	2019-12-01 18:58:01.553481+00	Make it clear to new users how to maintain access to their account	First off, please let me say that I very much like the eminently simple sign-up process, and I do believe that data minimization is a good thing.\r\n\r\nHowever, with how it works at present, if a user signs up and then lose their browser cookies, there appears to be no way to recover access to the account. If a user invests significantly in their account, this could become a problem.\r\n\r\nIt's fairly clear from reading the profile page in detail that the way to solve this is to securely record the account recovery token. However, the user does not appear to be presented with this information when they sign up. (I wasn't.)\r\n\r\nOnce a user signs up, they should ideally be presented with information on how to maintain access to their account even if their cookies are deleted. This could be a simple message along the lines of "to ensure continued access to your account, go into your profile and record your account recovery token in a safe, secure place" with a link to the profile page.	242	2019-12-01 18:58:01.553481+00	10	4	1	309	1322	2810	\N	0	0	0	\N	f	f	2	\N
1	96	369	2019-12-06 13:24:29.685769+00	View/filter questions by tag	Questions have tags, but there does not seem to be any usable way to use them yet.\r\n\r\n1. They should be usable to restrict search queries: `[faq] markdown` should only show me posts tagged *faq* matching markdown somewhere.\r\n\r\n1. Clicking on the tag should load activate a seach query for that tag, effectively filtering the question list by tag.	378	2019-12-06 13:24:29.685769+00	5	4	2	444	2221	4088	\N	0	0	0	\N	f	f	2	\N
2	45	191	2017-06-07 15:49:11+00	Why can it take up to 30 seconds to create a simple CCI rowgroup?	I was working on a demo involving CCIs when I noticed that some of my inserts were taking longer than expected. Table definitions to reproduce:\r\n\r\n    DROP TABLE IF EXISTS dbo.STG_1048576;\r\n    CREATE TABLE dbo.STG_1048576 (ID BIGINT NOT NULL);\r\n    INSERT INTO dbo.STG_1048576\r\n    SELECT TOP (1048576) ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) RN\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2;\r\n\r\n    DROP TABLE IF EXISTS dbo.CCI_BIGINT;\r\n    CREATE TABLE dbo.CCI_BIGINT (ID BIGINT NOT NULL, INDEX CCI CLUSTERED COLUMNSTORE);\r\n\r\nFor the tests I'm inserting all 1048576 rows from the staging table. That's enough to fill exactly one compressed rowgroup as long as it doesn't get trimmed for some reason.\r\n\r\nIf I insert all of the integers mod 17000 it takes less than a second:\r\n\r\n    TRUNCATE TABLE dbo.CCI_BIGINT;\r\n    \r\n    INSERT INTO dbo.CCI_BIGINT WITH (TABLOCK)\r\n    SELECT ID % 17000\r\n    FROM dbo.STG_1048576\r\n    OPTION (MAXDOP 1);\r\n\r\n> SQL Server Execution Times: CPU time = 359 ms,  elapsed time = 364 ms.\r\n\r\nHowever, if I insert the same integers mod 16000 it sometimes takes over 30 seconds:\r\n\r\n    TRUNCATE TABLE dbo.CCI_BIGINT;\r\n    \r\n    INSERT INTO dbo.CCI_BIGINT WITH (TABLOCK)\r\n    SELECT ID % 16000\r\n    FROM dbo.STG_1048576\r\n    OPTION (MAXDOP 1);\r\n\r\n> SQL Server Execution Times: CPU time = 32062 ms,  elapsed time = 32511 ms.\r\n\r\n\r\nThis is a repeatable test that has been done on multiple machines. There seems to be a clear pattern in elapsed time as the mod value changes:\r\n\r\n    MOD_NUM\tTIME_IN_MS\r\n    1000\t2036\r\n    2000\t3857\r\n    3000\t5463\r\n    4000\t6930\r\n    5000\t8414\r\n    6000\t10270\r\n    7000\t12350\r\n    8000\t13936\r\n    9000\t17470\r\n    10000\t19946\r\n    11000\t21373\r\n    12000\t24950\r\n    13000\t28677\r\n    14000\t31030\r\n    15000\t34040\r\n    16000\t37000\r\n    17000\t563\r\n    18000\t583\r\n    19000\t576\r\n    20000\t584\r\n\r\nIf you want to run tests yourself feel free to modify the test code that I wrote [here][1].\r\n\r\nI couldn't find anything interesting in sys.dm_os_wait_stats for the mod 16000 insert:\r\n\r\n    ╔════════════════════════════════════╦══════════════╗\r\n    ║             wait_type              ║ diff_wait_ms ║\r\n    ╠════════════════════════════════════╬══════════════╣\r\n    ║ XE_DISPATCHER_WAIT                 ║       164406 ║\r\n    ║ QDS_PERSIST_TASK_MAIN_LOOP_SLEEP   ║       120002 ║\r\n    ║ LAZYWRITER_SLEEP                   ║        97718 ║\r\n    ║ LOGMGR_QUEUE                       ║        97298 ║\r\n    ║ DIRTY_PAGE_POLL                    ║        97254 ║\r\n    ║ HADR_FILESTREAM_IOMGR_IOCOMPLETION ║        97111 ║\r\n    ║ SQLTRACE_INCREMENTAL_FLUSH_SLEEP   ║        96008 ║\r\n    ║ REQUEST_FOR_DEADLOCK_SEARCH        ║        95001 ║\r\n    ║ XE_TIMER_EVENT                     ║        94689 ║\r\n    ║ SLEEP_TASK                         ║        48308 ║\r\n    ║ BROKER_TO_FLUSH                    ║        48264 ║\r\n    ║ CHECKPOINT_QUEUE                   ║        35589 ║\r\n    ║ SOS_SCHEDULER_YIELD                ║           13 ║\r\n    ╚════════════════════════════════════╩══════════════╝\r\n\r\nWhy does the insert for `ID % 16000` take so much longer than the insert for `ID % 17000`?\r\n\r\n\r\n  [1]: https://pastebin.com/qXPhYFfE	198	2019-11-29 08:29:28.0174+00	0	4	1	266	403	1479	175682	0	0	0	2019-11-29 08:29:28.0174+00	f	f	1	1
1	234	388	2019-12-09 18:28:31.7406+00	Should we have user groups that can be pinged?	Is it possible to implement user groups. Let's say there is a group "squirrels". Everybody who has joined that group can be pinged with with @squirrels. It might make sense to make this feature only available to group members, i.e. the ping will only work if the message is from another member of the user group. (On the long run one may need the possibility to ban certain users from a given group, of course I hope this will never be necessary.) \r\n\r\nSo the question: is it technically possible to achieve this? If so, do others also see the need for this feature?	399	2019-12-09 18:28:31.7406+00	6	4	3	463	910	3464	\N	0	0	0	\N	f	f	2	\N
1	96	309	2019-12-05 15:28:15.433885+00	Copy/download code blocks	It would be really nice to have a plugin to the Markdown renderer that enabled "copy to clipboard" and/or "download as file" functions for code blocks.	317	2019-12-05 15:28:15.433885+00	9	4	2	384	2096	4136	\N	0	0	0	\N	f	f	2	\N
1	168	386	2019-12-09 12:56:18.807802+00	Can't login with second device	Today I tried to login with a second device for the first time. That didn't work out quite well. After entering the PIN or login key I get a small dialogue box flashing for a split second after which the site reloads and I'm still not logged in. I made a screenshot of said dialogue:\r\n\r\n![ta_error.png](/image?hash=d0fd0004ca723c7c8856606f3a0b132d8c5fca3d153c15407168f9b605310035)\r\n\r\nThe line with the checkbox next to it reads "Don't allow more dialogues from this site" (or something like that) in German (the localisation of that PC).	397	2019-12-09 12:56:18.807802+00	4	4	1	461	906	1364	\N	0	0	0	\N	f	f	2	\N
4	96	341	2019-12-06 05:49:53.854499+00	How can we enable TeX syntax highlighting as the default?	Most Markdown code block syntax highlighters have pretty good support for TeX based languages out of the box. I think it's reasonable to assume blocks otherwise unmarked on this site are going to be tex.\r\n\r\nSource:\r\n\r\n~~~markdown\r\n```\r\n\\textbf{bob}\r\n```\r\n~~~\r\n\r\nOutput:\r\n\r\n```\r\n\\textbf{bob}\r\n```\r\n\r\nOf course setting the language should also work:\r\n\r\n\r\nSource:\r\n\r\n~~~markdown\r\n```tex\r\n\\textbf{bob}\r\n```\r\n~~~\r\n\r\nOutput:\r\n\r\n```tex\r\n\\textbf{bob}\r\n```\r\n\r\nRight not it appears the language is just not recognized.	350	2019-12-06 06:19:39.094833+00	6	4	3	416	738	1371	\N	0	0	0	\N	f	f	2	\N
2	16	270	2018-05-09 23:51:32+00	Why does a plan with FULL optimization show simple parameterization?	I read that only [Trivial Plans can be Simple Parameterized][1], and that not all queries (even when the plan is Trivial) [can be Simple Parameterized][2].\r\n\r\nThen why is [this plan][3] showing Full Optimization, and Simple Parameterization at the same time?\r\n\r\n[![NUTS][4]][4]\r\n\r\n\r\n  [1]: https://blogs.msdn.microsoft.com/psssql/2013/12/04/how-simple-parameterization-works/\r\n  [2]: https://blogs.msdn.microsoft.com/sqlprogrammability/2007/01/11/4-0-query-parameterization/\r\n  [3]: https://www.brentozar.com/pastetheplan/?id=HkOMIb-CM\r\n  [4]: https://i.stack.imgur.com/a6g5N.png	278	2019-12-04 14:14:31.995389+00	0	4	1	345	627	1533	206304	0	0	0	2019-12-04 14:14:31.995389+00	f	f	1	1
2	250	355	2019-09-18 12:27:52+00	Сardinality estimation of partially covering range predicates	At the moment I'm trying to figure out how SQL Server evaluates the cardinality of range predicates that partially cover the histogram step.\r\n\r\nOn the Internet, at [cardinality-estimation-for-and-for-intra-step-statistics-value][1] I came across a similar question and Paul White gave a rather interesting answer to it.\r\n\r\nAccording to Paul's answer, the formulas for estimating cardinality for the predicates >= and > (in this case, I am only interested in the Cardinality estimator model of at least 120) are as follows:\r\n\r\nFor >:\r\n\r\n    Cardinality = EQ_ROWS + (AVG_RANGE_ROWS * (F * (DISTINCT_RANGE_ROWS - 1)))\r\n\r\nFor >=:\r\n\r\n    Cardinality = EQ_ROWS + (AVG_RANGE_ROWS * ((F * (DISTINCT_RANGE_ROWS - 1)) + 1))\r\n\r\nI tested the application of these formulas on the *[Production].[TransactionHistory]* table of the *AdventureWorks2014* database based on the range predicate using the *TransactionDate* column and datetime range between '20140614' and '20140618'.\r\n\r\nThe statistics for the histogram step of this range are as follows:\r\n\r\n[![Histogram][2]][2]\r\n \r\nAccording to the formula, I calculated the cardinality for the following query:\r\n\r\n    SELECT COUNT(1)\r\n    FROM [AdventureWorks2014].[Production].[TransactionHistory]\r\n    WHERE [TransactionDate] BETWEEN '20140615 00:00:00.000' AND '20140616 00:00:00.000'\r\n\r\nThe calculation was performed using the following code:\r\n\r\n      DECLARE @predStart DATETIME =  '20140615 00:00:00.000'\r\n      DECLARE @predEnd DATETIME = '20140616 00:00:00.000'\r\n    \r\n      DECLARE @stepStart DATETIME = '20140614 00:00:00.000'\r\n      DECLARE @stepEnd DATETIME = '20140618 00:00:00.000'\r\n    \r\n      DECLARE @predRange FLOAT = DATEDIFF(ms, @predStart, @predEnd)\r\n      DECLARE @stepRange FLOAT = DATEDIFF(ms, @stepStart, @stepEnd)\r\n    \r\n      DECLARE @F FLOAT = @predRange / @stepRange;\r\n    \r\n      DECLARE @avg_range_rows FLOAT = 100.3333\r\n      DECLARE @distinct_range_rows INT = 3\r\n      DECLARE @EQ_ROWS INT = 0\r\n    \r\n      SELECT @F AS 'F'\r\n      \r\n      --for new cardinality estimator\r\n    \r\n      SELECT @EQ_ROWS + @avg_range_rows * (@F * (@distinct_range_rows - 1) + 1) AS [new_card]\r\n\r\nAfter calculating, I got the following results:\r\n\r\n[![enter image description here][3]][3]\r\n \r\nAccording to the formula, it turned out 150.5, but the optimizer estimates the predicate at 225.75 rows, and if you change the upper border of the predicate to ‘20140617’, the optimizer will already evaluate 250.833 rows, while using the formula we get only 200.6666 rows.\r\n\r\nPlease tell me, how does Cardinality Estimator evaluate in this case, maybe I made a mistake somewhere in my understanding of the quoted formulas?\r\n\r\n\r\n  [1]: https://dba.stackexchange.com/questions/148523/cardinality-estimation-for-and-for-intra-step-statistics-value\r\n  [2]: https://i.stack.imgur.com/sMCyC.png\r\n  [3]: https://i.stack.imgur.com/loX3W.png	364	2019-12-06 09:12:21.570304+00	0	4	1	430	761	1551	249057	0	0	0	2019-12-06 09:12:21.570304+00	f	f	1	1
\.
