--
-- Data for Name: answer; Type: TABLE DATA; Schema: out; Owner: postgres
--

COPY "db".answer (answer_id, question_id, account_id, answer_at, answer_markdown, answer_change_at, answer_votes, license_id, codelicense_id, answer_se_answer_id, answer_flags, answer_crew_flags, answer_active_flags, answer_se_imported_at, answer_proposed_answer_id, answer_summary, answer_permit_later_license, answer_permit_later_codelicense) FROM stdin;
320	240	1	2019-12-05 19:07:28.847664+00	Similar to [@Caleb's list](/meta?q=240#a182), you will now get notifications:\r\n\r\n* When someone edits a question  of yours\r\n* When someone edits an answer of yours\r\n* When someone edits a question you are subscribed to\r\n* When someone edits an answer on a question you are subscribed to\r\n* When someone posts an answer on a question you are subscribed to\r\n\r\nYou are automatically subscribed to questions posted by you, but you can unsubscribe by clicking the bell icon at the bottom right:\r\n\r\n![Screenshot 2019-12-05 at 19.00.16.png](/image?hash=cbb421d4d133afb3c3c6d9dccb0d06b20352827d657febe326ab04e912496842)\r\n\r\nand you can subscribe to anyone's post or re-subscribe to your own by clicking the empty bell icon:\r\n\r\n![Screenshot 2019-12-05 at 18.59.41.png](/image?hash=5d4af60344b7efc4647697d4a828c7a8eb3405254d3db22ab08b75f0b06374ef)	2019-12-05 19:07:28.847664+00	8	1	1	\N	0	0	0	\N	\N	Similar to [@Caleb's list](/meta?q=240#a182), you will now get notifications:	f	f
182	240	96	2019-12-02 08:39:26.92514+00	Yes, the notification system needs to be expanded to handle more notifications. Perhaps each notification type should be optional, but probably defaulting them all to "on" is best.\r\n\r\n* New answer to question\r\n* Any own post edited by other user\r\n* Edited answers to question (something SE doesn't do, maybe default to off)\r\n* New participant in chat where you have a post, because they don't always ping the post owner... (default to off?)	2019-12-02 08:39:26.92514+00	2	4	2	\N	0	0	0	\N	\N	Yes, the notification system needs to be expanded to handle more notifications. Perhaps each notification type should be optional, but probably defaulting them all to "on" is best.	f	f
561	231	2	2020-01-03 16:41:05.221319+00	We've taken a few steps in the right direction…\r\n\r\n> I know I'm the odd one out sometimes in these matters, but I'd like to suggest that the site should be possible to read without having Javascript and XmlHttpRequest turned on.\r\n>\r\n> Currently, browsing to https://topanswers.xyz without Javascript and XHR on (for example, using uMatrix in a default-block configuration) results in a completely blank page. The site looks functional, but as if there's nothing there.\r\n\r\n…you no longer get a completely blank page — you can see questions, answers and even chat, but you do only see the raw markdown. It doesn't look too bad and it is readable enough — but we may go further later on and if we do I'll update this post.\r\n\r\n> By all means use Javascript to enhance the experience, but please make it possible to at least read the site without running Javascript and making extra asynchronous requests to the server.\r\n\r\nApart from reading the site, basic navigation should work, and not all of it does right now. This still isn't a major focus but again, we'll update this answer as and when we make more progress on this.	2020-01-03 16:41:05.221319+00	2	1	1	\N	0	0	0	\N	\N	We've taken a few steps in the right direction…	f	f
184	231	96	2019-12-02 11:27:08.173675+00	Yes please!\r\n\r\nThis also matters for crawlers and scrapers. Google's crawler can render and parse content on sites that load it via Javascript, but not all crawlers are there.\r\n\r\nProgressive enhancement takes a bit of thinking ahead of time to decide what to load when, but in the end it's a good thing. And for those of us that _do_ use Javascript, the experience is usually bettter, not worse, on sites that have done this right.	2019-12-02 11:27:17.815358+00	2	4	2	\N	0	0	0	\N	\N	Yes please!	f	f
614	570	751	2020-01-13 20:17:33+00	> is this a bug in SQL Server?\r\n\r\nYes, certainly, the `1` that is returned in all rows in your final result only exists in the first row of the outer input so shouldn't even be in scope for the subsequent rows. It looks like the same basic issue as looked at in detail by Paul White [here](https://topanswers.xyz/databases?q=573#a618).\r\n\r\nI executed your final query in dbfiddle (SQL Server 2019) and pasted the plan here https://www.brentozar.com/pastetheplan/?id=Sy4sBB5lI  It looks like the sort executes 4 times (once for each outer row) but for some reason it rewinds rather than rebinds so doesn't call the child operators of the sort more than once. This is a bug as the reference to the correlated parameter of the outer join (`Union1004`) should cause a rebind when this has changed value. As a result the reference to `Union1004` in Node 5 of the plan is never re-evaluated. \r\n\r\nI see you have now reported it here https://feedback.azure.com/forums/908035-sql-server/suggestions/39428632-microsoft-sql-server-2014-incorrect-result-when-s\r\n\r\n> Is there any possibility to force evaluation of the nested query for every row?\r\n\r\nAdding the query hint `OPTION (MERGE UNION)` works for your example, I don't know if this will necessarily be sufficient to avoid the bug in all cases but from the linked Paul White answer it appears it should work. In the case of your example it works as the sort is [pushed down lower][1] in the plan so it only rewinds the `TestCaseTemp` rows, not the entire unioned result. You could also add appropriate indexing to remove the sort entirely.\r\n\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=HJ--KIcxL	2020-01-15 08:33:00.377816+00	9	4	1	257327	0	0	0	2020-01-14 14:39:38.174643+00	\N	> is this a bug in SQL Server?	f	f
322	333	12	2019-12-05 22:53:29.788272+00	Not running the probe side of the join when the build is empty is an optimization. It is not available for *parallel row mode hash join* when the probe side has a child branch i.e. when there is an exchange operator.\r\n\r\nThere was a similar report many years ago by Adam Machanic on the now-defunct Connect feedback site. The scenario was a start-up Filter on the probe side, which ran its child operators unexpectedly. The answer from Microsoft was that the engine requires a guarantee that certain structures are initialized, and the only sane way to enforce that was to ensure the probe side operators are opened.\r\n\r\nMy own recollection of the details is that not initializing a sub-tree led to hard-to-fix parallel timing bugs. Ensuring the child branch started up was a work around for those problems.\r\n\r\nBatch mode hash join does not have this side effect because the way threads are managed is different.\r\n\r\nIn your particular case, the effect is more pronounced because the hash aggregate is blocking; it consumes its entire input during the iterator's Open() call. When there are only streaming operators on the probe side, the performance impact will often be more limited, depending on how much work is required to return the first row to the probe side of the hash join.	2019-12-06 09:49:06.988574+00	3	4	2	\N	0	0	0	\N	\N	Not running the probe side of the join when the build is empty is an optimization. It is not available for *parallel row mode hash join* when the probe side has a child branch i.e. when there is an exchange operator.	f	f
246	284	16	2019-06-26 18:49:09+00	XML is bonkers\r\n--\r\n\r\nWhen you add the concatenated string, you lose the "path element".\r\n\r\nFor example if you do this:\r\n\r\n    SELECT t.type + '/' AS type\r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML PATH('');\r\n\r\n    SELECT t.type + '/' \r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML PATH('type');\r\n\r\nYou get this back:\r\n\r\n``` xml\r\n<type>Green/</type>\r\n<type>Blue/</type>\r\n<type>Red/</type>\r\n```\r\n\r\nThe column name or alias acts as the path element. \r\n\r\nSome other examples that might help\r\n--\r\n\r\nUsing `RAW, ELEMENTS`\r\n\r\n    SELECT t.type + '/'\r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML RAW, ELEMENTS;\r\n    \r\n    SELECT t.type + '/' AS type\r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML RAW, ELEMENTS;\r\n\r\nIn the first example, you get the generic "row" element name, but in the second you get row/type.\r\n\r\nWhen using `RAW, TYPE`:\r\n\r\n    SELECT t.type + '/' AS type\r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML RAW, TYPE;\r\n    \r\n    SELECT t.type + '/'\r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML RAW, TYPE;\r\n\r\nThe first query returns valid-ish XML, the second throws an error because the path element lacks an identifier. \r\n\r\nUsing `AUTO`, the table alias and column name turns into the path:\r\n\r\n    SELECT type + '/' AS type\r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML AUTO;\r\n    \r\n    SELECT type \r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML AUTO;\r\n\r\nBut without an alias, you get a similar error:\r\n\r\n    SELECT type + '/'\r\n    FROM   ( VALUES ( 'Green' ), ( 'Blue' ), ( 'Red' )) AS t ( type )\r\n    FOR XML AUTO;\r\n\r\nI'd gin up an example with `FOR XML EXPLICIT` but it would be irresponsible for me to start drinking right now.	2019-12-04 14:28:55.059434+00	1	4	1	241486	0	0	0	2019-12-04 14:26:06.721346+00	\N	XML is bonkers	f	f
794	500	2	2020-02-10 17:27:06.771688+00	@@@ answer 365	2020-02-10 17:27:06.771688+00	0	1	1	\N	0	0	0	\N	\N	@@@ answer 365	t	f
188	245	12	2019-02-01 12:15:45+00	I was able to reproduce this, both with an empty statistic, and a populated statistic. I arranged for an automatic statistic to be created on an empty table, and the index was created later:\r\n\r\n```\r\nIF OBJECT_ID(N'dbo.Heap', N'U') IS NOT NULL\r\nBEGIN\r\n    DROP TABLE dbo.Heap;\r\nEND;\r\nGO\r\nCREATE TABLE dbo.Heap \r\n(\r\n    id integer NOT NULL IDENTITY,\r\n    val integer NOT NULL,\r\n);\r\nGO\r\n-- Add 1000 rows\r\nINSERT dbo.Heap\r\n    WITH (TABLOCKX)\r\n    (val)\r\nSELECT\r\n    SV.number\r\nFROM master.dbo.spt_values AS SV\r\nWHERE\r\n    SV.[type] = N'P'\r\n    AND SV.number BETWEEN 1 AND 1000;\r\nGO\r\nSELECT COUNT_BIG(*) \r\nFROM dbo.Heap AS H\r\nJOIN dbo.Heap AS H2\r\n    ON H2.id = H.id\r\nWHERE H.id > 0\r\nAND H2.id > 0;\r\nGO\r\n-- Empty table\r\nTRUNCATE TABLE dbo.Heap;\r\nGO\r\n-- Repeat exact same query (RT = 500 + 0.2 * 1000 = 700)\r\nGO\r\nSELECT COUNT_BIG(*) \r\nFROM dbo.Heap AS H\r\nJOIN dbo.Heap AS H2\r\n    ON H2.id = H.id\r\nWHERE H.id > 0\r\nAND H2.id > 0;\r\nGO\r\n-- Add 1000 rows\r\nINSERT dbo.Heap\r\n    WITH (TABLOCKX)\r\n    (val)\r\nSELECT\r\n    SV.number\r\nFROM master.dbo.spt_values AS SV\r\nWHERE\r\n    SV.[type] = N'P'\r\n    AND SV.number BETWEEN 1 AND 1000;\r\nGO\r\n-- Add index\r\nALTER TABLE dbo.Heap ADD \r\n    CONSTRAINT [PK dbo.Heap id]\r\n    PRIMARY KEY NONCLUSTERED (id);\r\nGO\r\nSELECT\r\n    S.[name],\r\n    S.auto_created,\r\n    DDSP.stats_id,\r\n    DDSP.last_updated,\r\n    DDSP.[rows],\r\n    DDSP.rows_sampled,\r\n    DDSP.steps,\r\n    DDSP.unfiltered_rows,\r\n    DDSP.modification_counter\r\nFROM sys.stats AS S\r\nCROSS APPLY sys.dm_db_stats_properties(S.[object_id], S.stats_id) AS DDSP\r\nWHERE \r\n    S.[object_id] = OBJECT_ID(N'dbo.Heap', N'U');\r\n```\r\n\r\n[![Output][1]][1]\r\n\r\nI found that modifications continue to be tracked accurately on all non-empty duplicates, but only one statistic is updated automatically (regardless of the asynchronous setting).\r\n\r\nAutomatic statistics updates only occur when the query optimizer needs a particular statistic, and finds that it is out of date (an optimality-related recompile).\r\n\r\nThe optimizer chooses from duplicate statistics as mentioned in the [Plan Caching and Recompilations in SQL Server 2012][2] paper:\r\n\r\n> An issue not directly related to the topic of this document is: given multiple statistics on the same set of columns in the same order, how does the query optimizer decide which ones to load during query optimization? The answer is not simple, but the query optimizer uses such guidelines as: Give preference to recent statistics over older statistics; Give preference to statistics computed using `FULLSCAN` option to those computed using sampling; and so on.\r\n\r\nThe point being that the optimizer chooses **one** of the available duplicate statistics (the "best one), and that one is automatically updated if found to be stale.\r\n\r\nI believe this is a change in behaviour from older releases - or at least the documentation suggests that all out-of-date statistics for an object would be updated as part of this process, but I have no idea when this changed. It was certainly after August 2013 when Matt Bowler posted [Duplicate Statistics][3], which contains a handy AdventureWorks-based repo. That script now results in only one of the statistics objects being updated, while at the time both were.\r\n\r\nThe above explanation matches all the behaviours I observed while attempting to reproduce your scenario, but I doubt it is explicitly documented anywhere. It does seem like a sensible optimization, since there is little value in keeping duplicates fully updated.\r\n\r\nThis is probably all at a level of detail below that which Microsoft are willing to support. This also means it could change without notice.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/H9iip.png\r\n  [2]: https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2012/dn148262(v=msdn.10)\r\n  [3]: https://mattsql.wordpress.com/2013/08/28/duplicate-statistics/	2019-12-02 19:02:38.082366+00	0	4	1	228673	0	0	0	2019-12-02 19:02:38.082366+00	\N	I was able to reproduce this, both with an empty statistic, and a populated statistic. I arranged for an automatic statistic to be created on an empty table, and the index was created later:	f	f
441	434	88	2017-02-08 21:48:21+00	This is the same as:\r\n\r\n    (\r\n      SELECT id AS delete_test_backup\r\n    )\r\n\r\nIn other words, you have merely assigned a column alias to `id`, and there is no reference at all to a table called `delete_test_backup`.\r\n\r\nAccording to the ANSI standard, what SQL Server is supposed to do in this case - when it has a subquery and does not find `id` at that scope - is traverse to the outer scope(s) until it finds one. If it does, it assumes that's the one you meant.\r\n\r\nSee:\r\n\r\n- [Best practices : Properly referencing columns](https://blogs.sentryone.com/aaronbertrand/best-practices-referencing-columns/)\r\n- ~~KB #298674~~\r\n- ~~Connect #735178~~\r\n- ~~Connect #624370~~\r\n- ~~Connect #392492~~\r\n- ~~Connect #362016~~\r\n- ~~Connect #265772~~\r\n- ~~Connect #772612~~\r\n\r\n> _* Microsoft decided to eradicate all of Connect instead of archiving the content, and also disappeared the knowledge base article for some reason._\r\n\r\nTo avoid the problem, always use table aliases, which allow you to be 100% explicit about which table a column comes from (not naming every key column `id` helps, too). Of course you can leave these prefixes out accidentally, just like you left out the `FROM`, but the following syntax would have failed as you expect, and most other omissions would have left you with a parsing error rather than accidental execution:\r\n\r\n    (\r\n      SELECT x.id FROM delete_test_backup AS x\r\n    )	2019-12-12 22:40:25.190389+00	2	4	1	163657	0	0	0	2019-12-12 22:39:06.771104+00	\N	This is the same as:	f	f
270	298	16	2018-04-25 13:32:34+00	**Hold on hold on hold on**\r\n\r\nWhile the performance and licensing aspects are interesting, they're not the only aspect of a workload to consider. \r\n\r\nOne thing that can have an impact on processor choice is worker threads.\r\n\r\n**Worker Threads?**\r\n\r\nYeah buddy! They're the things that your SQL Server will use to run your queries and do all the background stuff that it needs to do to keep things in shape.\r\n\r\nWhen you run out of worker threads, you hit [THREADPOOL][1] waits\r\n\r\n**THREADPOOL?**\r\n\r\nTHREADPOOL. This is one of the nastiest waits you can have on your server, along with [RESOURCE_SEMAPHORE and RESOURCE_SEMAPHORE_QUERY_COMPILE][2]. But those are memory waits, and this is a CPU question. \r\n\r\nSo back to why this is wiggity wack.\r\n\r\n[This is how SQL Server calculates worker threads][3]:\r\n\r\n[![NUTS][4]][4]\r\n\r\nNotice how doubling core counts doesn't double Max Worker Threads, and you get the same number with 1 core as you do with 4 cores? The equation is: `512 + ((logical CPUs - 4) * 16)`\r\n\r\nThat's a shame, because when core counts go up, clock speed usually nosedives to a generation or two back.\r\n\r\n[![NUTS][5]][5]\r\n\r\nTaking a look at any [recent line of Intel chips][6] will show a similar trend.\r\n\r\n**How do I know how many threads I need?**\r\n\r\nThis will depend a lot on:\r\n\r\n - Number of Users\r\n - Number of parallel queries\r\n - Number of serial queries\r\n - Number of databases and data synchronization (Mirroring, AGs, backups for Log Shipping)\r\n - If you leave MAXDOP and CTFP at the defaults\r\n\r\nIf you're not running out of them today, you're probably okay. \r\n\r\n**But how do you know if you are?**\r\n\r\nThere are good questions, and there are great questions, and lemme tell you something, that is a *GREAT QUESTION*.\r\n\r\nTHREADPOOL can manifest as [connection issues][7], and you may see messages in the error log about not being able to [spawn a thread][8].\r\n\r\nYou can also look at your server's wait stats using a free tool like [sp_Blitz or sp_BlitzFirst][9] (full disclosure, I contribute to this project).\r\n\r\n`EXEC sp_Blitz`\r\n\r\n[![NUTS][10]][10]\r\n\r\n`EXEC sp_BlitzFirst @SinceStartup = 1`\r\n\r\n[![NUTS][11]][11]\r\n\r\n**Can't I just increase Max Worker Threads?**\r\n\r\nIncreasing MWT can lead to increased `SOS_SCHEDULER_YIELD` waits. \r\n\r\nThat's not the end of the world, but think of it like adding a buncha screaming kids to a teacher's class. \r\n\r\nAll of a sudden, it's gonna be harder for each kid to get attention. \r\n\r\nWhen a process [exhausts its 4ms quantum][12], there will potentially be more threads ahead of it waiting to get on the CPU. \r\n\r\nPerformance might feel about the same.\r\n\r\n**How can I use fewer Worker Threads?**\r\n\r\nYou cruel [noun] of a [noun], those are workers with families to support! Mortgages! Dreams! \r\n\r\nBut alright, gotta respect the bottom line. You're the boss.\r\n\r\nThe easiest place to start is changing settings like MAXDOP and Cost Threshold For Parallelism from the defaults. \r\n\r\nIf you have questions about how to set those, head over here:\r\n\r\n - [MAXDOP setting algorithm for SQL Server][14]\r\n\r\n - [Why Cost Threshold For Parallelism Shouldn’t Be Set To 5][13]\r\n\r\nAfter that, your job gets a lot tougher. You've gotta figure out what's using all those threads. You can sometimes do that by looking at your wait stats. \r\n\r\nMore specifically, if you've got high waits on parallelism (`CXPACKET`) AND high waits on locks (`LCK_`), then you may be running into long blocking chains involving parallel queries. \r\n\r\nYou know what stinks? While all those parallel queries are waiting to get their locks, they don't give their allocated threads back. \r\n\r\nYou can almost hear that four core VM that your admin assured you was more than enough for any workload gasping for air, huh?\r\n\r\nUnfortunately, the type of query and index tuning you have to do to resolve that stuff is beyond the scope of the question. \r\n\r\nHope this helps!\r\n\r\n\r\n  [1]: https://www.sqlskills.com/help/waits/threadpool/\r\n  [2]: https://www.brentozar.com/archive/2018/01/pass-summit-2017-video-worked-development/\r\n  [3]: https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/configure-the-max-worker-threads-server-configuration-option?view=sql-server-2017\r\n  [4]: https://i.stack.imgur.com/hxsuq.png\r\n  [5]: https://i.stack.imgur.com/eGYRZ.png\r\n  [6]: https://ark.intel.com/products/series/93797/Intel-Xeon-Processor-E7-v4-Family\r\n  [7]: https://www.brentozar.com/archive/2018/01/network-issue-threadpool-waits/\r\n  [8]: https://social.msdn.microsoft.com/Forums/sqlserver/en-US/d511a15e-8f35-400c-a02e-4aa63dbc0ba9/sql-server-failed-with-error-code-0xc0000000-to-spawn-a-threaderror?forum=sqldatabaseengine\r\n  [9]: http://firstresponderkit.org\r\n  [10]: https://i.stack.imgur.com/UFmhq.jpg\r\n  [11]: https://i.stack.imgur.com/NZJWx.png\r\n  [12]: https://www.sqlskills.com/help/waits/sos_scheduler_yield/\r\n  [13]: https://www.brentozar.com/archive/2017/03/why-cost-threshold-for-parallelism-shouldnt-be-set-to-5/\r\n  [14]: https://dba.stackexchange.com/questions/36522/maxdop-setting-algorithm-for-sql-server	2019-12-04 22:50:19.045524+00	4	4	1	204912	0	0	0	2019-12-04 22:50:19.045524+00	\N	**Hold on hold on hold on**	f	f
269	298	37	2017-04-04 20:05:02+00	The general rule of thumb is keep the core count as low as possible, and the processor speed as high as possible.  The licensing math on that proves the point at ~$7,500 USD per core for Expensive Edition.\r\n\r\nBuying the correct hardware can pay for itself in reduced licensing costs. See [Processor Selection for SQL Server][1] by Glenn Berry. It's a great resource on how to choose a processor for SQL Server.\r\n\r\n  [1]: https://www.sqlskills.com/blogs/glenn/sqlskills-sql101-processor-selection-for-sql-server/\r\n\r\nOnce you take into consideration SQL Server's per-core licensing structure, it makes sense to always go with the fastest processor speed available, regardless of workload type, whether OLTP or analytics.  Having the fastest possible core speed is never going to be a problem.  Increase the core count as required, but never do that by reducing core speed.  \r\n\r\nIn other words, don't think of 16 x 2.2Ghz processors being the same as 8 x 4.5Ghz processors.  The cost savings of using the 2.2Ghz processors over the 4.5Ghz processors is likely to be a maximum of about $10,000 USD (for a typical Xeon-based two processor machine).  Jumping from 8 cores to 16 cores with SQL Server Enterprise Edition is likely to cost over $60,000 USD in licensing fees.  In other words, you might save $10,000 in hardware costs, but you'll lose an extra $50,000 in licensing.\r\n\r\nIf you decide you need a lot of parallel processing muscle, and decide you need 32 cores for the task at hand, going with the fastest cores will pay dividends in reduced processing time.  Nobody will fault you for that.\r\n\r\nHaving said all that, if the choice is one CPU or more than one CPU, always go with *more than one*.  Running SQL Server (or any DBMS) on a single CPU can cause all kinds of problems since the capability for concurrent operations is vastly limited.	2019-12-04 22:50:18.717751+00	3	4	1	169135	0	0	0	2019-12-04 22:50:18.717751+00	\N	The general rule of thumb is keep the core count as low as possible, and the processor speed as high as possible.  The licensing math on that proves the point at ~$7,500 USD per core for Expensive Edition.	f	f
409	407	288	2016-06-17 19:14:18+00	The sum of operator costs is more than 100% in execution plan is a [known bug and is closed as by design][1]!\r\n\r\n>Posted by Microsoft on 11/17/2010 at 5:28 PM\r\n>\r\n>Thanks for taking the time to file this observation and simple repro. The strange cost percentage values that you observed are an aritifact of the specific structure of the query plan that are a bit confusing but ultimately do make sense. They will not adversely affect the running of the query in any way.\r\n>\r\n>The concatenation operator has two children - the table scans. However the server expects that it will not be necessary to execute the second table scan. This is because it expects to find a row from the first table scan which would satisfy the "exists" part of the query. Therefore it does not include the cost of the second table scan when computing the total cost of the subtree rooted at the concatenation. However it still estimates and reports the cost of the second table scan, which the management tool the computes as a percentage of the total query cost as if it will be executed.\r\n>\r\n>In this case of course, it would be necessary to run the second table scan since the first is empty. However the engine operates on the conservative assumption that there will always be at least one row (which may only have been added after the query was compiled).\r\n>\r\n>Campbell  \r\n>SQL Development\r\n\r\nAaron Bertrand filed a similar bug - [SSMS : Execution plan sometimes exceeds 100%][2]\r\n\r\n>Posted by Microsoft on 5/28/2008 at 11:58 AM\r\n>\r\n>Thanks again for bringing this to our attention. This anomaly arise in the presence of "row goals" and the concatenation operator. Row goals is the term for when a subtree (perhaps the entire query) is not required to return all possible rows. This most commonly happens when the query has a "top" clause although there are other causes. The anomaly arises because we do not expect to execute the second child of the Concatenation (because we expect the first child to provide all the required rows). Those additional children are costed to assume they return a single row. The only alternative would be to give them 0 cost which would fix the anomaly but probably create other confusions and would lead the optimizer to not care about the plans - we don't want to do that just in case they are executed.\r\n>\r\n>Campbell Fraser, SQL Development.\r\n\r\nIf you want to understand [how plan costing works, Paul White explains it at his best here.][3]\r\n\r\nFrom the query processor team - [What’s this cost?][4]\r\n\r\nGeneral guidelines for better performance -\r\n\r\n - Have proper indexes & up-to-date stats.\r\n - Fetch data that is required ONLY along with relevant columns.\r\n - Have proper schema.table with proper indexes & datatypes \r\n - Use statistics time, IO before and after you make changes to measure the gain.\r\n - Get help / guidance from experts at this site :-)\r\n\r\nThat's why I use [SQL Sentry's FREE Plan Explorer][5]\r\n\r\n  [1]: https://web.archive.org/web/20131118011039/http://connect.microsoft.com/SQLServer/feedback/details/621330/sum-of-operator-costs-is-more-than-100-in-the-execution-plans\r\n  [2]: https://web.archive.org/web/20131119063847/https://connect.microsoft.com/SQLServer/feedback/details/267530/ssms-execution-plan-sometimes-exceeds-100\r\n  [3]: https://sqlkiwi.blogspot.com/2010/09/inside-the-optimizer-plan-costing.html\r\n  [4]: https://blogs.msdn.microsoft.com/sqlqueryprocessing/2006/10/11/whats-this-cost/\r\n  [5]: https://www.sentryone.com/plan-explorer	2019-12-11 04:32:37.659466+00	2	4	1	141566	0	0	0	2019-12-11 04:31:44.970624+00	\N	The sum of operator costs is more than 100% in execution plan is a [known bug and is closed as by design][1]!	f	f
404	402	90	2018-02-27 14:02:16+00	The calculation of the compute scalar operator is deferred to when it is actually used (the probe residual of Node ID = 1) and the rows that would have failed for you are filtered out by the previous Hash Match (Node ID = 3).\r\n\r\nHave a look at [Compute Scalars, Expressions and Execution Plan Performance](https://www.sql.kiwi/2012/09/compute-scalars-expressions-and-execution-plan-performance.html) by Paul White for more information.\r\n\r\nThere is no indication in execution plans that evaluation of an expression is deferred, or when (at which node) it was actually evaluated. You can normally code defensively around this using `CASE` (which comes with an evaluation order guarantee) and/or `TRY_CONVERT`.\r\n\r\nRelated Microsoft feedback: [SQL Server should not raise illogical errors][1]\r\n\r\n\r\n  [1]: https://feedback.azure.com/forums/908035-sql-server/suggestions/32912431-sql-server-should-not-raise-illogical-errors	2019-12-11 04:19:43.712599+00	2	4	1	198913	0	0	0	2019-12-11 04:18:19.79497+00	\N	The calculation of the compute scalar operator is deferred to when it is actually used (the probe residual of Node ID = 1) and the rows that would have failed for you are filtered out by the previous Hash Match (Node ID = 3).	f	f
436	424	14	2019-12-12 14:43:08.091493+00	In this specific case, the difference between the two plans can be explained by a *poor estimate* caused by local variables.  Kendra Little talks about this here: [Why Local Variables are Bad for Tuning Stored Procedures](https://www.brentozar.com/archive/2014/06/tuning-stored-procedures-local-variables-problems/)\r\n\r\n> Local variables effectively “anonymize” the value being passed in on a parameter in SQL Server. It’s like you’re using Optimize for Unknown all the time.\r\n\r\nThe stored procedure version shows a 1-row estimate on the (forward) clustered index seek:\r\n\r\n![sp-run.png](/image?hash=0d39a2e876c0a5cf6adadbb9131cfbfb8b742afdb038a90acd78da353f02f92c)\r\n\r\nThe optimizer is okay with "sorting" these results, because there will be so few, and letting the `TOP` operator do the rest.\r\n\r\nThe local variable version shows a 100-row *guess* on the (backward) clustered index seek:\r\n\r\n![local-var-run.png](/image?hash=47d18ddec7e8cd11b50ae769ea4fb0f836b60360acbbd128ce97669b1339f438)\r\n\r\nBecause the potential number of rows is higher, the optimizer decides to read the index backwards rather than sorting.\r\n\r\nYou can get the forward scan plan with the ad-hoc query by adding `OPTION (RECOMPILE)` or using the literal values in the query.\r\n\r\n---\r\n\r\nAs you noted in your question, the non-deterministic results are because of the lack of a unique sort column to be used for paging, and the different access patterns in the different plans.  In this case, the second plan isn't parallel at all really, and accesses the clustered index backwards from the first plan, which explains why you might see different results.\r\n\r\n---\r\n\r\nAs a funny side note, the odd no-op parallel branch in the second plan is due to a `FILTER` operator that was present in the plan at some point during optimization, but was later pushed into the seek as a residual predicate.  You can see the parallel `FILTER` by enabling trace flag 9130 to prevent the predicate push from happening:\r\n\r\n![filter-run.png](/image?hash=7d7797b7526203ad69247a0f2d24fc7b8ec1eae24b601d6ba8e76130ab537140)	2019-12-12 14:43:08.091493+00	7	4	1	\N	0	0	0	\N	\N	In this specific case, the difference between the two plans can be explained by a *poor estimate* caused by local variables.  Kendra Little talks about this here: [Why Local Variables are Bad for Tuning Stored Procedures](https://www.brentozar.com/archive/2014/06/tuning-stored-procedures-local-variables-problems/)	f	f
498	465	14	2018-12-04 21:15:03+00	# Yes!\r\n\r\nFor various reasons, it's always better to have the filtering column as part of the index: either in the keys, or in the includes\r\n\r\nThe following are some specific examples of filtered index query problems that are resolved by including the filtering columns in the index.\r\n\r\n## Key lookups when the query predicate doesn't match the filter expression\r\n\r\nFirst of all, [the documentation][1] has this to say about including filter expression columns:\r\n\r\n> - A column in the filtered index expression should be a key or included column in the filtered index definition if the query predicate uses the column in a comparison that is not equivalent to the filtered index expression.\r\n\r\nSo if you have an inequality filter expression like `Reputation > 400000`, but your *query* uses a predicate like `WHERE Reputation > 400000 AND Reputation < 450000;`, the filtered index might still be used - but a key lookup will be required to satisfy the query's predicate.\r\n\r\nIncluding the `Reputation` column in the index (key or includes) removes the need for this lookup.\r\n\r\nSee Erik Darling's post [Filtered Indexes: Just Add Includes][2] for additional details and an example of this situation.\r\n\r\nAnother example of this can be found in Paul White's answer here: [Unnecessary key lookup being performed while using filtered index][3]\r\n\r\n## Key lookups when the filtering column is included in the resultset\r\n\r\nThe documentation goes on to say this:\r\n\r\n> - A column in the filtered index expression should be a key or included column in the filtered index definition if the column is in the query result set.\r\n\r\nThis might feel like it goes without saying, but just to be complete: if your queries include the filtering column in the final resultset, you should probably include them in the index (key or includes).\r\n\r\n## Poor row estimates when using equality expressions\r\n\r\nThere are cases where useful row estimates based on actual statistics can be eliminated during the optimization process (specifically when the query plan produced by the optimizer is converted to a physical execution plan).  Including the filtering column can prevent these more-accurate estimates from being discarded.\r\n\r\nMore details, and an example, can be found in Paul White's answer here: [Incorrect row estimation given with a filtered index][4]\r\n\r\nAn additional example can be found here on dba.se: [Query using a filtered index but wrong estimated number of rows][6]\r\n\r\n## Key lookups when using `IS NULL` in the filtering expression\r\n\r\nCreating an index with a filtering expression that uses `IS NULL` can produce a completely unnecessary key lookup.  See this question, and the related bug report on SQL Server's feedback site: [Why filtered index on IS NULL value is not used?][5]\r\n\r\nAs you might have guessed, the workaround presented is to add the filtering column as an included column in the filtered index.\r\n\r\n[1]: https://docs.microsoft.com/en-us/sql/relational-databases/indexes/create-filtered-indexes?view=sql-server-2017\r\n[2]: https://www.brentozar.com/archive/2015/12/filtered-indexes-just-add-includes/\r\n[3]: https://answers.sqlperformance.com/questions/2894/unnecessary-key-lookup-being-performed-while-using.html\r\n[4]: https://answers.sqlperformance.com/questions/336/incorrect-row-estimation-given-with-a-filtered-ind.html\r\n[5]: https://dba.stackexchange.com/questions/217046/why-filtered-index-on-is-null-value-is-not-used\r\n[6]: https://dba.stackexchange.com/questions/208897/query-using-a-filtered-index-but-wrong-estimated-number-of-rows	2019-12-18 21:54:19.539903+00	4	4	1	224150	0	0	0	2019-12-18 21:54:19.539903+00	\N	Yes!	f	f
538	482	751	2018-12-18 15:59:05+00	You can see the role of this aggregate if no rows match the `WHERE` clause.\r\n\r\n    SELECT MAX(Revision)\r\n    FROM   dbo.TheOneders\r\n    WHERE  Id = 1\r\n           AND 1 = 1 /*To avoid auto parameterisation*/\r\n           AND Id%3 = 4  /*always false*/\r\n\r\nIn that case zero rows go into the aggregate but it still emits one as the correct semantics are to return `NULL` in this case.\r\n\r\n[![enter image description here][1]][1]\r\n\r\nThis is a scalar aggregate as opposed to a vector one. \r\n\r\nYour "logically equivalent" query is not equivalent. Adding `GROUP BY Id` would make it a vector aggregate and then the correct behaviour would be to return no rows.\r\n\r\nSee [Fun with Scalar and Vector Aggregates][2] for more about this.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/CSvmw.png\r\n  [2]: https://sqlkiwi.blogspot.com/2012/03/fun-with-aggregates.html	2019-12-24 10:49:51.113603+00	7	4	1	225283	0	0	0	2019-12-24 10:49:51.113603+00	\N	You can see the role of this aggregate if no rows match the `WHERE` clause.	f	f
229	273	16	2017-09-27 20:33:33+00	One common method is to use the `VALUES` clause, and `CROSS APPLY` the two columns aliased as a single column, then get the `MIN` and `MAX` of each.\r\n\r\n    SELECT MIN(x.CombinedDate) AS least, MAX(x.CombinedDate) AS greatest\r\n    FROM   dbo.Users AS u\r\n    CROSS APPLY ( VALUES ( u.CreationDate ), ( u.LastAccessDate )) AS x ( CombinedDate );\r\n\r\nThere are other ways of writing it, for example using `UNION ALL`\r\n\r\n    SELECT MIN(x.CombinedDate) AS least, MAX(x.CombinedDate) AS greatest\r\n    FROM   dbo.Users AS u\r\n    CROSS APPLY ( SELECT u.CreationDate UNION ALL SELECT u.LastAccessDate ) AS x(CombinedDate);\r\n\r\n\r\nHowever, the resulting [query plans][1] seem to be the same.\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=r1JWjFYiZ	2019-12-04 14:17:44.839211+00	2	4	1	187095	0	0	0	2019-12-04 14:17:44.839211+00	\N	One common method is to use the `VALUES` clause, and `CROSS APPLY` the two columns aliased as a single column, then get the `MIN` and `MAX` of each.	f	f
466	449	622	2013-04-04 18:11:42+00	The following C# code solves the problem:\r\n\r\n        var connString =\r\n            "Initial Catalog=MyDb;Data Source=MyServer;Integrated Security=SSPI;Application Name=Benchmarks;";\r\n\r\n        var stopWatch = new Stopwatch();\r\n        stopWatch.Start();\r\n\r\n        using (var conn = new SqlConnection(connString))\r\n        {\r\n            conn.Open();\r\n            var command = conn.CreateCommand();\r\n            command.CommandText = "dbo.GetAllTaskEvents";\r\n            command.CommandType = CommandType.StoredProcedure;\r\n            var gaps = new List<string>();\r\n            using (var dr = command.ExecuteReader())\r\n            {\r\n                var currentEvents = 0;\r\n                var gapStart = new DateTime();\r\n                var gapStarted = false;\r\n                while (dr.Read())\r\n                {\r\n                    var change = dr.GetInt32(1);\r\n                    if (change == -1 && currentEvents == 1)\r\n                    {\r\n                        gapStart = dr.GetDateTime(0);\r\n                        gapStarted = true;\r\n                    }\r\n                    else if (change == 1 && currentEvents == 0 && gapStarted)\r\n                    {\r\n                        gaps.Add(string.Format("({0},{1})", gapStart, dr.GetDateTime(0)));\r\n                        gapStarted = false;\r\n                    }\r\n                    currentEvents += change;\r\n                }\r\n            }\r\n            File.WriteAllLines(@"C:\\Temp\\Gaps.txt", gaps);\r\n        }\r\n\r\n        stopWatch.Stop();\r\n        System.Console.WriteLine("Elapsed: " + stopWatch.Elapsed);\r\n\r\nThis code invokes this stored procedure:\r\n\r\n    CREATE PROCEDURE dbo.GetAllTaskEvents\r\n    AS \r\n      BEGIN ;\r\n        SELECT  EventTime ,\r\n                Change\r\n        FROM    ( SELECT  StartedAt AS EventTime ,\r\n                          1 AS Change\r\n                  FROM    dbo.Tasks\r\n                  UNION ALL\r\n                  SELECT  FinishedAt AS EventTime ,\r\n                          -1 AS Change\r\n                  FROM    dbo.Tasks\r\n                ) AS TaskEvents\r\n        ORDER BY EventTime, Change DESC ;\r\n      END ;\r\n    GO\r\n\r\nIt finds and prints one gap in 2M intervals in the following times, warm cache:\r\n\r\n    1 gap: Elapsed: 00:00:01.4852029 00:00:01.4444307 00:00:01.4644152\r\n\r\nIt finds and prints 2M-1 gaps in 2M intervals in the following times, warm cache:\r\n\r\n    2M-1 gaps Elapsed: 00:00:08.8576637 00:00:08.9123053 00:00:09.0372344 00:00:08.8545477\r\n\r\nThis is a very simple solution - it took me 10 minutes to develop. A recent college graduate can come up with it. On the database side, execution plan is a trivial merge join which uses very little CPU and memory. \r\n\r\n**Edit:** to be realistic, I am running client and server on separate boxes.	2019-12-15 06:20:35.960313+00	3	4	1	39275	0	0	0	2019-12-15 06:20:35.960313+00	\N	The following C# code solves the problem:	f	f
467	449	623	2013-04-04 19:42:04+00	And a 1 second solution...\r\n        \r\n    ;WITH cteSource(StartedAt, FinishedAt)\r\n    AS (\r\n    \tSELECT\t\ts.StartedAt,\r\n    \t\t\te.FinishedAt\r\n    \tFROM\t\t(\r\n    \t\t\t\tSELECT\tStartedAt,\r\n    \t\t\t\t\tROW_NUMBER() OVER (ORDER BY StartedAt) AS rn\r\n    \t\t\t\tFROM\tdbo.Tasks\r\n    \t\t\t) AS s\r\n    \tINNER JOIN\t(\r\n    \t\t\t\tSELECT\tFinishedAt,\r\n    \t\t\t\t\tROW_NUMBER() OVER (ORDER BY FinishedAt) + 1 AS rn\r\n    \t\t\t\tFROM\tdbo.Tasks\r\n    \t\t\t) AS e ON e.rn = s.rn\r\n    \tWHERE\t\ts.StartedAt > e.FinishedAt\r\n    \r\n    \tUNION ALL\r\n    \r\n    \tSELECT\tMIN(StartedAt),\r\n    \t\tMAX(FinishedAt)\r\n    \tFROM\tdbo.Tasks\r\n    ), cteGrouped(theTime, grp)\r\n    AS (\r\n    \tSELECT\tu.theTime,\r\n    \t\t(ROW_NUMBER() OVER (ORDER BY u.theTime) - 1) / 2\r\n    \tFROM\tcteSource AS s\r\n    \tUNPIVOT\t(\r\n    \t\t\ttheTime\r\n    \t\t\tFOR theColumn IN (s.StartedAt, s.FinishedAt)\r\n    \t\t) AS u\r\n    )\r\n    SELECT\t\tMIN(theTime),\r\n    \t\tMAX(theTime)\r\n    FROM\t\tcteGrouped\r\n    GROUP BY\tgrp\r\n    ORDER BY\tgrp\r\n\r\n	2019-12-15 06:20:36.229823+00	2	4	1	39284	0	0	0	2019-12-15 06:20:36.229823+00	\N	And a 1 second solution...	f	f
468	449	623	2013-04-04 19:10:45+00	Here is a solution which runs in 4 seconds.\r\n\r\n         \r\n    WITH cteRaw(ts, type, e, s)\r\n    AS (\r\n    \tSELECT\tStartedAt,\r\n    \t\t1 AS type,\r\n    \t\tNULL,\r\n    \t\tROW_NUMBER() OVER (ORDER BY StartedAt)\r\n    \tFROM\tdbo.Tasks\r\n    \r\n    \tUNION ALL\r\n    \r\n    \tSELECT\tFinishedAt,\r\n    \t\t-1 AS type, \r\n    \t\tROW_NUMBER() OVER (ORDER BY FinishedAt),\r\n    \t\tNULL\r\n    \tFROM\tdbo.Tasks\r\n    ), cteCombined(ts, e, s, se)\r\n    AS (\r\n    \tSELECT\tts,\r\n    \t\te,\r\n    \t\ts,\r\n    \t\tROW_NUMBER() OVER (ORDER BY ts, type DESC)\r\n    \tFROM\tcteRaw\r\n    ), cteFiltered(ts, grpnum)\r\n    AS (\r\n    \tSELECT\tts, \r\n    \t\t(ROW_NUMBER() OVER (ORDER BY ts) - 1) / 2 AS grpnum\r\n    \tFROM\tcteCombined\r\n    \tWHERE\tCOALESCE(s + s - se - 1, se - e - e) = 0\r\n    )\r\n    SELECT\t\tMIN(ts) AS starttime,\r\n    \t\tMAX(ts) AS endtime\r\n    FROM\t\tcteFiltered\r\n    GROUP BY\tgrpnum;	2019-12-15 06:20:36.515067+00	0	4	1	39281	0	0	0	2019-12-15 06:20:36.515067+00	\N	Here is a solution which runs in 4 seconds.	f	f
469	449	624	2013-04-06 04:00:23+00	I think I have exhausted the limits of my knowledge in SQL server on this one....\r\n\r\nFor finding a gap in SQL server (what the C# code does), and you don't care about starting or ending gaps (those before the first start, or after the last finish), then the following query (or variants) is the fastest I could find:\r\n\r\n    SELECT e.FinishedAt as GapStart, s.StartedAt as GapEnd\r\n    FROM \r\n    (\r\n    \tSELECT StartedAt, ROW_NUMBER() OVER (ORDER BY StartedAt) AS rn\r\n    \tFROM dbo.Tasks\r\n    ) AS s\r\n    INNER JOIN  \r\n    (\r\n    \tSELECT  FinishedAt, ROW_NUMBER() OVER (ORDER BY FinishedAt) + 1 AS rn\r\n    \tFROM    dbo.Tasks\r\n    ) AS e ON e.rn = s.rn and s.StartedAt > e.FinishedAt\r\n\r\nWhich works though slight of hand that for each start-finish set, you can treat the start and finish as separate sequences, offset the finish by one and gaps are shown.\r\n\r\neg take (S1, F1), (S2, F2), (S3, F3), and order as: {S1, S2, S3, null} and {null, F1, F2, F3}\r\nThen compare row n to row n in each set, and gaps are where the F set value is less than the S set value... the problem I think is that in SQL server there is no way to join or compare two separate sets purely on the order of the values in the set... hence the use of the row_number function to allow us to merge based purely on row number... but there is no way to tell SQL server that these values are unique (without inserting them into a table var with an index on it - which takes longer - I tried it), so I think the merge join is less than optimal? (though hard to prove when it's faster than anything else I could do)\r\n\r\nI was able to get solutions using the LAG/LEAD functions:\r\n\r\n    select * from\r\n    (\r\n    \tSELECT top (100) percent StartedAt, FinishedAt, LEAD(StartedAt, 1, null) OVER (Order by FinishedAt) as NextStart\r\n    \tFROM dbo.Tasks\r\n    ) as x\r\n    where NextStart > FinishedAt\r\n(which by the way, I don't guarantee the results - it seems to work, but I think relies on StartedAt being in order in the Tasks table... and it was slower)\r\n\r\nUsing sum change:\r\n\r\n    select * from\r\n    (\r\n    \tSELECT EventTime, Change, SUM(Change) OVER (ORDER BY EventTime, Change desc ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as RunTotal --, x.*\r\n    \tFROM    \r\n    \t( \r\n    \t\tSELECT StartedAt AS EventTime, 1 AS Change\r\n    \t\tFROM dbo.Tasks\r\n    \tUNION ALL\r\n    \t\tSELECT  FinishedAt AS EventTime, -1 AS Change\r\n    \t\tFROM dbo.Tasks\r\n    \t) AS TaskEvents\r\n    ) as x\r\n    where x.RunTotal = 0 or (x.RunTotal = 1 and x.Change = 1)\r\n    ORDER BY EventTime, Change DESC\r\n(no surprise, also slower)\r\n\r\nI even tried a CLR aggregate function (to replace the sum - it was slower than sum and relied on row_number() to keep the order of the data), and CLR a table valued function (to open two result sets and compare values based purely on sequence)... and it too was slower.  I banged my head so many times on SQL, and CLR limitations, trying many other methods...\r\n\r\nAnd for what?\r\n\r\nRunning on the same machine, and spitting both the C# data, and SQL filtered data into a file (as per original C# code), the times are virtually the same.... approximately 2 seconds for the 1 gap data (C# usually faster), 8-10 seconds for the multi-gap data set (SQL usually faster).\r\n\r\n**NOTE**: Do not use the SQL Server Development Environment for timing comparison, as it's display to grid takes time.  As tested with SQL 2012, VS2010, .net 4.0 Client profile\r\n\r\nI will point out that both solutions perform pretty much the same sorting of data on the SQL server so the server load for fetch-sort will be similar, whichever solution you use, the only difference being the processing on the client (rather than server), and the transfer over the network.\r\n\r\nI don't know what the difference might be when partitioning by different staff members perhaps, or when you might need extra data with the gap information (though I can't think of much else other than a staff id), or of course if there is a **slow** data connection between the SQL server and client machine (or a **slow** client)...  Nor have I made a comparison of lock-times, or contention issues, or CPU/NETWORK issues for multiple users... So I don't know which one is more likely to be a bottleneck in this case.\r\n\r\nWhat I do know, is yes, SQL server is not good at this sort of set comparisons, and if you don't write the query right you will pay for it dearly.\r\n\r\nIs it easier or harder than writing the C# version?  I'm not entirely sure, the Change +/-1, running total solution is not entirely intuitive either, and I but it's not the first solution an average graduate would come to... once done it is easy enough to copy, but it takes insight to write in the first place... same can be said for the SQL version.\r\nWhich is harder?  Which is more robust to rogue data?  Which has more potential for parallel operations?  Does it really matter when the difference is so small compared to the programming effort?\r\n\r\nOne last note; there is an unstated constraint on the data - the StartedAt **must** be less than the FinishedAt, or you will get bad results.	2019-12-15 06:20:36.764266+00	0	4	1	39378	0	0	0	2019-12-15 06:20:36.764266+00	\N	I think I have exhausted the limits of my knowledge in SQL server on this one....	f	f
689	622	2	2020-01-27 22:38:28.946258+00	This only affects Firefox, and it wouldn't happen at all if we hadn't been experimenting with using community-themed colours for `scrollbar-color`.\r\n\r\nIt seems to be a bug with Firefox — I've confirmed this is visible even on [the MDN page about scrollbar-color](https://developer.mozilla.org/en-US/docs/Web/CSS/scrollbar-color) itself and reported it their BugZilla: https://bugzilla.mozilla.org/show_bug.cgi?id=1611854\r\n\r\nWe can workaround this bug in three ways: \r\n\r\n1. Ignore it until Firefox fixes the bug, as it is cosmetic.\r\n2. Stop using `scrollbar-color`.\r\n3. Use `scrollbar-width: thin;`, which doesn't display the arrows at all.\r\n\r\nWe are trialling option (3) at the moment, please let us know whether you think one of the other options would be a better bet.\r\n\r\n	2020-01-27 22:38:28.946258+00	4	1	1	\N	0	0	0	\N	\N	This only affects Firefox, and it wouldn't happen at all if we hadn't been experimenting with using community-themed colours for `scrollbar-color`.	f	f
82	110	12	2019-11-25 23:43:21.017364+00	`FAST_FORWARD` cursors [do not support parallelism][1] (though the server generating the plan would need to be 2012 or above to get `NonParallelPlanReason` as part of the showplan XML).\r\n\r\nWhen you specify `FAST_FORWARD`, [the optimizer chooses][2] between `STATIC` and `DYNAMIC` for you.\r\n\r\nThe provided execution plan shows the optimizer choosing a static-like plan.\r\nBecause the query contains aggregation, I doubt a dynamic cursor plan is even possible here. Nevertheless, requesting a `FAST_FORWARD` cursor type is preventing a parallel plan.\r\n\r\nYou should change the cursor type explicitly to either `STATIC` or `KEYSET`, for example. Both these cursor types can use parallelism.\r\n\r\nThat said, because this is an API cursor, changing the type of cursor would likely require an application change. Naturally, you would need to benchmark performance to check that changing the cursor type really is the best option for you.\r\n\r\n\r\n  [1]: http://blogs.msdn.com/b/psssql/archive/2013/08/29/10444849.aspx\r\n  [2]: http://blogs.msdn.com/b/sqlqueryprocessing/archive/2009/08/12/understanding-sql-server-fast-forward-server-cursors.aspx	2019-11-25 23:43:21.017364+00	1	4	2	\N	0	0	0	\N	\N	`FAST_FORWARD` cursors [do not support parallelism][1] (though the server generating the plan would need to be 2012 or above to get `NonParallelPlanReason` as part of the showplan XML).	f	f
408	406	12	2018-05-29 11:17:28+00	I can reproduce your results on SQL Server 2017 using the Stack Overflow 2010 database, but not (all of) your conclusions.\r\n\r\n[Minimal logging][1] to **the heap** is unavailable when using `INSERT...SELECT` with `TABLOCK` to a heap with a nonclustered index, which is **unexpected**. My guess is `INSERT...SELECT` cannot support bulk loads using `RowsetBulk` (heap) at the same time as `FastLoadContext` (b-tree). Only Microsoft would be able to confirm if this is a bug or by design.\r\n\r\nThe **nonclustered index** on the heap **is minimally logged** (assuming TF610 is on, or SQL Server 2016+ is used, enabling `FastLoadContext`) with the following caveats:\r\n\r\n* Only rows inserted to newly allocated pages are minimally logged.\r\n* Rows added to the first index page are not minimally logged, if the index was empty at the start of the operation.\r\n\r\nThe 497 `LOP_INSERT_ROWS` entries shown for the nonclustered index correspond to the first page of the index. Since the index was empty beforehand, these rows are fully logged. The remaining rows are all *minimally logged*. If documented trace flag 692 is enabled (2016+) to disable `FastLoadContext`, all nonclustered index rows are minimally logged.\r\n\r\n---\r\n\r\nI found that minimal logging **is applied** to **both** the heap and nonclustered index when bulk loading the same table (with index) using `BULK INSERT` from a file:\r\n\r\n    BULK INSERT dbo.PostsDestination\r\n    FROM 'D:\\SQL Server\\Posts.bcp'\r\n    WITH (TABLOCK, DATAFILETYPE = 'native');\r\n\r\nI note this for completeness. Bulk loading using `INSERT...SELECT` uses different code paths, so the fact the behaviours differ is not entirely unexpected.\r\n\r\n---\r\n\r\nFor **full details** about minimal logging using `RowsetBulk` and `FastLoadContext` with `INSERT...SELECT` see my three part series on SQLPerformance.com:\r\n\r\n1. [Minimal Logging with INSERT…SELECT into Heap Tables][2]\r\n2. [Minimal Logging with INSERT…SELECT into Empty Clustered Tables][3]\r\n3. [Minimal Logging with INSERT…SELECT and Fast Load Context][4]\r\n\r\n---\r\n\r\n## Other scenarios from your blog post\r\n\r\nComments are closed so I will address these briefly here.\r\n\r\n### Empty Clustered Index With Trace 610 Or 2016+\r\n\r\nThis is minimally logged using `FastLoadContext` without `TABLOCK`. The only rows fully logged are those inserted to the first page because the clustered index was empty at the start of the transaction.\r\n\r\n### Clustered Index With Data and Trace 610 OR 2016+\r\n\r\nThis is also minimally logged using `FastLoadContext`. Rows added to the existing page are fully logged, the remainder are minimally logged.\r\n\r\n### Clustered Index With NonClustered Indexes and TABLOCK Or Trace 610/SQL 2016+\r\n\r\nThis can also be minimally logged using `FastLoadContext` as long as the nonclustered index is maintained by a separate operator, `DMLRequestSort` is set to true, and the other conditions laid out in [my posts][4] are met.\r\n\r\n  [1]: https://blogs.msdn.microsoft.com/sql_server_team/sql-server-2016-minimal-logging-and-impact-of-the-batchsize-in-bulk-load-operations/\r\n  [2]: https://sqlperformance.com/2019/05/sql-performance/minimal-logging-insert-select-heap\r\n  [3]: https://sqlperformance.com/2019/05/sql-performance/minimal-logging-empty-clustered\r\n  [4]: https://sqlperformance.com/2019/05/sql-performance/minimal-logging-fast-load-context	2019-12-11 04:28:29.871588+00	3	4	1	208118	0	0	0	2019-12-11 04:28:29.871588+00	\N	I can reproduce your results on SQL Server 2017 using the Stack Overflow 2010 database, but not (all of) your conclusions.	f	f
166	227	12	2018-08-31 15:26:01+00	## Parallel heap scan\r\n\r\nYou might be expecting distribution among parallel threads as in the following toy example:\r\n\r\n    SELECT TOP (5 * 1000 * 1000)\r\n        n = ROW_NUMBER() OVER (ORDER BY @@SPID)\r\n    INTO #n\r\n    FROM sys.columns AS C\r\n    CROSS JOIN sys.columns AS C2\r\n    CROSS JOIN sys.columns AS C3;\r\n    \r\n    SELECT COUNT_BIG(*) \r\n    FROM #n AS N\r\n    GROUP BY N.n % 10\r\n    OPTION (USE HINT ('ENABLE_PARALLEL_PLAN_PREFERENCE'));\r\n\r\n[![parallel heap scan][1]][1]\r\n\r\nIn that plan, the heap table is indeed parallel scanned, with all threads coordinating to share the work of reading the whole table:\r\n\r\n[![Plan Explorer thread summary][2]][2]\r\n\r\nor in SSMS view:\r\n\r\n[![SSMS thread summary][3]][3]\r\n\r\n## Your case\r\n\r\nBut this is not the arrangement in your uploaded plan:\r\n\r\n[![uploaded plan][4]][4]\r\n\r\nThe heap scan is on the **inner side of a nested loops join** [^fn1], so each thread runs a serial copy of the inner side, meaning the there are DOP (degree of parallelism) independent copies of the Table Spool and Table Scan.\r\n\r\nAt DOP four, this means there are *four spools* and *four scans*, as evidenced by the `Number of Executions = 4` on the table scan. Indeed, the heap table is fully scanned four times (once per thread), giving 250,000 * 4 = 1,000,000 rows. The lazy spool caches the result of the scan per thread.\r\n\r\nSo the difference is that your parallel scan is **four serial scans in parallel**, rather than four threads **cooperating to parallel scan** the heap once (as in the toy example above).\r\n\r\nIt can be challenging to conceptualize the difference, but it is crucial. Once you see the branch between the two exchanges as DOP separate serial plans, it becomes easier to decode.\r\n\r\nOf course the plan is wildly inefficient, and the spool adds little value. Notice the join predicate is stuck at the nested loops join, rather than being pushed down the inner side (making the join an apply). This is due to the complex join conditions involving `ISNULL`.\r\n\r\nYou might get a slightly better-performing plan by making your `nc_assignedTouch` index clustered rather than nonclustered, but the bulk of the work would still be happening at the join, and the improvement would almost certainly be minimal. A query rewrite is probably necessary here. Ask a follow-up question if you want assistance expressing the query in a more execution-friendly way.\r\n\r\nFor more background on the parallelism aspects, see my article [Understanding and Using Parallelism in SQL Server][5].\r\n\r\n\r\n[^fn1]: There is one general exception to this, where one can see a true co-operative parallel scan (and exchanges) on the inner side of a nested loops join: The outer input must be *guaranteed* to produce at most one row, and the loop join cannot have any correlated parameters (outer references). Under these conditions, the execution engine will allow parallelism on the inner side because it will always produce correct results.  \r\nYou may also notice that operators below an Eager Spool on the inner side of a parallel nested loops join also execute only once. There are still DOP copies of these operators, but the runtime behaviour is that only one thread builds a shared indexed structure, which is then used by all instances of the Eager Spool. I do apologise that all this is so complicated.\r\n\r\n  [1]: https://i.stack.imgur.com/uDpkh.png\r\n  [2]: https://i.stack.imgur.com/CD1kW.png\r\n  [3]: https://i.stack.imgur.com/b0AsR.png\r\n  [4]: https://i.stack.imgur.com/wvf7j.png\r\n  [5]: https://www.red-gate.com/simple-talk/sql/learn-sql-server/understanding-and-using-parallelism-in-sql-server/	2019-12-01 17:56:05.587683+00	0	4	1	216434	0	0	0	2019-12-01 17:48:06.423848+00	\N	Parallel heap scan	f	f
700	623	811	2020-01-29 00:58:33.994285+00	## Cleaner and more consistent post meta info\r\n\r\n### Main post listing\r\nCurrently:\r\n![Current main listing](/image?hash=78322f1db9fad117079e68b85d76b2fd3904d1683a17051c84fc944b42ea8a2e "Current main listing")\r\nI suggest making the white OP bar a tad taller so the right side can fit two lines containing the authorship and star info, identical to that of the answer listing; and the tags (if any). This also allows us to get rid of the "Answer:" label which serves no real purpose, and eats up precious horizontal space on narrow screens:\r\n\r\nMock-up:\r\n![Proposed main listing](/image?hash=f849577c9bbf71b7b04a556a4bd3056c2dacbe2d273de2650de07a20f02e4e7b "Proposed main listing")\r\n\r\n### The page for a post and its answers\r\nCurrently:\r\n![Current post listing](/image?hash=0dd2fb1c961fd2e6db0939ba7b4a04fc95b8301a053574e988bbe507bcc87fb2 "Current post listing")\r\n\r\nThe OP bar should match the layout and styling of the answer bars:\r\n\r\n![Proposed post listing](/image?hash=dc1f107ca8ae3fc16f5a3776b5591e4c0b422f1d66afad84e2e5e9af0e1ee868 "Proposed post listing")	2020-01-29 13:05:28.501591+00	0	1	1	\N	0	0	0	\N	\N	Cleaner and more consistent post meta info	f	f
417	414	167	2019-12-11 15:54:19.740119+00	The problem with moving the content by a fixed amount of space for each section is that this does not take into acocunt different lenghts of section names.\r\n\r\nHowever using a similar approach as in https://topanswers.xyz/tex?q=415#a418 you can show only a fixed number of sections in the navigation bar, e.g. the current section +-2 other sections.\r\n\r\n\r\n```\r\n\\documentclass{beamer}\r\n\r\n\\usetheme{metropolis}\r\n\r\n\\setbeamercolor{section in head/foot}{bg=structure.fg,fg=white}\r\n\r\n\\makeatletter\r\n\\setbeamertemplate{footline}{%\r\n  \\begin{beamercolorbox}[wd=\\textwidth, sep=3ex]{footline}%\r\n    \\usebeamerfont{page number in head/foot}%\r\n    \\usebeamertemplate*{frame footer}\r\n    \\hfill%\r\n    \\usebeamertemplate*{frame numbering}\r\n  \\end{beamercolorbox}%\r\n\r\n  \\begin{beamercolorbox}[colsep=1.5pt]{upper separation line head}\r\n  \\end{beamercolorbox}\r\n  \\begin{beamercolorbox}{section in head/foot}\r\n    \\vskip2pt\\insertnavigation{\\paperwidth}\\vskip2pt\r\n  \\end{beamercolorbox}%\r\n  \\begin{beamercolorbox}[colsep=1.5pt]{lower separation line head}\r\n  \\end{beamercolorbox}\r\n}\r\n\r\n% based on the original definitions in beamerbasenavigation.sty\r\n\\def\\sectionentry#1#2#3#4#5{% section number, section title, page\r\n%\r\n\\newcount\\mymin%\r\n\\mymin=3\r\n\\ifnum\\c@section=1%\r\n    \\mymin=5\r\n\\fi%\r\n\\ifnum\\c@section=2%\r\n    \\mymin=4\r\n\\fi%\r\n%\r\n\\newcount\\mymax%\r\n\\mymax=3\r\n\\ifnum\\c@section=\\beamer@sectionmax%\r\n    \\mymax=5\r\n\\fi%\r\n\\ifnum\\c@section=\\numexpr\\beamer@sectionmax-1%\r\n    \\mymax=4\r\n\\fi%\r\n%\r\n    \\ifnum\\numexpr\\c@section-#1<\\mymax%\r\n        \\ifnum\\numexpr#1-\\c@section<\\mymin%\r\n            \\ifnum#5=\\c@part%\r\n                \\beamer@section@set@min@width\r\n                \\box\\beamer@sectionbox\\hskip1.875ex plus 1fill%\r\n                \\beamer@xpos=0\\relax%\r\n                \\beamer@ypos=1\\relax%\r\n                \\setbox\\beamer@sectionbox=\r\n                \\hbox{\r\n                    \\def\\insertsectionhead{#2}%\r\n                    \\def\\insertsectionheadnumber{#1}%\r\n                    \\def\\insertpartheadnumber{#5}%\r\n\r\n                    {%\r\n                        \\usebeamerfont{section in head/foot}\\usebeamercolor[fg]{section in head/foot}%\r\n                        \\ifnum\\c@section=#1%\r\n                            \\hyperlink{Navigation#3}{{\\usebeamertemplate{section in head/foot}}}%\r\n                        \\else%\r\n                            \\hyperlink{Navigation#3}{{\\usebeamertemplate{section in head/foot shaded}}}%\r\n                        \\fi%    \r\n                    }%\r\n                }%\r\n                \\ht\\beamer@sectionbox=1.875ex%\r\n                \\dp\\beamer@sectionbox=0.75ex%\r\n            \\fi%\r\n        \\fi%\r\n    \\fi%\r\n    \\ignorespaces%\r\n}\r\n\r\n\\def\\slideentry#1#2#3#4#5#6{%\r\n    %section number, subsection number, slide number, first/last frame, page number, part number\r\n    %\r\n    \\newcount\\mymin%\r\n    \\mymin=3\r\n    \\ifnum\\c@section=1%\r\n        \\mymin=5\r\n    \\fi%\r\n    \\ifnum\\c@section=2%\r\n        \\mymin=4\r\n    \\fi%\r\n        %\r\n    \\newcount\\mymax%\r\n    \\mymax=3\r\n    \\ifnum\\c@section=\\beamer@sectionmax%\r\n        \\mymax=5\r\n    \\fi%\r\n    \\ifnum\\c@section=\\numexpr\\beamer@sectionmax-1%\r\n        \\mymax=4\r\n    \\fi%\r\n    %\r\n    \\ifnum\\numexpr\\c@section-#1<\\mymax%\r\n        \\ifnum\\numexpr#1-\\c@section<\\mymin%\r\n          \\ifnum#6=\\c@part\\ifnum#2>0\\ifnum#3>0%\r\n            \\ifbeamer@compress%\r\n              \\advance\\beamer@xpos by1\\relax%\r\n            \\else%\r\n              \\beamer@xpos=#3\\relax%\r\n              \\beamer@ypos=#2\\relax%\r\n            \\fi%\r\n          \\hbox to 0pt{%\r\n            \\beamer@tempdim=-\\beamer@vboxoffset%\r\n            \\advance\\beamer@tempdim by-\\beamer@boxsize%\r\n            \\multiply\\beamer@tempdim by\\beamer@ypos%\r\n            \\advance\\beamer@tempdim by -.05cm%\r\n            \\raise\\beamer@tempdim\\hbox{%\r\n              \\beamer@tempdim=\\beamer@boxsize%\r\n              \\multiply\\beamer@tempdim by\\beamer@xpos%\r\n              \\advance\\beamer@tempdim by -\\beamer@boxsize%\r\n              \\advance\\beamer@tempdim by 1pt%\r\n              \\kern\\beamer@tempdim\r\n              \\global\\beamer@section@min@dim\\beamer@tempdim\r\n              \\hbox{\\beamer@link(#4){%\r\n                  \\usebeamerfont{mini frame}%\r\n                  \\ifnum\\c@section=#1%\r\n                    \\ifnum\\c@subsection=#2%\r\n                      \\usebeamercolor[fg]{mini frame}%\r\n                      \\ifnum\\c@subsectionslide=#3%\r\n                        \\usebeamertemplate{mini frame}%\\beamer@minislidehilight%\r\n                      \\else%\r\n                        \\usebeamertemplate{mini frame in current subsection}%\\beamer@minisliderowhilight%\r\n                      \\fi%\r\n                    \\else%\r\n                      \\usebeamercolor{mini frame}%\r\n                      %\\color{fg!50!bg}%\r\n                      \\usebeamertemplate{mini frame in other subsection}%\\beamer@minislide%\r\n                    \\fi%\r\n                  \\else%\r\n                    \\usebeamercolor{mini frame}%\r\n                    %\\color{fg!50!bg}%\r\n                    \\usebeamertemplate{mini frame in other subsection}%\\beamer@minislide%\r\n                  \\fi%\r\n                }}}\\hskip-10cm plus 1fil%\r\n          }\\fi\\fi%\r\n          \\else%\r\n          \\fakeslideentry{#1}{#2}{#3}{#4}{#5}{#6}%\r\n         \\fi%\r\n        \\fi%\r\n    \\fi%\r\n    \\ignorespaces%\r\n}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\t\r\n\\section{section 1}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\t\r\n\\section{section 2}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\r\n\\section{section 3}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\r\n\\section{section 4}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\r\n\\section{section 5}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\r\n\\section{section 6}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\r\n\\section{section 7}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\r\n\\section{section 8}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\r\n\\section{section 9}\r\n\\begin{frame}\r\n\tabc\r\n\\end{frame}\t\r\n\t\r\n\t\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2019-12-11 at 17.08.07.png](/image?hash=b37ac84da4b508661b67ee0cbbbc3d233d2a50a3fdede184b7b3ffcc8c42e4d6)\r\n\r\n(I would prefer if this answer would **not** be copied to tex.se)	2019-12-14 13:50:44.832123+00	6	4	3	\N	0	0	0	\N	\N	The problem with moving the content by a fixed amount of space for each section is that this does not take into acocunt different lenghts of section names.	f	f
484	451	12	2019-12-16 12:52:36+00	When the update statement qualifies for a trivial plan, the optimizer rule that expands the instead-of trigger part of the statement (`ExpandInsteadOfTriggerUpd`) includes the part of the plan that reads from the base table. This rewrite includes adding an `UPDLOCK` hint to the base read. As usual, the `UPDLOCK` hint means that update locks are taken and held to the end of the transaction.\r\n\r\nWhen the statement does not qualify for a trivial plan, the `ExpandInsteadOfTriggerUpd` rule only [rewrites the write-cursor portion][1] of the plan, leaving the base table reads untouched - no `UPDLOCK` hint is added.\r\n\r\nMy guess is that this trivial plan behaviour exists to avoid a deadlock scenario.\r\n\r\n\r\n  [1]: https://sqlperformance.com/2015/09/sql-plan/instead-of-triggers	2019-12-16 13:02:53.650877+00	2	4	1	255702	0	0	0	2019-12-16 13:02:53.650877+00	\N	When the update statement qualifies for a trivial plan, the optimizer rule that expands the instead-of trigger part of the statement (`ExpandInsteadOfTriggerUpd`) includes the part of the plan that reads from the base table. This rewrite includes adding an `UPDLOCK` hint to the base read. As usual, the `UPDLOCK` hint means that update locks are taken and held to the end of the transaction.	f	f
133	192	12	2017-07-25 17:03:06+00	## Background\r\n\r\nData for the statistics object are gathered using a statement of the form:\r\n\r\n    SELECT \r\n        StatMan([SC0], [SC1], [SB0000]) \r\n    FROM \r\n    (\r\n        SELECT TOP 100 PERCENT \r\n            [SC0], [SC1], STEP_DIRECTION([SC0]) OVER (ORDER BY NULL) AS [SB0000]\r\n        FROM \r\n        (\r\n            SELECT \r\n                [TextValue] AS [SC0], \r\n                [Id] AS [SC1] \r\n            FROM [dbo].[Test] \r\n                TABLESAMPLE SYSTEM (2.223684e+001 PERCENT) \r\n                WITH (READUNCOMMITTED) \r\n        ) AS _MS_UPDSTATS_TBL_HELPER \r\n        ORDER BY \r\n            [SC0], \r\n            [SC1], \r\n            [SB0000] \r\n    ) AS _MS_UPDSTATS_TBL\r\n    OPTION (MAXDOP 1)\r\n\r\nYou can collect this statement with Extended Events or Profiler (`SP:StmtCompleted`).\r\n\r\nStatistics generation queries often access the base table (rather than a nonclustered index) to avoid the clustering of values that naturally occurs on nonclustered index pages.\r\n\r\nThe number of rows sampled depends on the number of whole pages selected for sampling. Each page of the table is either selected or it is not. All rows on selected pages contribute to the statistics.\r\n\r\n## Random numbers\r\n\r\nSQL Server uses a random number generator to decide if a page qualifies or not. The generator used in this instance is the [Lehmer random number generator][1] with parameter values as shown below:\r\n\r\n**X~next~ = X~seed~ * 7^5^ mod (2^31^ - 1)**\r\n\r\nThe value of **X~seed~** is computed as the sum of:\r\n\r\n* The low integer part of the (`bigint`) base table's `partition_id` e.g.\r\n\r\n        SELECT\r\n            P.[partition_id] & 0xFFFFFFFF\r\n        FROM sys.partitions AS P\r\n        WHERE\r\n            P.[object_id] = OBJECT_ID(N'dbo.Test', N'U')\r\n            AND P.index_id = 1;\r\n\r\n* The value specified in the `REPEATABLE` clause\r\n * For sampled `UPDATE STATISTICS`, the `REPEATABLE` value is 1.\r\n * This value is exposed in the `m_randomSeed` element of the access method's internal debugging information shown in execution plans when trace flag 8666 is enabled, for example `<Field FieldName="m_randomSeed" FieldValue="1" />`\r\n\r\nFor SQL Server 2012, this calculation occurs in `sqlmin!UnOrderPageScanner::StartScan`:\r\n\r\n``` none\r\nmov     edx,dword ptr [rcx+30h]\r\nadd     edx,dword ptr [rcx+2Ch]\r\n```\r\n\r\nwhere memory at `[rcx+30h]` contains the low 32 bits of the partition id and memory at `[rcx+2Ch]` contains the `REPEATABLE` value in use.\r\n\r\nThe random number generator is initialized later in the same method, calling `sqlmin!RandomNumGenerator::Init`, where the instruction:\r\n\r\n``` none\r\nimul    r9d,r9d,41A7h\r\n```\r\n\r\n...multiplies the seed by `41A7` hex (16807 decimal = 7^5^) as shown in the equation above.\r\n\r\nLater random numbers (for individual pages) are generated using the same basic code inlined into `sqlmin!UnOrderPageScanner::SetupSubScanner`.\r\n\r\n## StatMan\r\n\r\nFor the example `StatMan` query shown above, the same pages will be collected as for the T-SQL statement:\r\n\r\n    SELECT \r\n        COUNT_BIG(*) \r\n    FROM dbo.Test AS T \r\n        TABLESAMPLE SYSTEM (2.223684e+001 PERCENT)  -- Same sample %\r\n        REPEATABLE (1)                              -- Always 1 for statman\r\n        WITH (INDEX(0));                            -- Scan base object\r\n\r\nThis will match the output of:\r\n\r\n    SELECT \r\n        DDSP.rows_sampled\r\n    FROM sys.stats AS S\r\n    CROSS APPLY sys.dm_db_stats_properties(S.[object_id], S.stats_id) AS DDSP\r\n    WHERE \r\n        S.[object_id] = OBJECT_ID(N'dbo.Test', N'U')\r\n        AND S.[name] = N'IX_Test_TextValue';\r\n\r\n## Edge case\r\n\r\nOne consequence of using the MINSTD Lehmer random number generator is that seed values zero and int.max should not be used as this will result in the algorithm producing a sequence of zeroes (selecting every page).\r\n\r\nThe code detects zero, and uses a value from the system 'clock' as the seed in that case. It does not do the same if the seed is int.max (`0x7FFFFFFF` = 2^31^ - 1).\r\n\r\nWe can engineer this scenario since the initial seed is calculated as the sum of the low 32 bits of the partition id and the `REPEATABLE` value. The `REPEATABLE` value that will result in the seed being int.max and therefore every page being selected for sample is:\r\n\r\n    SELECT\r\n        0x7FFFFFFF - (P.[partition_id] & 0xFFFFFFFF)\r\n    FROM sys.partitions AS P\r\n    WHERE\r\n        P.[object_id] = OBJECT_ID(N'dbo.Test', N'U')\r\n        AND P.index_id = 1;\r\n\r\nWorking that into a complete example:\r\n\r\n    DECLARE @SQL nvarchar(4000) = \r\n        N'\r\n        SELECT\r\n            COUNT_BIG(*) \r\n        FROM dbo.Test AS T \r\n            TABLESAMPLE (0 PERCENT) \r\n            REPEATABLE (' +\r\n            (\r\n                SELECT TOP (1)\r\n                    CONVERT(nvarchar(11), 0x7FFFFFFF - P.[partition_id] & 0xFFFFFFFF)\r\n                FROM sys.partitions AS P\r\n                WHERE\r\n                    P.[object_id] = OBJECT_ID(N'dbo.Test', N'U')\r\n                    AND P.index_id = 1\r\n            ) + ')\r\n            WITH (INDEX(0));';\r\n    \r\n    PRINT @SQL;\r\n    --EXECUTE (@SQL);\r\n\r\nThat will select every row on every page whatever the `TABLESAMPLE` clause says (even zero percent).\r\n\r\n  [1]: https://en.wikipedia.org/wiki/Lehmer_random_number_generator	2019-11-29 08:51:20.153496+00	1	4	1	181747	0	0	0	2019-11-29 08:45:10.640837+00	\N	Background	f	f
486	453	167	2019-12-16 22:36:43.060919+00	Instead of trying to re-invent the wheel, you could let `biblatex` do the hard work for you. With an numeric style, each of your references will have it's unique number:\r\n\r\n```\r\n\\documentclass[11pt]{beamer}\r\n%\\documentclass[handout,11pt]{beamer}\r\n\\usepackage[utf8]{inputenc}\r\n\\usepackage[T1]{fontenc}\r\n\\usetheme{Madrid}\r\n\r\n\\begin{filecontents}[overwrite]{\\jobname.bib}\r\n@article{Harshman1970,\r\n    author = {Harshman, Richard A and others},\r\n    doi = {10.1134/S0036023613040165},\r\n    file = {:C$\\backslash$:/Users/Isi/Documents/Research/Tensors/TensorsDocumentation/Literature/Harshman{\\_}CPD.pdf:pdf},\r\n    issn = {00360236},\r\n    journal = {UCLA Working Papers in Phonetics},\r\n    pages = {1--84},\r\n    title = {{Foundations of the PARAFAC procedure: Models and conditions for an" explanatory" multimodal factor analysis}},\r\n    volume = {16},\r\n    year = {1970}\r\n}\r\n@article{Hitchcock1927,\r\n    author = {Hitchcock, Frank L.},\r\n    doi = {10.1002/sapm192761164},\r\n    file = {:C$\\backslash$:/Users/Isi/Documents/Research/Tensors/TensorsDocumentation/Literature/Hitchcock{\\_}cpd.pdf:pdf},\r\n    issn = {0097-1421},\r\n    journal = {Journal of Mathematics and Physics},\r\n    number = {1-4},\r\n    pages = {164--189},\r\n    title = {{The Expression of a Tensor or a Polyadic as a Sum of Products}},\r\n    volume = {6},\r\n    year = {1927}\r\n}\r\n@article{Carroll1970,\r\n    doi = {10.1007/BF02310791},\r\n    file = {:C$\\backslash$:/Users/Isi/Documents/Research/Tensors/TensorsDocumentation/Literature/Carrol{\\_}Chang{\\_}CPD.pdf:pdf},\r\n    issn = {00333123},\r\n    journal = {Psychometrika},\r\n    number = {3},\r\n    pages = {283--319},\r\n    title = {{Analysis of individual differences in multidimensional scaling via an n-way generalization of "Eckart-Young" decomposition}},\r\n    volume = {35},\r\n    year = {1970}\r\n}\r\n\r\n\r\n\r\n\\end{filecontents}\r\n\r\n\\usepackage[style=numeric,sorting=none]{biblatex}\r\n\r\n\\addbibresource{\\jobname.bib}\r\n\r\n\\setbeamertemplate{bibliography item}[text]\r\n\r\n\\makeatletter\r\n\\DeclareCiteCommand{\\cite}[\\mkbibsuperscript]\r\n  {\\usebibmacro{prenote}}\r\n  {\\usebibmacro{citeindex}%\r\n   \\usebibmacro{cite}}\r\n  {\\multicitedelim}\r\n  {\\usebibmacro{postnote}}\r\n\r\n\\renewbibmacro*{cite}{%\r\n  \\printtext[bibhyperref]{%\r\n    \\printfield{labelprefix}%\r\n    \\printfield{labelnumber}%\r\n    \\ifbool{bbx:subentry}{%\r\n        \\printfield{entrysetcount}\r\n    }{}%\r\n  }%\r\n  \\footnotetext[\\thefield{labelnumber}]{%\r\n    \\usedriver{\\DeclareNameAlias{sortname}{default}}{\\thefield{entrytype}}\r\n  }%\r\n}\r\n\r\n\\DeclareBibliographyDriver{article}{%\r\n  \\usebibmacro{bibindex}%\r\n  \\usebibmacro{begentry}%\r\n  \\usebibmacro{author/translator+others}%\r\n  \\setunit{\\printdelim{nametitledelim}}\\newblock\r\n  \\usebibmacro{title}%\r\n  \\newunit\r\n  \\printlist{language}%\r\n  \\newunit\\newblock\r\n  \\usebibmacro{byauthor}%\r\n  \\newunit\\newblock\r\n  \\usebibmacro{date}%\r\n  \\usebibmacro{finentry}}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\r\n    \\begin{frame}{Slide 1\\cite{Harshman1970}\\cite{Hitchcock1927}\\cite{Carroll1970}}\r\n        some text\r\n    \\end{frame}\r\n\r\n    \\begin{frame}{Slide 2\\cite{Hitchcock1927}\\cite{Carroll1970}}\r\n    some figure\r\n    \\end{frame}\r\n    \r\n    \\begin{frame}\r\n    \\printbibliography\r\n    \\end{frame}\r\n\r\n\\end{document}\r\n```\r\n![Screen Shot 2019-12-16 at 23.35.47.png](/image?hash=d5d470fecabd57648344b69c909b458f7b70da15ac3b9c13c81b9297432976ee)\r\n\r\n\r\n(I would prefer if this answer would **not** be reposted on tex.se)	2019-12-19 12:04:44.564298+00	4	4	3	\N	0	0	0	\N	\N	Instead of trying to re-invent the wheel, you could let `biblatex` do the hard work for you. With an numeric style, each of your references will have it's unique number:	f	f
104	149	12	2016-05-08 14:07:58+00	Not all reads are equal. SQL Server knows that accessing LOB data is expensive, and tries to avoid it when possible. There are also detailed differences in the way the LOB data is read in each case:\r\n\r\n### Summary\r\n\r\nThe numbers are different because:\r\n\r\n* The select reads the LOB in packet-sized *chunks*\r\n* The variable assignment test *does not read* the LOB at all\r\n* The "select into" test reads the LOB in *whole pages*\r\n\r\n### Detail\r\n\r\n1. **Plain `SELECT`**\r\n\r\n [![Select plan][1]][1]\r\n\r\n The Clustered Index Scan does not read any LOB data. It only assigns a storage engine LOB *handle*. The handle isn't used until control returns to the root of the plan.\r\n\r\n The current row's LOB content is read in TDS packet sized chunks and streamed to the client. Logical reads count the number of times a page is touched, so:\r\n\r\n *The number of reads reported equals the number of chunked reads performed, plus one for each time a LOB page transition occurs.*\r\n\r\n For example: A logical read is counted at the start of each chunk as the process touches the page corresponding to the current position of the stream. Where packets are smaller than a database page (the usual case), several logical reads are counted for the same page. If the packet size were so large that the entire LOB could fit in one chunk, the number of logical reads reported would be the number of LOB pages.\r\n\r\n2. **Variable assignment**\r\n\r\n [![Variable plan][2]][2]\r\n\r\n The Clustered Index Scan assigns a LOB *handle* as before. At the root of the plan, the LOB handle is copied to the variable. The LOB data itself is never accessed (zero LOB reads), because the variable is never read. Even if it were, it would only be via the LOB handle last assigned.\r\n\r\n *There are no LOB reads because the LOB data is never accessed.*\r\n\r\n3. **`SELECT INTO`**\r\n\r\n [![Select Into Plan][3]][3]\r\n\r\n This plan uses the bulk rowset provider to copy the LOB data from the source table to the new table. It processes a complete LOB page on each read (no streaming or chunking).\r\n\r\n *The number of logical reads corresponds to the number of LOB pages in the test table.*\r\n\r\n  [1]: https://i.stack.imgur.com/pGEAe.png\r\n  [2]: https://i.stack.imgur.com/V8cwI.png\r\n  [3]: https://i.stack.imgur.com/3DlKX.png	2019-11-27 13:02:16.515341+00	2	4	1	137908	0	0	0	2019-11-27 10:21:42.256417+00	\N	Not all reads are equal. SQL Server knows that accessing LOB data is expensive, and tries to avoid it when possible. There are also detailed differences in the way the LOB data is read in each case:	f	f
640	587	167	2018-08-16 21:47:37+00	One could draw a simple progress bar with subsection marks in tikz:\r\n\r\n    \\documentclass{beamer}\r\n    \r\n    \\usepackage{totcount}\r\n    \\newtotcounter{mysub}\r\n    \\AtBeginSubsection{\\addtocounter{mysub}{1}\\label{mysub:\\themysub}}\r\n    \\newcounter{foo}\r\n    \\usepackage{refcount}\r\n    \\usepackage{tikz}\r\n    \r\n    \\setbeamertemplate{footline}{%\r\n        \\begin{tikzpicture}\r\n        \t\\draw[ultra thick] (0,0) -- (\\thepage/\\insertdocumentendpage*\\paperwidth,0);\r\n         \t\\foreach \\x in {1,...,\\totvalue{mysub}}{%\r\n         \t\t\\setcounterpageref{foo}{mysub:\\x}\r\n    \t\t\t\t\\draw[thick] (\\thefoo/\\insertdocumentendpage*\\paperwidth,0) -- (\\thefoo/\\insertdocumentendpage*\\paperwidth,0.5);\t\r\n    \r\n         \t}\r\n        \\end{tikzpicture}\r\n    \r\n    }\r\n    \r\n    \\begin{document}\r\n    \r\n    \\section{section name}\r\n    \\subsection{section name}\r\n    \\begin{frame}\r\n    \\end{frame}\r\n    \r\n    \\begin{frame}\r\n    \\end{frame}\r\n    \r\n    \\subsection{section name}\r\n    \\begin{frame}\r\n    \\end{frame}\r\n    \r\n    \\subsection{section name}\r\n    \\begin{frame}\r\n    \\end{frame}\r\n    \r\n    \\section{section name}\r\n    \\subsection{section name}\r\n    \\begin{frame}\r\n    \\end{frame}\r\n    \r\n    \\begin{frame}\r\n    \\end{frame}\r\n    \r\n    \\end{document}\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/qd6Ba.gif	2020-01-20 16:43:09.346408+00	2	4	1	446354	0	0	0	2020-01-20 16:35:55.020031+00	\N	One could draw a simple progress bar with subsection marks in tikz:	f	f
452	443	607	2017-01-10 17:54:27+00	Bulk Insert operations in the manner you are describing are not supported for encrypted column via SSMS.\r\n\r\nPlease refer to [Encrypting Existing Data with Always Encrypted](https://blogs.msdn.microsoft.com/sqlsecurity/2015/07/28/encrypting-existing-data-with-always-encrypted/) by Jakub Szymaszek to migrate your existing data to Always Encrypted\r\n\r\nAlso, please note that doing bulk inserts through a C# (.NET 4.6.1+ client) app is supported.\r\n\r\nYou can do this in C# using `SqlBulkCopy` specifically using `SqlBulkCopy.WriteToServer(IDataReader)` Method. I am assuming you are trying to load data from csv file to a table with encrypted column (say encryptedTable). I would do the following:\r\n\r\n - Create a new table (say unencryptedTable, for security purposes, you might consider creating this table in a local sql server instance) with the same schema without any column encryption.\r\n - Load the csv data into unencryptedTable, using the method that you described in the question.\r\n - Do `select * from unencryptedTable` to load the data in a SqlDataReader then use SqlBulkCopy to load it to the encryptedTable using `SqlBulkCopy.WriteToServer(IDataReader)` Method.	2019-12-14 06:59:10.651891+00	2	4	1	160605	0	0	0	2019-12-14 06:54:18.256413+00	\N	Bulk Insert operations in the manner you are describing are not supported for encrypted column via SSMS.	f	f
30	53	2	2019-11-16 23:48:19.578702+00	From [the online documentation](https://docs.microsoft.com/en-us/sql/t-sql/functions/power-transact-sql), the implication is that whatever you pass as the first parameter is going to be implicitly cast to a `float(53)` *before* the function is executed. :\r\n\r\n>     POWER ( float_expression , y )  \r\n> ## Arguments\r\n> float_expression\r\nIs an expression of type float or of a type ***that can be implicitly converted to float***\r\n\r\nHowever, this [is not (always?) the case](https://dba.stackexchange.com/a/6902/1396).\r\n\r\nIf it were the case, it would explain the loss of precision:\r\n\r\n> Conversion of float values that use scientific notation to decimal or numeric is restricted to values of precision 17 digits only. Any value with precision higher than 17 rounds to zero.\r\n\r\nOn the other hand, the literal `2.` is type `numeric`…:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2016&fiddle=2468f9ee4e8494fbe568ea8298a7ad84\r\n\r\n…and the multiply operator [returns the data type of the argument with the higher precedence](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/multiply-transact-sql).\r\n\r\nIt appears that on 2016 (SP1), all the precision is retained:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2016&fiddle=79ed50a23c712d9d448800220ba79a84\r\n\r\n…but on 2014 (SP2), it is not:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2014&fiddle=79ed50a23c712d9d448800220ba79a84\r\n\r\n\r\n\r\n	2019-11-25 15:24:28.35979+00	2	1	1	\N	0	0	0	\N	\N	From [the online documentation](https://docs.microsoft.com/en-us/sql/t-sql/functions/power-transact-sql), the implication is that whatever you pass as the first parameter is going to be implicitly cast to a `float(53)` *before* the function is executed. :	f	f
32	53	12	2019-11-17 12:54:06.17293+00	The result of 2^64^ is exactly representable in `float` (and `real` for that matter).\r\n\r\nThe problem arises when this precise result is converted back to `numeric` (the type of the first `POWER` operand).\r\n\r\nBefore database compatibility level 130 was introduced, SQL Server rounded `float` to `numeric` implicit conversions to a maximum of 17 digits.\r\n\r\nUnder compatibility level 130, as much precision as possible is preserved during the conversion. This is documented in the Knowledge Base article:\r\n\r\n[SQL Server 2016 improvements in handling some data types and uncommon operations][1]\r\n\r\nTo take advantage of this in Azure SQL Database, you need to set the `COMPATIBILITY_LEVEL` to 130:\r\n\r\n```sql\r\nALTER DATABASE CURRENT SET COMPATIBILITY_LEVEL = 130;\r\n```\r\n\r\n---\r\n\r\nWorkload testing is needed because the new arrangement is not a panacea. For example:\r\n\r\n```sql\r\nSELECT POWER(10., 38);\r\n```\r\n\r\n...ought to throw an error because 10^38^ cannot be stored in `numeric` (maximum precision of 38). An overflow error results under 120 compatibility, but the result under 130 is:\r\n\r\n~~~\r\n99999999999999997748809823456034029568 -- (38 digits)\r\n~~~\r\n\r\n  [1]: https://support.microsoft.com/en-us/help/4010261/sql-server-2016-improvements-in-handling-some-data-types-and-uncommon-	2019-11-17 12:54:06.17293+00	3	4	2	\N	0	0	0	\N	\N	The result of 2^64^ is exactly representable in `float` (and `real` for that matter).	f	f
419	416	14	2019-08-30 00:29:35+00	## Query Store plan forcing does NOT affect queries on the secondary\r\n\r\nUsing Query Store to force a plan on the primary certainly *looks* like it forces the plan on the secondary.  \r\n\r\nI tried running a query on a non-prod server, and then flushing the query store with `sp_query_store_flush_db` (which was required to get the data to sync across to the secondary).  Here's the secondary on the left (note the circled warning about being "read only"), and the primary on the right:\r\n\r\n[![screenshot of query store UI][1]][1]\r\n\r\nNow I'll click "Force Plan" on the right, and then refresh both views:\r\n\r\n[![screenshot of query store UI showing both forced plans][2]][2]\r\n\r\nSo the "forcing" at least carried over in the underlying Query Store tables.  This makes sense, given that the articles quoted in the OP make the point that query forcing should remain in-place after a failover:\r\n\r\n> Question: Will QDS retain FORCED Plan information when Database failover from Primary replica to secondary Replica?\r\n> \r\n> Answer: Yes, QDS store Forced Plan information in sys.query_store_plan table, so in case of failover you will continue to see same behavior on new Primary.\r\n\r\nBut does the forcing *behavior* actually take place?  I'll now run the same query on both servers.  On the primary, as expected, the "UsePlan" attribute is there in the plan XML:\r\n\r\n```lang-xml\r\n<QueryPlan DegreeOfParallelism="1" MemoryGrant="11096" CachedPlanSize="288" CompileTime="82"\r\n           CompileCPU="78" CompileMemory="2104" UsePlan="true">\r\n```\r\n\r\nAnd in the UI:\r\n\r\n[![screenshot of execution plan in SSMS showing the "use plan" attribute][3]][3]\r\n\r\nOn the secondary (note the different server name), the plan was *not forced*.  Here's the same plan XML snippet:\r\n\r\n```lang-xml\r\n<QueryPlan DegreeOfParallelism="1" MemoryGrant="11096" CachedPlanSize="288" CompileTime="32" \r\n           CompileCPU="28" CompileMemory="1656">\r\n```\r\n\r\n[![screenshot of execution plan in SSMS showing the no "use plan" attribute][4]][4]\r\n\r\n## Plan Guides do NOT affect queries on the secondary\r\n\r\nI created a plan guide on the primary using this code (table names changed to protect the innocent):\r\n\r\n```\r\nEXEC sp_create_plan_guide \r\n\t@name = 'plan-guide-test',\r\n\t@stmt = N'SELECT TOP (1000) * \r\nFROM dbo.TableName t \r\nWHERE \r\n\tNOT EXISTS \r\n\t(\r\n\t\tSELECT NULL \r\n\t\tFROM dbo.OtherTable o \r\n\t\tWHERE t.Id = o.TableName\r\n\t);',\r\n\t@type = N'SQL',\r\n\t@module_or_batch = NULL,\r\n\t@hints = N'OPTION (MAXDOP 1)';\r\n```\r\nThe plan guide was, of course, effective on the primary, as evidenced by the execution plan:\r\n\r\n```lang-xml\r\n<StmtSimple StatementCompId="1" StatementEstRows="1000" ... StatementType="SELECT" \r\n            PlanGuideDB="..._UAT" PlanGuideName="plan-guide-test" ...>\r\n```\r\n\r\n[![screenshot of execution plan in SSMS showing plan guide attributes][5]][5]\r\n\r\nI did confirm at this point that the plan guide was replicated to the secondary. \r\n\r\nRunning the same query on the secondary, the execution plan is missing all the signs of being forced by a plan guide:\r\n\r\n```lang-xml\r\n<StmtSimple StatementCompId="1" StatementEstRows="1000" ... StatementType="SELECT" \r\n            QueryHash="0xECF8A24F126EE77A" QueryPlanHash="0x0E93CF7FEAC1B6EA" \r\n            RetrievedFromCache="true" SecurityPolicyApplied="false">\r\n```\r\n\r\n[![screenshot of execution plan in XML with missing plan guide attributes][6]][6]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/5i0sj.png\r\n  [2]: https://i.stack.imgur.com/S6Jaz.png\r\n  [3]: https://i.stack.imgur.com/FA7Je.png\r\n  [4]: https://i.stack.imgur.com/3XLFP.png\r\n  [5]: https://i.stack.imgur.com/5aZU8.png\r\n  [6]: https://i.stack.imgur.com/WWJ1k.png	2019-12-11 16:14:36.05791+00	2	4	1	246595	0	0	0	2019-12-11 16:14:36.05791+00	\N	Query Store plan forcing does NOT affect queries on the secondary	f	f
212	262	12	2019-07-17 20:47:55+00	*Hash join* and *hash aggregate* both use the same operator code internally, though a hash aggregate uses only a single (build) input. The basic operation of *hash aggregate* is [described by Craig Freedman][1]:\r\n\r\n>As with hash join, the hash aggregate requires memory.  Before executing a query with a hash aggregate, SQL Server uses cardinality estimates to estimate how much memory we need to execute the query.  With a hash join, we store each build row, so the total memory requirement is proportional to the number and size of the build rows.  The number of rows that join and the output cardinality of the join have no effect on the memory requirement of the join.  With a hash aggregate, we store one row for each group, so the total memory requirement is actually proportional to the number and size of the output groups or rows.  If we have fewer unique values of the group by column(s) and fewer groups, we need less memory.  If we have more unique values of the group by column(s) and more groups, we need more memory.\r\n\r\nHe goes on to talk about hash recursion:\r\n\r\n>So, what happens if we run out of memory?  Again, like hash join, if we run out of memory, we must begin spilling rows to tempdb.  We spill one or more buckets or partitions including any partially aggregated results along with any additional new rows that hash to the spilled buckets or partitions.  Although we do not attempt to aggregate the spilled new rows, we do hash them and divide them up into several buckets or partitions.  Once we’ve finished processing all input groups, we output the completed in-memory groups and repeat the algorithm by reading back and aggregating one spilled partition at a time.  By dividing the spilled rows into multiple partitions, we reduce the size of each partition and, thus, reduce the risk that the algorithm will need to repeat many times.\r\n\r\n### Bailout\r\n\r\n*Hash bailout* is lightly documented, but mentioned by Nacho Alonso Portillo in [What’s the maximum level of recursion for the hash iterator before forcing bail-out?][2]\r\n\r\n>The value is a constant, hard coded in the product, and its value is five (5). This means that before the hash scan operator resorts to a sort based algorithm for any given subpartition that doesn’t fit into the granted memory from the workspace, five previous attempts to subdivide the original partition into smaller partitions must have happened.\r\n\r\nThe "hash scan operator" mentioned there is a reference to the internal class `CQScanHash` in `sqlmin.dll`. This class heads the implementation of the hash operator (in all its forms, including partial aggregates and flow distinct) we see in execution plans.\r\n\r\n### Bailout algorithm\r\n\r\nThis brings us to the heart of your questions - what exactly does the bailout algorithm do? Is it "sort based" or based on "a sort of nested loops thing"?\r\n\r\nIt is arguably both, depending on your point of view. When hash recursion reaches level 5, the in-memory hash partition changes from being a hash table to an initially empty b-tree index on the hash values. Each row from a single previously-spilled hash partition is looked up in the b-tree index and inserted (new group) or updated (maintaining aggregates) as appropriate.\r\n\r\nThis series of unordered inserts to a b-tree can equally well be seen as an [insertion sort][3] or as an indexed nested loops lookup.\r\n\r\nIn any case, this fallback algorithm is guaranteed to complete eventually without allocating more memory. It may require multiple passes if the space available for the b-tree is not sufficient to hold all the grouping keys and aggregates from the overflow partition.\r\n\r\nOnce the memory available to hold the b-tree index is exhausted, any further rows (from the current spilled partition) are sent to a single new *tempdb* partition (which is guaranteed to be smaller) and the process repeats as necessary. The spill level remains at 5 because hash *recursion* has ended. Some processing details can be observed with undocumented trace flag 7357.\r\n\r\n  [1]: https://blogs.msdn.microsoft.com/craigfr/2006/09/20/hash-aggregate/\r\n  [2]: https://blogs.msdn.microsoft.com/ialonso/2012/09/05/whats-the-maximum-level-of-recursion-for-the-hash-iterator-before-forcing-bail-out/\r\n  [3]: https://en.wikipedia.org/wiki/Insertion_sort	2019-12-04 11:43:10.182585+00	4	4	1	243133	0	0	0	2019-12-04 11:43:10.182585+00	\N	*Hash join* and *hash aggregate* both use the same operator code internally, though a hash aggregate uses only a single (build) input. The basic operation of *hash aggregate* is [described by Craig Freedman][1]:	f	f
471	450	88	2012-10-11 15:21:18+00	I'm old school, and find `CASE` easier to work out in my head than `PIVOT`. I'm sure bluefeet will show up shortly and put me to shame, but in the meantime you can play with this dynamic SQL query. Assuming your table stores `DATE` and not `DATETIME` (or even worse, `VARCHAR`):\r\n\r\n    USE tempdb;\r\n    GO\r\n\r\n    CREATE TABLE dbo.a\r\n    (\r\n       Person_Id INT, \r\n       ArrivalDate DATE, \r\n       DepartureDate DATE\r\n    );\r\n    \r\n    INSERT dbo.a SELECT 123456, '2012-01-01', '2012-01-04'\r\n    UNION ALL    SELECT 213415, '2012-01-02', '2012-01-07';\r\n    \r\n    DECLARE @sql NVARCHAR(MAX) = N'SELECT Person_Id';\r\n    \r\n    ;WITH dr AS\r\n    (\r\n      SELECT MinDate = MIN(ArrivalDate),\r\n             MaxDate = MAX(DepartureDate)\r\n      FROM dbo.a\r\n    ),\r\n    n AS\r\n    (\r\n      SELECT TOP (DATEDIFF(DAY, (SELECT MinDate FROM dr), (SELECT MaxDate FROM dr)) + 1)\r\n       d = DATEADD(DAY, ROW_NUMBER() OVER (ORDER BY [object_id])-1, \r\n         (SELECT MinDate FROM dr))\r\n     FROM sys.all_objects\r\n    )\r\n    SELECT @sql += ',\r\n      ' + QUOTENAME(d) + ' = CASE WHEN ''' + CONVERT(CHAR(10), d, 120) \r\n      + ''' BETWEEN ArrivalDate AND DepartureDate THEN ''1'' ELSE '''' END' FROM n;\r\n    \r\n    SELECT @sql += ' FROM dbo.a;'\r\n    \r\n    EXEC sp_executesql @sql;\r\n    GO\r\n\r\n    DROP TABLE dbo.a;\r\n\r\nOne of the very few cases, BTW, where I could justify using `BETWEEN` for date range queries.	2019-12-15 06:24:21.323009+00	2	4	1	25813	0	0	0	2019-12-15 06:24:21.323009+00	\N	I'm old school, and find `CASE` easier to work out in my head than `PIVOT`. I'm sure bluefeet will show up shortly and put me to shame, but in the meantime you can play with this dynamic SQL query. Assuming your table stores `DATE` and not `DATETIME` (or even worse, `VARCHAR`):	f	f
470	450	80	2012-10-11 16:00:19+00	You can use the `PIVOT` function to perform this query.  My answer will include both a *static* and a *dynamic* version because sometimes it is easier to understand it using a static version.\r\n\r\nA *static pivot* is when you hard-code all of the values that you want to transform into columns.\r\n\r\n    -- first into into a #temp table the list of dates that you want to turn to columns\r\n    ;with cte (datelist, maxdate) as\r\n    (\r\n    \tselect min(arrivaldate) datelist, max(departuredate) maxdate\r\n    \tfrom BookingsPerPerson\r\n    \tunion all\r\n    \tselect dateadd(dd, 1, datelist), maxdate\r\n    \tfrom cte\r\n    \twhere datelist < maxdate\r\n    ) \r\n    select c.datelist\r\n    into #tempDates\r\n    from cte c\r\n    \r\n    select *\r\n    from\r\n    (\r\n    \tselect b.person_id, b.arrivaldate, b.departuredate,\r\n    \t\td.datelist,\r\n    \t\tconvert(CHAR(10), datelist, 120) PivotDate\r\n    \tfrom #tempDates d\r\n    \tleft join BookingsPerPerson b\r\n    \t\ton d.datelist between b.arrivaldate and b.departuredate\r\n    ) x\r\n    pivot\r\n    (\r\n    \tcount(datelist)\r\n    \tfor PivotDate in ([2012-01-01], [2012-01-02], [2012-01-03],\r\n    \t\t\t  [2012-01-04], [2012-01-05], [2012-01-06] , [2012-01-07])\r\n    ) p;\r\n\r\nResults (See [SQL Fiddle With Demo][1]):\r\n\r\nPERSON_ID | ARRIVALDATE | DEPARTUREDATE | 2012-01-01 | 2012-01-02 | 2012-01-03 | 2012-01-04 | 2012-01-05 | 2012-01-06 | 2012-01-07\r\n--|--|--|--|--|--|--|--|--|--\r\n123456    | 2012-01-01  | 2012-01-04    | 1          | 1          | 1          | 1          | 0          | 0          | 0\r\n213415    | 2012-01-02  | 2012-01-07    | 0          | 1          | 1          | 1          | 1          | 1          | 1\r\n\r\nThe dynamic version will generate the list of values to transform to columns:\r\n\r\n    DECLARE @cols AS NVARCHAR(MAX),\r\n        @query  AS NVARCHAR(MAX)\r\n    \r\n    ;with cte (datelist, maxdate) as\r\n    (\r\n    \tselect min(arrivaldate) datelist, max(departuredate) maxdate\r\n    \tfrom BookingsPerPerson\r\n    \tunion all\r\n    \tselect dateadd(dd, 1, datelist), maxdate\r\n    \tfrom cte\r\n    \twhere datelist < maxdate\r\n    ) \r\n    select c.datelist\r\n    into #tempDates\r\n    from cte c\r\n    \r\n    \r\n    select @cols = STUFF((SELECT distinct ',' + QUOTENAME(convert(CHAR(10), datelist, 120)) \r\n                        from #tempDates\r\n                FOR XML PATH(''), TYPE\r\n                ).value('.', 'NVARCHAR(MAX)') \r\n            ,1,1,'')\r\n    \r\n    set @query = 'SELECT person_id, arrivaldate, departuredate, ' + @cols + ' from \r\n                 (\r\n                   \tselect b.person_id, b.arrivaldate, b.departuredate,\r\n    \t\t\t\t\td.datelist,\r\n    \t\t\t\t\tconvert(CHAR(10), datelist, 120) PivotDate\r\n    \t\t\t\tfrom #tempDates d\r\n    \t\t\t\tleft join BookingsPerPerson b\r\n    \t\t\t\t\ton d.datelist between b.arrivaldate and b.departuredate\r\n                ) x\r\n                pivot \r\n                (\r\n    \t\t\t\tcount(datelist)\r\n                    for PivotDate in (' + @cols + ')\r\n                ) p '\r\n    \r\n    execute(@query)\r\n\r\nThe results are the same (see [SQL Fiddle With Demo][2]):\r\n\r\nPERSON_ID | ARRIVALDATE | DEPARTUREDATE | 2012-01-01 | 2012-01-02 | 2012-01-03 | 2012-01-04 | 2012-01-05 | 2012-01-06 | 2012-01-07\r\n--|--|--|--|--|--|--|--|--|--\r\n123456    | 2012-01-01  | 2012-01-04    | 1          | 1          | 1          | 1          | 0          | 0          | 0\r\n213415    | 2012-01-02  | 2012-01-07    | 0          | 1          | 1          | 1          | 1          | 1          | 1\r\n\r\n\r\n  [1]: http://sqlfiddle.com/#!3/8857c/9\r\n  [2]: http://sqlfiddle.com/#!3/8857c/14	2019-12-15 06:28:13.192201+00	4	4	1	25818	0	0	0	2019-12-15 06:24:20.819549+00	\N	You can use the `PIVOT` function to perform this query.  My answer will include both a *static* and a *dynamic* version because sometimes it is easier to understand it using a static version.	f	f
349	362	167	2019-12-06 12:16:02.261691+00	The problem seems to be the expansion of `\\beamer@sidebarside` inside the `\\AtBeginSection{...}` macro. As a simple workaround, you could "hide" the command in another macro.\r\n\r\nTo restrict the change to the section slide, you will also need to wrap it in an additional group and in case you not only want to remove the content of the sidebar, but also the space reserved for it, you can add\r\n\r\n```\r\n\\hoffset=-\\beamer@leftsidebar\r\n\\advance\\textwidth\\beamer@sidebarwidth\r\n\\hsize\\textwidth\r\n\\columnwidth\\textwidth\t\r\n```\r\n\r\nFull MWE:\r\n\r\n```\r\n\\documentclass{beamer}\r\n\r\n\\useoutertheme[%\r\nwidth=3cm,\r\nheight=1cm,\r\nhideothersubsections,\r\n]{sidebar}\r\n\r\n\\makeatletter\r\n\\newcommand{\\hidesidebar}{%\r\n\t\\setbeamertemplate{sidebar \\beamer@sidebarside}{}\r\n\t\\hoffset=-\\beamer@leftsidebar\r\n\t\\advance\\textwidth\\beamer@sidebarwidth\r\n\t\\hsize\\textwidth\r\n\t\\columnwidth\\textwidth\t\r\n\t}\r\n\\makeatother\r\n\r\n\\AtBeginSection[]{%\r\n\t\\begingroup\r\n\t\t\\hidesidebar\r\n\t  \\begin{frame}\r\n\t    {\\thesection\\\\[.4ex]\\insertsectionhead}\r\n\t  \\end{frame}\r\n  \\endgroup\r\n}\r\n\r\n\\begin{document}\r\n\r\n\\section{First section}\r\n\r\n\\begin{frame}{Title}\r\n  Some content\r\n\\end{frame}\r\n\r\n\\begin{frame}{Title}\r\n  Some content\r\n\\end{frame}\r\n\r\n\\section{Second section}\r\n\r\n\\begin{frame}{Title}\r\n  Some content\r\n\\end{frame}\r\n\r\n\\begin{frame}{Title}\r\n  Some content\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n\r\n(I would prefer if this answer would **not** be copied to tex.se)	2019-12-11 16:26:09.975264+00	7	4	3	\N	0	0	0	\N	\N	The problem seems to be the expansion of `\\beamer@sidebarside` inside the `\\AtBeginSection{...}` macro. As a simple workaround, you could "hide" the command in another macro.	f	f
495	462	751	2013-02-27 21:38:29+00	You can use `CROSS APPLY ... VALUES` to `UNPIVOT` multiple columns\r\n\r\n    SELECT ID,\r\n           OptionID,\r\n           OptionName,\r\n           OptionLName\r\n    FROM   [dbo].[Base]\r\n           CROSS APPLY (VALUES([Option1ID], [Option1Name], [Option1LName]),\r\n                              ([Option2ID], [Option2Name], [Option2LName]),\r\n                              ([Option3ID], [Option3Name], [Option3LName])) \r\n                         V( OptionID, OptionName, OptionLName) \r\n\r\nThe execution plan for this has one scan of `Base`. The plan is in fact the same as for the 2005 compatible rewrite that uses `UNION ALL`\r\n\r\n\r\n    SELECT ID,\r\n           OptionID,\r\n           OptionName,\r\n           OptionLName\r\n    FROM   [dbo].[Base]\r\n           CROSS APPLY (SELECT [Option1ID], [Option1Name], [Option1LName] UNION ALL\r\n                        SELECT [Option2ID], [Option2Name], [Option2LName] UNION ALL\r\n                        SELECT [Option3ID], [Option3Name], [Option3LName]) \r\n                         V( OptionID, OptionName, OptionLName)  \r\n\r\nBut I presume the `UNION ALL` you were trying to avoid was the multiple scans of \r\n\r\n\r\n    SELECT ID,\r\n           [Option1ID],\r\n           [Option1Name],\r\n           [Option1LName]\r\n    FROM   [dbo].[Base]\r\n    UNION ALL\r\n    SELECT ID,\r\n           [Option2ID],\r\n           [Option2Name],\r\n           [Option2LName]\r\n    FROM   [dbo].[Base]\r\n    UNION ALL\r\n    SELECT ID,\r\n           [Option3ID],\r\n           [Option3Name],\r\n           [Option3LName]\r\n    FROM   [dbo].[Base] 	2019-12-18 07:41:47.14755+00	3	4	1	35621	0	0	0	2019-12-18 07:41:47.14755+00	\N	You can use `CROSS APPLY ... VALUES` to `UNPIVOT` multiple columns	f	f
455	445	751	2016-07-07 10:24:44+00	This is certainly a bug. The fact that the `col1` values happened to be the result of an expression involving random numbers clearly doesn't change what the the correct value for `col2` is supposed to be. `DBCC CHECKDB` returns an error if this is run against a permanent table.\r\n\r\n    create table test (\r\n        Col1 INT,\r\n        Contains2 AS CASE WHEN 2 IN (Col1) THEN 1 ELSE 0 END PERSISTED);\r\n    \r\n    INSERT INTO test (Col1) VALUES\r\n        (ABS(CHECKSUM(NEWID()) % 5)),\r\n        (ABS(CHECKSUM(NEWID()) % 5)),\r\n        (ABS(CHECKSUM(NEWID()) % 5)),\r\n        (ABS(CHECKSUM(NEWID()) % 5)),\r\n        (ABS(CHECKSUM(NEWID()) % 5));\r\n    \r\n    DBCC CHECKDB\r\n\r\nGives (for my test run which had one "impossible" row)\r\n\r\n    Msg 2537, Level 16, State 106, Line 17\r\n    Table error: object ID 437576597, index ID 0, partition ID 72057594041008128, alloc unit ID 72057594046251008 (type In-row data), page (1:121), row 0. The record check (valid computed column) failed. The values are 2 and 0.\r\n    DBCC results for 'test'.\r\n    There are 5 rows in 1 pages for object "test".\r\n    CHECKDB found 0 allocation errors and 1 consistency errors in table 'test' (object ID 437576597).\r\n\r\nIt does also report that \r\n\r\n> repair_allow_data_loss is the minimum repair level for the errors\r\n> found by DBCC CHECKDB\r\n\r\nAnd if taken up on the repair option unceremoniously deletes the whole row as it has no way of telling which column is corrupted.\r\n\r\nAttaching a debugger shows that the `NEWID()` is being evaluated twice per inserted row. Once before the `CASE` expression is evaluated and once inside it.\r\n\r\n[![enter image description here][1]][1]\r\n\r\nA possible workaround might be to use\r\n\r\n    INSERT INTO @test\r\n                (Col1)\r\n    SELECT ( ABS(CHECKSUM(NEWID()) % 5) )\r\n    FROM   (VALUES (1),(1),(1),(1),(1)) V(X); \r\n\r\nWhich for one reason or another avoids the issue and only evaluates the expression once per row. \r\n\r\n  [1]: https://i.stack.imgur.com/HHKF7.png	2019-12-14 07:10:07.095444+00	3	4	1	143216	0	0	0	2019-12-14 07:10:07.095444+00	\N	This is certainly a bug. The fact that the `col1` values happened to be the result of an expression involving random numbers clearly doesn't change what the the correct value for `col2` is supposed to be. `DBCC CHECKDB` returns an error if this is run against a permanent table.	f	f
534	481	751	2012-07-08 11:43:52+00	I've come across another case where `CASE` / `COALESCE` do not short circuit. The following TVF will raise a PK violation if passed `1` as a parameter.\r\n\r\n    CREATE FUNCTION F (@P INT)\r\n    RETURNS @T TABLE (\r\n      C INT PRIMARY KEY)\r\n    AS\r\n      BEGIN\r\n          INSERT INTO @T\r\n          VALUES      (1),\r\n                      (@P)\r\n    \r\n          RETURN\r\n      END\r\n    \r\nIf called as follows\r\n    \r\n    DECLARE @Number INT = 1\r\n    \r\n    SELECT COALESCE(@Number, (SELECT number\r\n                              FROM   master..spt_values\r\n                              WHERE  type = 'P'\r\n                                     AND number = @Number), \r\n                             (SELECT TOP (1)  C\r\n                              FROM   F(@Number))) \r\n\r\nOr as\r\n\r\n    DECLARE @Number INT = 1\r\n    \r\n    SELECT CASE\r\n             WHEN @Number = 1 THEN @Number\r\n             ELSE (SELECT TOP (1) C\r\n                   FROM   F(@Number))\r\n           END \r\n\r\nBoth give the result\r\n\r\n> Violation of PRIMARY KEY constraint 'PK__F__3BD019A800551192'. Cannot\r\n> insert duplicate key in object 'dbo.@T'. The duplicate key value is\r\n> (1).\r\n\r\nshowing that the `SELECT` (or at least the table variable population) is still carried out and raises an error even though that branch of the statement should never be reached. The plan for the `COALESCE` version is below.\r\n\r\n[![Plan][1]][1]\r\n\r\nThis rewrite of the query appears to avoid the issue\r\n\r\n    SELECT COALESCE(Number, (SELECT number\r\n                              FROM   master..spt_values\r\n                              WHERE  type = 'P'\r\n                                     AND number = Number), \r\n                             (SELECT TOP (1)  C\r\n                              FROM   F(Number))) \r\n    FROM (VALUES(1)) V(Number)   \r\n\r\nWhich gives plan\r\n\r\n\r\n[![Plan2][2]][2]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/JSoTc.png\r\n  [2]: https://i.stack.imgur.com/5IKdF.png	2019-12-24 00:59:41.635287+00	4	4	1	20536	0	0	0	2019-12-24 00:59:41.635287+00	\N	I've come across another case where `CASE` / `COALESCE` do not short circuit. The following TVF will raise a PK violation if passed `1` as a parameter.	f	f
535	481	751	2013-09-24 21:05:20+00	Another example\r\n\r\n    CREATE TABLE T1 (C INT PRIMARY KEY)\r\n    \r\n    CREATE TABLE T2 (C INT PRIMARY KEY)\r\n    \r\n    INSERT INTO T1 \r\n    OUTPUT inserted.* INTO T2\r\n    VALUES (1),(2),(3);\r\n\r\n\r\nThe query\r\n\r\n    SET STATISTICS IO ON;\r\n\r\n    SELECT T1.C,\r\n           COALESCE(T1.C , CASE WHEN EXISTS (SELECT * FROM T2 WHERE T2.C = T1.C)  THEN -1 END)\r\n    FROM T1\r\n    OPTION (LOOP JOIN)\r\n\r\nShows no reads against `T2` at all. \r\n\r\nThe seek of `T2` is under a pass through predicate and the operator is never executed. But\r\n\r\n    SELECT T1.C,\r\n           COALESCE(T1.C , CASE WHEN EXISTS (SELECT * FROM T2 WHERE T2.C = T1.C)  THEN -1 END)\r\n    FROM T1\r\n    OPTION (MERGE JOIN)\r\n\r\n**Does** show that `T2` is read. Even though no value from `T2` is ever actually needed. \r\n\r\nOf course this is not really surprising but I thought worth adding to the counter example repository if only because it raises the issue of what short circuiting even means in a set based declarative language.	2019-12-24 00:59:41.890373+00	5	4	1	50443	0	0	0	2019-12-24 00:59:41.890373+00	\N	Another example	f	f
536	481	76	2012-02-09 05:24:42+00	I just wanted to mention a strategy you may not have considered. It may not be a match here, but it does come in handy sometimes. See if this modification gives you any better performance:\r\n\r\n    SELECT COALESCE(c.FirstName\r\n                ,(SELECT TOP 1 b.FirstName\r\n                  FROM TableA a \r\n                  JOIN TableB b ON .....\r\n                  WHERE C.FirstName IS NULL) -- this is the changed part\r\n                )\r\n\r\nAnother way to do it could be this (basically equivalent, but allows you to access more columns from the other query if necessary):\r\n\r\n    SELECT COALESCE(c.FirstName, x.FirstName)\r\n    FROM\r\n       TableC c\r\n       OUTER APPLY (\r\n          SELECT TOP 1 b.FirstName\r\n          FROM\r\n             TableA a \r\n             JOIN TableB b ON ...\r\n          WHERE\r\n             c.FirstName IS NULL -- the important part\r\n       ) x\r\n\r\nBasically this is a technique of "hard" joining tables but including the condition on when any rows at all should be JOINed. In my experience this has really helped execution plans at times.	2019-12-24 00:59:42.153546+00	1	4	1	12947	0	0	0	2019-12-24 00:59:42.153546+00	\N	I just wanted to mention a strategy you may not have considered. It may not be a match here, but it does come in handy sometimes. See if this modification gives you any better performance:	f	f
531	481	681	2011-09-19 15:10:23+00	**Nope**.  Here's a simple test:\r\n\r\n    SELECT COALESCE(1, (SELECT 1/0)) -- runs fine\r\n    SELECT COALESCE(NULL, (SELECT 1/0)) -- throws error\r\n\r\nIf the second condition is evaluated, an exception is thrown for divide-by-zero.\r\n\r\nPer the [MSDN Documentation][1] this is related to how `COALESCE` is viewed by the interpreter - it's just an easy way to write a `CASE` statement.  \r\n\r\n`CASE` is well known to be one of the only functions in SQL Server that (mostly) reliably short circuits.\r\n\r\nThere are some exceptions when comparing to scalar variables and aggregations as shown by Aaron Bertrand in another answer here (and this would apply both to `CASE` and `COALESCE`):\r\n\r\n    DECLARE @i INT = 1;\r\n    SELECT CASE WHEN @i = 1 THEN 1 ELSE MIN(1/0) END;\r\n\r\nwill generate a division by zero error.\r\n\r\nThis should be considered a bug, and as a rule `COALESCE` will parse from left to right.\r\n\r\n\r\n  [1]: http://msdn.microsoft.com/en-us/library/ms190349.aspx	2019-12-24 00:59:40.83728+00	3	4	1	12943	0	0	0	2019-12-24 00:59:40.83728+00	\N	**Nope**.  Here's a simple test:	f	f
533	481	12	2011-09-19 18:43:33+00	The documentation makes it reasonably clear that the *intention* is for `CASE` to short-circuit.  As [Aaron mentions](https://topanswers.xyz/databases?q=481#a532), there have been several reported instances where this has been shown to not always be true. So far, most of these have been acknowledged as bugs and fixed.\r\n\r\nThere are other issues with `CASE` (and therefore `COALESCE`) where side-effecting functions or sub-queries are used.  Consider:\r\n\r\n    SELECT COALESCE((SELECT CASE WHEN RAND() <= 0.5 THEN 999 END), 999);\r\n    SELECT ISNULL((SELECT CASE WHEN RAND() <= 0.5 THEN 999 END), 999);\r\n\r\nThe `COALESCE` form often returns null, as described in [a bug report][2] by Hugo Kornelis.\r\n\r\nThe demonstrated issues with optimizer transforms and common-expression-tracking mean that it is impossible to guarantee that `CASE` will short-circuit in all circumstances.\r\n\r\nI think you can be reasonably confident that `CASE` will short-circuit in general (particularly if a reasonably-skilled person inspects the execution plan, and that execution plan is 'enforced' with a plan guide or hints) but if you need an absolute guarantee, you have to write SQL that does not include the expression at all.\r\n\r\n  [2]: https://web.archive.org/web/20140329083016/https://connect.microsoft.com/SQLServer/feedback/details/546437/coalesce-subquery-1-may-return-null	2019-12-24 09:42:42.081098+00	5	4	1	12946	0	0	0	2019-12-24 00:59:41.383716+00	\N	The documentation makes it reasonably clear that the *intention* is for `CASE` to short-circuit.  As [Aaron mentions](https://topanswers.xyz/databases?q=481#a532), there have been several reported instances where this has been shown to not always be true. So far, most of these have been acknowledged as bugs and fixed.	f	f
537	481	682	2018-12-17 18:50:25+00	The actual standard says that all of the WHEN clauses (as well as the ELSE  clause) have to be parsed to determine the data type of the expression as a whole. I'd really have to get out some of my old notes to determine how an error is handled. But just off hand, 1/0  uses integers, so I would assume that while it's an error. It's an error with the integer data type. When you only have nulls in the coalesce list, it's a little trickier to determine the data type, and that's another problem.	2019-12-24 00:59:45.422278+00	1	4	1	225191	0	0	0	2019-12-24 00:59:45.422278+00	\N	The actual standard says that all of the WHEN clauses (as well as the ELSE  clause) have to be parsed to determine the data type of the expression as a whole. I'd really have to get out some of my old notes to determine how an error is handled. But just off hand, 1/0  uses integers, so I would assume that while it's an error. It's an error with the integer data type. When you only have nulls in the coalesce list, it's a little trickier to determine the data type, and that's another problem.	f	f
532	481	88	2011-09-19 17:47:15+00	How about this one - as reported to me by Itzik Ben-Gan, who was [told about it by Jaime Lafargue](http://aprendiendosqlserver.blogspot.com.es/2011/09/bug-en-sql-server-case-ejecuta.html)?\r\n\r\n    DECLARE @i INT = 1;\r\n    SELECT CASE WHEN @i = 1 THEN 1 ELSE MIN(1/0) END;\r\n\r\nResult:\r\n\r\n    Msg 8134, Level 16, State 1, Line 2\r\n    Divide by zero error encountered.\r\n\r\nThere are trivial workarounds of course, but the point is still that `CASE` does not *always* guarantee left-to-right evaluation / short-circuiting. [I reported the bug here](http://web.archive.org/web/20140422050537/http://connect.microsoft.com/SQLServer/feedback/details/690017/case-coalesce-wont-always-evaluate-in-textual-order) and it was closed as "by design." Paul White subsequently filed [this Connect item](http://web.archive.org/web/20140912144142/http://connect.microsoft.com/SQLServer/feedback/details/691535/aggregates-dont-follow-the-semantics-of-case), and it was closed as Fixed. Not because it was fixed per se, but because they updated Books Online with a more accurate description of the scenario where aggregates can change the evaluation order of a `CASE` expression. I recently [blogged more about this here](http://sqlperformance.com/2014/06/t-sql-queries/dirty-secrets-of-the-case-expression).\r\n\r\nWhile I agree that these are edge cases, that *most of the time* you can rely on left-to-right evaluation and short-circuiting, and that these are bugs that contradict the documentation and will probably eventually be fixed (this isn't definite - see the follow-up conversation on [Bart Duncan's blog post][1] to see why), I have to disagree when folks say that something is always true even if there is a single edge case that disproves it. If Itzik and others can find solitary bugs like this, it makes it at least in the realm of possibility that there are other bugs as well. And since we don't know the rest of the OP's query, we can't say for certain that he will rely on this short-circuiting but end up being bitten by it. So to me, the safer answer is:\r\n\r\nYou can *usually* rely on `CASE` to evaluate left-to-right and short-circuit, as described in the documentation, but it is not accurate to say that you can always do so. There are two demonstrated cases on this page where it is not true, and neither bug has been fixed in any publicly available version of SQL Server.\r\n\r\n[Here is another case](http://web.archive.org/web/20140329081147/http://connect.microsoft.com/SQLServer/feedback/details/780132/) (I need to stop doing that) where a `CASE` expression does not evaluate in the order you would expect, even though no aggregates are involved.\r\n\r\n  [1]: http://bartduncansql.wordpress.com/2011/03/03/dont-depend-on-expression-short-circuiting-in-t-sql-not-even-with-case/	2019-12-24 09:44:14.408498+00	5	4	1	12945	0	0	0	2019-12-24 00:59:41.104525+00	\N	How about this one - as reported to me by Itzik Ben-Gan, who was [told about it by Jaime Lafargue](http://aprendiendosqlserver.blogspot.com.es/2011/09/bug-en-sql-server-case-ejecuta.html)?	f	f
650	581	2	2020-01-22 11:30:22.109111+00	@@@ answer 362	2020-01-22 11:30:22.109111+00	4	1	1	\N	0	0	0	\N	\N	@@@ answer 362	f	f
503	467	665	2019-12-19 19:00:31+00	With constant trickle inserts, you very well may end up with numerous open deltastore rowgroups.  The reason for this is that when an insert starts, a new rowgroup is created if all of the existing ones are locked.  From [Stairway to Columnstore Indexes Level 5: Adding New Data To Columnstore Indexes](https://www.sqlservercentral.com/steps/stairway-to-columnstore-indexes-level-5-adding-new-data-to-columnstore-indexes)\r\n\r\n> Any insert of 102,399 or fewer rows is considered a "trickle insert". These rows are added to an open deltastore if one is available (and not locked), or else a new deltastore rowgroup is created for them.\r\n\r\nIn general, the columnstore index design is optimized for bulk inserts, and when using trickle inserts you'll need to run the reorg on a periodic basis.\r\n\r\nAnother option, recommended in the Microsoft documentation, is to trickle into a staging table (heap), and when it gets over 102,400 rows, insert those rows into the columstore index.  See[ Columnstore indexes - Data loading guidance](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-data-loading-guidance).\r\n\r\nIn any case, after deleting a lot of data, a reorg is recommended on a columnstore index so that the data will actually be deleted, and the resulting deltastore rowgroups will get cleaned up.	2019-12-19 21:39:10.615612+00	3	4	1	255988	0	0	0	2019-12-19 21:37:45.869741+00	\N	With constant trickle inserts, you very well may end up with numerous open deltastore rowgroups.  The reason for this is that when an insert starts, a new rowgroup is created if all of the existing ones are locked.  From [Stairway to Columnstore Indexes Level 5: Adding New Data To Columnstore Indexes](https://www.sqlservercentral.com/steps/stairway-to-columnstore-indexes-level-5-adding-new-data-to-columnstore-indexes)	f	f
542	467	686	2019-12-23 15:38:54+00	This looks like an edge case for the Clustered Columnstore Indexes, and in the end this is more an *HTAP* scenario under current Microsoft consideration - meaning a NCCI would be a better solution. Yeah, I imagine that loosing that Columnstore compression on the clustered index would be really bad storage-wise, but if your main storage are Delta-Stores than you are running non-compressed anyway.\r\n\r\nAlso:\r\n\r\n - What happens when you lower DOP of the DELETE statements ?\r\n - Did you try to add secondary Rowstore Nonclustered Indexes to lower blocking (yeah, there will be impact on the compression quality)\r\n	2019-12-27 04:58:04.381653+00	0	4	1	256180	0	0	0	2019-12-27 04:58:04.381653+00	\N	This looks like an edge case for the Clustered Columnstore Indexes, and in the end this is more an *HTAP* scenario under current Microsoft consideration - meaning a NCCI would be a better solution. Yeah, I imagine that loosing that Columnstore compression on the clustered index would be really bad storage-wise, but if your main storage are Delta-Stores than you are running non-compressed anyway.	f	f
541	467	45	2019-12-26 16:13:40+00	> Why would a table with a Clustered Columnstore Index have many open rowgroups?\r\n\r\nThere are many different scenarios that can cause this. I'm going to pass on answering the generic question in favor of addressing your specific scenario, which I think is what you want.\r\n\r\n> Is it possibly memory pressure or contention between the insert and the delete?\r\n\r\nIt's not memory pressure. SQL Server won't ask for a memory grant when inserting a single row into a columnstore table. It knows that the row will be inserted into a delta rowgroup so the memory grant isn't needed. It is possible to get more delta rowgroups than one might expect when inserting more than 102399 rows per `INSERT` statement and hitting the fixed 25 second memory grant timeout. That memory pressure scenario is for bulk loading though, not trickle loading.\r\n\r\nIncompatible locks between the `DELETE` and `INSERT` is a plausible explanation for what you're seeing with your table. Keep in mind I don't do trickle inserts in production, but the current locking implementation for deleting rows from a delta rowgroup seems to require a UIX lock. You can see this with a simple demo:\r\n\r\nThrow some rows into the delta store in the first session:\r\n\r\n    DROP TABLE IF EXISTS dbo.LAMAK;\r\n    \r\n    CREATE TABLE dbo.LAMAK (\r\n    ID INT NOT NULL,\r\n    INDEX C CLUSTERED COLUMNSTORE\r\n    );\r\n    \r\n    INSERT INTO dbo.LAMAK\r\n    SELECT TOP (64000) ROW_NUMBER() OVER (ORDER BY (SELECT NULL))\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2;\r\n\r\nDelete a row in the second session, but don't commit the change yet:\r\n\r\n    BEGIN TRANSACTION;\r\n    \r\n    DELETE FROM dbo.LAMAK WHERE ID = 1;\r\n\r\nLocks for the `DELETE` per `sp_whoisactive`:\r\n\r\n    <Lock resource_type="HOBT" request_mode="UIX" request_status="GRANT" request_count="1" />\r\n    <Lock resource_type="KEY" request_mode="X" request_status="GRANT" request_count="1" />\r\n    <Lock resource_type="OBJECT" request_mode="IX" request_status="GRANT" request_count="1" />\r\n    <Lock resource_type="OBJECT.INDEX_OPERATION" request_mode="S" request_status="GRANT" request_count="1" />\r\n    <Lock resource_type="PAGE" page_type="*" request_mode="IX" request_status="GRANT" request_count="1" />\r\n    <Lock resource_type="ROWGROUP" resource_description="ROWGROUP: 5:100000004080000:0" request_mode="UIX" request_status="GRANT" request_count="1" />\r\n\r\nInsert a new row in the first session:\r\n\r\n    INSERT INTO dbo.LAMAK\r\n    VALUES (0);\r\n\r\nCommit the changes in the second session and check `sys.dm_db_column_store_row_group_physical_stats`:\r\n\r\n[![dr dmv][1]][1]\r\n\r\nA new rowgroup was created because the insert requests an IX lock on the rowgroup that it changes. An IX lock is not compatible with a UIX lock. This seems to be the current internal implementation, and perhaps Microsoft will change it over time.\r\n\r\nIn terms of what to do how to fix it, you should consider how this data is used. Is it important for the data to be as compressed as possible? Do you need good rowgroup elimination on the `[CreationDate]` column? Would it be okay for new data to not show up in the table for a few hours? Would end users prefer if duplicates never showed up in the table as opposed to existing in it for up to four hours?\r\n\r\nThe answers to all of those questions determines the right path to addressing the issue. Here are a few options:\r\n\r\n 1. Run a `REORGANIZE` with the `COMPRESS_ALL_ROW_GROUPS = ON` option against the columnstore once a day. On average this will mean that the table won't exceed a million rows in the delta store. This is a good option if you don't need the best possible compression, you don't need the best rowgroup elimination on the `[CreationDate]` column, and you want to maintain the status quo of deleting duplicate rows every four hours.\r\n\r\n 2. Break the `DELETE` into separate `INSERT` and `DELETE` statements. Insert the rows to delete into a temp table as a first step and delete them with `TABLOCKX` in the second query. This doesn't need to be in one transaction based on your data loading pattern (only inserts) and the method that you use to find and remove duplicates. Deleting a few hundred rows should be very fast with good elimination on the `[CreationDate]` column, which you will eventually get with this approach. The advantage of this approach is that your compressed rowgroups will have tight ranges for `[CreationDate]`, assuming that the date for that column is the current date. The disadvantage is that your trickle inserts will be blocked from running for maybe a few seconds.\r\n\r\n 3. Write new data to a staging table and flush it into the columnstore every X minutes. As part of the flush process you can skip inserting duplicates, so the main table will never contain duplicates. The other advantage is that you control how often the data flushes so you can get rowgroups of the desired quality. The disadvantage is that new data will be delayed from appearing in the `[dbo].[NetworkVisits]` table. You could try a view that combines the tables but then you have to be careful that your process to flush data will result in a consistent view of the data for end users (you don't want rows to disappear or to show up twice during the process).\r\n\r\nFinally, I do not agree with other answers that a redesign of the table should be considered. You're only inserting 9 rows per second on average into the table which just isn't a high rate. A single session can do 1500 singleton inserts per second into a columnstore table with six columns. You may want to change the table design once you start to see numbers around that.\r\n\r\n  [1]: https://i.stack.imgur.com/GsSbi.png	2019-12-27 04:58:03.85035+00	3	4	1	256316	0	0	0	2019-12-27 04:58:03.85035+00	\N	> Why would a table with a Clustered Columnstore Index have many open rowgroups?	f	f
151	209	12	2018-06-09 03:03:31+00	> Is SQL Server allowed to evaluate `A <> B` as `A < B OR A > B`, even if one of the expressions is non-deterministic?\r\n\r\nThis is a somewhat controversial point, and the answer is a qualified "yes".\r\n\r\nThe best discussion I am aware of was given in answer to Itzik Ben-Gan's Connect bug report [Bug with NEWID and Table Expressions][1], which was closed as won't fix. Connect has since been retired, so the link there is to a web archive. Sadly, a lot of useful material was lost (or made harder to find) by the demise of Connect. Anyway, the most useful quotes from Jim Hogg of Microsoft there are:\r\n\r\n>This hits to the very heart of the issue - is optimization allowed to change a program's semantics? Ie: if a program yields certain answers, but runs slowly, is it legitimate for a Query Optimizer make that program run faster, yet also change the results given?\r\n>\r\n>Before shouting "NO!" (my own personal inclination too :-), consider: the good news is that, in 99% of cases, the answers ARE the same. So Query Optimization is a clear win. The bad news is that, if the query contains side-effecting code, then different plans CAN indeed yield different results. And NEWID() is one such side-effecting (non-deterministic) 'function' that exposes the difference. [Actually, if you experiment, you can devise others - for example, short-circuit evaluation of AND clauses: make the second clause throw an arithmetic divide-by-zero - different optimizations may execute that second clause BEFORE the first clause] This reflects Craig's explanation, elsewhere in this thread, that SqlServer does not guarantee when scalar operators are executed.\r\n>\r\n>So, we have a choice: if we want to guarantee a certain behavior in the presence of non-deterministic (side-effecting) code - so that results of JOINs, for example, follow the semantics of a nested-loop execution - then we can use appropriate OPTIONs to force that behavior - as UC points out. But the resulting code will run slow - that's the cost of, in effect, hobbling the Query Optimizer.\r\n>\r\n>All that said, we are moving the Query Optimizer in the direction of "as expected" behavior for NEWID() - trading off performance for "results as expected".\r\n\r\nOne example of the changing of behaviour in this regard over time is [NULLIF works incorrectly with non-deterministic functions such as RAND()][2]. There are also other similar cases using e.g. `COALESCE` with a subquery that can produce unexpected results, and which are also being addressed gradually.\r\n\r\nJim continues:\r\n\r\n>Closing the loop . . . I've discussed this question with the Dev team. And eventually we have decided not to change current behavior, for the following reasons:\r\n>\r\n>1) The optimizer does not guarantee timing or number of executions of scalar functions. This is a long-estabilished tenet. It's the fundamental 'leeway' tha allows the optimizer enough freedom to gain significant improvements in query-plan execution.\r\n>\r\n>2) This "once-per-row behavior" is not a new issue, although it's not widely discussed. We started to tweak its behavior back in the Yukon release. But it's quite hard to pin down precisely, in all cases, exactly what it means! For example, does it a apply to interim rows calculated 'on the way' to the final result? - in which case it clearly depends on the plan chosen. Or does it apply only to the rows that will eventually appear in the completed result? - there's a nasty recursion going on here, as I'm sure you'll agree!\r\n>\r\n>3) As I mentioned earlier, we default to "optimize performance" - which is good for 99% of cases. The 1% of cases where it might change results are fairly easy to spot - side-effecting 'functions' such as NEWID - and easy to 'fix' (trading perf, as a consequence). This default to "optimize performance" again, is long-established, and accepted. (Yes, it's not the stance chosen by compilers for conventional programming languages, but so be it).\r\n>\r\n>So, our recommendations are:\r\n>\r\n>a) Avoid reliance on non-guaranteed timing and number-of-executions semantics.  \r\nb) Avoid using NEWID() deep in table expressions.  \r\nc) Use OPTION to force a particular behavior (trading perf)\r\n>\r\n>Hope this explanation helps clarify our reasons for closing this bug as "won't fix".\r\n\r\n---\r\n\r\n>Interestingly, `AND NOT (s_guid = NEWID())` yields the same execution plan\r\n\r\nThis is a consequence of normalization, which happens very early during query compilation. Both expressions compile to exactly the same normalized form, so the same execution plan is produced.\r\n\r\n  [1]: https://web.archive.org/web/20160626085155/https://connect.microsoft.com/SQLServer/feedbackdetail/view/350485/bug-with-newid-and-table-expressions\r\n  [2]: https://feedback.azure.com/forums/908035-sql-server/suggestions/32886796-nullif-works-incorrectly-with-non-deterministic-fu	2019-11-30 13:03:18.568793+00	2	4	1	209170	0	0	0	2019-11-30 13:01:29.525041+00	\N	> Is SQL Server allowed to evaluate `A <> B` as `A < B OR A > B`, even if one of the expressions is non-deterministic?	f	f
465	448	621	2015-10-14 21:15:51+00	Remus has helpfully pointed out that the max length of the `VARCHAR` column impacts the estimated row size and therefore memory grants that SQL Server provides.\r\n\r\nI tried to do a bit more research to expand on the "from this on things cascade" part of his answer. I don't have a complete or concise explanation, but here is what I found.\r\n\r\n**Repro script**\r\n\r\n[I created a full script][1] that generates a fake data set on which index creation takes roughly 10x as long on my machine for the `VARCHAR(256)` version. The data used is exactly the same, but the first table uses the actual max lengths of `18`, `75`, `9`, `15`, `123`, and `5`, while all columns use a max length of `256` in the second table.\r\n\r\n**Keying the original table**\r\n\r\nHere we see that the original query completes in about 20 seconds and the logical reads are equal to the table size of `~1.5GB` (195K pages, 8K per page).\r\n\r\n\t-- CPU time = 37674 ms,  elapsed time = 19206 ms.\r\n\t-- Table 'testVarchar'. Scan count 9, logical reads 194490, physical reads 0\r\n\tCREATE CLUSTERED INDEX IX_testVarchar\r\n\tON dbo.testVarchar (s1, s2, s3, s4)\r\n\tWITH (MAXDOP = 8) -- Same as my global MAXDOP, but just being explicit\r\n\tGO\r\n\r\n**Keying the VARCHAR(256) table**\r\n\r\nFor the `VARCHAR(256)` table, we see that the elapsed time has increased dramatically. \r\n\r\nInterestingly, neither the CPU time nor the logical reads increase. This makes sense given that the table has the exact same data, but it doesn't explain why the elapsed time is so much slower.\r\n\r\n\t-- CPU time = 33212 ms,  elapsed time = 263134 ms.\r\n\t-- Table 'testVarchar256'. Scan count 9, logical reads 194491\r\n\tCREATE CLUSTERED INDEX IX_testVarchar256\r\n\tON dbo.testVarchar256 (s1, s2, s3, s4)\r\n\tWITH (MAXDOP = 8) -- Same as my global MAXDOP, but just being explicit\r\n\tGO\r\n\r\n**I/O and wait stats: original**\r\n\r\nIf we capture a bit more detail (using [p_perfMon, a procedure that I wrote][2]), we can see that the vast majority of the I/O is performed on the `LOG` file. We see a relatively modest amount of I/O on the actual `ROWS` (the main data file), and the primary wait type is `LATCH_EX`, indicating in-memory page contention.\r\n\r\nWe can also see that my spinning disk is somewhere between "bad" and "shockingly bad", [according to Paul Randal][3] :)\r\n\r\n[![enter image description here][4]][4]\r\n\r\n**I/O and wait stats: VARCHAR(256)**\r\n\r\nFor the `VARCHAR(256)` version, the I/O and wait stats look completely different! Here we see a huge increase in the I/O on the data file (`ROWS`), and the stall times now make Paul Randal simply say "WOW!".\r\n\r\nIt's not surprising that the #1 wait type is now `IO_COMPLETION`. But why is so much I/O generated?\r\n\r\n[![enter image description here][5]][5]\r\n\r\n**Actual query plan: VARCHAR(256)**\r\n\r\nFrom the query plan, we can see that the `Sort` operator has a recursive spill (5 levels deep!) in the `VARCHAR(256)` version of the query. (There is no spill at all in the original version.)\r\n\r\n[![enter image description here][6]][6]\r\n\r\n**Live query progress: VARCHAR(256)**\r\n\r\nWe can [use sys.dm_exec_query_profiles to view live query progress in SQL 2014+][7]. In the original version, the entire `Table Scan` and `Sort` are processed without any spills (`spill_page_count` remains `0` throughout).\r\n\r\nIn the `VARCHAR(256)` version, however, we can see that page spills quickly accumulate for the `Sort` operator. Here is a snapshot of the query progress just before the query completes. The data here is aggregated across all threads.\r\n\r\n[![enter image description here][8]][8]\r\n\r\nIf I dig into each thread individually, I see that 2 threads complete the sort within about 5 seconds (@20 seconds overall, after 15 seconds spent on the table scan). If all threads progressed at this rate, the `VARCHAR(256)` index creation would have completed in roughly the same time as the original table.\r\n\r\nHowever, the remaining 6 threads progress at a much slower rate. This may be due to the way that memory is allocated and the way that the threads are being held up by I/O as they are spilling data. I don't know for sure though.\r\n\r\n[![enter image description here][9]][9]\r\n\r\n**What can you do?**\r\n\r\nThere are a number of things you might considering trying:\r\n\r\n* Work with the vendor to roll back to a previous version. If that's not possible, let the vendor that you are not happy with this change so that they can consider reverting it in a future release.\r\n* When adding your index, consider using `OPTION (MAXDOP X)` where `X` is a lower number than your current server-level setting. When I used `OPTION (MAXDOP 2)` on this specific data set on my machine, the `VARCHAR(256)` version completed in `25 seconds` (compared to 3-4 minutes with 8 threads!). It's possible that the spilling behavior is exacerbated by higher parallelism.\r\n* If additional hardware investment is a possibility, profile the I/O (the likely bottleneck) on your system and consider using an SSD to reduce the latency of the I/O incurred by spills.\r\n\r\n**Further reading**\r\n\r\nPaul White has a nice blog post on [the internals of SQL Server sorts][10] that may be of interest. It does talk a little bit about spilling, thread skew, and memory allocation for parallel sorts.\r\n\r\n\r\n  [1]: https://gist.github.com/anonymous/71ee72101c806441676f\r\n  [2]: https://gist.github.com/anonymous/ec12170fbe9b76224fae\r\n  [3]: http://www.sqlskills.com/blogs/paul/are-io-latencies-killing-your-performance/\r\n  [4]: https://i.stack.imgur.com/hIhBS.png\r\n  [5]: https://i.stack.imgur.com/UJ2v8.png\r\n  [6]: https://i.stack.imgur.com/bASQk.png\r\n  [7]: https://dba.stackexchange.com/questions/113124/query-is-slow-in-sql-server-2014-fast-in-sql-server-2012/113126#113126\r\n  [8]: https://i.stack.imgur.com/GG3yq.png\r\n  [9]: https://i.stack.imgur.com/QkLu7.png\r\n  [10]: http://sqlperformance.com/2015/04/sql-plan/internals-of-the-seven-sql-server-sorts-part-1	2019-12-15 06:17:49.366641+00	7	4	1	118077	0	0	0	2019-12-15 06:16:32.35705+00	\N	Remus has helpfully pointed out that the max length of the `VARCHAR` column impacts the estimated row size and therefore memory grants that SQL Server provides.	f	f
464	448	620	2015-10-08 14:46:11+00	The intermediate sort table will be differently estimated between the two cases. This will lead to different memory grant requests (`VARCHAR(256)` will be bigger) and likely a much smaller actual grant, percent wise, compared with the 'ideal' request. I guess this leads to spills during sort. \r\n\r\nTesting the script from Geoff (on 100k rows only) I can clearly see difference in the sort estimated row size (141B vs. 789B). From this on things cascade.	2019-12-15 06:16:32.070648+00	2	4	1	117420	0	0	0	2019-12-15 06:16:32.070648+00	\N	The intermediate sort table will be differently estimated between the two cases. This will lead to different memory grant requests (`VARCHAR(256)` will be bigger) and likely a much smaller actual grant, percent wise, compared with the 'ideal' request. I guess this leads to spills during sort.	f	f
592	542	15	2019-07-23 00:49:36+00	> What does that indicate?\r\n\r\nIt indicates that invalid page flags for protection are set, mostly likely caused by something in the hardware, Windows I/O stack, or 3rd party software. In *supported* SQL Server versions there are three options for page protections:\r\n\r\n 1. None\r\n 2. Torn Page\r\n 3. Checksum\r\n\r\nImagine reading a page from disk and doing basic sanity checks against it. One check might be, let's check to make sure the page protection options are set appropriately. If there are three options, having a 4th value set or having multiple options set would be invalid.\r\n\r\n>I'm just curious about the "root cause" piece, and what could possibly cause this flavor of the logical consistency-based I/O error.\r\n\r\nIt indicates that something scribbled the data on disk or the data being read from the disk into memory. The root cause for on-disk corruption is extremely hard to get if it isn't reproducible, so at best it's a bad piece of hardware or software in the I/O stack. Worst is it's another application not playing nice and doing whatever it wants to files on that server. 	2020-01-10 20:27:31.623553+00	5	4	1	243493	0	0	0	2020-01-10 20:27:31.623553+00	\N	> What does that indicate?	f	f
506	470	751	2013-12-25 21:42:33+00	There is in fact no useful way to do this as far as I can see.\r\n\r\nThe other answer mentions `DBCC PAGE` and leaves it up to the reader to figure out the details. From experimentation I assume they mean `bUse1`.\r\n\r\nThis fails to take account that `DBCC PAGE` is itself a use of the page and the value gets updated **before** it is shown to us.\r\n\r\nA script demonstrating this is below (takes 12 seconds to run).\r\n\r\n    USE tempdb;\r\n    \r\n    CREATE TABLE T(X INT);\r\n    \r\n    INSERT INTO T VALUES(1);\r\n    \r\n    DECLARE @DBCCPAGE NVARCHAR(100);\r\n    \r\n    SELECT @DBCCPAGE = 'DBCC PAGE(0,' + CAST(file_id AS VARCHAR) + ',' + CAST(page_id AS VARCHAR) + ',0) WITH TABLERESULTS;'\r\n    FROM   T CROSS APPLY  sys.fn_PhysLocCracker (%%physloc%%)\r\n    \r\n    DECLARE @DbccResults TABLE \r\n    (\r\n          ID INT IDENTITY,\r\n          ParentObject VARCHAR(1000)NULL,\r\n          Object VARCHAR(4000)NULL,\r\n          Field VARCHAR(1000)NULL,\r\n          ObjectValue VARCHAR(MAX)NULL\r\n    )    \r\n    INSERT INTO @DbccResults EXEC(@DBCCPAGE)  \r\n    WAITFOR DELAY '00:00:07'\r\n    INSERT INTO @DbccResults EXEC(@DBCCPAGE)  \r\n    WAITFOR DELAY '00:00:05'\r\n    INSERT INTO @DbccResults EXEC(@DBCCPAGE)             \r\n    \r\n    SELECT *\r\n    FROM @DbccResults   \r\n    WHERE Field = 'bUse1'    \r\n    ORDER BY ID\r\n    \r\n    EXEC(@DBCCPAGE) \r\n    \r\n    DROP TABLE T\r\n\r\nTypical results are \r\n\r\n| ID | ParentObject |         Object          | Field | ObjectValue |\r\n|----|--------------|-------------------------|-------|-------------|\r\n|  8 | BUFFER:      | BUF @0x00000002FE1F1440 | bUse1 |       54938 |\r\n| 49 | BUFFER:      | BUF @0x00000002FE1F1440 | bUse1 |       54945 |\r\n| 90 | BUFFER:      | BUF @0x00000002FE1F1440 | bUse1 |       54950 |\r\n\r\nWith the second result being\r\n\r\n|||||\r\n|---------|-------------------------|--------------|--------------------|\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bpage        | 0x00000002F4968000 |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bhash        | 0x0000000000000000 |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bpageno      | (1:120)            |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bdbid        | 8                  |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | breferences  | 0                  |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bcputicks    | 0                  |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bsampleCount | 0                  |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bUse1        | 54950              |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bstat        | 0x9                |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | blog         | 0x1c9a             |\r\n| BUFFER: | BUF @0x00000002FE1F1440 | bnext        | 0x0000000000000000 |\r\n\r\nThe output after the 7 second delay is incremented by 7 and after the 5 second delay by 5.\r\n\r\nSo it seems clear that these LRU values are seconds since some epoch. Restarting the SQL Server service does not alter the epoch but restarting the machine does.\r\n\r\nThe value rolls over every 65,536 seconds so I presume that it just uses something like `system_up_time mod 65536`\r\n\r\nThis does leave one unanswered questions in my mind (any takers?). SQL Server uses `LRU-K` with `K=2` according to the internals book. Shouldn't there be a `bUse2`? If so where is that?\r\n\r\nThere is one way of observing the `bUse1` value without changing it that I know of though and that is [demonstrated by Bob Ward][1] here.\r\n\r\nAttach a debugger to the SQL Server process and display referenced memory for the memory address of the buffer structure (shown to be `0x00000002FE1F1440` above).\r\n\r\nI did this immediately after running the script above and saw the following.\r\n\r\n![enter image description here][2]\r\n\r\n(From previous experimentation I'd found the highlighted bytes were the only ones that changed between runs so these are definitely the right ones).\r\n\r\nOne surprising aspect is that `SELECT CAST(0xc896 as int)` = `51350`.\r\n\r\nThis is exactly 3600 (one hour) less than reported by `DBCC PAGE`.\r\n\r\nI believe this to be some attempt to disfavour pages being kept in cache by calling `DBCC PAGE` itself. For a "normal" page select this one hour adjustment does not occur. After running\r\n\r\n    SELECT *\r\n    FROM T\r\n    \r\n    SELECT ((ms_ticks) % 65536000) / 1000 AS [Roughly Expected Value]\r\n    FROM sys.dm_os_sys_info\r\n\r\nThe value shown in memory is as expected.\r\n\r\nThe `DBCC` command actually updates that value twice. Once at \r\n\r\n```none\r\nsqlmin.dll!BPool::Touch()  + 0x3bfe bytes\t\r\nsqlmin.dll!BPool::Get()  + 0x12e bytes\t\r\nsqlmin.dll!LatchedBuf::ReadLatch()  + 0x14f bytes\t\r\nsqlmin.dll!UtilDbccDumpPage()  + 0x364 bytes\t\r\nsqlmin.dll!DbccPage()  + 0xfa bytes\t\r\nsqllang.dll!DbccCommand::Execute()  + 0x153 bytes\r\n```\r\n\r\nWith the higher value then again at \r\n\r\n```none\r\nsqlmin.dll!LatchedBuf::FreeAndUnlatch()  + 0x71 bytes\r\nsqlmin.dll!UtilDbccDumpPage()  + 0x545 bytes\r\nsqlmin.dll!DbccPage()  + 0xfa bytes\r\nsqllang.dll!DbccCommand::Execute()  + 0x153 bytes\r\n```\r\n\r\nWith the lower one.\r\n\r\nI'm not aware of any way to get buffer addresses for pages without using `DBCC BUFFER`/ `DBCC PAGE` any way though and using both of these changes the value we are trying to inspect!\r\n\r\n  [1]: http://youtu.be/9n6FzIf5Hy4?t=22m36s\r\n  [2]: https://i.stack.imgur.com/MNIz0.png	2019-12-22 09:22:20.957542+00	5	4	1	55595	0	0	0	2019-12-22 09:17:41.439835+00	\N	There is in fact no useful way to do this as far as I can see.	f	f
202	256	12	2019-06-05 20:46:41+00	## Summary\r\n\r\nSQL Server uses the correct join (inner or outer) and adds projections where necessary to **honour all the semantics** of the original query when performing [internal translations][1] between *apply* and *join*.\r\n\r\nThe differences in the plans can all be explained by the [different semantics][2] of aggregates with and without a group by clause in SQL Server.\r\n\r\n---\r\n\r\n## Details\r\n\r\n### Join vs Apply\r\n\r\nWe will need to be able to distinguish between an *apply* and a *join*:\r\n\r\n* **Apply** \r\n\r\n The inner (lower) input of the *apply* is run for each row of the outer (upper) input, with one or more inner side parameter values provided by the current outer row. The overall result of the *apply* is the combination (union all) of all the rows produced by the parameterized inner side executions. The presence of parameters means *apply* is sometimes referred to as a correlated join.\r\n\r\n An *apply* is always implemented in execution plans by the *Nested Loops* operator. The operator will have an *Outer References* property rather than join predicates. The outer references are the parameters passed from the outer side to the inner side on each iteration of the loop.\r\n\r\n* **Join**\r\n\r\n A join evaluates its join predicate at the join operator. The join may generally be implemented by *Hash Match*, *Merge*, or *Nested Loops* operators in SQL Server.\r\n\r\n When *Nested Loops* is chosen, it can be distinguished from an *apply* by the lack of *Outer References* (and usually the presence of a join predicate). The inner input of a *join* never references values from the outer input - the inner side is still executed once for each outer row, but inner side executions do not depend on any values from the current outer row.\r\n\r\nFor more details see my post [Apply versus Nested Loops Join][1].\r\n\r\n>...why is there an **outer** join in the execution plan instead of an **inner** join?\r\n\r\nThe outer join arises when the optimizer transforms an *apply* to a *join* (using a rule called `ApplyHandler`) to see if it can find a cheaper join-based plan. The join is required to be an outer join for *correctness* when the *apply* contains a *scalar aggregate*. An inner join would not be *guaranteed* to produce the same results as the original *apply* as we will see.\r\n\r\n### Scalar and Vector Aggregates\r\n\r\n* An aggregate without a corresponding `GROUP BY` clause is a **scalar** aggregate.\r\n* An aggregate with a corresponding `GROUP BY` clause is a **vector** aggregate.\r\n\r\nIn SQL Server, a *scalar* aggregate will always produce a row, even if it is given no rows to aggregate. For example, the scalar `COUNT` aggregate of no rows is zero. A *vector* `COUNT` aggregate of no rows is the empty set (no rows at all).\r\n\r\nThe following toy queries illustrate the difference. You can also read more about scalar and vector aggregates in my article [Fun with Scalar and Vector Aggregates][2].\r\n\r\n```\r\n-- Produces a single zero value\r\nSELECT COUNT_BIG(*) FROM #MyTable AS MT WHERE 0 = 1;\r\n\r\n-- Produces no rows\r\nSELECT COUNT_BIG(*) FROM #MyTable AS MT WHERE 0 = 1 GROUP BY ();\r\n```\r\n\r\nDemo:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=ae2b672d78d5bd51ec8b29abe301e078\r\n\r\n### Transforming apply to join\r\n\r\nI mentioned before that the join is required to be an outer join for *correctness* when the original *apply* contains a *scalar aggregate*. To show why this is the case in detail, I will use a simplified example of the question query:\r\n\r\n```\r\nDECLARE @A table (A integer NULL, B integer NULL);\r\nDECLARE @B table (A integer NULL, B integer NULL);\r\n\r\nINSERT @A (A, B) VALUES (1, 1);\r\nINSERT @B (A, B) VALUES (2, 2);\r\n\r\nSELECT * FROM @A AS A\r\nCROSS APPLY (SELECT c = COUNT_BIG(*) FROM @B AS B WHERE B.A = A.A) AS CA;\r\n```\r\n\r\nThe correct result for column `c` is **zero**, because the `COUNT_BIG` is a **scalar** aggregate. When translating this apply query to join form, SQL Server generates an internal alternative that would look similar to the following if it were expressed in T-SQL:\r\n\r\n```\r\nSELECT A.*, c = COALESCE(J1.c, 0)\r\nFROM @A AS A\r\nLEFT JOIN\r\n(\r\n    SELECT B.A, c = COUNT_BIG(*) \r\n    FROM @B AS B\r\n    GROUP BY B.A\r\n) AS J1\r\n    ON J1.A = A.A;\r\n```\r\n\r\nTo rewrite the apply as an uncorrelated join, we have to introduce a `GROUP BY` in the derived table (otherwise there could be no `A` column to join on). The join has to be an **outer** join so each row from table `@A` continues to produce a row in the output. The left join will produce a `NULL` for column `c` when the join predicate does not evaluate to true. That `NULL` needs to be translated to zero by `COALESCE` to complete a correct transformation from *apply*.\r\n\r\nThe demo below shows how both outer join and `COALESCE` are required to produce the same results using *join* as the original *apply* query:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=8302f12448fffe6ed0ce192791cebd4c\r\n\r\n### With the `GROUP BY`\r\n\r\n>...why does uncommenting the group by clause result in an inner join?\r\n\r\nContinuing the simplified example, but adding a `GROUP BY`:\r\n\r\n```\r\nDECLARE @A table (A integer NULL, B integer NULL);\r\nDECLARE @B table (A integer NULL, B integer NULL);\r\n\r\nINSERT @A (A, B) VALUES (1, 1);\r\nINSERT @B (A, B) VALUES (2, 2);\r\n\r\n-- Original\r\nSELECT * FROM @A AS A\r\nCROSS APPLY \r\n(SELECT c = COUNT_BIG(*) FROM @B AS B WHERE B.A = A.A GROUP BY B.A) AS CA;\r\n\r\n```\r\n\r\nThe `COUNT_BIG` is now a **vector** aggregate, so the correct result for an empty input set is no longer zero, it is **no row at all**. In other words, running the statements above produces no output.\r\n\r\nThese semantics are much easier to honour when translating from *apply* to *join*,  since `CROSS APPLY` naturally rejects any outer row that generates no inner side rows. We can therefore safely use an inner join now, with no extra expression projection:\r\n\r\n```\r\n-- Rewrite\r\nSELECT A.*, J1.c \r\nFROM @A AS A\r\nJOIN\r\n(\r\n    SELECT B.A, c = COUNT_BIG(*) \r\n    FROM @B AS B\r\n    GROUP BY B.A\r\n) AS J1\r\n    ON J1.A = A.A;\r\n```\r\n\r\nThe demo below shows that the inner join rewrite produces the same results as the original apply with vector aggregate:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=43a1f0fd09c650e201e0441c77847248\r\n\r\nThe optimizer happens to choose a merge inner join with the small table because it finds a cheap *join* plan quickly (good enough plan found). The cost based optimizer may go on to rewrite the join back to an apply - perhaps finding a cheaper apply plan, as it will here if a loop join or forceseek hint is used - but it is not worth the effort in this case.\r\n\r\n### Notes\r\n\r\nThe simplified examples use different tables with different contents to show the semantic differences more clearly.\r\n\r\nOne could argue that the optimizer ought to be able to reason about a self-join not being capable generating any mismatched (non-joining) rows, but it does not contain that logic today. Accessing the same table multiple times in a query is not guaranteed to produce the same results in general anyway, depending on isolation level and concurrent activity.\r\n\r\nThe optimizer worries about these semantics and edge cases so you don't have to.\r\n\r\n---\r\n\r\n### Bonus: Inner *Apply* Plan\r\n\r\nSQL Server **can** produce an inner *apply* plan (not an inner *join* plan!) for the example query, it just chooses not to for cost reasons. The cost of the outer join plan shown in the question is **0.02898** units on my laptop's SQL Server 2017 instance.\r\n\r\nYou can force an *apply* (correlated join) plan using undocumented and unsupported trace flag 9114 (which disables `ApplyHandler` etc.) just for illustration:\r\n\r\n```\r\nSELECT      *\r\nFROM        #MyTable AS mt\r\nCROSS APPLY \r\n(\r\n    SELECT COUNT_BIG(DISTINCT mt2.Col_B) AS dc\r\n    FROM   #MyTable AS mt2\r\n    WHERE  mt2.Col_A = mt.Col_A \r\n    --GROUP BY mt2.Col_A\r\n) AS ca\r\nOPTION (QUERYTRACEON 9114);\r\n```\r\n\r\nThis produces an *apply* nested loops plan with a lazy index spool. The total estimated cost is **0.0463983** (higher than the selected plan):\r\n\r\n[![Index Spool apply plan][6]][6]\r\n\r\nNote that the execution plan using *apply* nested loops produces correct results using "inner join" semantics regardless of the presence of the `GROUP BY` clause.\r\n\r\nIn the real world, we would typically have an index to support a seek on the inner side of the *apply* to encourage SQL Server to choose this option naturally, for example:\r\n\r\n```\r\nCREATE INDEX i ON #MyTable (Col_A, Col_B);\r\n```\r\n\r\nDemo:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=b314493c20a1b5206391c540400a1dae\r\n\r\n\r\n  [1]: https://www.sql.kiwi/2019/06/apply-versus-nested-loops-join.html\r\n  [2]: https://www.sql.kiwi/2012/03/fun-with-aggregates.html\r\n  [3]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=ae2b672d78d5bd51ec8b29abe301e078\r\n  [4]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=8302f12448fffe6ed0ce192791cebd4c\r\n  [5]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=43a1f0fd09c650e201e0441c77847248\r\n  [6]: https://i.stack.imgur.com/TCrjK.png\r\n  [7]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=b314493c20a1b5206391c540400a1dae	2019-12-04 01:17:47.369736+00	2	4	1	239907	0	0	0	2019-12-04 01:12:23.041752+00	\N	Summary	f	f
39	26	8	2019-11-18 06:50:52.673797+00	I agree.\r\n\r\nEven if we allow the users to select a font in their preferences the default font for users upon first visit or for unregistered users should be a sans-serif.	2019-11-18 06:50:52.673797+00	5	4	1	\N	0	0	0	\N	\N	I agree.	f	f
31	26	2	2019-11-17 10:54:55.025919+00	We could try and find a font that suits most people the most, and perhaps we can do better than Quattrocento.\r\n\r\nBefore we do, I think we should consider just letting everone choose their own font, so please let us know what you think of that suggestion [on this other meta post](/meta?q=56).	2019-11-17 10:54:55.025919+00	2	1	1	\N	0	0	0	\N	\N	We could try and find a font that suits most people the most, and perhaps we can do better than Quattrocento.	f	f
153	26	96	2019-11-30 14:14:19.124235+00	I don't think it's a given that "sans serif" is better for screens. There are screen optimized serif and sans-serif fonts, and many of the same benefits of serif fonts for print work such as books apply to body content on the web as well.\r\n\r\nIt is true that many serif fonts optimized for print do _not_ offer good readability on digital screens, especically at small sizes.\r\n\r\nI think the default for body content of posts should actually be a serif font, but one with good screen rendering properties.\r\n\r\nI would consider making site Chrome/UI elements sans to diferenciate them from user generated content (posts, comments).\r\n\r\nAdditionally a user preference would be a welcome change over most sites which force you to use a single upstream choice that is not necessarily the best for all users.	2019-11-30 14:14:19.124235+00	1	4	2	\N	0	0	0	\N	\N	I don't think it's a given that "sans serif" is better for screens. There are screen optimized serif and sans-serif fonts, and many of the same benefits of serif fonts for print work such as books apply to body content on the web as well.	f	f
5	15	12	2019-11-09 11:03:31.778586+00	SQL Server makes **no guarantees** about when, and how many times, a scalar expression will be evaluated at execution time.\r\n\r\nThis was confirmed by Jim Hogg of Microsoft in response to a [bug report][1] filed by Itzik Ben-Gan:\r\n\r\n> The optimizer does not guarantee timing or number of executions of scalar functions. This is a long-estabilished tenet. It's the fundamental 'leeway' tha (sic) allows the optimizer enough freedom to gain significant improvements in query-plan execution.\r\n>\r\n> So, our recommendations are:\r\n>\r\n> a) Avoid reliance on non-guaranteed timing and number-of-executions semantics.\r\n\r\nRuntime behaviour depends on many complicated (and undocumented) interacting details within the query optimizer and execution engine. The placement, execution, and caching of scalar expressions is difficult to predict, even for people with advanced knowledge of the internals.\r\n\r\nTo take one example, evaluation of a deterministic expression may be deferred from the point it appears in an execution plan, to be evaluated when a later operation requires the expression result. For more details and examples see my article [Compute Scalars, Expressions and Execution Plan Performance][2]\r\n\r\nThe cost-based optimizer makes little effort to avoid repeated scalar computations. This is because scalar operations are hardly costed at all, as explained by optimizer architect Conor Cunningham in [SQL Server Scalar Operator Costing aka “umm, what costing?”][3]\r\n\r\nIt is best to treat the behaviour here as **undefined**. There is nothing much in execution plans to help you figure out what happened in detail at runtime, and it will not often be convenient to attach a debugger to examine detailed operations, as I did in my blog post.\r\n\r\nOften, it will not matter too much if simple scalar expressions are evaluated more often than necessary. Where an important performance impact is suspected, the most robust way to work around the issue is to explicitly materialize the result, e.g. in a variable or temporary table.\r\n\r\nViews, derived tables, and common table expressions are not "optimization fences" in SQL Server, so while using these *may* encourage the runtime behaviour you are after, it does not come with any particular guarantee.\r\n\r\n[1]: https://web.archive.org/web/20150506160803/http://connect.microsoft.com/SQLServer/feedback/details/350485/bug-with-newid-and-table-expressions\r\n[2]: https://www.sql.kiwi/2012/09/compute-scalars-expressions-and-execution-plan-performance.html\r\n[3]: https://www.sqlskills.com/blogs/conor/sql-server-scalar-operator-costing-aka-umm-what-costing/	2019-11-12 08:54:38.843075+00	5	4	1	\N	0	0	0	\N	\N	SQL Server makes **no guarantees** about when, and how many times, a scalar expression will be evaluated at execution time.	f	f
603	556	160	2019-04-26 15:17:55+00	>Assuming I have 100 million tables, I calculate less than a 1-in-1-trillion chance of a collision\r\n\r\nRemember this is the "[birthday problem][1]".  You're not trying to generate a collision for a single given hash, but rather measuring the probability that none of the many pairs of values will collide.\r\n\r\nSo with N tables, there are N*(N-1)/2 pairs, so here about 10^16^ pairs.  If the probability of a collision is 2^-64^, the probability of a single pair not colliding is 1-2^-64^, but with so many pairs, the probability of having no collisions here is about ((1-2^-64^)^10^)^16^, or more like 1/10,000.  See eg https://preshing.com/20110504/hash-collision-probabilities/\r\n\r\nAnd if it's only a 32-bit hash the probability of a collision crosses 1/2 at only 77k values.\r\n\r\n\r\n  [1]: https://en.wikipedia.org/wiki/Birthday_problem	2020-01-12 14:00:41.495767+00	2	4	1	236797	0	0	0	2020-01-12 13:56:58.853192+00	\N	>Assuming I have 100 million tables, I calculate less than a 1-in-1-trillion chance of a collision	f	f
602	556	751	2019-04-26 15:08:42+00	> Can SQL Server create collisions in system generated constraint names?\r\n\r\nThis depends on the type of constraint and version of SQL Server.\r\n\r\n    CREATE TABLE T1\r\n    (\r\n    A INT PRIMARY KEY CHECK (A > 0),\r\n    B INT DEFAULT -1 REFERENCES T1,\r\n    C INT UNIQUE,\r\n    CHECK (C > A)\r\n    )\r\n    \r\n    SELECT name, \r\n           object_id, \r\n    \t   CAST(object_id AS binary(4)) as object_id_hex,\r\n    \t   CAST(CASE WHEN object_id >= 16000057  THEN object_id -16000057 ELSE object_id +2131483591 END AS BINARY(4)) AS object_id_offset_hex\r\n    FROM sys.objects\r\n    WHERE parent_object_id = OBJECT_ID('T1')\r\n    ORDER BY name;\r\n    \r\n    drop table T1\r\n\r\n## Example Results 2008\r\n\r\n\r\n|           name           | object_id | object_id_hex | object_id_offset_hex |\r\n|--------------------------|-----------|---------------|----------------------|\r\n| CK__T1__1D498357         | 491357015 | 0x1D498357    | 0x1C555F1E           |\r\n| CK__T1__A__1A6D16AC      | 443356844 | 0x1A6D16AC    | 0x1978F273           |\r\n| DF__T1__B__1B613AE5      | 459356901 | 0x1B613AE5    | 0x1A6D16AC           |\r\n| FK__T1__B__1C555F1E      | 475356958 | 0x1C555F1E    | 0x1B613AE5           |\r\n| PK__T1__3BD019AE15A8618F | 379356616 | 0x169C85C8    | 0x15A8618F           |\r\n| UQ__T1__3BD019A91884CE3A | 427356787 | 0x1978F273    | 0x1884CE3A           |\r\n\r\n## Example Results 2017\r\n\r\n|           name           | object_id  | object_id_hex | object_id_offset_hex |\r\n|--------------------------|------------|---------------|----------------------|\r\n| CK__T1__59FA5E80         | 1509580416 | 0x59FA5E80    | 0x59063A47           |\r\n| CK__T1__A__571DF1D5      | 1461580245 | 0x571DF1D5    | 0x5629CD9C           |\r\n| DF__T1__B__5812160E      | 1477580302 | 0x5812160E    | 0x571DF1D5           |\r\n| FK__T1__B__59063A47      | 1493580359 | 0x59063A47    | 0x5812160E           |\r\n| PK__T1__3BD019AE0A4A6932 | 1429580131 | 0x5535A963    | 0x5441852A           |\r\n| UQ__T1__3BD019A981F522E0 | 1445580188 | 0x5629CD9C    | 0x5535A963           |\r\n\r\n\r\nFor default constraints, check constraints and foreign key constraints the last 4 bytes of the auto generated name are a hexadecimal version of the objectid of the constraint. As `objectid` are guaranteed unique the name must also be unique. [In Sybase too][1] these use `tabname_colname_objectid`\r\n\r\nFor unique constraints and primary key constraints Sybase uses \r\n\r\n> tabname_colname_tabindid, where tabindid is a string concatenation of\r\n> the table ID and index ID\r\n\r\nThis too would guarantee uniqueness. \r\n\r\nSQL Server doesn't use this scheme.\r\n\r\nIn both SQL Server 2008 and 2017 it uses an 8 byte string at the end of the system generated name however the algorithm has changed as to how the last 4 bytes of that are generated.\r\n\r\nIn 2008 the last 4 bytes represent a signed integer counter that is offset from the `object_id` by `-16000057` with any negative value wrapping around to max signed int. (The significance of `16000057` is that this is [the increment applied between successively created `object_id`][2]). This still guarantees uniqueness.\r\n\r\nOn 2012 upwards I don't see any pattern at all between the object_id of the constraint and the integer obtained by treating the last 8 characters of the name as the hexadecimal representation of a signed int.\r\n\r\nThe function names in the call stack in 2017 shows that it now creates a GUID as part of the name generation process (On 2008 I see no mention of `MDConstraintNameGenerator`). I guess this is to provide some source of randomness. Clearly it isn't using the whole 16 bytes from the GUID in that 4 bytes that changes between constraints however.\r\n\r\n![enter link description here][3]\r\n\r\nI presume the new algorithm was done for some efficiency reason at the expense of some increased possibility of collisions in extreme cases such as yours. \r\n\r\nThis is quite a pathological case as it requires the table name prefix and column name of the PK (insofar as this affects the 8 characters preceding the final 8) to be identical for tens of thousands of tables before it becomes probable but can be reproduced quite easily with the below.\r\n\r\n    CREATE OR ALTER PROC #P\r\n    AS\r\n        SET NOCOUNT ON;\r\n    \r\n        DECLARE @I INT = 0;\r\n    \r\n    \r\n        WHILE 1 = 1\r\n          BEGIN\r\n              EXEC ('CREATE TABLE abcdefghijklmnopqrstuvwxyz' + @I + '(C INT PRIMARY KEY)');\r\n              SET @I +=1;\r\n          END \r\n    \r\n    GO\r\n    \r\n    EXEC #P\r\n\r\nAn example run on SQL Server 2017 against a newly created database failed in just over a minute (after 50,931 tables had been created)\r\n\r\n> Msg 2714, Level 16, State 30, Line 15 There is already an object named\r\n> 'PK__abcdefgh__3BD019A8175067CE' in the database. Msg 1750, Level 16,\r\n> State 1, Line 15 Could not create constraint or index. See previous\r\n> errors.\r\n\r\n\r\n  [1]: http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc36272.1570/html/commands/X80969.htm\r\n  [2]: https://raresql.com/2013/01/29/sql-server-all-about-object_id/\r\n  [3]: https://i.stack.imgur.com/hNbgX.png	2020-01-12 14:05:19.592465+00	4	4	1	236796	0	0	0	2020-01-12 13:56:58.582619+00	\N	> Can SQL Server create collisions in system generated constraint names?	f	f
356	367	258	2015-03-24 10:57:38+00	The indigestion is avoided by passing the `unicode` option to `hyperref`:\r\n\r\n    \\documentclass{scrartcl}\r\n    \\usepackage{polyglossia}\r\n    \\setmainlanguage{turkish}\r\n    \\makeatletter\r\n    \\usepackage[hidelinks,unicode]{hyperref}\r\n    \\AtBeginDocument{%\r\n        \\hypersetup{%\r\n            pdftitle = {\\@title}\r\n        }\r\n    }{}\r\n    \\makeatother\r\n    \\title{RAB'BİN GÜNÜ}\r\n    \\begin{document}\r\n    Title in document properties should match: RAB'BİN GÜNÜ\r\n    \\end{document}\r\n\r\n![enter image description here][1]\r\n\r\nHere's the output of `pdfinfo -enc UTF-8 test.pdf` (only the relevant parts)\r\n\r\n### LuaLaTeX\r\n\r\n    Title:          RAB'BİN GÜNÜ\r\n    Subject:        \r\n    Keywords:       \r\n    Author:         \r\n    Creator:        LaTeX with hyperref package\r\n    Producer:       LuaTeX-0.79.1\r\n    CreationDate:   Tue Mar 24 12:01:09 2015\r\n    ModDate:        Tue Mar 24 12:01:09 2015\r\n\r\n### XeLaTeX\r\n\r\n    Title:          RAB'BİN GÜNÜ\r\n    Creator:        LaTeX with hyperref package\r\n    Producer:       xdvipdfmx (20140317)\r\n    CreationDate:   Tue Mar 24 11:58:05 2015\r\n\r\n  [1]: https://i.stack.imgur.com/swT87.png\r\n\r\n	2019-12-06 12:46:24.447861+00	1	4	1	234781	0	0	0	2019-12-06 12:46:24.447861+00	\N	The indigestion is avoided by passing the `unicode` option to `hyperref`:	f	f
357	367	249	2015-04-08 09:41:45+00	The reason why the behavior differs when LuaTeX or XeTeX is used, is explained [here][1]. The XeTeX uses `xdvipdfmx` and this converter sets the UTF8 to UCS2 conversion automatically. On the other hand, when we are using direct pdfTeX primitives (like in LuaTeX) then the UTF8 to UCS2 conversion must be done at macro level. And the `hyperref` package does this when the mentioned `unicode` option is set.\r\n\r\n\r\n  [1]: https://tex.stackexchange.com/questions/236863/incorrect-encoding-in-pdf-outlines/237277#237277	2019-12-06 12:46:24.740982+00	0	4	1	237458	0	0	0	2019-12-06 12:46:24.740982+00	\N	The reason why the behavior differs when LuaTeX or XeTeX is used, is explained [here][1]. The XeTeX uses `xdvipdfmx` and this converter sets the UTF8 to UCS2 conversion automatically. On the other hand, when we are using direct pdfTeX primitives (like in LuaTeX) then the UTF8 to UCS2 conversion must be done at macro level. And the `hyperref` package does this when the mentioned `unicode` option is set.	f	f
477	357	630	2013-09-09 10:40:38+00	I tried to make the total be pi or `\\pi` as well but `beamer` didn't want to play, this does the digit counting though.\r\n\r\n    \\documentclass[t]{beamer}\r\n    \r\n    \\usepackage[absolute,overlay]{textpos}\r\n    \\setbeamertemplate{navigation symbols}{}\r\n    \r\n    \\setbeamertemplate{footline}[frame number]\r\n    \r\n    \\newcommand{\\pifoot}[1]{\r\n        \\begin{textblock*}{120mm}(0mm,84.3mm)\r\n            \\raggedleft #1\r\n        \\end{textblock*}\r\n    }\r\n    \\def\\pilist{3{.1}4159265.....}\r\n    \\makeatletter\r\n    \\def\\insertframenumber{\\csname pi-\\the\\c@framenumber\\endcsname}\r\n    \\@namedef{pi-1}{3}\r\n    \\@namedef{pi-2}{3.1}\r\n    \\@namedef{pi-3}{3.14}\r\n    \\@namedef{pi-4}{3.141}\r\n    \\@namedef{pi-5}{3.1415}\r\n    \\makeatother\r\n    \r\n    \r\n    % this doesn't work:-)\r\n    \\def\\inserttotalframenumber{pi}\r\n    \r\n    \\begin{document}\r\n    \r\n        \\begin{frame}\r\n            \\pifoot{3}\r\n        \\end{frame}\r\n    \r\n        \\begin{frame}\r\n            \\pifoot{3.1}\r\n        \\end{frame}\r\n    \r\n        \\begin{frame}\r\n            \\pifoot{3.14}\r\n        \\end{frame}\r\n    \r\n    \\end{document}	2019-12-15 16:02:25.361554+00	2	4	1	132378	0	0	0	2019-12-15 16:02:25.361554+00	\N	I tried to make the total be pi or `\\pi` as well but `beamer` didn't want to play, this does the digit counting though.	f	f
481	357	634	2013-09-09 23:35:26+00	Here I present a ConTeXt solution which admittedly cheats a little\r\nand resorts to Perl, instead of using Lua to calculate Pi. The idea can easily be ported to LaTeX as well.\r\n\r\nFirst the TeX macro `\\PrintPi` is created, which generates one more\r\ndigit with every consecutive call. It only keeps track of the page\r\nnumber and calls Perl. This macro is then placed in the header of\r\nevery page.\r\n\r\nThe performance should be sufficient for presentations with less\r\nthan 400 slides.\r\n\r\n    \\setuppapersize [S6]  %% screen size for slides\r\n\r\n    \\define\\PrintPi\r\n      {\\startluacode\r\n        userdata     = userdata or {}\r\n        userdata.num = userdata.num or 1\r\n        userdata.num = userdata.num + 1\r\n\r\n        f = io.popen("perl -Mbignum=bpi -wle 'print 0+substr(bpi("..userdata.num.."),0,-1)'")\r\n        context( f:read("*a") )\r\n      \\stopluacode}\r\n\r\n    \\setupheadertexts [\\PrintPi]\r\n\r\n    \\starttext\r\n      \\dorecurse{80}\r\n        {\\input ward\\page}\r\n    \\stoptext\r\n\r\nPage 69:\r\n\r\n![screenshot][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/gCw46.png	2019-12-15 16:02:26.335731+00	2	4	1	132496	0	0	0	2019-12-15 16:02:26.335731+00	\N	Here I present a ConTeXt solution which admittedly cheats a little	f	f
482	357	635	2013-10-15 19:21:51+00	This can also be done by returning `\\pgfmathresult` of pi with the precision determined by the frame number. I kinda like that `\\pgfmathprintnumber` rounds the output, so the fourth frame will be 3.142, the fifth will be 3.1416, the sixth 3.14159, and so on.\r\n\r\n    \\documentclass{beamer}\r\n    \\setbeamertemplate{footline}{%\r\n    \t\\usebeamerfont{page number in head/foot}\r\n    \t\\pgfkeys{/pgf/number format/.cd,fixed,precision=\\thepage}\r\n    \t\\pgfmathprintnumber{\\pgfmathresult}\r\n    \t\\vskip.5ex %\r\n    }\r\n    % Define pi to as many digits as you need\r\n    \\pgfmathparse{3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067}\r\n    \\setcounter{page}{0}\r\n    \r\n    \\begin{document}\r\n    \\frame{}\\frame{}\\frame{}\\frame{}\\frame{}\\frame{}\r\n    \\end{document}\r\n	2019-12-15 16:02:26.602488+00	2	4	1	139015	0	0	0	2019-12-15 16:02:26.602488+00	\N	This can also be done by returning `\\pgfmathresult` of pi with the precision determined by the frame number. I kinda like that `\\pgfmathprintnumber` rounds the output, so the fourth frame will be 3.142, the fifth will be 3.1416, the sixth 3.14159, and so on.	f	f
475	357	258	2013-09-09 11:16:12+00	Here's a way with LaTeX3 macros:\r\n\r\n\r\n    \\documentclass[t]{beamer}\r\n    \\usepackage{xparse}\r\n    \r\n    \\setbeamertemplate{navigation symbols}{}\r\n\r\n     % just add the frame numbers\r\n    \\setbeamertemplate{footline}{\\hfill\\Large\\strut\\pagepi{\\arabic{framenumber}}\\hspace*{1pc}}\r\n    \r\n    \\ExplSyntaxOn\r\n    \\tl_const:Nn \\c_pidigits_tl {1415926535897932384626433832795028842}\r\n    \\DeclareExpandableDocumentCommand{\\pagepi}{m}\r\n     {\r\n      \\int_compare:nTF { #1 < 2 }\r\n        { 3 }\r\n        {\r\n         3.\r\n         \\int_step_function:nnnN { 1 } { 1 } { #1-1 } \\my_print_digits:n\r\n        }\r\n     }\r\n    \\cs_new:Npn \\my_print_digits:n #1\r\n     {\r\n      \\tl_item:Nn { \\c_pidigits_tl } { #1 }\r\n     }\r\n    \\ExplSyntaxOff\r\n    \\begin{document}\r\n    \r\n    \\begin{frame}\r\n    a\\pause\r\n    b\r\n    \\end{frame}\r\n    \\begin{frame}\r\n    a\\pause\r\n    b\r\n    \\end{frame}\r\n    \\begin{frame}\r\n    a\\pause\r\n    b\r\n    \\end{frame}\r\n    \r\n    \\end{document}\r\n\r\n![enter image description here][1]\r\n\r\n---\r\n\r\nA different implementation using a property list; here 250 digits are available, much more than a presentation should need.\r\n\r\n    \\documentclass[t]{beamer}\r\n    \\usepackage{xparse}\r\n    \r\n    \\setbeamertemplate{navigation symbols}{}\r\n    \\setbeamertemplate{footline}{\\hfill\\Large\\strut\\pagepi{\\arabic{framenumber}}\\hspace*{1pc}}\r\n    \r\n    \\ExplSyntaxOn\r\n    \\tl_const:Nn \\c_pidigits_tl \r\n     {3%. % source: http://www.eveandersson.com/pi/digits\r\n      1415926535897932384626433\r\n      8327950288419716939937510\r\n      5820974944592307816406286\r\n      2089986280348253421170679\r\n      8214808651328230664709384\r\n      4609550582231725359408128\r\n      4811174502841027019385211\r\n      0555964462294895493038196\r\n      4428810975665933446128475\r\n      6482337867831652712019091\r\n     }\r\n    \\tl_new:N \\l__pidigits_cumulate_tl\r\n    \\tl_set:Nn \\l__pidigits_cumulate_tl { 3. }\r\n    \\prop_new:N \\g_pidigits_prop\r\n    \\prop_gput:Nnn \\g_pidigits_prop { 1 } { 3 }\r\n    \\int_step_inline:nnnn { 2 } { 1 } { \\tl_count:N \\c_pidigits_tl }\r\n     {\r\n      \\tl_set:Nx \\l__pidigits_cumulate_tl\r\n       { \\l__pidigits_cumulate_tl \\tl_item:Nn \\c_pidigits_tl { #1 } }\r\n      \\prop_gput:NnV \\g_pidigits_prop { #1 } \\l__pidigits_cumulate_tl \r\n     }\r\n    \\prop_show:N \\g_pidigits_prop\r\n    \\DeclareExpandableDocumentCommand{\\pagepi}{m}\r\n     {\r\n      \\prop_get:Nf \\g_pidigits_prop { #1 } % #1 is \\arabic{framenumber}\r\n     }\r\n    \\cs_generate_variant:Nn \\prop_get:Nn { Nf }\r\n    \\ExplSyntaxOff\r\n    \r\n    \\begin{document}\r\n    \r\n    \\begin{frame}\r\n    a\\pause\r\n    b\r\n    \\end{frame}\r\n    \\begin{frame}\r\n    a\\pause\r\n    b\r\n    \\end{frame}\r\n    \\begin{frame}\r\n    a\\pause\r\n    b\r\n    \\end{frame}\r\n    \r\n    \\end{document}\r\n\r\n  [1]: https://i.stack.imgur.com/CZcnJ.png\r\n	2019-12-15 16:02:24.822456+00	2	4	1	132381	0	0	0	2019-12-15 16:02:24.822456+00	\N	Here's a way with LaTeX3 macros:	f	f
480	357	633	2013-09-09 10:15:53+00	Here's an approach using `stringstrings` package.\r\n\r\n    \\documentclass[t]{beamer}\r\n    \\usepackage{stringstrings}\r\n    \\usepackage[absolute,overlay]{textpos}\r\n    \\setbeamertemplate{navigation symbols}{}\r\n    \\def\\PI{3.141592653589793238462643383279502884197169}\r\n    \\newcounter{sigdigits}\r\n    \\setcounter{sigdigits}{0}\r\n    \\newcommand{\\pifoot}{%\r\n       \\if1\\thesigdigits\\stepcounter{sigdigits}\\fi%\r\n       \\stepcounter{sigdigits}%\r\n        \\begin{textblock*}{120mm}(0mm,84.3mm)\r\n            \\raggedleft \\substring{\\PI}{1}{\\thesigdigits}%\r\n        \\end{textblock*}\r\n    }\r\n    \r\n    \\begin{document}\r\n    \r\n        \\begin{frame}\r\n            \\pifoot\r\n        \\end{frame}\r\n    \r\n        \\begin{frame}\r\n            \\pifoot\r\n        \\end{frame}\r\n    \r\n        \\begin{frame}\r\n            \\pifoot\r\n        \\end{frame}\r\n    \r\n    \\end{document}	2019-12-15 16:02:26.154445+00	2	4	1	132375	0	0	0	2019-12-15 16:02:26.154445+00	\N	Here's an approach using `stringstrings` package.	f	f
479	357	632	2013-09-09 11:55:32+00	Here is a solution using `lua`. With this solution you don't have to bother about which slide is it, or whether you have given enough digits at the beginning. Obviously, it can be easily integrated into one of the themes of `beamer`, so you can typeset it in a fancy way as well.\r\n\r\n    \\documentclass[t]{beamer}\r\n    \r\n    \\usepackage[absolute,overlay]{textpos}\r\n    \\setbeamertemplate{navigation symbols}{}\r\n    \r\n    \\usepackage{luatextra}\r\n    \r\n    \\newcommand{\\pifoot}{\r\n      \\begin{textblock*}{120mm}(0mm,84.3mm)\r\n        \\raggedleft $\\luaexec{\r\n          slidenumber = \\insertframenumber\r\n          if tonumber(slidenumber) == 1 then\r\n             numbertoinsert = tostring(3)\r\n          else\r\n             numbertoinsert = "\\%." .. tostring(slidenumber-1) .. "f"\r\n          end\r\n          tex.sprint(string.format(numbertoinsert,math.pi))\r\n        }$\r\n      \\end{textblock*}\r\n    }\r\n    \r\n    \r\n    \\begin{document}\r\n    \r\n    \\begin{frame}\r\n      \\pifoot\r\n    \\end{frame}\r\n    \r\n    \\begin{frame}\r\n      \\pifoot\r\n    \\end{frame}\r\n    \r\n    \\begin{frame}\r\n      \\pifoot\r\n    \\end{frame}\r\n    \r\n    \\end{document}\r\n\r\nHas to be processed using `lualatex`.	2019-12-15 16:02:25.89916+00	2	4	1	132389	0	0	0	2019-12-15 16:02:25.89916+00	\N	Here is a solution using `lua`. With this solution you don't have to bother about which slide is it, or whether you have given enough digits at the beginning. Obviously, it can be easily integrated into one of the themes of `beamer`, so you can typeset it in a fancy way as well.	f	f
476	357	629	2013-09-09 11:55:25+00	Without PSTricks. Only up to 19 slides!\r\n\r\n    \\documentclass{beamer}\r\n    \\usepackage[nomessages]{fp}\r\n    \\usepackage{multido}\r\n    \r\n    \\begin{document}\r\n    \\multido{\\i=0+1}{19}{%\r\n    \\FPtrunc{\\x}{\\FPpi}{\\i}%\r\n    \\begin{frame}{\\x}\r\n    \\end{frame}}\r\n    \\end{document}\r\n\r\n\r\n![enter image description here][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/6leIU.gif	2019-12-15 16:02:25.09166+00	2	4	1	132388	0	0	0	2019-12-15 16:02:25.09166+00	\N	Without PSTricks. Only up to 19 slides!	f	f
478	357	631	2013-09-09 10:06:28+00	A slight modification to your code using the [`xstring`][1] package can do the trick. You have to enter `Pi` as a string, though:\r\n\r\n    \\documentclass[t]{beamer}\r\n    \\usepackage{xstring}\r\n    \\usepackage[absolute,overlay]{textpos}\r\n    \\setbeamertemplate{navigation symbols}{}\r\n\r\n    \\newcounter{Pi}\r\n\r\n    \\newcommand{\\pifoot}{\r\n        \\begin{textblock*}{120mm}(0mm,84.3mm)\r\n            \\raggedleft \\StrChar{3141592653589793238462643383279502884197169}{\\arabic{Pi}}\r\n        \\end{textblock*}\r\n        \\stepcounter{Pi}\r\n    }\r\n\r\n\r\n    \\begin{document}\r\n\r\n        \\begin{frame}\r\n            \\pifoot\r\n        \\end{frame}\r\n\r\n        \\begin{frame}\r\n            \\pifoot\r\n        \\end{frame}\r\n\r\n        \\begin{frame}\r\n            \\pifoot\r\n        \\end{frame}\r\n\r\n    \\end{document}\r\n\r\n\r\n  [1]: http://www.ctan.org/pkg/xstring	2019-12-15 16:02:25.625762+00	2	4	1	132373	0	0	0	2019-12-15 16:02:25.625762+00	\N	A slight modification to your code using the [`xstring`][1] package can do the trick. You have to enter `Pi` as a string, though:	f	f
474	357	628	2013-09-09 10:17:24+00	## EDIT ##\r\n\r\nI did not success in using an algorithm to calculate the pi digits but I can get them by the web using the `\\write18` command of `LaTeX` (require `--shell-enabled`) and the shell command `wget` and `sed`.\r\nAll you need to do is substitute the row\r\n\r\n    \\CatchFileDef{\\PiG}{pi.txt}\r\n\r\nin the code below with:\r\n\r\n    \\IfFileExists{./digits.txt}{}\r\n        {\r\n        \\immediate\\write18{\\detokenize{wget  http://www.geom.uiuc.edu/~huberty/math5337/groupe/digits.html -O digits.txt}}\r\n        \\immediate\\write18{sed -i '/[0-9]$/!d' digits.txt} \r\n        \\immediate\\write18{sed -i '1,13!d' digits.txt}\r\n        \\immediate\\write18{sed -i 's/ //g' digits.txt}\r\n        }\r\n    \\CatchFileDef{\\PiG}{digits.txt}\r\n\r\n----------\r\n\r\nThis is the correct modification to @user36411 answer. Just create a file `pi.txt` with the number of pi digits you need.\r\n\r\n    \\documentclass[t]{beamer}\r\n    \\usepackage{xstring,ifthen,catchfile,forloop}\r\n    \\usepackage[absolute,overlay]{textpos}\r\n    \\setbeamertemplate{navigation symbols}{}\r\n    \r\n    \\CatchFileDef{\\PiG}{pi.txt}\r\n    \\newcounter{Pi}\r\n    \\setcounter{Pi}{1}\r\n    \\newcommand{\\pifoot}{\r\n    \t\\ifthenelse{\\arabic{Pi}=2}{\\stepcounter{Pi}}{}\r\n        \\begin{textblock*}{120mm}(0mm,84.3mm)\r\n            \\raggedleft \\StrLeft{\\PiG}{\\arabic{Pi}}\r\n        \\end{textblock*}\r\n        \\stepcounter{Pi}\r\n    }\r\n    \r\n    \r\n    \\begin{document}\r\n    \\newcounter{framenum}\r\n    \\forloop{framenum}{1}{\\value{framenumber} < 50}{\r\n        \\begin{frame}\r\n            \\pifoot\r\n        \\end{frame}\r\n    }\r\n    \\end{document}	2019-12-15 16:02:24.555268+00	2	4	1	132376	0	0	0	2019-12-15 16:02:24.555268+00	\N	EDIT	f	f
509	473	168	2019-12-22 17:02:56+00	You can think of it like this:\r\n\r\n```latex\r\n\\def\\foo#1;{}\r\n```\r\n\r\nwill read everything after `\\foo` until there is a `;`, so `\\foo 123;` will read `123` as `#1` and remove `;` from the input stream as well.\r\n\r\nSimilarly\r\n\r\n```latex\r\n\\def\\foo#1#{}\r\n```\r\n\r\nwill create a macro that has a right delimited argument, but instead of `;` this right delimiter is `{`, so it reads everything up to the next opening brace, *but* contrarily to the first case, in this case `{` will not be removed from the input stream (well, actually it will be removed but reinserted by `\\foo`'s replacement text). So `\\def\\foo#1#{}` behaves like `\\def\\foo#1;{;}` but uses `{` as the right delimiter instead of `;`.\r\n\r\nYou can see that the opening brace is actually removed and later reinserted by taking a look at the `\\meaning` (or `\\show`ing the definition):\r\n\r\n```latex\r\n\\def\\foo#1#{}\\show\\foo\r\n\\def\\foo#1#{foo}\\show\\foo\r\n```\r\n\r\nwill print\r\n\r\n>```latex\r\n>> \\foo=macro:\r\n>#1{->{.\r\n>> \\foo=macro:\r\n>#1{->foo{.\r\n>```\r\n\r\nto the terminal.\r\n\r\n----\r\n\r\n**EDIT:** This tries to answer the comment\r\n\r\n>As I understand it, the parameter text is a regular expression. In this regular expression the opening parenthesis `{` is a delimiter of the replacement text and is therefore removed when replacing. This is not the case when it is immediately preceded by `#`. What is the purpose of this rule?\r\n\r\nIf TeX would also remove that opening brace what would be left would be an unbalanced token list (an unmatched closing brace). Therefore if the following macros aren't created very carefully, this would throw an error, and creating macros with the same logic would be much harder. So the only purpose of this rule is that you can create macros which are right delimited by a token of category code 1 (a `{` in normal catcodes) without needing a lot of macros to sanitize the now unbalanced input stream.\r\n\r\nImagine the following situation:\r\n\r\n```latex\r\n\\def\\foo#1#{}\r\n\\foo 123{abc}\r\n```\r\n\r\nAfter the definition of `\\foo` and its expansion what would be left in the input stream if the `{` wasn't reinserted would be\r\n\r\n```latex\r\nabc}\r\n```\r\n\r\nand we'd have to somehow sanitize that unmatched closing brace. Say we want to create a macro which reads everything up to an opening brace and the next group, what we'd have to do now would be to create a macro that grabs every token until it meets a closing brace, but `\\def\\bar#1}{}` will throw an error as well, so how should we create this? What we'd need to do would be something like the following (note that I create the unbalanced text by expanding an `\\iffalse{\\fi` in the following):\r\n\r\n```latex\r\n\\documentclass[]{article}\r\n\r\n\\makeatletter\r\n\\long\\def\\grabuntilclosingbrace@fi@firstoftwo\\fi\\@secondoftwo#1#2{\\fi#1}\r\n\\def\\grabuntilclosingbrace\r\n  {%\r\n    \\begingroup\r\n    \\aftergroup\\grabuntilclosingbrace@done\r\n    \\grabuntilclosingbrace@a\r\n  }\r\n\\def\\grabuntilclosingbrace@a\r\n  {%\r\n    \\futurelet\\grabuntilclosingbrace@tok\\grabuntilclosingbrace@b\r\n  }\r\n\\def\\grabuntilclosingbrace@b\r\n  {%\r\n    \\ifx\\grabuntilclosingbrace@tok\\egroup\r\n      \\grabuntilclosingbrace@fi@firstoftwo\r\n    \\fi\r\n    \\@secondoftwo\r\n    {%\r\n      \\afterassignment\\grabuntilclosingbrace@final\r\n      \\let\\afterassignment@tok=%\r\n    }\r\n    {%\r\n      \\grabuntilclosingbrace@c\r\n    }%\r\n  }\r\n\\def\\grabuntilclosingbrace@final\r\n  {%\r\n    \\aftergroup\\grabuntilclosingbrace@end\r\n    \\endgroup\r\n  }\r\n\\long\\def\\grabuntilclosingbrace@done#1\\grabuntilclosingbrace@end\r\n  {Argument was: \\texttt{#1}}\r\n\\long\\def\\grabuntilclosingbrace@c#1{\\aftergroup#1\\grabuntilclosingbrace@a}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\\expandafter\\grabuntilclosingbrace\\iffalse{\\fi abc}\r\n\\end{document}\r\n```\r\n\r\nAnd this macro can't deal with spaces or nested groups. See how complicated life would be, if TeX didn't give us the super handy `\\def\\foo#1#{}` rule?\r\n\r\n\r\n----\r\n\r\n# If you want to know, what this rule can be used for:\r\n\r\nSay we have some macro that can't deal with nested groups in its argument, so we have to test whether the argument has a group, after all we want to give a helpful error message instead of just letting our macro fail. So we need to create a test that tests for a nested group. With the logic of `\\def\\foo#1#{}` we can reduce this to a test whether an argument is empty (this reuses code/ideas from https://topanswers.xyz/tex?q=474#a510).\r\n\r\n```latex\r\n\\documentclass[]{article}\r\n\r\n\\makeatletter\r\n\\long\\def\\ifgroupin#1%\r\n  {%\r\n    \\ifgroupin@a#1{}\\ifgroupin@tokB\\ifgroupin@false\r\n    \\ifgroupin@tokA\\ifgroupin@tokB\\@firstoftwo\r\n  }\r\n\\long\\def\\ifgroupin@a#1#{\\ifgroupin@b}\r\n\\long\\def\\ifgroupin@b#1{\\ifgroupin@c\\ifgroupin@tokA}\r\n\\long\\def\\ifgroupin@c#1\\ifgroupin@tokA\\ifgroupin@tokB{}\r\n\\long\\def\\ifgroupin@false\\ifgroupin@tokA\\ifgroupin@tokB\\@firstoftwo#1#2{#2}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\\ifgroupin{abc}{true}{false}\r\n\r\n\\ifgroupin{a{b}c}{true}{false}\r\n\\end{document}\r\n```\r\n\r\nAn alternative version which uses the classic `\\if\\relax\\detokenize{#1}\\relax` empty test, because this might be easier to understand (but takes about 160% the time of the previous implementation):\r\n\r\n```latex\r\n\\makeatletter\r\n\\long\\def\\ifgroupin#1%\r\n  {%\r\n    \\if\\relax\r\n      \\detokenize\\expandafter\\expandafter\\expandafter{\\ifgroupin@a#1{}}\\relax\r\n      \\expandafter\\@secondoftwo\r\n    \\else\r\n      \\expandafter\\@firstoftwo\r\n    \\fi\r\n  }\r\n\\long\\def\\ifgroupin@a#1#{\\@gobble}\r\n\\makeatother\r\n```	2019-12-22 19:49:17.411967+00	6	4	1	521453	0	0	0	2019-12-22 19:44:43.520989+00	\N	You can think of it like this:	f	f
447	438	167	2019-12-13 09:59:19.729873+00	Under the hood `l3draw` uses `l3color` for the colour support, therefore on can use the `\\color_set:nnn` or `\\color_set:nn` macro to define new colours.\r\n\r\nOne example with an rgb colour (the colour models `gray`, `cmyk` and `spot` are also available) and based on existing colours.\r\n\r\n```\r\n\\documentclass{standalone}\r\n\r\n\\usepackage{l3draw}\r\n\r\n\\begin{document}\r\n\r\n\\ExplSyntaxOn\r\n\r\n\\color_set:nnn {foo} {rgb} {0.5,0.8,1}\r\n\\color_set:nn {bar} {yellow!50!red}\r\n\r\n\\draw_begin: \r\n\r\n\t\\draw_path_circle:nn {0cm,0cm} {1cm}\r\n\t\\draw_color_fill:n { foo }\r\n\t\\draw_path_use_clear:n { fill, stroke }\r\n\t\r\n\t\\draw_path_circle:nn {1cm,0cm} {1cm}\r\n\t\\draw_color_fill:n { bar }\r\n\t\\draw_path_use_clear:n { fill, stroke }\t\r\n\t\r\n\\draw_end:\r\n\\ExplSyntaxOff \r\n\r\n\\end{document}\r\n```	2019-12-13 21:54:26.90989+00	7	4	3	\N	0	0	0	\N	\N	Under the hood `l3draw` uses `l3color` for the colour support, therefore on can use the `\\color_set:nnn` or `\\color_set:nn` macro to define new colours.	f	f
414	410	96	2019-12-11 11:52:57.220415+00	I think this suggestion is picking up the snake from the wrong end. The site interface should neither promote nor demote content based soley on the user who posted it. If *we* as participants can't step away and ignore a user we don't like, no amount of tooling is going to completely solve this problem[^1]. Sure the tooling can make it a bit easier, but I think the first line of defense is to not put too much focus on the individual(s) involved in the first place. Keep the focus on the content. Interact with the content first, and lets not build to many tools that focus on the personalities.\r\n\r\nGiven the high visibility of chat pings, it might make sense to add a "mute" feature. I would first make this a per-room (that is, per question thread) mute. Sometimes discussions get going and it isn't just one user that's a problem, you just want to stay out of the whole conversation. That should be the first step.\r\n\r\nSecondarily muting all pings / notifications generated by a specific user might be a reasonable step. I don't think hiding the content is a good idea at all given the way all content is at least somewhat interdependent. Posts would all display as usual, voting would be as usual, but the "comment" link as well as any avatar links that would normally ping the user would be disabled for anybody you had on a mute list, and no actions of that user would generate notifications for you.\r\n\r\nOnly as a last resort, adding the ability to hide chat messages entirely (again not posts, only chat messages) from the active room and (not transcripts).\r\n\r\nIf there is still a problem at that point then moderators should step in. If multiple people on a site are ignoring the same user, that should come to the attention of somebody with some moderator priviledges and some kind of warning system. What that system looks like should be discussied elsewhere, perhaps [on this question about moderator tools](https://topanswers.xyz/meta?q=182).\r\n\r\n[^1]: Incidentally I think this issue is a large factor in where SE jumped the shark. Trying to blame only one end of an interaction for the friction is a loosing battle, both parties have to have some culpability. Rather than complaining about others and trying to legistlate what they can and can't say, we have to realize people are going to disagree with us and have that be okay. Sometimes people are [wrong on the internet](https://www.xkcd.com/386/) and you just have to let them be wrong and go to bed.	2019-12-11 12:03:33.570943+00	6	4	2	\N	0	0	0	\N	\N	I think this suggestion is picking up the snake from the wrong end. The site interface should neither promote nor demote content based soley on the user who posted it. If *we* as participants can't step away and ignore a user we don't like, no amount of tooling is going to completely solve this problem[^1](Incidentally I think this issue is a large factor in where SE jumped the shark. Trying to blame only one end of an interaction for the friction is a loosing battle, both parties have to have some culpability. Rather than complaining about others and trying to legistlate what they can and can't say, we have to realize people are going to disagree with us and have that be okay. Sometimes people are [wrong on the internet](https://www.xkcd.com/386/) and you just have to let them be wrong and go to bed.). Sure the tooling can make it a bit easier, but I think the first line of defense is to not put too much focus on the individual(s) involved in the first place. Keep the focus on the content. Interact with the content first, and lets not build to many tools that focus on the personalities.	f	f
433	410	17	2019-12-12 09:23:54.699859+00	To me this is an X-Y problem.\r\n\r\nI feel the site rules should foster an environment in which this situation doesn't occur - we're here to curate content not argue with each other.\r\n\r\nI feel it's important to remember the target users of the sites - professionals and experts. A set of rules around engagement (i.e. be kind) should be enough, with moderation for individual breaches of those rules. We can't look to constrain behaviour *too* much - this is where SE failed for me. In this respect as Caleb mentions, we should focus on content, not people.\r\n\r\nI would give a high level of personal ownership to the decisions made by the moderation team, but ensure the userbase within each site has the ability to flag and deal with systemic moderation issues via some democratic process.\r\n\r\nIf the flagging process works correctly, this situation should be dealt with quickly anyway.	2019-12-12 09:41:59.870881+00	7	4	1	\N	0	0	0	\N	\N	To me this is an X-Y problem.	f	f
641	588	167	2020-01-20 16:36:48.51398+00	One can modify https://topanswers.xyz/tex?q=587#a640 a bit to add the section names and numbers:\r\n\r\n\r\n```\r\n\\documentclass[xcolor={rgb}]{beamer}\r\n\r\n\\usepackage{totcount}\r\n\\newtotcounter{mysec}\r\n\\AtBeginSection{\\addtocounter{mysec}{1}\\label{mysec:\\themysec}}\r\n\\newcounter{foo}\r\n\\usepackage{refcount}\r\n\\usepackage{tikz}\r\n\\usetikzlibrary{patterns.meta}\r\n\r\n\\setbeamertemplate{headline}{%\r\n\t\\vskip4pt\r\n\t\\begin{tikzpicture}\r\n\t\t\\path[preaction={fill, lightgray},pattern={Lines[angle=45,line width=3pt,distance=6pt]},pattern color=lightgray!50] (0,0) rectangle (\\paperwidth,0.3);\r\n\t\t\\path[preaction={fill, green!50!teal},pattern={Lines[angle=45,line width=3pt,distance=6pt]},pattern color=green!50!teal!50] (0,0) rectangle ({(\\thepage-1)/(\\insertdocumentendpage-1)*\\paperwidth},0.3);\r\n\t\t\\ifnum\\thesection>0\r\n\t\t\t\\path[preaction={fill, cyan!50!blue},pattern={Lines[angle=45,line width=3pt,distance=6pt]},pattern color=cyan!50!blue!50] (0,0) rectangle ({(\\insertsectionstartpage-1)/(\\insertdocumentendpage-1)*\\paperwidth},0.3);\r\n\t\t\\fi\r\n\t\t\\foreach \\x in {1,...,\\totvalue{mysec}}{%\r\n\t\t\t\\setcounterpageref{foo}{mysec:\\x}\r\n\t\t\t\\colorlet{seccol}{green!50!teal}\r\n\t\t\t\\ifnum\\thesection=\\x\r\n\t\t\t\t\\colorlet{seccol}{green!50!teal}\r\n\t\t\t\\else\r\n\t\t\t\t\\ifnum\\thesection<\\x\r\n\t\t\t\t\t\\colorlet{seccol}{lightgray}\r\n\t\t\t\t\\else\r\n\t\t\t\t\t\\colorlet{seccol}{cyan!50!blue}\r\n\t\t\t\t\\fi\r\n\t\t\t\\fi\t\t\r\n\t\t\t\\node[circle,fill=seccol,draw=white,line width=1.5pt,minimum size=15pt] at ({(\\thefoo-1)/(\\insertdocumentendpage-1)*\\paperwidth},0.15) {\\x};\r\n\t\t\t\\node[seccol] at ({(\\thefoo-1)/(\\insertdocumentendpage-1)*\\paperwidth},-0.35) {\\nameref{mysec:\\x}};\r\n\t\t}\r\n\t\\end{tikzpicture}%\r\n}\r\n\r\n\\begin{document}\r\n\r\n\\begin{frame}\r\n\\titlepage\r\n\\end{frame}\r\n\r\n\\section{Introduction}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\r\n\\section{Motivation}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\r\n\\section{Methodology}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\r\n\r\n\\section{Results}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n\r\n![document.gif](/image?hash=323c6c62a3fa8226c5c7ed0a009a25db375c7e24cdc8917108fb6ae9b14a7060)\r\n\r\n### Second version with rounded corners\r\n\r\n```\r\n\\documentclass[xcolor={rgb}]{beamer}\r\n\r\n\\usepackage{totcount}\r\n\\newtotcounter{mysec}\r\n\\AtBeginSection{\\addtocounter{mysec}{1}\\label{mysec:\\themysec}}\r\n\\newcounter{foo}\r\n\\usepackage{refcount}\r\n\\usepackage{tikz}\r\n\\usetikzlibrary{patterns.meta}\r\n\\usetikzlibrary{fadings,patterns,shadows}\r\n\r\n\\setbeamertemplate{headline}{%\r\n\t\\vskip6pt\r\n\t\\centering\r\n\t\\begin{tikzpicture}\r\n\t\t\\path[draw=gray,rounded corners,preaction={fill, lightgray},pattern={Lines[angle=45,line width=3pt,distance=6pt]},pattern color=lightgray!50] (0,0) rectangle (.9\\paperwidth,0.3);\r\n\t\t\\path[rounded corners,preaction={fill, green!50!teal},pattern={Lines[angle=45,line width=3pt,distance=6pt]},pattern color=green!50!teal!50] (0,0) rectangle ({(\\thepage-1)/(\\insertdocumentendpage-1)*.9\\paperwidth},0.3);\r\n\t\t\\ifnum\\thesection>0\r\n\t\t\t\\path[rounded corners,preaction={fill, cyan!50!blue},pattern={Lines[angle=45,line width=3pt,distance=6pt]},pattern color=cyan!50!blue!50] (0,0) rectangle ({(\\insertsectionstartpage-1)/(\\insertdocumentendpage-1)*.9\\paperwidth},0.3);\r\n\t\t\\fi\r\n\t\t\\foreach \\x in {1,...,\\totvalue{mysec}}{%\r\n\t\t\t\\setcounterpageref{foo}{mysec:\\x}\r\n\t\t\t\\colorlet{seccol}{green!50!teal}\r\n\t\t\t\\ifnum\\thesection=\\x\r\n\t\t\t\t\\colorlet{seccol}{green!50!teal}\r\n\t\t\t\\else\r\n\t\t\t\t\\ifnum\\thesection<\\x\r\n\t\t\t\t\t\\colorlet{seccol}{lightgray}\r\n\t\t\t\t\\else\r\n\t\t\t\t\t\\colorlet{seccol}{cyan!50!blue}\r\n\t\t\t\t\\fi\r\n\t\t\t\\fi\t\t\r\n\t\t\t\\node[circle,fill=seccol,draw=white,line width=1.5pt,minimum size=15pt] at ({(\\thefoo-1)/(\\insertdocumentendpage-1)*.9\\paperwidth},0.15) {\\x};\r\n\t\t\t\\node[seccol] at ({(\\thefoo-1)/(\\insertdocumentendpage-1)*.9\\paperwidth},-0.35) {\\nameref{mysec:\\x}};\r\n\t\t}\r\n\t\\end{tikzpicture}%\r\n\t\\par%\r\n}\r\n\r\n\\begin{document}\r\n\r\n\\begin{frame}\r\n\\titlepage\r\n\\end{frame}\r\n\r\n\\section{Introduction}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\r\n\\section{Motivation}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\r\n\\section{Methodology}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\r\n\r\n\\section{Results}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\\begin{frame}\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n\r\n![document.gif](/image?hash=8054d6b3fed1ec6f7b48fcd8c2d69e0e7d5cb5a56b39a22b06f510d8c016f1e2)	2020-01-21 10:00:53.382577+00	2	4	3	\N	0	0	0	\N	\N	One can modify https://topanswers.xyz/tex?q=587#a640 a bit to add the section names and numbers:	f	f
271	299	16	2019-09-27 12:42:36+00	SQL Server 2019 has introduced a wait stat to track this. On earlier versions, you're left to traces/XE. See my posts about the new wait stat: \r\n\r\n- [SQL Server 2019: WAIT_ON_SYNC_STATISTICS_REFRESH][2]\r\n- [SQL Server 2019: WAIT_ON_SYNC_STATISTICS_REFRESH Redux][1] \r\n\r\nTo use XE on prior versions, the event you want to look for is auto_stats. A bare minimum session to get you started would look like this:\r\n\r\n    CREATE EVENT SESSION auto_stats\r\n        ON SERVER\r\n        ADD EVENT sqlserver.auto_stats\r\n        ADD TARGET package0.event_file\r\n        ( SET filename = N'auto_stats' );\r\n\r\nThough you'd probably want to configure it with some specificity to databases or tables that you care about, and I'd definitely want to filter on duration (`WHERE [duration] > 1000000` would be one second) or something, because it'd be pretty noisy otherwise.\r\n\r\n  [1]: https://www.erikdarlingdata.com/2019/09/sql-server-2019-wait_on_sync_statistics_refresh-redux/\r\n  [2]: https://www.erikdarlingdata.com/2019/08/sql-server-2019-wait_on_sync_statistics_refresh/	2019-12-04 22:54:11.664028+00	3	4	1	249777	0	0	0	2019-12-04 22:54:11.664028+00	\N	SQL Server 2019 has introduced a wait stat to track this. On earlier versions, you're left to traces/XE. See my posts about the new wait stat:	f	f
287	314	16	2018-09-14 17:42:02+00	Repro!\r\n--\r\nI was able to reproduce your issue using [SQL Query Stress][1], both using your example, and changing things to create the indexes inline, as Paul suggested:\r\n\r\n    CREATE TABLE #temp_table_name_goes_here\r\n         (\r\n             the_first_col BIGINT NULL,\r\n             the_second_col INT NULL,\r\n             another_col VARCHAR(20) NULL,\r\n             INDEX tmp_indx_temp_table_name_goes_here ( the_first_col ) \r\n                   WHERE the_first_col IS NOT NULL,\r\n             INDEX tmp_indx_outstanding_inventory ( the_second_col ) \r\n                   WHERE the_second_col IS NOT NULL\r\n         );\r\n\r\n[![NUTS][2]][2]\r\n\r\nOf course, SQS was running 200 concurrent sessions for 20 iterations., so it took some work to get there.\r\n\r\nI'm on SQL Server 2017, so the situation was not changed by enabling [Trace Flag 3427][3].\r\n\r\nIf tempdb is not configured optimally on your system, I'd start there:\r\n\r\n - One data file per core up to 8 cores\r\n - Enable Trace Flags 1117 and 1118\r\n\r\nIf you still have issues, I'd open a support case with Microsoft. Scalability issues like this should be front and center.\r\n\r\nFollow Up\r\n--\r\nAfter stumbling on [this question][4], I decided to re-try the test using table variables. Under the same circumstances as above, the same contention was observed.\r\n\r\n    DECLARE @temp_table_name_goes_here TABLE\r\n         (\r\n             the_first_col BIGINT NULL,\r\n             the_second_col INT NULL,\r\n             another_col VARCHAR(20) NULL,\r\n             INDEX tmp_indx_temp_table_name_goes_here ( the_first_col ) \r\n                   WHERE the_first_col IS NOT NULL,\r\n             INDEX tmp_indx_outstanding_inventory ( the_second_col ) \r\n                   WHERE the_second_col IS NOT NULL\r\n         );\r\n\r\nI also re-ran the temp table test with an explicit DROP IF EXISTS (or the old style check for a NOT NULL object_id) at the beginning, per [Jonathan Fite's][5] comment:\r\n\r\n> I've hit this error before. My solution was to add explicit drop of\r\n> the indexes and the temp table (DROP IF EXISTS) before creating it. It\r\n> only happens when the same session runs the stored procedure multiple\r\n> times.\r\n\r\nThis *did* help. Contention was either non-existent or minimized.\r\n\r\nAdding an explicit DROP TABLE, or DROP IF EXISTS at the end had an odd effect: curiously, the contention switched to dropping the temp tables:\r\n\r\n[![NUTS][6]][6]\r\n\r\n\r\n  [1]: https://github.com/ErikEJ/SqlQueryStress\r\n  [2]: https://i.stack.imgur.com/X4eyv.jpg\r\n  [3]: https://blogs.msdn.microsoft.com/sql_server_team/tempdb-files-and-trace-flags-and-updates-oh-my/\r\n  [4]: https://dba.stackexchange.com/q/13392/32281\r\n  [5]: https://dba.stackexchange.com/users/46773/jonathan-fite\r\n  [6]: https://i.stack.imgur.com/iOHc8.jpg	2019-12-05 16:44:30.425194+00	3	4	1	217651	0	0	0	2019-12-05 16:44:30.425194+00	\N	Repro!	f	f
265	295	16	2017-12-19 16:52:18+00	The magic of [NULLIF][1] seems to do the trick for the test case in your question. Since you used a different example than in your SQL Fiddle, I don't know if that's what you want there too.\r\n\r\n    CREATE TABLE dbo.Ids\r\n    (\r\n        Id INT NOT NULL IDENTITY(1, 1),\r\n        Value INT,\r\n        Name NVARCHAR(3)\r\n    );\r\n    INSERT INTO dbo.Ids ( Name, Value )\r\n    VALUES ( 'a', 1 );\r\n    INSERT INTO dbo.Ids ( Name, Value )\r\n    VALUES ( 'a', 2 );\r\n    INSERT INTO dbo.Ids ( Name, Value )\r\n    VALUES ( 'b', 0 );\r\n    INSERT INTO dbo.Ids ( Name, Value )\r\n    VALUES ( 'b', 1 );\r\n    \r\n    SELECT   Name,\r\n             CASE WHEN MIN(Value) = 0 THEN 0\r\n                  WHEN MIN(Value) > 0 THEN EXP(SUM(LOG(NULLIF(Value, 0)))) -- trying to get the product of all rows in this column\r\n             END AS Product\r\n    FROM     Ids\r\n    GROUP BY Name;\r\n\r\nReturns:\r\n\r\n    Name\tProduct\r\n    a\t    2\r\n    b\t    0\r\n\r\n---\r\n\r\nIf you need a more general solution that handles negative numbers and other edge cases, see for example [The Product Aggregate in T-SQL Versus the CLR][2] by Scott Burkow. One T-SQL construction from that article is:\r\n\r\n    EXP(SUM(LOG(NULLIF(ABS([Value]), 0))))\r\n    *\r\n    IIF(SUM(IIF([Value] = 0, 1, NULL)) > 0, 0, 1)\r\n    *\r\n    IIF(SUM(IIF([Value] < 0, 1, 0)) % 2 = 1, -1, 1)\r\n\r\n---\r\n\r\nAs to why your original `CASE` expression did not work as expected, from the documentation for [CASE (Transact-SQL)][3] (emphasis added):\r\n\r\n>You should only depend on order of evaluation of the WHEN conditions for scalar expressions (including non-correlated sub-queries that return scalars), **not for aggregate expressions**.\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/language-elements/nullif-transact-sql\r\n  [2]: https://www.codeproject.com/Articles/548395/The-Product-Aggregate-in-T-SQL-Versus-the-CLR\r\n  [3]: https://docs.microsoft.com/en-us/sql/t-sql/language-elements/case-transact-sql#remarks	2019-12-04 22:43:48.908132+00	2	4	1	193552	0	0	0	2019-12-04 22:43:48.908132+00	\N	The magic of [NULLIF][1] seems to do the trick for the test case in your question. Since you used a different example than in your SQL Fiddle, I don't know if that's what you want there too.	f	f
620	574	12	2020-01-15 15:48:06.548827+00	I'm a fan of **linking** to "duplicate" Q & A but not so much of **closing** (= preventing answers) at the same time. This is something I think the SE model *almost* got right, but not quite. In most cases I have encountered, a question is rarely an **exact** duplicate (such that no useful answer to the specific question is possible).\r\n\r\nWhere the suggested "duplicate" answers the question, the querent may still need **specific guidance** on how to adapt the **general solution** to their particular circumstance. If nothing else, this provides an extra example for future readers.\r\n\r\nIf something new and interesting arises from the more specific question, the conscientious answer-writer would also update the more general linked Q & A as appropriate.\r\n\r\nI would favour a list of "related" Q & A immediately below the question, with a short header text to describe its purpose, and to guide people on when local answers should be added (and not).\r\n\r\nIf something is truly an *exact* duplicate, the link would suffice and no more specific answer need be added. We just shouldn't enforce that with some closure-like restriction.	2020-01-15 15:48:06.548827+00	9	4	2	\N	0	0	0	\N	\N	I'm a fan of **linking** to "duplicate" Q & A but not so much of **closing** (= preventing answers) at the same time. This is something I think the SE model *almost* got right, but not quite. In most cases I have encountered, a question is rarely an **exact** duplicate (such that no useful answer to the specific question is possible).	f	f
798	574	2	2020-02-10 17:52:59.683925+00	You can now post an answer which simply links to another answer within the same community:\r\n\r\nFor example:\r\n\r\n```\r\n@@@ answer 620\r\n```\r\n\r\nproduces:\r\n\r\n@@@ answer 620\r\n\r\nIn some cases an answer linking to another answer and with no other text would be perfectly appropriate. In other cases you can add your own comments as well. In either case, you deserve the credit for discovering the other answer and linking it for the OP's benefit here.\r\n\r\nWe are hoping this will eventually result in a natural ranking of 'canonical' answers — rather than subjectively tagging them, canonical answers will just be those linked most often.	2020-02-10 17:52:59.683925+00	4	1	1	\N	0	0	0	\N	\N	You can now post an answer which simply links to another answer within the same community:	t	f
74	96	12	2019-11-24 11:04:46.856364+00	This is a **bug** in SQL Server (from 2008 to 2014 inclusive).\r\n\r\nMy bug report is **[here][1]**.\r\n\r\nThe filtering condition is pushed down into the scan operator as a residual predicate, but the memory granted for the sort is erroneously calculated based on the **pre-filter cardinality estimate**.\r\n\r\nTo illustrate the issue, we can use *(undocumented and unsupported)* trace flag 9130 to prevent the Filter from being **pushed down into the scan** operator. The memory granted to the sort is now correctly based on the estimated cardinality of the Filter output, not the scan:\r\n\r\n```sql\r\nSELECT\r\n    T.TID,\r\n    T.FilterMe,\r\n    T.SortMe,\r\n    T.Unused\r\nFROM dbo.Test AS T \r\nWHERE \r\n    T.FilterMe = 567\r\nORDER BY \r\n    T.SortMe\r\nOPTION (QUERYTRACEON 9130); -- Not for production systems!\r\n```\r\n\r\n![Estimated plan][2]\r\n\r\nFor a **production system**, steps will need to be taken to **avoid** the problematic plan shape (a filter pushed into a scan with a sort on another column). One way to do this is to provide an index on the filter condition and/or to provide the required sort order.\r\n\r\n```sql\r\n-- Index on the filter condition only\r\nCREATE NONCLUSTERED INDEX IX_dbo_Test_FilterMe\r\nON dbo.Test (FilterMe);\r\n```\r\n\r\nWith this index in place, the desired memory grant for the sort is only **928KB**:\r\n\r\n![With filter index][3]\r\n\r\nGoing further, the following index can avoid the sort completely (**zero** memory grant):\r\n\r\n```sql\r\n-- Provides filtering and sort order\r\n-- nvarchar(max) column deliberately not INCLUDEd\r\nCREATE NONCLUSTERED INDEX IX_dbo_Test_FilterMe_SortMe\r\nON dbo.Test (FilterMe, SortMe);\r\n```\r\n\r\n![With filter and sort index][4]\r\n\r\nTested and **bug confirmed** on the following builds of SQL Server x64 Developer Edition:\r\n\r\n~~~\r\n2014   : 12.00.2430 (RTM CU4)\r\n2012   : 11.00.5556 (SP2 CU3)\r\n2008R2 : 10.50.6000 (SP3)\r\n2008   : 10.00.6000 (SP4)\r\n~~~\r\n\r\nThe bug was **fixed** in [SQL Server 2016 Service Pack 1][5]. The release notes include the following: \r\n\r\n>VSTS bug number 8024987  \r\nTable scans and index scans with push down predicate tend to overestimate memory grant for the parent operator.\r\n\r\nTested and confirmed fixed on: \r\n\r\n* `Microsoft SQL Server 2016 (SP1) - 13.0.4001.0 (X64) Developer Edition`\r\n* `Microsoft SQL Server 2014 (SP2-CU3) 12.0.5538.0 (X64) Developer Edition`\r\n\r\n...under both cardinality estimation (CE) models.\r\n\r\n  [1]: https://feedback.azure.com/forums/908035-sql-server/suggestions/32909110-cardinality-estimation-error-with-pushed-predicate\r\n  [2]: https://i.stack.imgur.com/DgEfd.png\r\n  [3]: https://i.stack.imgur.com/upDb8.png\r\n  [4]: https://i.stack.imgur.com/U2aXO.png\r\n  [5]: https://support.microsoft.com/en-us/kb/3182545	2019-11-24 11:08:53.35813+00	2	4	2	\N	0	0	0	\N	\N	This is a **bug** in SQL Server (from 2008 to 2014 inclusive).	f	f
463	447	618	2013-10-28 13:05:39+00	Although this might be a bit of a co-incidence due to the artificial nature of your test data, being as you mentioned SQL 2012 I tried a rewrite:\r\n\r\n    SELECT\tID,\r\n    \t\tComp,\r\n    \t\tD,\r\n    \t\tD2 = LEAD(D) OVER(PARTITION BY COMP ORDER BY D)\r\n    FROM\tdbo.T \r\n    WHERE\tD IS NOT NULL\r\n    ORDER BY Comp;\r\n\r\nThis yielded a nice low-cost plan using your index and with significantly lower reads than the other options (and the same results for your test data).\r\n\r\n![Plan Explorer costs for four options: Original; original with hint; outer apply and Lead][1]\r\n\r\nI suspect your real data is more complicated so there might be some scenarios where this query behaves semantically different to yours, but it does show sometimes the new features can make a real difference.\r\n\r\nI did experiment with some more varied data and found some scenarios to match and some not:\r\n\r\n    --Example 1: results matched\r\n    TRUNCATE TABLE dbo.t\r\n    \r\n    -- Generate some more interesting test data\r\n    ;WITH cte AS\r\n    (\r\n    SELECT TOP 1000 ROW_NUMBER() OVER ( ORDER BY ( SELECT 1 ) ) rn\r\n    FROM master.sys.columns c1\r\n    \tCROSS JOIN master.sys.columns c2\r\n    \tCROSS JOIN master.sys.columns c3\r\n    )\r\n    INSERT T (A, B, C, D)\r\n    SELECT\t'A' + CAST( a.rn AS VARCHAR(5) ),\r\n    \t\t'B' + CAST( a.rn AS VARCHAR(5) ),\r\n    \t\t'C' + CAST( a.rn AS VARCHAR(5) ),\r\n    \t\tDATEADD(DAY, a.rn + b.rn, '1 Jan 2013')\r\n    FROM cte a\r\n    \tCROSS JOIN cte b\r\n    WHERE a.rn % 3 = 0\r\n     AND b.rn % 5 = 0\r\n    ORDER BY 1, 2, 3\r\n    GO\r\n    \r\n    \r\n    -- Original query\r\n    SELECT\tt1.ID,\r\n    \t\tt1.Comp,\r\n    \t\tt1.D,\r\n    \t\tD2 = (\tSELECT\tTOP 1 D\r\n    \t\t\t\tFROM\tdbo.T t2\r\n    \t\t\t\tWHERE\tt2.Comp = t1.Comp\r\n    \t\t\t\tAND\t\tt2.D > t1.D\r\n                  \tORDER BY D\r\n    \t\t\t)\r\n    INTO #tmp1\r\n    FROM\tdbo.T t1 \r\n    WHERE\tt1.D IS NOT NULL\r\n    ORDER BY t1.Comp;\r\n    GO\r\n    \r\n    SELECT\tID,\r\n    \t\tComp,\r\n    \t\tD,\r\n    \t\tD2 = LEAD(D) OVER(PARTITION BY COMP ORDER BY D)\r\n    INTO #tmp2\r\n    FROM\tdbo.T \r\n    WHERE\tD IS NOT NULL\r\n    ORDER BY Comp;\r\n    GO\r\n    \r\n    \r\n    -- Checks ...\r\n    SELECT * FROM #tmp1\r\n    EXCEPT\r\n    SELECT * FROM #tmp2\r\n    \r\n    SELECT * FROM #tmp2\r\n    EXCEPT\r\n    SELECT * FROM #tmp1\r\n\r\n\r\n    Example 2: results did not match\r\n    TRUNCATE TABLE dbo.t\r\n    \r\n    -- Generate some more interesting test data\r\n    ;WITH cte AS\r\n    (\r\n    SELECT TOP 1000 ROW_NUMBER() OVER ( ORDER BY ( SELECT 1 ) ) rn\r\n    FROM master.sys.columns c1\r\n    \tCROSS JOIN master.sys.columns c2\r\n    \tCROSS JOIN master.sys.columns c3\r\n    )\r\n    INSERT T (A, B, C, D)\r\n    SELECT\t'A' + CAST( a.rn AS VARCHAR(5) ),\r\n    \t\t'B' + CAST( a.rn AS VARCHAR(5) ),\r\n    \t\t'C' + CAST( a.rn AS VARCHAR(5) ),\r\n    \t\tDATEADD(DAY, a.rn, '1 Jan 2013')\r\n    FROM cte a\r\n    \r\n    -- Add some more data\r\n    INSERT dbo.T (A, B, C, D)\r\n    SELECT A, B, C, D \r\n    FROM dbo.T\r\n    WHERE DAY(D) In ( 3, 7, 9 )\r\n    \r\n    \r\n    INSERT dbo.T (A, B, C, D)\r\n    SELECT A, B, C, DATEADD( day, 1, D )\r\n    FROM dbo.T\r\n    WHERE DAY(D) In ( 12, 13, 17 )\r\n    \r\n    \r\n    SELECT * FROM #tmp1\r\n    EXCEPT\r\n    SELECT * FROM #tmp2\r\n    \r\n    SELECT * FROM #tmp2\r\n    EXCEPT\r\n    SELECT * FROM #tmp1\r\n    \r\n    SELECT * FROM #tmp2\r\n    INTERSECT\r\n    SELECT * FROM #tmp1\r\n    \r\n    \r\n    select * from #tmp1\r\n    where comp = 'A2-B2-C2'\r\n    \r\n    select * from #tmp2\r\n    where comp = 'A2-B2-C2'\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/7tpdL.png\r\n	2019-12-15 06:13:00.529908+00	5	4	1	52326	0	0	0	2019-12-15 06:13:00.529908+00	\N	Although this might be a bit of a co-incidence due to the artificial nature of your test data, being as you mentioned SQL 2012 I tried a rewrite:	f	f
462	447	12	2013-10-28 00:39:21+00	>*Why is a Key Lookup required to get A, B and C when they are not referenced in the query at all? I assume they are being used to calculate Comp, but why?*\r\n\r\nColumns `A, B, and C` *are* referenced in the query plan - they are used by the seek on `T2`.\r\n\r\n>*Also, why can the query use the index on t2, but not on t1?*\r\n\r\nThe optimizer decided that scanning the clustered index was cheaper than scanning the filtered nonclustered index and then performing a lookup to retrieve the values for columns A, B, and C.\r\n\r\n### Explanation\r\n\r\nThe real question is why the optimizer felt the need to retrieve A, B, and C for the index seek at all. We would expect it to read the `Comp` column using a nonclustered index scan, and then perform a seek on the same index (alias T2) to locate the Top 1 record.\r\n\r\nThe query optimizer expands computed column references before optimization begins, to give it a chance to assess the costs of various query plans. For some queries, expanding the definition of a computed column allows the optimizer to find more efficient plans.\r\n\r\nWhen the optimizer encounters a correlated subquery, it attempts to 'unroll it' to a form it finds easier to reason about. If it cannot find a more effective simplification, it resorts to rewriting the correlated subquery as an apply (a correlated join):\r\n\r\n![Apply rewrite][1]\r\n\r\nIt just so happens that this apply unrolling puts the logical query tree into a form that does not work well with *project normalization* (a later stage that looks to match general expressions to computed columns, among other things).\r\n\r\nIn your case, the way the query is written interacts with internal details of the optimizer such that the expanded expression definition is not matched back to the computed column, and you end up with a seek that references columns `A, B, and C` instead of the computed column, `Comp`. This is the root cause.\r\n\r\n### Workaround\r\n\r\nOne idea to workaround this side-effect is to write the query as an apply manually:\r\n\r\n    SELECT\r\n        T1.ID,\r\n        T1.Comp,\r\n        T1.D,\r\n        CA.D2\r\n    FROM dbo.T AS T1\r\n    CROSS APPLY\r\n    (  \r\n        SELECT TOP (1)\r\n            D2 = T2.D\r\n        FROM dbo.T AS T2\r\n        WHERE\r\n            T2.Comp = T1.Comp\r\n            AND T2.D > T1.D\r\n        ORDER BY\r\n            T2.D ASC\r\n    ) AS CA\r\n    WHERE\r\n        T1.D IS NOT NULL -- DON'T CARE ABOUT INACTIVE RECORDS\r\n    ORDER BY\r\n        T1.Comp;\r\n\r\nUnfortunately, this query will not use the filtered index as we would hope either. The inequality test on column `D` inside the apply rejects `NULLs`, so the apparently redundant predicate `WHERE T1.D IS NOT NULL` is optimized away.\r\n\r\nWithout that explicit predicate, the filtered index matching logic decides it cannot use the filtered index. There are a number of ways to work around this second side-effect, but the easiest is probably to change the cross apply to an outer apply (mirroring the logic of the rewrite the optimizer performed earlier on the correlated subquery):\r\n\r\n    SELECT\r\n        T1.ID,\r\n        T1.Comp,\r\n        T1.D,\r\n        CA.D2\r\n    FROM dbo.T AS T1\r\n    OUTER APPLY\r\n    (  \r\n        SELECT TOP (1)\r\n            D2 = T2.D\r\n        FROM dbo.T AS T2\r\n        WHERE\r\n            T2.Comp = T1.Comp\r\n            AND T2.D > T1.D\r\n        ORDER BY\r\n            T2.D ASC\r\n    ) AS CA\r\n    WHERE\r\n        T1.D IS NOT NULL -- DON'T CARE ABOUT INACTIVE RECORDS\r\n    ORDER BY\r\n        T1.Comp;\r\n\r\nNow the optimizer does not need to use the apply rewrite itself (so the computed column matching works as expected) and the predicate is not optimized away either, so the filtered index can be used for both data access operations, and the seek uses the `Comp` column on both sides:\r\n\r\n![Outer Apply Plan][2]\r\n\r\nThis would generally be preferred over adding A, B, and C as `INCLUDEd` columns in the filtered index, because it addresses the root cause of the problem, and does not require widening the index unnecessarily.\r\n\r\n### Persisted computed columns\r\n\r\nAs a side note, it is not necessary to mark the computed column as `PERSISTED`, if you don't mind repeating its definition in a `CHECK` constraint:\r\n\r\n    CREATE TABLE dbo.T \r\n    (\t\r\n    \tID integer IDENTITY(1, 1) NOT NULL,\r\n    \tA varchar(20) NOT NULL,\r\n    \tB varchar(20) NOT NULL,\r\n    \tC varchar(20) NOT NULL,\r\n    \tD date NULL,\r\n    \tE varchar(20) NULL,\r\n    \tComp AS A + '-' + B + '-' + C,\r\n    \r\n        CONSTRAINT CK_T_Comp_NotNull\r\n            CHECK (A + '-' + B + '-' + C IS NOT NULL),\r\n    \r\n        CONSTRAINT PK_T_ID \r\n            PRIMARY KEY (ID)\r\n    );\r\n\r\n    CREATE NONCLUSTERED INDEX IX_T_Comp_D\r\n    ON dbo.T (Comp, D) \r\n    WHERE D IS NOT NULL;\r\n\r\nThe computed column is only required to be `PERSISTED` in this case if you want to use a `NOT NULL` constraint or to reference the `Comp` column directly (instead of repeating its definition) in a `CHECK` constraint.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/BPYGL.png\r\n  [2]: https://i.stack.imgur.com/J3aud.png	2019-12-15 06:13:00.251956+00	2	4	1	52297	0	0	0	2019-12-15 06:13:00.251956+00	\N	>*Why is a Key Lookup required to get A, B and C when they are not referenced in the query at all? I assume they are being used to calculate Comp, but why?*	f	f
510	474	168	2019-11-20 12:55:00+00	# General\r\n\r\nThere are a few considerations when it comes to performance of TeX code:\r\n\r\n1. argument grabbing costs time, don't grab arguments unnecessarily\r\n1. `\\expandafter` is slow, if you can work around it with the same amount of expansions it's faster, so instead of\r\n    ```latex\r\n    \\if...\r\n      \\expandafter\\@firstoftwo\r\n    \\else\r\n      \\expandafter\\@secondoftwo\r\n    \\fi\r\n    ```\r\n    we'd use (this uses an aspect of the first point, too, namely if false only the contents of the true branch will be gobbled)\r\n    ```latex\r\n    \\long\\def\\my@fi@firstoftwo\\fi#1#2#3{\\fi#2}\r\n    \\if...\r\n      \\my@fi@firstoftwo\r\n    \\fi\r\n    \\@secondoftwo\r\n    ```\r\n1. gobbling tokens explicitly as delimiters for arguments is faster than gobbling them as an argument which is delimited, so the above example can further be optimized:\r\n    ```latex\r\n    \\long\\def\\my@fi@firstoftwo\\fi\\@secondoftwo#1#2{\\fi#1}\r\n    \\if...\r\n      \\my@fi@firstoftwo\r\n    \\fi\r\n    \\@secondoftwo\r\n    ```\r\n    But be aware that this way code becomes less readable, less reusable, and less maintainable, so the small performance gain comes at a cost.\r\n\r\n`\\if...` can represent any if test that results in a TeX-syntax if, such as `\\ifx AB`, `\\iftrue`, etc.\r\n\r\nAlso `\\if` tests can be slow (depending on the used test) and so is `\\detokenize`, if we can get around those, we should. Another thing to consider is that `\\if` tests are not robust if their arguments contains other `\\if` tests, `\\else` or `\\fi`. To overcome this the standard test for an empty argument does `\\detokenize` the argument with:\r\n\r\n```latex\r\n\\long\\def\\ifemptyStandard#1%\r\n  {%\r\n    \\if\\relax\\detokenize{#1}\\relax\r\n      \\expandafter\\@firstoftwo\r\n    \\else\r\n      \\expandafter\\@secondoftwo\r\n    \\fi\r\n  }\r\n```\r\n\r\nThis yields an unbeatable robustness, as the only possible argument that might fail this test would be an unbalanced input, which needs to be actively created, such as `\\expandafter\\ifemptyStandard\\expandafter{\\iffalse{\\fi}}{true}{false}` (but who would do that anyway).\r\n\r\nOf all the if tests built into TeX, `\\ifx` is probably the fastest. So a naive test `\\ifx <some-token>#1<some-token>` would be pretty fast, unfortunately this would not be robust. Cases for which it'd fail would be if `\\if...`, `\\else`, or `\\fi` would be part of the argument or if `#1` starts with `<some-token>` (though we can make `<some-token>` pretty unlikely).\r\n\r\n# Fast `\\ifempty`\r\n\r\nThe following is a fast test, that considers some of the above mentioned aspects. We don't use any `\\if...` test, but instead do the branching through TeX's argument grabbing logic:\r\n\r\n```latex\r\n\\long\\def\\ifempty@true\\ifempty@A\\ifempty@B\\@secondoftwo#1#2{#1}\r\n\\long\\def\\ifempty@#1\\ifempty@A\\ifempty@B{}\r\n\\long\\def\\ifempty#1%\r\n  {%\r\n    \\ifempty@\\ifempty@A#1\\ifempty@B\\ifempty@true\r\n      \\ifempty@A\\ifempty@B\\@secondoftwo\r\n  }\r\n```\r\n\r\nSo if `#1` is empty `\\ifempty@` will gobble only the first `\\ifempty@A` and `\\ifempty@B` and `\\ifempty@true` will be executed, gobbling the following `\\ifempty@A\\ifempty@B\\@secondoftwo` and the false-branch. On the other hand, if `#1` is not empty everything up to `\\@secondoftwo` (non-inclusive) will be gobbled and `\\@secondoftwo` will execute the false-branch.\r\n\r\nThis way we get a fast testing macro (taking about 70% the time of the `\\if\\relax\\detokenize{#1}\\relax` test during my benchmarks), that's fairly robust (only input which contains `\\ifempty@A\\ifempty@B` will fail the test, and that should be rare).\r\n\r\nAnd of course, we can use tokens which are even more unlikely than `\\ifempty@A` and `\\ifempty@B`, e.g., why not use a `<DEL>` characters for both but with different category codes (that should be pretty very very unlikely to ever be part of a valid argument):\r\n\r\n```latex\r\n\\begingroup\r\n\\lccode`\\&=127\r\n\\lccode`\\$=127\r\n\\catcode`\\&=12\r\n\\catcode`\\$=11\r\n\\lowercase{\\endgroup\r\n\\long\\def\\ifempty@true&$\\@secondoftwo#1#2{#1}\r\n\\long\\def\\ifempty@#1&${}\r\n\\long\\def\\ifempty#1{\\ifempty@&#1$\\ifempty@true&$\\@secondoftwo}\r\n}\r\n```\r\n\r\n# Fast `\\ifblank`\r\n\r\nAs a small addition, we can also create a fast `\\ifblank` test based on the aforementioned thoughts. The standard `\\ifblank` looks something like the following:\r\n\r\n```latex\r\n\\long\\def\\ifblankStandard#1%\r\n  {%\r\n    \\if\\relax\\detokenize\\expandafter{\\@gobble #1.}\\relax\r\n      \\expandafter\\@firstoftwo\r\n    \\else\r\n      \\expandafter\\@secondoftwo\r\n    \\fi\r\n  }\r\n```\r\n\r\nSo essentially the same as `\\ifemptyStandard` but with an `\\expandafter` and a `\\@gobble #1.` added. But we could do the same as for our fast `\\ifempty` test with just some small additions (I'll just add this to the slightly obfuscated variant using the `<DEL>` tokens). And we don't want to use some `\\expandafter`s (remember they are slow) so we use `\\ifblank@` to gobble one token and insert the necessary tests of `\\ifempty`.\r\n\r\n```latex\r\n\\begingroup\r\n\\lccode`\\&=127\r\n\\lccode`\\$=127\r\n\\catcode`\\&=12\r\n\\catcode`\\$=11\r\n\\lowercase{\\endgroup\r\n\\long\\def\\ifempty@true&$\\@secondoftwo#1#2{#1}\r\n\\long\\def\\ifempty@#1&${}\r\n\\long\\def\\ifempty#1{\\ifempty@&#1$\\ifempty@true&$\\@secondoftwo}\r\n\\long\\def\\ifblank@#1{\\ifempty@&}\r\n\\long\\def\\ifblank#1{\\ifblank@#1.$\\ifempty@true&$\\@secondoftwo}\r\n}\r\n```\r\n	2019-12-22 19:47:44.509345+00	4	4	1	517265	0	0	0	2019-12-22 19:47:44.509345+00	\N	General	f	f
247	285	16	2019-05-13 17:38:14+00	Why Yours Is Better\r\n--\r\nIn general, your pattern is a better idea. \r\n\r\nPerformance will depend on indexes, predicate selectivity, and table size though.\r\n\r\nThe reason your pattern is a better idea comes down to the concept of SARGability, which is a fancy word for if your search arguments can be used as seek predicates, or even fully pushed predicates, (i.e. not being processed in Filter operators after accessing an index).\r\n\r\nSome examples of where this can hurt in joins and where clauses are:\r\n\r\n* function(column) = something\r\n* column + column = something\r\n* column + value = something\r\n* column = @something or @something IS NULL\r\n* column like ‘%something%’\r\n* column = case when …\r\n\r\nWhen you do stuff like this, your queries can end up with all sorts of bad side effects:\r\n\r\n* Increased CPU (burn baby burn)\r\n* Index Scans (when you could have Seeks)\r\n* Implicit Conversion (if your predicates produce a different data type)\r\n* Poor Cardinality Estimates (poking the optimizer in the eye)\r\n* Inappropriate Plan Choices (because the optimizer is blind now, you jerk)\r\n* Long Running Queries (yay job security!)\r\n\r\nBetter Options\r\n---\r\nSARGable options for what you're looking for would include:\r\n\r\n`WHERE [ID] LIKE 'G%'`\r\n\r\nor\r\n\r\n`WHERE [ID] >= 'G' AND [ID] < 'H'`\r\n\r\nAn alternative solution would be to add a computed column in just the table you're searching:\r\n\r\n    ALTER TABLE [Table] \r\n        ADD Lefty AS Left([ID], 1);\r\n    \r\n    CREATE INDEX ix_whatever \r\n        ON [Table] (CategoryID , Lefty);\r\n\r\nThough like I said before, the performance difference may not be dramatic with smaller tables. \r\n\r\nIt's also possible that this index won't be used since your example query is selecting all of the table columns, and this index doesn't cover them. But that's a story for a different day.\r\n	2019-12-04 14:29:03.354673+00	2	4	1	238046	0	0	0	2019-12-04 14:29:03.354673+00	\N	Why Yours Is Better	f	f
607	560	751	2013-10-04 23:26:41+00	The value for `dbi_maxDbTimestamp` is stored on the database boot page. (page 9 in the primary data file).\r\n\r\nThis is not written to every time a timestamp value is allocated. Instead SQL Server reserves a few thousand at a time. \r\n\r\nFor example if `@@DBTS` is `2000` and the `dbi_maxDbTimestamp` is also `2000` then SQL Server updates the value written in the boot page to `6000` the next time it needs a timestamp value.\r\n\r\nThe values from `2001 - 5999` are allocated in memory and "lost" if the database is set offline and then online again.\r\n\r\nThe backup contains the copy of the boot page that has been updated to `6000`. So upon restoring it the timestamp values will start from this number. It knows nothing about any lost intermediate values.\r\n\r\nTo see this\r\n\r\n    CREATE DATABASE DbtsTest\r\n    \r\n    GO\r\n    \r\n    USE DbtsTest\r\n    \r\n    GO\r\n    \r\n    DBCC TRACEON(3604);\r\n    \r\n    CREATE TABLE T (X ROWVERSION)\r\n    \r\n    SELECT CAST(@@dbts AS BIGINT);\r\n    \r\n    DBCC PAGE(DbtsTest,1,9,1)\r\n\r\nOn my system for a newly created database `@@dbts` is `2,000`. The `DBCC PAGE` output from above is\r\n\r\n![DBCC Page 1][1]\r\n\r\nI have highlighted the timestamp value. `CAST(CAST(REVERSE(0xD007000000000000) AS BINARY(8)) AS BIGINT)` = `2000`\r\n\r\n    INSERT INTO T DEFAULT VALUES\r\n    \r\n    SELECT CAST(@@dbts AS BIGINT);\r\n    DBCC PAGE(DbtsTest,1,9,1)\r\n\r\nNow the `@@dbts` is reported as `2001` but looking at the page itself. \r\n\r\n![DBCC Page 2][2]\r\n\r\nthe timestamp value has changed. `CAST(CAST(REVERSE(0x7017000000000000) AS BINARY(8)) AS BIGINT)` = `6000`.\r\n\r\nRunning\r\n\r\n    DBCC DBTABLE('DbtsTest')\r\n\r\nat this point to view the [`DBTABLE` structure][3] shows both values\r\n\r\n    dbt_maxDbTimestamp = 6000           \r\n    dbt_dbTimestamp = 2001\r\n\r\nFinally \r\n\r\n    BACKUP DATABASE [DbtsTest] TO  \r\n    DISK = N'C:\\Program Files\\Microsoft SQL Server\\MSSQL11.SQL2012\\MSSQL\\Backup\\DbtsTest.bak' \r\n    WITH NOFORMAT, \r\n         NOINIT,  \r\n    \t NAME = N'DbtsTest-Full Database Backup', \r\n    \t SKIP, \r\n    \t NOREWIND, \r\n    \t NOUNLOAD,  \r\n    \t STATS = 10\r\n\r\nThen looking at the backup shows it is the 6,000 figure that is written.\r\n\r\n[![enter image description here][4]][5]\r\n\r\nRestoring the database and querying `SELECT CAST(@@DBTS AS BIGINT)` returns `6,000` as expected.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/tE1br.png\r\n  [2]: https://i.stack.imgur.com/xZozd.png\r\n  [3]: http://www.scheduler-usage.com/Uploads/philmalmaison/mdcache_whitepaper.pdf\r\n  [4]: https://i.stack.imgur.com/H4Uts.png\r\n  [5]: https://i.stack.imgur.com/H4Uts.png	2020-01-12 14:19:29.288381+00	5	4	1	51064	0	0	0	2020-01-12 14:19:29.288381+00	\N	The value for `dbi_maxDbTimestamp` is stored on the database boot page. (page 9 in the primary data file).	f	f
751	665	2	2020-02-03 19:58:23.82123+00	I can't find the conversation behind it now, but on our todo list is a job to make tag edits bump questions but only on meta.ta, so as things stand…\r\n\r\n> But it seems as if tag edits, either adding or deleting tags, does not bump the Q&A in the site's sort order.\r\n>\r\n> Is this correct?\r\n\r\n…yes that's correct.\r\n\r\n> I actually hope it is, as the re-tagging changes nothing in terms of Q&A content, only refines the "meta" information about it. (I always thought tag-edits on SE that bumped Q&As were a bit of a pain, frankly.)\r\n\r\nThat's usually the case but on meta with tags like `in-progress` and `completed`, perhaps it still makes sense to have tag edits bump questions here? Or perhaps that's moot because most significant status changes also involve an answer (or answer edit) anyway, what do you think?	2020-02-03 19:58:23.82123+00	1	1	1	\N	0	0	0	\N	\N	I can't find the conversation behind it now, but on our todo list is a job to make tag edits bump questions but only on meta.ta, so as things stand…	t	f
736	664	2	2020-02-02 11:54:44.728942+00	This suggestion has obviously been well received so we've added the new question type 'Contest' and made the linked question have that type:\r\n\r\n@@@ answer 583\r\n\r\nAs a trial to demonstrate the feature, we've applied the option to have a minimum number of stars before answers are allowed (the answer already posted isn't affected, only new answers).\r\n\r\nIf you like that we can keep it, if not we'll remove it and (optionally) prevent voting on contest questions at all again.\r\n\r\nWe could also implement an *upper* limit so that further voting on the question isn't allowed once it has 'launched'.\r\n\r\nI think one important bit of information that I don't have is: "how hard is it to post a good contest, and how much do you want to reward those that do" — on Code Golf, posting good challenges is apparently quite a lot harder than answering them.	2020-02-02 11:54:44.728942+00	10	1	1	\N	0	0	0	\N	\N	This suggestion has obviously been well received so we've added the new question type 'Contest' and made the linked question have that type:	t	f
514	242	1	2019-12-23 11:50:51.106936+00	We have just released the first step towards this. Now notifications and chat:\r\n* are in separate panels\r\n* have a border around them just like the border around question/answer posts\r\n* both have titles describing what they are ('Notifications', 'Chat', or 'Comments' if it is a room for a question)\r\n\r\nIn addition, the stack of room icons now shows a count of unread messages in the room, and it polls for changes. We think this request is mostly `completed` now…	2020-02-10 17:19:10.622053+00	6	1	1	\N	0	0	0	\N	\N	We have just released the first step towards this. Now notifications and chat:	f	f
445	436	603	2018-05-10 08:29:42+00	A good friend of mine Grzegorz (Stasiek) Domański found the solution that makes the query check startup predicate at first. Here is the code (similar to Erik solution).\r\n\r\n     create or alter function dbo.test_fn1(@Par1 varchar(100), @Par2 varchar(1))\r\n     returns @t table(item varchar(100))\r\n     as\r\n     begin\r\n       insert into @t (item) select value from STRING_SPLIT(@Par1, @Par2);\r\n       return\r\n     end\r\n     GO\r\n     create or alter function dbo.test_fn(@Par1 varchar(100), @Par2 varchar(100))\r\n     returns table\r\n     as\r\n     return (\r\n       select sx.*\r\n       from (select top 1 null x) a\r\n       cross apply(\r\n         select s.*\r\n           from dbo.test_fn1(@Par2,';') x\r\n           inner join sys.objects s on s.name = x.item\r\n       ) sx\r\n       where @Par1 = 'CASE1'\r\n     )\r\n     GO\r\n     create or alter function dbo.test_fnx(@Par1 varchar(100), @Par2 \r\n        varchar(100))\r\n     returns table\r\n     as\r\n     return (\r\n       select sx.*\r\n       from (select top 1 null x) a\r\n       cross apply(\r\n         select s.*\r\n           from sys.objects s\r\n           inner join dbo.test_fn1(@Par2,';') x ON s.name = x.item\r\n       ) sx\r\n       where @Par1 = 'CASE1'\r\n     )\r\n     GO\r\n     declare @Par1 varchar(100), @Par2 varchar(100)\r\n     select @Par1 = 'CASE2', @Par2 = 'test1;test2'\r\n     \r\n     select * from dbo.test_fn(@Par1, @Par2)\r\n     select * from dbo.test_fnx(@Par1, @Par2)\r\n\r\nHere are the execution plans. Marked places are the moments of startup predicate checks. And these are actual plans so no records are considered as long as startup predicate is false. And that is we expected to have.\r\n\r\n[![Execution plan for first statement and the position of startup predicate][1]][1]\r\n\r\n\r\n[![Execution plan for second statement and the position of startup predicate][2]][2]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/fmKHM.png\r\n  [2]: https://i.stack.imgur.com/kVijh.png	2019-12-13 09:50:15.344941+00	1	4	1	206329	0	0	0	2019-12-13 09:50:15.344941+00	\N	A good friend of mine Grzegorz (Stasiek) Domański found the solution that makes the query check startup predicate at first. Here is the code (similar to Erik solution).	f	f
444	436	16	2018-05-08 23:22:17+00	> Is there any whitepaper or documentation on how SQL Server is placing startup predicates?\r\n\r\nNo, not that I've ever seen. They don't get written about all that much. The one time I blogged about them, I did a lot of head scratching.\r\n\r\nNow, I have an answer -- it's not guaranteed to always work. It's playing some tricks on the optimizer that you can learn more about by watching [Query Tuning Mastery: Clash of the Row Goals][1], a talk by Adam Machanic.\r\n\r\nWith that out of the way, we can control to some degree where the optimizer decides to stick predicates by using TOP.\r\n\r\n**Function rewrite 1:**\r\n\r\nIn this one, the `WHERE` clause appears inside the `CROSS APPLY`.\r\n\r\n    CREATE OR ALTER FUNCTION dbo.test_erik ( @Par1 VARCHAR(100), @Par2 VARCHAR(100))\r\n    RETURNS TABLE\r\n    AS\r\n        RETURN (   SELECT s.*\r\n                   FROM   sys.objects AS s\r\n                   CROSS APPLY\r\n                          (   SELECT TOP (2147483647) *\r\n                              FROM   dbo.test_fn1(@Par2, ';') AS x\r\n                              WHERE  s.name = x.item\r\n                              AND    @Par1 = 'CASE1' ) AS ca );\r\n    GO\r\n\r\n**Function rewrite 2:**\r\n\r\nIn this one, the `WHERE` clause appears outside the `CROSS APPLY`.\r\n\r\n    CREATE OR ALTER FUNCTION dbo.test_erikx ( @Par1 VARCHAR(100), @Par2 VARCHAR(100))\r\n    RETURNS TABLE\r\n    AS\r\n        RETURN (   SELECT s.*\r\n                   FROM   sys.objects AS s\r\n                   CROSS APPLY\r\n                          (   SELECT TOP (2147483647) *\r\n                              FROM   dbo.test_fn1(@Par2, ';') AS x\r\n                              WHERE  s.name = x.item ) AS ca\r\n                   WHERE  @Par1 = 'CASE1' );\r\n    GO\r\n\r\nYou can see the [resulting plans here][2].\r\n\r\nIn plan 1, the startup expression predicate appears inside by the TOP:\r\n\r\n[![NUTS][3]][3]\r\n\r\nIn plan 2, it appears outside.\r\n\r\n[![NUTS][4]][4]\r\n\r\nIf you want the Startup Expression Predicate to act as a constant scan, try this:\r\n\r\n    CREATE OR ALTER FUNCTION dbo.test_erik_filter ( @Par1 VARCHAR(100), @Par2 VARCHAR(100))\r\n    RETURNS TABLE\r\n    AS\r\n        RETURN (   \r\n    \t\t\t   SELECT ca.*\r\n    \t\t\t   FROM (\r\n    \t\t\t\t\t SELECT TOP (1) 1 AS n \r\n    \t\t\t\t\t WHERE @Par1 = 'CASE1'\r\n    \t\t\t\t\t) AS x\r\n    \t\t\t   CROSS APPLY \r\n    \t\t\t\t(\r\n    \t\t\t   SELECT     s.*\r\n                   FROM       sys.objects AS s\r\n                   INNER JOIN dbo.test_fn1(@Par2, ';') AS x\r\n                       ON s.name = x.item\t\t\t   \t   \r\n    \t\t\t   ) AS ca\r\n    \t\t\t   \r\n    \t\t );\r\n    GO\r\n\r\nIf we run two different test cases, the plans will be slightly different:\r\n\r\n    DECLARE @Par1 VARCHAR(100), @Par2 VARCHAR(100);\r\n    SELECT @Par1 = 'CASE2', @Par2 = 'test1;test2';\r\n    \r\n    select * from dbo.test_erik_filter(@Par1, @Par2)\r\n    \r\n    GO \r\n    \r\n    DECLARE @Par1 VARCHAR(100), @Par2 VARCHAR(100);\r\n    SELECT @Par1 = 'CASE1', @Par2 = 'test1;test2';\r\n    \r\n    select * from dbo.test_erik_filter(@Par1, @Par2)\r\n    \r\n    GO\r\n\r\nLooking at the live query plans ([here are the regular ones][5]), there are a couple _early_ important differences.\r\n\r\nNo rows come out of this, which means the start up expression isn't true.\r\n[![NUTS][6]][6]\r\n\r\nRows will flow out of this one, because it does.\r\n[![NUTS][7]][7]\r\n\r\nRemember that data flows from right to left in a query plan. Putting the filter further towards the left side of the plan has no advantage for doing less work.\r\n\r\nAaron Bertrand discusses a similar issue with regular filters here: \r\n\r\n - [Should I use NOT IN, OUTER APPLY, LEFT OUTER JOIN, EXCEPT, or NOT EXISTS?][8]\r\n\r\nAs an aside, if your real-life code doesn't just select from system tables/views, you should consider adding the [SCHEMABINDING][9] attribute to it, which could aid in filter placement.\r\n\r\nHope this helps!\r\n\r\n\r\n  [1]: https://sqlbits.com/Sessions/Event14/Query_Tuning_Mastery_Clash_of_the_Row_Goals\r\n  [2]: https://www.brentozar.com/pastetheplan/?id=H17Exh1RG\r\n  [3]: https://i.stack.imgur.com/Pv15m.jpg\r\n  [4]: https://i.stack.imgur.com/xcPsS.jpg\r\n  [5]: https://www.brentozar.com/pastetheplan/?id=rysZBSeRM\r\n  [6]: https://i.stack.imgur.com/muafR.jpg\r\n  [7]: https://i.stack.imgur.com/9PBwE.jpg\r\n  [8]: https://sqlperformance.com/2012/12/t-sql-queries/left-anti-semi-join\r\n  [9]: https://dba.stackexchange.com/questions/140381/is-there-any-benefit-to-schemabinding-a-function-beyond-halloween-protection	2019-12-13 09:50:15.06093+00	3	4	1	206207	0	0	0	2019-12-13 09:50:15.06093+00	\N	> Is there any whitepaper or documentation on how SQL Server is placing startup predicates?	f	f
85	120	12	2019-11-26 16:07:17.029189+00	Use `DBCC UPDATEUSAGE` with the `COUNT_ROWS` option.\r\n\r\n    DBCC UPDATEUSAGE \r\n    (   { database_name | database_id | 0 } \r\n        [ , { table_name | table_id | view_name | view_id } \r\n        [ , { index_name | index_id } ] ] \r\n    ) [ WITH [ NO_INFOMSGS ] [ , ] [ COUNT_ROWS ] ] \r\n\r\n[Documentation][1]\r\n\r\n\r\n  [1]: https://msdn.microsoft.com/en-us/library/ms188414.aspx\r\n	2019-11-26 16:07:17.029189+00	3	4	2	\N	0	0	0	\N	\N	Use `DBCC UPDATEUSAGE` with the `COUNT_ROWS` option.	f	f
492	460	88	2012-02-13 18:04:13+00	In SQL Server 2012 and above, you can use [`sys.dm_exec_describe_first_result_set`](https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-describe-first-result-set-transact-sql) locally, assuming the result set you are after is the *first* result:\r\n\r\n    DECLARE @sql NVARCHAR(MAX) = N'';\r\n    \r\n    SELECT @sql += ',' + CHAR(13) + CHAR(10) + CHAR(9)\r\n    \t+ name + ' ' + system_type_name\r\n    \tFROM sys.dm_exec_describe_first_result_set('sp_who', NULL, 1);\r\n    \r\n    SELECT @sql = N'CREATE TABLE #f\r\n    (' + STUFF(@sql, 1, 1, N'') + '\r\n    );';\r\n    \r\n    PRINT @sql;\r\n\r\nResult:\r\n\r\n    CREATE TABLE #f\r\n    (\r\n    \tspid smallint,\r\n    \tecid smallint,\r\n    \tstatus nchar(30),\r\n    \tloginame nvarchar(128),\r\n    \thostname nchar(128),\r\n    \tblk char(5),\r\n    \tdbname nvarchar(128),\r\n    \tcmd nchar(16),\r\n    \trequest_id int\r\n    );\r\n\r\nNote there is a limitation: if your stored procedure creates #temp tables, the metadata functionality does not work. This is why I did not use sp_who2. :-)	2019-12-18 07:29:22.774008+00	4	4	1	12805	0	0	0	2019-12-18 07:29:22.774008+00	\N	In SQL Server 2012 and above, you can use [`sys.dm_exec_describe_first_result_set`](https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-describe-first-result-set-transact-sql) locally, assuming the result set you are after is the *first* result:	f	f
491	460	751	2012-02-13 09:29:55+00	If the procedure just returns one result set and the [ad hoc distributed queries][1] option is enabled.\r\n\r\n    SELECT * \r\n    INTO #T \r\n    FROM OPENROWSET('SQLNCLI', \r\n                    'Server=(local)\\MSSQL2008;Trusted_Connection=yes;',\r\n                     'SET FMTONLY OFF;EXEC sp_who')\r\n     \r\n\r\nOr you can set up a loopback linked server and use that instead.\r\n\r\n    EXEC sp_addlinkedserver @server = 'LOCALSERVER',  @srvproduct = '',\r\n                            @provider = 'SQLNCLI', @datasrc = @@servername\r\n    \r\n    SELECT *\r\n    INTO  #T\r\n    FROM OPENQUERY(LOCALSERVER, \r\n                   'SET FMTONLY OFF;\r\n                   EXEC sp_who')\r\n\r\n  [1]: http://msdn.microsoft.com/en-us/library/ms187569.aspx	2019-12-18 07:29:22.510514+00	3	4	1	12745	0	0	0	2019-12-18 07:29:22.510514+00	\N	If the procedure just returns one result set and the [ad hoc distributed queries][1] option is enabled.	f	f
489	459	751	2017-02-19 20:15:12+00	    SELECT thing, \r\n           sql_variant_property(thing,'basetype') AS basetype,\r\n           sql_variant_property(thing,'precision') AS precision, \r\n           sql_variant_property(thing,'scale') AS scale\r\n    FROM (VALUES (2147483648)) V(thing)\r\n\r\nShows you that the literal `2147483648` is interpreted as `numeric(10,0)`. This behaviour pre-dates the introduction of the `bigint` in SQL Server (2000).\r\n\r\nThere is no syntax to indicate that a literal should be treated as `bigint` - adding an explicit `CAST` is the best solution. The article [Dynamic Seeks and Hidden Implicit Conversions][3] discusses the rest of the apparatus in the plan.\r\n\r\nThe plan itself shows that the nested loops has a seek predicate on\r\n\r\n```none\r\nSeek Keys[1]:\r\n    Start: [tempdb].[dbo].[Table1].col1 > Scalar Operator([Expr1005]), \r\n    End: [tempdb].[dbo].[Table1].col1 < Scalar Operator([Expr1006])\r\n```\r\n\r\nYou can use an extended events  session on [`query_trace_column_values`][1] to see that these are as follows.\r\n\r\n[![enter image description here][2]][2]\r\n\r\nThe XML in the plan also shows this\r\n\r\n```xml\r\n<DefinedValue>\r\n    <ValueVector>\r\n        <ColumnReference Column="Expr1005" />\r\n        <ColumnReference Column="Expr1006" />\r\n        <ColumnReference Column="Expr1004" />\r\n    </ValueVector>\r\n    <ScalarOperator ScalarString="GetRangeWithMismatchedTypes((2147483648.),NULL,(6))">\r\n        <Intrinsic FunctionName="GetRangeWithMismatchedTypes">\r\n        <ScalarOperator>\r\n            <Const ConstValue="(2147483648.)" />\r\n        </ScalarOperator>\r\n        <ScalarOperator>\r\n            <Const ConstValue="NULL" />\r\n        </ScalarOperator>\r\n        <ScalarOperator>\r\n            <Const ConstValue="(6)" />\r\n        </ScalarOperator>\r\n        </Intrinsic>\r\n    </ScalarOperator>\r\n</DefinedValue>\r\n```\r\n\r\nThis does not mean that it is literally doing a comparison `< null` rather \r\n\r\n>  The range boundary expressions use NULL to represent 'unbounded' at either end. ([Source][4])\r\n\r\nSo the net effect is that your query predicate of `b.col1 > CAST(2147483648 AS NUMERIC(10, 0))` still ends up with a seek against `b.col1 > CAST(2147483648 AS BIGINT)`\r\n\r\n>  Does it affect bound parameters to prepared statements (from jtds JDBC) as well?\r\n\r\nI haven't used jtds JDBC but I presume it allows you to define parameter datatypes? If so just make sure the parameters are the correct datatype that match the column (`bigint`) so there's no need for SQL Server to deal with mismatched datatypes.\r\n\r\n  [1]: http://www.queryprocessor.com/query-trace-column-values/\r\n  [2]: https://i.stack.imgur.com/UJd9B.png\r\n  [3]: https://sql.kiwi/2012/01/dynamic-seeks-and-hidden-implicit-conversions.html\r\n  [4]: https://topanswers.xyz/databases?q=437#a446	2019-12-18 07:24:55.322281+00	4	4	1	164849	0	0	0	2019-12-18 07:20:42.752341+00	\N	SELECT thing,	f	f
490	459	656	2017-02-21 15:21:59+00	In relation to my question about JDBC prepared statements. [jTDS][1] uses `sp_prepare`/`sp_execute` (in default `prepareSQL=3` mode).\r\n\r\nWith the following query ([source][2]):\r\n\r\n    use database\r\n    select\r\n        cp.objtype, st.text\r\n    from sys.dm_exec_cached_plans cp\r\n    cross apply sys.dm_exec_sql_text(cp.plan_handle) st\r\n    where cp.objtype = 'prepared' and st.text like '%TABLE%'\r\n\r\nI can see the prepared statement as issued by JTDS, and it does declare the variable as `(@P0 bigint)...` as expected.\r\n\r\nSo this is all good and I need to remember that when trying out the execution plans that it is better to actually define local typed variables instead of replacing them with literals (and/or using `sp_execute` to tap into the cached execution plan).\r\n\r\n  [1]: http://jtds.sourceforge.net/faq.html\r\n  [2]: https://dba.stackexchange.com/questions/21385/list-prepared-statements	2019-12-18 07:20:43.052124+00	2	4	1	165037	0	0	0	2019-12-18 07:20:43.052124+00	\N	In relation to my question about JDBC prepared statements. [jTDS][1] uses `sp_prepare`/`sp_execute` (in default `prepareSQL=3` mode).	f	f
160	223	12	2018-08-04 11:24:34+00	>1NF rule says that we should not keep data of different types in one single column.\r\n\r\nThere's no general agreement about exactly what *first normal form* (1NF) means in detail. Personally, I prefer Chris Date's current interpretation, which simply states that all relation variables (relvars) are in 1NF by definition.\r\n\r\n>Does it mean that sql_variant is not compatible with first normal form hence should not be used?\r\n\r\nThis is a bit imprecise since normal forms are properties of the logical model, whereas `sql_variant` is a physical (SQL Server-specific) type. We would need to define a mapping from relational model *type* (or domain) to physical implementation for this to truly make sense.\r\n\r\nNevertheless, I will say that `sql_variant` is not *inherently* an indication that the associated model violates 1NF - so long as the model defines the range of allowable attribute values to match those storable in `sql_variant` (with or without suitable constraints). The fact that `sql_variant` can store values of several different underlying types is neither here nor there. We might as well argue that *float* types violate 1NF since they can contain integers as well as reals etc.\r\n\r\nIn other words, a physical table using `sql_variant` *can* be a valid representation of a relation variable (within the general limitations of SQL anyway). It simply depends on what the underlying logical model says about the range of values allowed in the attribute.\r\n\r\nConsider also that whatever we might physically store in a `sql_variant` might also be stored in a string representation, or as a binary value. String and binary values do not necessarily violate 1NF, so why should `sql_variant`? Similar arguments can be made about `xml` or `json` (and array types for databases that support such things).\r\n\r\nAll of the above is orthogonal to the question of whether using `sql_variant` is a feature of a "good" design/implementation [or not][1].\r\n\r\nFurther reading:\r\n\r\n* [Facts and Fallacies about First Normal Form][2] by Anith Sen\r\n* [First Normal Form in Theory and Practice][3] by Fabian Pascal\r\n* [The Third Manifesto][4] - C J Date and Hugh Darwen\r\n* https://dba.stackexchange.com/q/161119 (Q & A)\r\n\r\n#### Footnote\r\n\r\n[Walter Mitty](https://dba.stackexchange.com/users/1050/walter-mitty) said:\r\n\r\n\r\n> In relational maths, some domains may have relations as elements.  If such a domain were used as the basis for an attribute in another relation, the result would not be in what Codd originally without reducing the expressive power of the model was the whole point.  \r\n>   \r\n> Relvars eliminate this kind of nesting by definition, as stated.  Beginners often get around the restriction in physical design by storing CSV strings in columns. There are consequences of such designs with regard to keyed access to data.\r\n\r\n\r\n  [1]: https://brentozar.com/archive/2017/03/no-seriously-dont-use-sql_variant/\r\n  [2]: https://www.red-gate.com/simple-talk/sql/learn-sql-server/facts-and-fallacies-about-first-normal-form/\r\n  [3]: http://www.dbdebunk.com/2016/03/real-data-science-first-normal-form-in.html\r\n  [4]: http://www.thethirdmanifesto.com/	2019-12-01 15:17:15.363114+00	0	4	1	214057	0	0	0	2019-12-01 15:14:08.441027+00	\N	>1NF rule says that we should not keep data of different types in one single column.	f	f
100	146	60	2012-08-16 00:35:28+00	Enhancing Jack Douglas's answer to avoid the need for PL/PgSQL looping and bytea concatenation, you can use:\r\n\r\n    CREATE OR REPLACE FUNCTION random_bytea(bytea_length integer)\r\n    RETURNS bytea AS $body$\r\n        SELECT decode(string_agg(lpad(to_hex(width_bucket(random(), 0, 1, 256)-1),2,'0') ,''), 'hex')\r\n        FROM generate_series(1, $1);\r\n    $body$\r\n    LANGUAGE 'sql'\r\n    VOLATILE\r\n    SET search_path = 'pg_catalog';\r\n\r\nIt's a simple `SQL` function that's cheaper to call than PL/PgSQL.\r\n\r\nThe difference in performance due to the changed aggregation method is immense for larger `bytea` values. Though the original function is actually up to 3x faster for sizes < 50 bytes, this one scales much better for larger values.\r\n\r\n**Or use a C extension function**:\r\n\r\nI've implemented a random bytea generator as a simple C extension function. It's in my [scrapcode repository on GitHub](https://github.com/ringerc/scrapcode/tree/master/postgresql/random_bytea). See the README there.\r\n\r\nIt nukes the performance of the above SQL version:\r\n\r\n```none\r\nregress=# \\a\r\nregress=# \\o /dev/null\r\nregress=# \\timing on\r\nregress=# select random_bytea(2000000);\r\nTime: 895.972 ms\r\nregress=# drop function random_bytea(integer);\r\nregress=# create extension random_bytea;\r\nregress=# select random_bytea(2000000);\r\nTime: 24.126 ms\r\n```	2019-11-27 01:28:18.966729+00	1	4	1	22571	0	0	0	2019-11-27 01:21:53.035797+00	\N	Enhancing Jack Douglas's answer to avoid the need for PL/PgSQL looping and bytea concatenation, you can use:	f	f
101	146	2	2012-08-15 10:08:11+00	> I would like to be able to generate random bytea fields of arbitrary length\r\n\r\nThis function will do it, but 1Gb will take a long time because it does not scale linearly with output length:\r\n\r\n<>http://dbfiddle.uk/?rdbms=postgres_9.6&fiddle=7b55d9b493c5930cdc1c601304ca738d	2019-11-27 01:29:33.433608+00	0	4	1	22513	0	0	0	2019-11-27 01:21:53.298914+00	\N	> I would like to be able to generate random bytea fields of arbitrary length	f	f
77	100	12	2019-11-25 10:16:16.662118+00	> *Anyone ever see Cost Threshold for Parallelism being ignored?*\r\n\r\nIt is not being ignored. During the compilation process, the optimizer first considers a serial plan. If the estimated cost of that plan exceeds the Threshold, the optimizer goes on to look for a parallel plan. If the resulting parallel plan is costed below the best serial one, it will be chosen.\r\n\r\nSo, the parallel plan will have a lower cost that the serial one (which you cannot see). It is perfectly possible for the final parallel plan to have an estimated cost below the Threshold - the point is *the best serial plan candidate exceeded the Threshold*.\r\n\r\nAn example can be seen in my [blog post][1] on parallel plan bitmaps.\r\n\r\n  [1]: https://sqlkiwi.blogspot.com/2011/07/bitmap-magic.html	2019-11-25 10:16:16.662118+00	2	4	2	\N	0	0	0	\N	\N	> *Anyone ever see Cost Threshold for Parallelism being ignored?*	f	f
596	550	37	2017-08-31 14:13:06+00	The certificate used by TDE to encrypt the database encryption key is actually stored in the `master` database, which is in turn encrypted by the `database master key` stored in the *`master`* database. \r\n\r\nThe query to see which certificate is used to decrypt the `my_db` TDE-encrypted database should be:\r\n\r\n    SELECT DatabaseName = d.name\r\n    \t, ddek.encryptor_type\r\n        , ddek.opened_date\r\n        , c.name\r\n        , c.cert_serial_number\r\n        , c.pvt_key_encryption_type_desc\r\n        , c.subject\r\n    FROM sys.dm_database_encryption_keys ddek\r\n    \tINNER JOIN sys.databases d ON ddek.database_id = d.database_id\r\n        LEFT JOIN master.sys.certificates c ON ddek.encryptor_thumbprint = c.thumbprint\r\n    WHERE d.name <> 'tempdb' /* tempdb is auto-encrypted by SQL Server */\r\n\r\nNote, the only change is to reference `sys.certificates` in the *`master`* database.\r\n\r\nThe results of that query shows:\r\n\r\n```\r\n╔══════════════╦════════════════╦═════════════════════════╦════════════════════╦════════════════════╦══════════════════════════════╦═══════════════════════════╗\r\n║ DatabaseName ║ encryptor_type ║ opened_date             ║ CertName           ║ cert_serial_number ║ pvt_key_encryption_type_desc ║ Certsubject               ║\r\n╠══════════════╬════════════════╬═════════════════════════╬════════════════════╬════════════════════╬══════════════════════════════╬═══════════════════════════╣\r\n║ my_db        ║ CERTIFICATE    ║ 2017-09-20 11:24:13.590 ║ db_encryption_cert ║ <redacted>         ║ ENCRYPTED_BY_MASTER_KEY      ║ DB Encryption Certificate ║\r\n╚══════════════╩════════════════╩═════════════════════════╩════════════════════╩════════════════════╩══════════════════════════════╩═══════════════════════════╝\r\n```\r\n\r\nNote the query shows the certificate is `ENCRYPTED_BY_MASTER_KEY` - the master key referenced here is the master key for the *`master`* database.\r\n\r\nIn order to restore `my_db` onto another server, you'll need to backup the the certificate (with its private key) used to encrypt the database, then restore it onto the target SQL Server.  \r\n\r\nThis should happen on the source SQL Server:\r\n\r\n    BACKUP CERTIFICATE db_encryption_cert\r\n    TO FILE = 'C:\\db_encryption_cert'\r\n    WITH PRIVATE KEY (\r\n        FILE = 'C:\\db_encryption_cert_private_key'\r\n        , ENCRYPTION BY PASSWORD = 'new private key password'\r\n        );\r\n\r\nStore the resulting certificate file and private key and it's password on a secure file system offsite.\r\n\r\nThis should happen on the target SQL Server:\r\n\r\n    CREATE CERTIFICATE db_encryption_cert\r\n    FROM FILE = 'C:\\db_encryption_cert'\r\n    WITH PRIVATE KEY (\r\n        FILE = 'C:\\db_encryption_cert_private_key'\r\n        , DECRYPTION BY PASSWORD = 'new private key password'\r\n        );\r\n\r\nOnce you've created the certificate on the target server, you should be able to restore the database without issue.\r\n\r\nIf you are preparing for disaster recovery, and intend on being able to restore the source SQL Server's master database, etc, you should also backup the service master key and the master database master key:\r\n\r\n    USE master;\r\n    GO\r\n    BACKUP SERVICE MASTER KEY \r\n    TO FILE = 'C:\\service_master_key'\r\n    ENCRYPTION BY PASSWORD = 'new service master key password';\r\n    \r\n    BACKUP MASTER KEY\r\n    TO FILE = 'C:\\master_database_master_key'\r\n    ENCRYPTION BY PASSWORD = 'new master database master key password';\r\n    \r\nThese keys, and their associated encryption passwords, should be stored in a secure location off-site.	2020-01-10 22:01:40.003286+00	2	4	1	184806	0	0	0	2020-01-10 22:00:09.646137+00	\N	The certificate used by TDE to encrypt the database encryption key is actually stored in the `master` database, which is in turn encrypted by the `database master key` stored in the *`master`* database.	f	f
161	224	128	2018-08-04 13:48:48+00	It is true that one can read the same row more than once during a `SELECT` query running under the `READ COMMITTED` isolation level (without the `READ COMMITTED_SNAPSHOT` database option) but ***the subsequent row is not a duplicate***. The "duplicate" is actually a modified version of the row originally read. One or more of the row's column values will be different than that of the initial read, which is how non-repeatable read is defined under the ISO/ANSI SQL standard per Paul's quote.\r\n\r\nThis phenomenon can occur in SQL Server when an index key value of a row is updated during a `SELECT` query scanning that index using ordered scan. I'm not aware of a way for this to occur with an allocation ordered scan of a heap because the original row location (with forwarding pointer) remains unchanged when the physical row must be moved to a new page to accommodate a larger row length.\r\n	2019-12-01 15:19:01.289821+00	1	4	1	214066	0	0	0	2019-12-01 15:19:01.289821+00	\N	It is true that one can read the same row more than once during a `SELECT` query running under the `READ COMMITTED` isolation level (without the `READ COMMITTED_SNAPSHOT` database option) but ***the subsequent row is not a duplicate***. The "duplicate" is actually a modified version of the row originally read. One or more of the row's column values will be different than that of the initial read, which is how non-repeatable read is defined under the ISO/ANSI SQL standard per Paul's quote.	f	f
162	224	12	2018-08-04 11:52:47+00	The SQL Standard isolation level definitions are (probably deliberately) imprecise. They are given in terms of allowable concurrency phenomena:\r\n\r\n* P1 - dirty read\r\n* P2 - non-repeatable read\r\n* P3 - phantom\r\n\r\nAt `READ COMMITTED` isolation, only P1 is disallowed.\r\n\r\n[![extract from ISO/IEC 9075-2][1]][1]\r\n\r\n*extract from ISO/IEC 9075-2*\r\n\r\nReading the "same record" more than once due to movement within a physical structure is therefore not disallowed, so long as each read is not a dirty read (i.e. the read is performed on committed data).\r\n\r\nTherefore, SQL Server's locking implementation of `READ COMMITTED` isolation conforms with the standard.\r\n\r\nI understand the behaviour may be surprising, but the root of that surprise is in misunderstanding what this isolation level does, and does not, guarantee. For example, one might expect a concurrent `UPDATE` never to change the number of records returned by a `SELECT` query. This is a reasonable expectation, but not one that must be met to conform with the standard.\r\n\r\nIf you want a point-in-time (stable) view of the data, locking read committed is not the isolation level implementation you need. While not mandated by the standard, read committed using row versioning (RCSI) happens to provide a statement-level point-in-time consistent view of the committed state of the database. For a transaction-level point-in-time consistent view of the committed state of the database, you would need snapshot isolation (SI).\r\n\r\n> As far as I know only MS SQL Server behaves this way.\r\n\r\nIt would be possible in any database that implements read committed using locks (specifically short-term locks for reads) *and* where a concurrent modification might cause a record to move ahead of the current scan position. In other words, it is implementation-specific.\r\n\r\nRelated reading:\r\n\r\n* [A Critique of ANSI SQL Isolation Levels][2] by Hal Berenson, Phil Bernstein, Jim Gray, Jim Melton, Elizabeth O'Neil, and Patrick O'Neil.\r\n* [The Read Committed Isolation Level][3] by me\r\n\r\n  [1]: https://i.stack.imgur.com/gxAIH.png\r\n  [2]: https://www.microsoft.com/en-us/research/publication/a-critique-of-ansi-sql-isolation-levels/\r\n  [3]: https://sqlperformance.com/2014/04/t-sql-queries/the-read-committed-isolation-level	2019-12-03 08:29:21.360888+00	2	4	1	214060	0	0	0	2019-12-01 15:19:01.542411+00	\N	The SQL Standard isolation level definitions are (probably deliberately) imprecise. They are given in terms of allowable concurrency phenomena:	f	f
494	461	751	2016-01-06 20:20:04+00	It determines the logical tables involved in the join.\r\n\r\nWith a simple example\r\n\r\n    SELECT w1.WidgetID,\r\n           w2.SomeValue,\r\n           wp.PropertyName\r\n    FROM   #widgets1 w1\r\n           LEFT JOIN #widgets2 w2\r\n             ON w2.WidgetID = w1.WidgetID\r\n           JOIN #widgetProperties wp\r\n             ON w2.WidgetID = wp.WidgetID\r\n                AND wp.PropertyName = 'b'\r\n    ORDER  BY w1.WidgetID \r\n\r\n`#widgets1` is left outer joined to `#widgets2` - the result of that forms a virtual table that is inner joined to `#widgetProperties`. The predicate `w2.WidgetID = wp.WidgetID` will mean that any null extended rows from the initial outer join are filtered out, effectively making all the joins inner joins.\r\n\r\n\r\nThis differs from q2...\r\n\r\n    SELECT w1.WidgetID,\r\n           w2.SomeValue,\r\n           wp.PropertyName\r\n    FROM   #widgets1 w1\r\n           LEFT JOIN #widgets2 w2 --no ON clause here\r\n                     JOIN #widgetProperties wp\r\n                       ON w2.WidgetID = wp.WidgetID\r\n                          AND wp.PropertyName = 'b'\r\n             ON w2.WidgetID = w1.WidgetID\r\n    ORDER  BY w1.WidgetID\r\n\r\n`#widgets2` is inner joined onto `#widgetProperties`. The virtual table resulting from that join is then the right hand table in the Left Outer Join on `#widgets1`\r\n\r\nThe same result can be achieved by using a derived table or Common Table Expression...\r\n\r\n\r\n    WITH VT2\r\n         AS (SELECT w2.WidgetID,\r\n                    w2.SomeValue,\r\n                    wp.PropertyName\r\n             FROM   #widgets2 w2 \r\n                    JOIN #widgetProperties wp\r\n                      ON w2.WidgetID = wp.WidgetID\r\n                         AND wp.PropertyName = 'b')\r\n    SELECT w1.WidgetID,\r\n           VT2.SomeValue,\r\n           VT2.PropertyName\r\n    FROM   #widgets1 w1\r\n           LEFT JOIN VT2\r\n             ON VT2.WidgetID = w1.WidgetID\r\n    ORDER  BY w1.WidgetID \r\n\r\n... Or alternatively you could re-order the virtual tables and use a `RIGHT JOIN` instead.\r\n\r\n    SELECT w1.WidgetID,\r\n           w2.SomeValue,\r\n           wp.PropertyName\r\n    FROM   #widgets2 w2\r\n           INNER JOIN #widgetProperties wp\r\n                   ON w2.WidgetID = wp.WidgetID\r\n                      AND wp.PropertyName = 'b'\r\n           RIGHT JOIN #widgets1 w1\r\n                   ON w2.WidgetID = w1.WidgetID\r\n    ORDER  BY w1.WidgetID \r\n\r\n\r\nThis is [covered by Itzik Ben Gan here][1] \r\n\r\n> ... the JOIN conditions must follow a chiastic relationship to the table\r\n> order. That is, if you specify tables T1, T2, T3, and T4 in that order\r\n> and the JOIN conditions match T1 with T2, T2 with T3, and T3 with T4,\r\n> you must specify the JOIN conditions in the order opposite to the\r\n> table order, like this:\r\n\r\n    FROM   T1\r\n           <join_type> T2 T2\r\n                      <join_type> T3 T3\r\n                                 <join_type> T4\r\n                                   ON T4.key = T3.key\r\n                        ON T3.key = T2.key\r\n             ON T2.key = T1.key \r\n\r\n> To look at this join technique in a different way,\r\n> a given JOIN condition can refer only to the table names right above\r\n> it or table names that earlier JOIN conditions already referred to and\r\n> resolved.\r\n\r\nbut the article has a number of inaccuracies, see the [follow up letter by Lubor Kollar][2] as well.\r\n\r\n\r\n  [1]: https://www.itprotoday.com/software-development/take-control-joins\r\n  [2]: https://www.itprotoday.com/software-development/letters-february-2004	2019-12-18 07:33:53.553519+00	5	4	1	125423	0	0	0	2019-12-18 07:33:53.553519+00	\N	It determines the logical tables involved in the join.	f	f
493	461	659	2016-01-06 20:49:58+00	If you look at the [`FROM` clause syntax diagram][1] you will see that there is only one place for the `ON` clause:\r\n\r\n    <joined_table> ::= \r\n    {\r\n        <table_source> <join_type> <table_source> ON <search_condition> \r\n        ...\r\n    }\r\n\r\nWhat you find confusing is simple recursion, because `<table_source>` in `<joined_table`> above can be another `<joined_table`>:\r\n\r\n    [ FROM { <table_source> } [ ,...n ] ] \r\n    <table_source> ::= \r\n    {\r\n        table_or_view_name ... \r\n        ...\r\n        | <joined_table> \r\n        ...\r\n    }\r\n\r\nTo avoid confusion you should use parentheses in non-obvious cases (like your examples) to visually separate `<table_sources>`; they are not necessary for the query parser but useful for humans.\r\n\r\n  [1]: https://technet.microsoft.com/en-us/library/ms177634%28v=sql.105%29.aspx\r\n\r\n\r\n	2019-12-18 07:33:53.283795+00	5	4	1	125426	0	0	0	2019-12-18 07:33:53.283795+00	\N	If you look at the [`FROM` clause syntax diagram][1] you will see that there is only one place for the `ON` clause:	f	f
60	84	12	2019-11-23 08:16:28.593853+00	>*What is the logic behind requiring the same length when using UNPIVOT?*\r\n\r\nThis question may only be truly answerable by the people who worked on the implementation of `UNPIVOT`. You might be able to obtain this by [contacting them for support][1]. The following is my understanding of the reasoning, which may not be 100% accurate:\r\n\r\n---\r\n\r\nT-SQL contains any number of instances of weird semantics and other counter-intuitive behaviours. Some of these will eventually go away as part of deprecation cycles, but others may never be 'improved' or 'fixed'. Quite aside from anything else, applications exist that depend on these behaviours, so backward compatibility has to be preserved.\r\n\r\nThe rules for implicit conversions, and expression type derivation account for a significant proportion of the weirdness mentioned above. I do not envy the testers who have to ensure that the weird (and often undocumented) behaviours are preserved (under all combinations of `SET` session values and so on) for new versions.\r\n\r\nThat said, there is no good reason not to make improvements, and avoid past mistakes, when introducing new language features (with obviously no backward compatibility baggage). New features like recursive common table expressions (as mentioned by [Andriy M][2] in a comment) and `UNPIVOT` were free to have relatively sane semantics and clearly-defined rules.\r\n\r\nThere will be a range of views as to whether including the length in the type is taking explicit typing too far, but personally I welcome it. In my view, the types `varchar(25)` and `varchar(50)` are **not** the same, any more than `decimal(8)` and `decimal(10)` are. Special casing string type conversion complicates things unnecessarily and adds no real value, in my opinion.\r\n\r\nOne could argue that only implicit conversions that might lose data should be required to be explicitly stated, but there are edge-cases there too. Ultimately, a conversion is going to be needed, so we might as well make it explicit.\r\n\r\nIf the implicit conversion from `varchar(25)` to `varchar(50)` were allowed, it would just be another (most likely hidden) implicit conversion, with all the usual weird edge cases and `SET` setting sensitivities. Why not make the implementation the simplest and most explicit possible? (Nothing is perfect, however, and it is a shame that hiding `varchar(25)` and `varchar(50)` inside a `sql_variant` is allowed.)\r\n\r\nRewriting the `UNPIVOT` with `APPLY` and `UNION ALL` avoids the (better) type behaviour because the rules for `UNION` are subject to backward compatibility, and are documented in Books Online as allowing different types so long as they are comparable using implicit conversion (for which the arcane rules of data type precedence are used, and so on).\r\n\r\nThe workaround involves being explicit about the data types and adding explicit conversions where necessary. This looks like progress to me :)\r\n\r\nOne way to write the explicitly-typed workaround:\r\n\r\n```sql\r\nSELECT\r\n    U.PersonId,\r\n    U.ColumnName,\r\n    U.Value\r\nFROM dbo.People AS P\r\nCROSS APPLY\r\n(\r\n    VALUES (CONVERT(varchar(50), Lastname))\r\n) AS CA (Lastname)\r\nUNPIVOT\r\n(\r\n    Value FOR\r\n    ColumnName IN (P.Firstname, CA.Lastname)\r\n) AS U;\r\n```\r\n\r\nRecursive CTE example:\r\n\r\n```\r\n-- Fails\r\nWITH R AS\r\n(\r\n    SELECT Dummy = 'A row'\r\n    UNION ALL\r\n    SELECT 'Another row'\r\n    FROM R\r\n    WHERE Dummy = 'A row'\r\n)\r\nSELECT Dummy\r\nFROM R;\r\n    \r\n-- Succeeds\r\nWITH R AS\r\n(\r\n    SELECT Dummy = CONVERT(varchar(11), 'A row')\r\n    UNION ALL\r\n    SELECT CONVERT(varchar(11), 'Another row')\r\n    FROM R\r\n    WHERE Dummy = 'A row'\r\n)\r\nSELECT Dummy\r\nFROM R;\r\n```\r\n\r\nFinally, note that the rewrite using `CROSS APPLY` in the question is not quite the same as the `UNPIVOT`, because it does not reject `NULL` attributes.\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/sql-server/sql-server-get-help\r\n  [2]: https://dba.stackexchange.com/users/6965/andriy-m	2019-11-23 08:18:57.50317+00	2	4	2	\N	0	0	0	\N	\N	>*What is the logic behind requiring the same length when using UNPIVOT?*	f	f
165	226	12	2018-08-21 08:32:34+00	>Why would the scalar operator be fed an estimate of ~14000 rows, then estimate an output of 5? Is this a problem or a red herring?\r\n\r\nThis is counter-intuitive, but a natural consequence of the way the query optimizer explores the plan space. As it generates new, logically-equivalent, alternatives for a particular plan operator or subtree, it may need to derive a new cardinality estimate.\r\n\r\nSince estimation is a statistical process, there is no guarantee that estimates derived on logically-equivalent (but physically different) trees will produce the same number, in fact in the majority of cases, they won't. There is normally no obvious way to prefer one estimate over another.\r\n\r\nWhen optimization reaches its end point, the best physical alternatives found are 'stitched together' to form the final plan. This plan can have 'inconsistencies' as a result, simply because estimates were computed on different logic structures at different times. For example, a Compute Scalar might have started out as a logical aggregate, which was later simplified.\r\n\r\nI wrote more about this in my article [Indexed Views and Statistics][1].\r\n\r\nIf you suspect the cardinality mis-estimate is affecting plan choice (in an important way), you may choose to split the query up manually or use hints. Materializing the small intermediate set at or around node 27 into a temporary table may well improve plan quality, since the optimizer can see accurate cardinality at that point and create automatic statistics. The query writer can also choose to add indexing to the temporary table.\r\n\r\n>Is it anything to do with the conversions it is performing? I can understand that affecting a join, but why would it affect the output of the conversion?\r\n\r\nNot usually, no, though it is best to avoid conversions wherever possible. Certainly conversions *can* affect cardinality estimation, but there is little indication it is the cause here.\r\n\r\n\r\n  [1]: https://sqlperformance.com/2014/01/sql-plan/indexed-views-and-statistics	2019-12-01 15:27:17.906042+00	2	4	1	215454	0	0	0	2019-12-01 15:27:17.906042+00	\N	>Why would the scalar operator be fed an estimate of ~14000 rows, then estimate an output of 5? Is this a problem or a red herring?	f	f
231	275	16	2017-09-06 22:16:55+00	> SQL Server 2012 and 2016 Standard: If I put if-else logic in a stored procedure to execute one of two branches of code, depending on the value of a parameter, does the engine cache the latest version? \r\n\r\nNo, it caches _all_ versions. Or rather, it caches one version with _all paths_ explored, compiled with passed in variables. \r\n\r\nHere's a quick demo, using the Stack Overflow database.\r\n\r\nCreate an index:\r\n\r\n    CREATE INDEX ix_yourmom ON dbo.Users (Reputation) INCLUDE (Id, DisplayName);\r\n    GO \r\n\r\nCreate a stored procedure with an index hint that points to an index that doesn't exist, in branched code.\r\n\r\n\r\n    CREATE OR ALTER PROCEDURE dbo.YourMom (@Reputation INT)\r\n    AS \r\n    BEGIN\r\n    \r\n    \tIF @Reputation = 1\r\n    \tBEGIN\r\n    \t\tSELECT u.Id, u.DisplayName, u.Reputation\r\n    \t\tFROM dbo.Users AS u WITH (INDEX = PK_Users_Id)\r\n    \t\tWHERE u.Reputation = @Reputation;\r\n    \tEND;\r\n    \t\r\n    \tIF @Reputation > 1\r\n    \tBEGIN\r\n    \t\tSELECT u.Id, u.DisplayName, u.Reputation\r\n    \t\tFROM dbo.Users AS u WITH (INDEX = ix_yourdad)\r\n    \t\tWHERE u.Reputation = @Reputation;\r\n    \t\r\n    \tEND;\r\n    \r\n    END;\r\n\r\nIf I execute that stored proc looking for Reputation = 1, I get an error.\r\n\r\n    EXEC dbo.YourMom @Reputation = 1;\r\n\r\n> Msg 308, Level 16, State 1, Procedure YourMom, Line 14 [Batch Start\r\n> Line 32] Index 'ix_yourdad' on table 'dbo.Users' (specified in the\r\n> FROM clause) does not exist.\r\n\r\nIf we fix the index name and re-run the query, the [cached plan][1] looks like this:\r\n\r\n[![Nuts][2]][2]\r\n\r\nInside, the XML will have two references to the `@Reputation` variable.\r\n\r\n    <ColumnReference Column="@Reputation" ParameterDataType="int" ParameterCompiledValue="(1)" />\r\n\r\nA slightly simpler test would be to just get an estimated plan for the stored proc. You can see the optimizer exploring both paths:\r\n\r\n[![Nuts][3]][3]\r\n\r\n> And if on the following execution, the value of the parameter changes,\r\n> will it re-compile and re-cache the stored procedure, because a\r\n> different branch of the code must be executed? (This query is quite\r\n> expensive to compile.) Thank you.\r\n\r\nNo, it will retain the runtime value of the first compilation.\r\n\r\nIf we re-execute with a different `@Reputation`:\r\n\r\n    EXEC dbo.YourMom @Reputation = 2;\r\n\r\nFrom the [actual plan][4]:\r\n\r\n    <ColumnReference Column="@Reputation" ParameterDataType="int" ParameterCompiledValue="(1)" ParameterRuntimeValue="(2)" />\r\n\r\nWe still have a compiled value of 1, but now a runtime value of 2.\r\n\r\nIn the plan cache, which you can check out with a free tool like the one my company develops, [sp_BlitzCache][5]:\r\n\r\n[![Nuts][6]][6]\r\n\r\nThe stored procedure has been called twice, and each statement in it has been called once.\r\n\r\nSo what do we have? One cached plan for both queries in the stored procedure.\r\n\r\nIf you _want_ this sort of branched logic, you'd have to call sub-stored procedures:\r\n\r\n    CREATE OR ALTER PROCEDURE dbo.YourMom (@Reputation INT)\r\n    AS \r\n    BEGIN\r\n    \r\n    \tIF @Reputation = 1\r\n    \tBEGIN\r\n    \t\t\r\n    \t\tEXEC dbo.Reputation1Query;\r\n    \r\n    \tEND;\r\n    \t\r\n    \tIF @Reputation > 1\r\n    \tBEGIN\r\n    \t\t\r\n    \t\tEXEC dbo.ReputationGreaterThan1Query;\r\n    \t\r\n    \tEND;\r\n    \r\n    END;\r\n\r\n\r\nOr dynamic SQL:\r\n\r\n    DECLARE @sql NVARCHAR(MAX) = N''\r\n    \r\n    SET @sql +=\r\n    N'\r\n    SELECT u.Id, u.DisplayName, u.Reputation\r\n    \t\tFROM dbo.Users AS u '\r\n    IF @Reputation = 1\r\n    BEGIN\r\n        SET @sql += N' (INDEX = PK_Users_Id)\r\n    \t\tWHERE u.Reputation = @Reputation;'\r\n    END;\r\n    \r\n    \r\n    IF @Reputation > 1 \r\n    BEGIN\r\n    \r\n    SET @sql += ' WITH (INDEX = ix_yourmom)\r\n    \t\tWHERE u.Reputation = @Reputation;'\r\n    \r\n    END;\r\n    \r\n    \r\n    EXEC sys.sp_executesql @sql;\r\n\r\nHope this helps!\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=HJR9gxCFW\r\n  [2]: https://i.stack.imgur.com/jFSeB.jpg\r\n  [3]: https://i.stack.imgur.com/MPatW.jpg\r\n  [4]: https://www.brentozar.com/pastetheplan/?id=S1asZgCFZ\r\n  [5]: http://firstresponderkit.org\r\n  [6]: https://i.stack.imgur.com/rPyQs.jpg\r\n	2019-12-04 14:18:45.668444+00	2	4	1	185254	0	0	0	2019-12-04 14:18:45.668444+00	\N	> SQL Server 2012 and 2016 Standard: If I put if-else logic in a stored procedure to execute one of two branches of code, depending on the value of a parameter, does the engine cache the latest version?	f	f
276	303	12	2019-08-31 11:06:54+00	Like other execution plan warnings, this one is informational. If your query performed slowly, or you noticed that cardinality estimates were incorrect, the warning would give you information about where to look for a possible cause.\r\n\r\nAs a purely practical matter, that is pretty much the end of it. The precise conditions that trigger this optimizer warning are undocumented.\r\n\r\nThat said, I will go into a little detail to satisfy a little of your curiosity.\r\n\r\n>1\\. An explanation of why I get this warning despite nothing of interest going on - is it just that SQL Server will be conservative and report even when it won't affect plan choice?  \r\n\r\n There *are* things going on of interest to the cardinality estimation component of the query optimizer. This component keeps track of a wide range of properties and other information for each logical operator in the query tree. This includes histograms (from disk, or derived entirely in-memory), domain information, functional dependencies etc.\r\n\r\nIn your specific example, the warning is triggered when the input to the convert is known to be [monotone][1] but the output is not. Conversion from `integer` literal to `bigint` retains this property. Conversion from `bigint` to `nvarchar(50)` does not.\r\n\r\nWhen there is a single literal integer constant, parse-time constant-folding converts this to `nvarchar` before optimization starts. This avoids type conversion in the plan, and the associated warning.\r\n\r\nThere are detailed internal implementation differences between the original cardinality estimator and the new cardinality estimator that mean some statements will generate a warning under one CE but not the other.\r\n\r\n>2\\. What cardinality estimate is actually at risk here, and what operation would change based off of inaccuracies in that cardinality estimate?\r\n\r\nIn your example statements, none. As noted, the conversion can affect other internal data, which might affect e.g. other plan operators, or the code path taken through the optimizer. The broad point is that there is more than a raw cardinality estimate to consider. Internal differences can affect things like indexed view or computed column matching, among other possibilities.\r\n\r\n>3\\. Is there a scenario where this could affect plan choice? Obviously if I start joining or filtering on the converted column it could, but as-is?\r\n\r\nThis appears to be the same as the previous question.\r\n\r\n>4\\. Is there anything that can be done to prevent it from warning, besides changing data types (assume this is a requirement of how the data models interact)\r\n\r\nThere is no option to turn off these warnings.\r\n\r\n  [1]: https://en.wikipedia.org/wiki/Monotonic_function	2019-12-05 02:36:50.881731+00	3	4	1	246693	0	0	0	2019-12-05 02:36:50.881731+00	\N	Like other execution plan warnings, this one is informational. If your query performed slowly, or you noticed that cardinality estimates were incorrect, the warning would give you information about where to look for a possible cause.	f	f
288	315	16	2019-05-28 15:02:43+00	A common query rewrite that helps with `OR` predicates looks like this:\r\n\r\n```\r\nSELECT\r\n    tTitulo.CdContaCartao,\r\n    tTitulo.CdStatus,\r\n    MAX(DiariaMaxima)\r\nINTO #DiariaMaxima\r\nFROM Sistema.Titulo AS tTitulo\r\nCROSS APPLY\r\n(   \r\n    SELECT MAX(DiariaMaxima)\r\n    FROM \r\n    (\r\n    \tSELECT tTIPM.DtDiaria\r\n        FROM Sistema.TaxaIndice_PagamentoMensal_ContaCartao AS tTIPM_Cartao\r\n        JOIN Sistema.TaxaIndice_PagamentoMensal AS tTIPM\r\n            ON tTIPM.CdTaxaIndice_PagamentoMensal = tTIPM_Cartao.CdTaxaIndice_PagamentoMensal\r\n        WHERE tTIPM_Cartao.CdContaCartao = tTitulo.CdContaCartao\r\n    \t\t       \r\n        UNION ALL\r\n    \t\t       \r\n        SELECT tTIPM.DtDiaria\r\n        FROM Sistema.TaxaIndice_PagamentoMensal_ContaCartao AS tTIPM_Cartao\r\n        JOIN Sistema.TaxaIndice_PagamentoMensal AS tTIPM\r\n            ON tTIPM.CdTaxaIndice_PagamentoMensal = tTIPM_Cartao.CdTaxaIndice_PagamentoMensal\r\n        WHERE tTIPM_Cartao.CdContaCartao = tTitulo.CdContaCartao_Visa\r\n    ) AS x (DiariaMaxima)        \r\n) AS DiariaMaxima (DiariaMaxima);\r\n```\r\n\r\n`Apply` is not always the best method for this, though I've often had success with it over using a regular join.\r\n\r\nSome background on similar problems here:\r\n\r\n- [Analysing A Query Plan][1]\r\n- [How to Optimise Query][2]\r\n\r\n\r\n  [1]: https://dba.stackexchange.com/questions/23773/analysing-a-query-plan\r\n  [2]: https://dba.stackexchange.com/questions/23766/how-to-optimise-query	2019-12-05 21:18:21.585679+00	3	4	1	239228	0	0	0	2019-12-05 16:44:47.764242+00	\N	A common query rewrite that helps with `OR` predicates looks like this:	f	f
84	112	12	2019-11-26 10:47:12.081669+00	> *I would have guessed that when a query includes TOP n the database\r\n> engine would run the query ignoring the the TOP clause, and then at\r\n> the end just shrink that result set down to the n number of rows that\r\n> was requested. The graphical execution plan seems to indicate this is\r\n> the case -- TOP is the "last" step. But it appears there is more going\r\n> on.*\r\n\r\nThe way the above is phrased makes me think you may have an incorrect mental picture of how a query executes.  An operator in a query plan is not a *step* (where the full result set of a previous *step* is evaluated by the next one.\r\n\r\nSQL Server uses a *pipelined* execution model, where each operator exposes methods like *Init()*, *GetRow()*, and *Close()*.  As the *GetRow()* name suggests, an operator produces one row at a time on demand (as required by its parent operator).  This is documented in the Books Online [Logical and Physical Operators reference][1], with more detail in my blog post [Why Query Plans Run Backwards][2].  This row-at-a-time model is essential in forming a sound intuition for query execution.\r\n\r\n> My question is, how (and why) does a `TOP` n clause impact the execution\r\n> plan of a query?\r\n\r\nSome logical operations like `TOP`, semi joins and the `FAST n` [query hint][3] affect the way the query optimizer costs execution plan alternatives.  The basic idea is that one possible plan shape might return the first *n* rows more quickly than a different plan that was optimized to return all rows.\r\n\r\nFor example, indexed nested loops join is often the fastest way to return a small number of rows, though hash or merge join with scans might be more efficient on larger sets.  The way the query optimizer reasons about these choices is by setting a [Row Goal][4] at a particular point in the logical tree of operations.\r\n\r\nA row goal modifies the way query plan alternatives are costed.  The essence of it is that the optimizer starts by costing each operator as if the full result set were required, sets a row goal at the appropriate point, and then works back down the plan tree estimating the number of rows it expects to need to examine to meet the row goal.\r\n\r\nFor example, a logical `TOP(10)` sets a row goal of 10 at a particular point in the logical query tree.  The costs of operators leading up to the row goal are modified to estimate how many rows they need to produce to meet the row goal.  This calculation can become complex, so it is easier to understand all this with a [fully worked example][5] and annotated execution plans.  Row goals can affect more than the choice of join type or whether seeks and lookups are preferred to scans.  More details on that [here][6].\r\n\r\nAs always, an execution plan selected on the basis of a row goal is subject to the optimizer's reasoning abilities and the quality of information provided to it.  Not every plan with a row goal will produce the required number of rows faster in practice, but according to the costing model it will.\r\n\r\nWhere a row goal plan proves not to be faster, there are usually ways to modify the query or provide better information to the optimizer such that the naturally selected plan is best.  Which option is appropriate in your case depends on the details of course.  The row goal feature is generally very effective (though there is a [bug][7] to watch out for when used in parallel execution plans).\r\n\r\nYour particular query and plan may not be suitable for detailed analysis here (by all means provide an actual execution plan if you wish) but hopefully the ideas outlined here will allow you to make forward progress.\r\n\r\n  [1]: https://msdn.microsoft.com/en-us/library/ms191158.aspx\r\n  [2]: https://www.sql.kiwi/2010/08/iterators-query-plans-and-why-they-run-backwards.html\r\n  [3]: http://msdn.microsoft.com/en-us/library/ms181714.aspx\r\n  [4]: https://blogs.msdn.com/b/queryoptteam/archive/2006/03/30/564912.aspx\r\n  [5]: https://www.sql.kiwi/2010/08/inside-the-optimiser-row-goals-in-depth.html\r\n  [6]: https://www.sql.kiwi/2010/08/row-goals-and-grouping.html\r\n  [7]: https://www.sql.kiwi/2012/05/parallel-row-goals-gone-rogue.html	2019-11-26 10:48:43.139862+00	2	4	2	\N	0	0	0	\N	\N	> *I would have guessed that when a query includes TOP n the database	f	f
78	101	12	2019-11-25 12:08:52.637551+00	> *I am unsure if its necessary to add the `TABLOCK` table hint to an empty temporary table, defined with a clustered index in order to get minimal logging.*\r\n\r\nNo. Local temporary tables (`#temp`) are *private* to the creating session, so a table lock hint is not required. A table lock hint would be required for a global temporary table (`##temp`) or a regular table (`dbo.temp`) created in `tempdb`, because these can be accessed from multiple sessions.\r\n\r\nNote that even where the full minimal logging optimizations are not applied, tables created in `tempdb` benefit from other optimizations like not needing to log `REDO` information. You can test to see whether rows or pages are being logged using the undocumented `sys.fn_dblog`. Regular logging will feature row-logged records like `LOP_INSERT_ROWS`.\r\n\r\n---\r\n\r\nNote that adding `TABLOCK` to a local temporary table is required to get parallel execution with `INSERT...SELECT` queries in SQL Server 2016, see the Microsoft Knowledge Base article:\r\n\r\n[Poor performance when you run INSERT.. SELECT operations in SQL Server 2016][1]\r\n\r\n\r\n  [1]: https://support.microsoft.com/en-us/kb/3180087	2019-11-25 12:08:52.637551+00	1	4	2	\N	0	0	0	\N	\N	> *I am unsure if its necessary to add the `TABLOCK` table hint to an empty temporary table, defined with a clustered index in order to get minimal logging.*	f	f
136	195	12	2017-08-04 15:17:49+00	Reusing the example table and indexed view from my article [Another Reason to Use `NOEXPAND` hints in Enterprise Edition][1]:\r\n\r\n    CREATE TABLE dbo.T\r\n    (\r\n        col1 integer NOT NULL\r\n    );\r\n    GO\r\n    INSERT dbo.T WITH (TABLOCKX)\r\n        (col1)\r\n    SELECT \r\n        SV.number\r\n    FROM master.dbo.spt_values AS SV\r\n    WHERE \r\n        SV.type = N'P';\r\n    GO\r\n    CREATE VIEW dbo.VT\r\n    WITH SCHEMABINDING\r\n    AS\r\n    SELECT T.col1 \r\n    FROM dbo.T AS T;\r\n\r\n## Repro\r\n\r\nThis query matches the indexed view (albeit with a redundant aggregate):\r\n\r\n    SELECT DISTINCT\r\n        VT.col1 \r\n    FROM dbo.VT AS VT;\r\n\r\n[![Indexed view matched][2]][2]\r\n\r\nAdding a `READPAST` hint results in accessing the base table:\r\n\r\n    SELECT DISTINCT\r\n        VT.col1 \r\n    FROM dbo.VT AS VT \r\n        WITH (READPAST);\r\n\r\n[![Indexed view not matched][3]][3]\r\n\r\n## Explanation\r\n\r\nThe `READPAST` hint is semantic-affecting. The optimizer resists rewriting queries such that the results change. To illustrate:\r\n\r\nThe following query executes without problems:\r\n\r\n    SELECT DISTINCT\r\n        VT.col1 \r\n    FROM dbo.VT AS VT \r\n        WITH (READPAST);\r\n\r\nHowever:\r\n\r\n    SELECT DISTINCT\r\n        VT.col1 \r\n    FROM dbo.VT AS VT \r\n        WITH (READPAST)\r\n    OPTION \r\n        (TABLE HINT (VT, FORCESCAN));\r\n\r\nProduces the error:\r\n\r\n``` none\r\nMsg 8722, Level 16, State 1, Line 42\r\nCannot execute query.\r\nSemantic affecting hint 'readpast' appears in the 'WITH' clause of object 'VT'\r\nbut not in the corresponding 'TABLE HINT' clause.\r\nChange the OPTION (TABLE HINTS...) clause so the semantic affecting hints\r\nmatch the WITH clause.\r\n```\r\n\r\nWhen you reference the indexed view without the `NOEXPAND` hint, the view is expanded (before compilation and optimization begins) to reference the underlying objects instead. Later in the process, the optimizer may consider matching the query tree back to an indexed view, in whole or in part.\r\n\r\nWhen `READPAST` is used without `NOEXPAND`, the hint [propagates][4] to the base table, preventing view matching (different semantics).\r\n\r\nWith `NOEXPAND`, the hint applies to the view directly, so there is no problem.\r\n\r\n\r\n  [1]: https://sqlperformance.com/2015/12/sql-performance/noexpand-hints\r\n  [2]: https://i.stack.imgur.com/N3XZM.png\r\n  [3]: https://i.stack.imgur.com/z26Zv.png\r\n  [4]: https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-table#remarks	2019-11-29 12:36:45.707684+00	1	4	1	182706	0	0	0	2019-11-29 12:35:49.826691+00	\N	Reusing the example table and indexed view from my article [Another Reason to Use `NOEXPAND` hints in Enterprise Edition][1]:	f	f
281	306	16	2017-10-05 22:14:57+00	This really depends on indexes and data types. \r\n\r\nUsing the Stack Overflow database as an example, this is what the Users table looks like:\r\n\r\n[![NUTS][1]][1]\r\n\r\nIt has a PK/CX on the Id column. So it's the entirety of the table data sorted by Id.\r\n\r\nWith that as the only index, SQL has to read that whole thing (sans the LOB columns) into memory if it's not already there.\r\n\r\n    DBCC DROPCLEANBUFFERS-- Don't run this anywhere near prod.\r\n    \r\n    SET STATISTICS TIME, IO ON \r\n    \r\n    SELECT u.Id\r\n    INTO  #crap1\r\n    FROM dbo.Users AS u\r\n\r\nThe stats time and io profile looks like this:\r\n\r\n    Table 'Users'. Scan count 7, logical reads 80846, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 2406 ms,  elapsed time = 446 ms.\r\n\r\n\r\nIf I add an additional nonclustered index on just Id\r\n\r\n    CREATE INDEX ix_whatever ON dbo.Users (Id)\r\n\r\nI now have a much smaller index that satisfies my query.\r\n\r\n    DBCC DROPCLEANBUFFERS-- Don't run this anywhere near prod.\r\n    \r\n    SELECT u.Id\r\n    INTO  #crap2\r\n    FROM dbo.Users AS u\r\n\r\nThe profile here:\r\n\r\n    Table 'Users'. Scan count 7, logical reads 6587, physical reads 0, read-ahead reads 6549, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 2344 ms,  elapsed time = 384 ms.\r\n\r\nWe're able to do far fewer reads and save a little CPU time. \r\n\r\nWithout more information about your table definition, I can't really try to reproduce what you're trying to measure any better.\r\n\r\n> But you're saying that unless there is a specific index on that lone column, the other columns/ fields will also be scanned? Is this just a drawback inherent to the design of rowstore tables? Why would irrelevant fields be scanned?\r\n\r\nYes, this is specific to rowstore tables. Data is stored by the row on data pages. Even if other data on the page is irrelevant to your query, that whole row > page > index needs to be read into memory. I wouldn't say that the other columns are "scanned" so much as the pages they exist on are scanned to retrieve the single value on them relevant to the query.\r\n\r\nUsing the ol' phonebook example: even if you're just reading phone numbers, when you turn the page, you're turning last name, first name, address, etc along with the phone number.\r\n\r\n  [1]: https://i.stack.imgur.com/n9Zbd.jpg\r\n	2019-12-05 13:48:13.586538+00	2	4	1	187810	0	0	0	2019-12-05 13:48:13.586538+00	\N	This really depends on indexes and data types.	f	f
68	91	667	2014-12-17 15:47:27+00	In DBMS, like MySQL, that do not have window functions or `CROSS APPLY`, the way to do this would be to use standard SQL (89). The slow way would be a triangular cross join with aggregate. The faster way (but still and probably not as efficient as using cross apply or the row_number function) would be what I call the ***"poor man's `CROSS APPLY`"***. It would be interesting to compare this query with the others:\r\n\r\n*Assumption: `Orders (CustomerID, OrderDate)` has a `UNIQUE` constraint:*\r\n\r\n    DECLARE @top INT;\r\n    SET @top = 5;\r\n \r\n    SELECT o.CustomerID, o.OrderID, o.OrderDate\r\n      FROM dbo.Customers AS c\r\n        JOIN dbo.Orders AS o\r\n          ON  o.CustomerID = c.CustomerID\r\n          AND o.OrderID IN\r\n              ( SELECT TOP (@top) oi.OrderID\r\n                FROM dbo.Orders AS oi\r\n                WHERE oi.CustomerID = c.CustomerID\r\n                ORDER BY oi.OrderDate DESC\r\n              )\r\n      ORDER BY CustomerID, OrderDate DESC ;\r\n\r\nFor the extra problem of customized top rows per group:\r\n\r\n    SELECT o.CustomerID, o.OrderID, o.OrderDate\r\n      FROM dbo.Customers AS c\r\n        JOIN dbo.Orders AS o\r\n          ON  o.CustomerID = c.CustomerID\r\n          AND o.OrderID IN\r\n              ( SELECT TOP (c.Number_of_Recent_Orders_to_Show) oi.OrderID\r\n                FROM dbo.Orders AS oi\r\n                WHERE oi.CustomerID = c.CustomerID\r\n                ORDER BY oi.OrderDate DESC\r\n              )\r\n      ORDER BY CustomerID, OrderDate DESC ;\r\n\r\n*Note: In MySQL, instead of `AND o.OrderID IN (SELECT TOP(@top) oi.OrderID ...)` one would use `AND o.OrderDate >= (SELECT oi.OrderDate ... LIMIT 1 OFFSET (@top - 1))`. SQL-Server added `FETCH / OFFSET` syntax in 2012 version. The queries here were adjusted with `IN (TOP...)` to work with earlier versions.*	2019-11-27 15:18:27.204528+00	1	4	1	86421	0	0	0	2019-11-24 09:59:58.697831+00	\N	In DBMS, like MySQL, that do not have window functions or `CROSS APPLY`, the way to do this would be to use standard SQL (89). The slow way would be a triangular cross join with aggregate. The faster way (but still and probably not as efficient as using cross apply or the row_number function) would be what I call the ***"poor man's `CROSS APPLY`"***. It would be interesting to compare this query with the others:	f	f
66	91	89	2014-12-22 11:28:40+00	Let's start with the basic scenario.\r\n\r\nIf I want to get some number of rows out of a table, I have two main options: ranking functions; or `TOP`.\r\n\r\nFirst, let's consider the whole set from `Production.TransactionHistory` for a particular `ProductID`:\r\n\r\n    SELECT h.TransactionID, h.ProductID, h.TransactionDate\r\n    FROM Production.TransactionHistory h\r\n    WHERE h.ProductID = 800;\r\n\r\nThis returns 418 rows, and the plan shows that it checks every row in the table looking for this - an unrestricted Clustered Index Scan, with a Predicate to provide the filter. 797 reads here, which is ugly.\r\n\r\n![Expensive Scan with 'Residual' Predicate][1]\r\n\r\nSo let's be fair to it, and create an index that would be more useful. Our conditions call for an equality match on `ProductID`, followed by a search for the most recent by `TransactionDate`. We need the `TransactionID` returned too, so let's go with: `CREATE INDEX ix_FindingMostRecent ON Production.TransactionHistory (ProductID, TransactionDate) INCLUDE (TransactionID);`.\r\n\r\nHaving done this, our plan changes significantly, and drops the reads down to just 3. So we're already improving things by over 250x or so...\r\n\r\n![Improved plan][2]\r\n\r\nNow that we've levelled the playing field, let's look at the top options - ranking functions and `TOP`.\r\n\r\n    WITH Numbered AS\r\n    (\r\n    SELECT h.TransactionID, h.ProductID, h.TransactionDate, ROW_NUMBER() OVER (ORDER BY TransactionDate DESC) AS RowNum\r\n    FROM Production.TransactionHistory h\r\n    WHERE h.ProductID = 800\r\n    )\r\n    SELECT TransactionID, ProductID, TransactionDate\r\n    FROM Numbered\r\n    WHERE RowNum <= 5;\r\n    \r\n    SELECT TOP (5) h.TransactionID, h.ProductID, h.TransactionDate\r\n    FROM Production.TransactionHistory h\r\n    WHERE h.ProductID = 800\r\n    ORDER BY TransactionDate DESC;\r\n\r\n![Two plans - basic TOP\\RowNum][3]\r\n\r\nYou will notice that the second (`TOP`) query is much simpler than the first, both in query and in plan. But very significantly, they both use `TOP` to limit the number of rows actually being pulled out of the index. The costs are only estimates and worth ignoring, but you can see a lot of similarity in the two plans, with the `ROW_NUMBER()` version doing a tiny amount of extra work to assign numbers and filter accordingly, and both queries end up doing just 2 reads to do their work. The Query Optimizer certainly recognises the idea of filtering on a `ROW_NUMBER()` field, realising that it can use a Top operator to ignore rows that aren't going to be needed. Both these queries are good enough - `TOP` isn't so much better that it's worth changing code, but it is simpler and probably clearer for beginners.\r\n\r\nSo this work across a single product. But we need to consider what happens if we need to do this across multiple products.\r\n\r\nThe iterative programmer is going to consider the idea of looping through the products of interest, and calling this query multiple times, and we can actually get away with writing a query in this form - not using cursors, but using `APPLY`. I'm using `OUTER APPLY`, figuring that we might want to return the Product with NULL, if there are no Transactions for it.\r\n\r\n    SELECT p.Name, p.ProductID, t.TransactionID, t.TransactionDate\r\n    FROM \r\n    Production.Product p\r\n    OUTER APPLY (\r\n        SELECT TOP (5) h.TransactionID, h.ProductID, h.TransactionDate\r\n        FROM Production.TransactionHistory h\r\n        WHERE h.ProductID = p.ProductID\r\n        ORDER BY TransactionDate DESC\r\n    ) t\r\n    WHERE p.Name >= 'M' AND p.Name < 'S';\r\n\r\nThe plan for this is the iterative programmers' method - Nested Loop, doing a Top operation and Seek (those 2 reads we had before) for each Product. This gives 4 reads against Product, and 360 against TransactionHistory.\r\n\r\n![APPLY plan][4]\r\n\r\nUsing `ROW_NUMBER()`, the method is to use `PARTITION BY` in the `OVER` clause, so that we restart the numbering for each Product. This can then be filtered like before. The plan ends up being quite different. The logical reads are about 15% lower on TransactionHistory, with a full Index Scan going on to get the rows out.\r\n\r\n    WITH Numbered AS\r\n    (\r\n    SELECT p.Name, p.ProductID, h.TransactionID, h.TransactionDate, ROW_NUMBER() OVER (PARTITION BY h.ProductID ORDER BY h.TransactionDate DESC) AS RowNum\r\n    FROM Production.Product p\r\n    LEFT JOIN Production.TransactionHistory h ON h.ProductID = p.ProductID\r\n    WHERE p.Name >= 'M' AND p.Name < 'S'\r\n    )\r\n    SELECT Name, ProductID, TransactionID, TransactionDate\r\n    FROM Numbered n\r\n    WHERE RowNum <= 5;\r\n\r\n![ROW_NUMBER plan][5]\r\n\r\nSignificantly, though, this plan has an expensive Sort operator. The Merge Join doesn't seem to maintain the order of rows in TransactionHistory, the data must be resorted to be able to find the rownumbers. It's fewer reads, but this blocking Sort could feel painful. Using `APPLY`, the Nested Loop will return the first rows very quickly, after just a few reads, but with a Sort, `ROW_NUMBER()` will only return rows after a most of the work has been finished.\r\n\r\nInterestingly, if the `ROW_NUMBER()` query uses `INNER JOIN` instead of `LEFT JOIN`, then a different plan comes up.\r\n\r\n![ROW_NUMBER() with INNER JOIN][6]\r\n\r\nThis plan uses a Nested Loop, just like with `APPLY`. But there's no Top operator, so it pulls all the transactions for each product, and uses a lot more reads than before - 492 reads against TransactionHistory. There isn't a good reason for it not to choose the Merge Join option here, so I guess the plan was considered 'Good Enough'. Still - it doesn't block, which is nice - just not as nice as `APPLY`.\r\n\r\nThe `PARTITION BY` column that I used for `ROW_NUMBER()` was `h.ProductID` in both cases, because I had wanted to give the QO the option of producing the RowNum value before joining to the Product table. If I use `p.ProductID`, we see the same shape plan as with the `INNER JOIN` variation.\r\n\r\n    WITH Numbered AS\r\n    (\r\n    SELECT p.Name, p.ProductID, h.TransactionID, h.TransactionDate, ROW_NUMBER() OVER (PARTITION BY p.ProductID ORDER BY h.TransactionDate DESC) AS RowNum\r\n    FROM Production.Product p\r\n    LEFT JOIN Production.TransactionHistory h ON h.ProductID = p.ProductID\r\n    WHERE p.Name >= 'M' AND p.Name < 'S'\r\n    )\r\n    SELECT Name, ProductID, TransactionID, TransactionDate\r\n    FROM Numbered n\r\n    WHERE RowNum <= 5;\r\n\r\nBut the Join operator says 'Left Outer Join' instead of 'Inner Join'. The number of reads is still just under 500 reads against the TransactionHistory table.\r\n\r\n![PARTITION BY on p.ProductID instead of h.ProductID][7]\r\n\r\nAnyway - back to the question at hand...\r\n\r\nWe've answered **question 1**, with two options that you could pick and choose from. Personally, I like the `APPLY` option.\r\n\r\nTo extend this to use a variable number (**question 2**), the `5` just needs to be changed accordingly. Oh, and I added another index, so that there was an index on `Production.Product.Name` that included the `DaysToManufacture` column.\r\n\r\n    WITH Numbered AS\r\n    (\r\n    SELECT p.Name, p.ProductID, p.DaysToManufacture, h.TransactionID, h.TransactionDate, ROW_NUMBER() OVER (PARTITION BY h.ProductID ORDER BY h.TransactionDate DESC) AS RowNum\r\n    FROM Production.Product p\r\n    LEFT JOIN Production.TransactionHistory h ON h.ProductID = p.ProductID\r\n    WHERE p.Name >= 'M' AND p.Name < 'S'\r\n    )\r\n    SELECT Name, ProductID, TransactionID, TransactionDate\r\n    FROM Numbered n\r\n    WHERE RowNum <= 5 * DaysToManufacture;\r\n    \r\n    SELECT p.Name, p.ProductID, t.TransactionID, t.TransactionDate\r\n    FROM \r\n    Production.Product p\r\n    OUTER APPLY (\r\n        SELECT TOP (5 * p.DaysToManufacture) h.TransactionID, h.ProductID, h.TransactionDate\r\n        FROM Production.TransactionHistory h\r\n        WHERE h.ProductID = p.ProductID\r\n        ORDER BY TransactionDate DESC\r\n    ) t\r\n    WHERE p.Name >= 'M' AND p.Name < 'S';\r\n\r\nAnd both plans are almost identical to what they were before!\r\n\r\n![Variable rows][8]\r\n\r\nAgain, ignore the estimated costs - but I still like the TOP scenario, as it is so much more simple, and the plan has no blocking operator. The reads are less on TransactionHistory because of the high number of zeroes in `DaysToManufacture`, but in real life, I doubt we'd be picking that column. ;)\r\n\r\nOne way to avoid the block is to come up with a plan that handles the `ROW_NUMBER()` bit to the right (in the plan) of the join. We can persuade this to happen by doing the join outside the CTE.\r\n\r\n    WITH Numbered AS\r\n    (\r\n    SELECT h.TransactionID, h.ProductID, h.TransactionDate, ROW_NUMBER() OVER (PARTITION BY ProductID ORDER BY TransactionDate DESC) AS RowNum\r\n    FROM Production.TransactionHistory h\r\n    )\r\n    SELECT p.Name, p.ProductID, t.TransactionID, t.TransactionDate\r\n    FROM Production.Product p\r\n    LEFT JOIN Numbered t ON t.ProductID = p.ProductID\r\n        AND t.RowNum <= 5 * p.DaysToManufacture\r\n    WHERE p.Name >= 'M' AND p.Name < 'S';\r\n\r\nThe plan here looks simpler - it's not blocking, but there's a hidden danger.\r\n\r\n![Joining outside CTE][9]\r\n\r\nNotice the Compute Scalar that's pulling data from the Product table. This is working out the `5 * p.DaysToManufacture` value. This value isn't being passed into the branch that's pulling data from the TransactionHistory table, it's being used in the Merge Join. As a Residual.\r\n\r\n![Sneaky Residual!][11]\r\n\r\nSo the Merge Join is consuming ALL the rows, not just the first however-many-are-needed, but all of them and then doing a residual check. This is dangerous as the number of transactions increases. I'm not a fan of this scenario - residual predicates in Merge Joins can quickly escalate. Another reason why I prefer the `APPLY/TOP` scenario.\r\n\r\nIn the special case where it's exactly one row, for **question 3**, we can obviously use the same queries, but with `1` instead of `5`. But then we have an extra option, which is to use regular aggregates.\r\n\r\n    SELECT ProductID, MAX(TransactionDate)\r\n    FROM Production.TransactionHistory\r\n    GROUP BY ProductID;\r\n\r\nA query like this would be a useful start, and we could easily modify it to pull out the TransactionID as well for tie-break purposes (using a concatenation which would then be broken down), but we either look at the whole index, or we dive in product by product, and we don't really get a big improvement on what we had before in this scenario.\r\n\r\nBut I should point out that we're looking at a particular scenario here. With real data, and with an indexing strategy that may not be ideal, mileage may vary considerably. Despite the fact that we've seen that `APPLY` is strong here, it can be slower in some situations. It rarely blocks though, as it has a tendency to use Nested Loops, which many people (myself included) find very appealing.\r\n\r\nI haven't tried to explore parallelism here, or dived very hard into question 3, which I see as a special case that people rarely want based on the complication of concatenating and splitting. The main thing to consider here is that these two options are both very strong.\r\n\r\nI prefer `APPLY`. It's clear, it uses the Top operator well, and it rarely causes blocking.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/NjxAr.png\r\n  [2]: https://i.stack.imgur.com/SuSo4.png\r\n  [3]: https://i.stack.imgur.com/9O7zx.png\r\n  [4]: https://i.stack.imgur.com/jXxSl.png\r\n  [5]: https://i.stack.imgur.com/ipmb0.png\r\n  [6]: https://i.stack.imgur.com/rBft7.png\r\n  [7]: https://i.stack.imgur.com/bLMqE.png\r\n  [8]: https://i.stack.imgur.com/NIgfd.png\r\n  [9]: https://i.stack.imgur.com/KeQyu.png\r\n  [10]: https://i.stack.imgur.com/u830P.png\r\n  [11]: https://i.stack.imgur.com/rSBwi.png	2019-11-27 15:14:06.417934+00	3	4	1	86765	0	0	0	2019-11-24 09:54:40.965511+00	\N	Let's start with the basic scenario.	f	f
67	91	88	2014-12-17 14:55:41+00	The typical way to do this in SQL Server 2005 and up is to use a CTE and windowing functions. For top n per group you can simply use `ROW_NUMBER()` with a `PARTITION` clause, and filter against that in the outer query. So, for example, the top 5 most recent orders per customer could be displayed this way:\r\n\r\n    DECLARE @top INT;\r\n    SET @top = 5;\r\n \r\n    ;WITH grp AS \r\n    (\r\n       SELECT CustomerID, OrderID, OrderDate,\r\n         rn = ROW_NUMBER() OVER\r\n         (PARTITION BY CustomerID ORDER BY OrderDate DESC)\r\n       FROM dbo.Orders\r\n    )\r\n    SELECT CustomerID, OrderID, OrderDate\r\n      FROM grp\r\n      WHERE rn <= @top\r\n      ORDER BY CustomerID, OrderDate DESC;\r\n\r\nYou can also do this with `CROSS APPLY`:\r\n\r\n    DECLARE @top INT;\r\n    SET @top = 5;\r\n\r\n    SELECT c.CustomerID, o.OrderID, o.OrderDate\r\n    FROM dbo.Customers AS c\r\n    CROSS APPLY \r\n    (\r\n        SELECT TOP (@top) OrderID, OrderDate \r\n    \tFROM dbo.Orders AS o\r\n        WHERE CustomerID = c.CustomerID\r\n    \tORDER BY OrderDate DESC\r\n    ) AS o\r\n    ORDER BY c.CustomerID, o.OrderDate DESC;\r\n\r\nWith the additional option Paul specified, say the Customers table has a column indicating how many rows to include per customer:\r\n\r\n    ;WITH grp AS \r\n    (\r\n       SELECT CustomerID, OrderID, OrderDate,\r\n         rn = ROW_NUMBER() OVER\r\n         (PARTITION BY CustomerID ORDER BY OrderDate DESC)\r\n       FROM dbo.Orders\r\n    )\r\n    SELECT c.CustomerID, grp.OrderID, grp.OrderDate\r\n      FROM grp \r\n      INNER JOIN dbo.Customers AS c\r\n      ON grp.CustomerID = c.CustomerID\r\n      AND grp.rn <= c.Number_of_Recent_Orders_to_Show\r\n      ORDER BY c.CustomerID, grp.OrderDate DESC;\r\n\r\nAnd again, using `CROSS APPLY` and incorporating the added option that the number of rows for a customer be dictated by some column in the customers table:\r\n\r\n    SELECT c.CustomerID, o.OrderID, o.OrderDate\r\n    FROM dbo.Customers AS c\r\n    CROSS APPLY \r\n    (\r\n        SELECT TOP (c.Number_of_Recent_Orders_to_Show) OrderID, OrderDate \r\n    \tFROM dbo.Orders AS o\r\n        WHERE CustomerID = c.CustomerID\r\n    \tORDER BY OrderDate DESC\r\n    ) AS o\r\n    ORDER BY c.CustomerID, o.OrderDate DESC;\r\n\r\nNote that these will perform differently depending on data distribution and the availability of supporting indexes, so optimizing the performance and getting the best plan will really depend on local factors.\r\n\r\nPersonally, I prefer the CTE and windowing solutions over the `CROSS APPLY` / `TOP` because they separate the logic better and are more intuitive (to me). In general (both in this case and in my general experience), the CTE approach produces more efficient plans (examples below), but this should not be taken as a universal truth - you should always test your scenarios, especially if indexes have changed or data has skewed significantly.\r\n\r\n----\r\n\r\n## AdventureWorks examples - without any changes\r\n\r\n> 1. List the five most recent recent transaction dates and IDs from the `TransactionHistory` table, for each product that starts with a letter from M to R inclusive.\r\n\r\n    -- CTE / OVER()\r\n\r\n    ;WITH History AS\r\n    (\r\n      SELECT p.ProductID, p.Name, t.TransactionID, t.TransactionDate,\r\n        rn = ROW_NUMBER() OVER \r\n        (PARTITION BY t.ProductID ORDER BY t.TransactionDate DESC)\r\n      FROM Production.Product AS p\r\n      INNER JOIN Production.TransactionHistory AS t\r\n      ON p.ProductID = t.ProductID\r\n      WHERE p.Name >= N'M' AND p.Name < N'S'\r\n    )\r\n    SELECT ProductID, Name, TransactionID, TransactionDate\r\n    FROM History \r\n    WHERE rn <= 5;\r\n\r\n    -- CROSS APPLY\r\n    \r\n    SELECT p.ProductID, p.Name, t.TransactionID, t.TransactionDate\r\n    FROM Production.Product AS p\r\n    CROSS APPLY\r\n    (\r\n      SELECT TOP (5) TransactionID, TransactionDate\r\n      FROM Production.TransactionHistory\r\n      WHERE ProductID = p.ProductID\r\n      ORDER BY TransactionDate DESC\r\n    ) AS t\r\n    WHERE p.Name >= N'M' AND p.Name < N'S';\r\n\r\nComparison of these two in runtime metrics:\r\n\r\n![enter image description here][1]\r\n\r\nCTE / `OVER()` plan:\r\n\r\n![enter image description here][2]\r\n\r\n`CROSS APPLY` plan:\r\n\r\n![enter image description here][3]\r\n\r\nThe CTE plan looks more complicated, but it's actually much more efficient. Pay little attention to the estimated cost % numbers, but focus on more important *actual* observations, such as far fewer reads and a much lower duration. I also ran these without parallelism, and this wasn't the difference. Runtime metrics and the CTE plan (the `CROSS APPLY` plan remained the same):\r\n\r\n![enter image description here][4]\r\n\r\n![enter image description here][5]\r\n\r\n> 2. Same again, but with `n` history lines per product, where `n` is five times the `DaysToManufacture` Product attribute.\r\n\r\nVery minor changes required here. For the CTE, we can add a column to the inner query, and filter on the outer query; for the `CROSS APPLY`, we can perform the calculation inside the correlated `TOP`. You'd think this would lend some efficiency to the `CROSS APPLY` solution, but that doesn't happen in this case. Queries:\r\n\r\n    -- CTE / OVER()\r\n\r\n    ;WITH History AS\r\n    (\r\n      SELECT p.ProductID, p.Name, p.DaysToManufacture, t.TransactionID, t.TransactionDate,\r\n        rn = ROW_NUMBER() OVER \r\n        (PARTITION BY t.ProductID ORDER BY t.TransactionDate DESC)\r\n      FROM Production.Product AS p\r\n      INNER JOIN Production.TransactionHistory AS t\r\n      ON p.ProductID = t.ProductID\r\n      WHERE p.Name >= N'M' AND p.Name < N'S'\r\n    )\r\n    SELECT ProductID, Name, TransactionID, TransactionDate\r\n    FROM History \r\n    WHERE rn <= (5 * DaysToManufacture);\r\n\r\n    -- CROSS APPLY\r\n    \r\n    SELECT p.ProductID, p.Name, t.TransactionID, t.TransactionDate\r\n    FROM Production.Product AS p\r\n    CROSS APPLY\r\n    (\r\n      SELECT TOP (5 * p.DaysToManufacture) TransactionID, TransactionDate\r\n      FROM Production.TransactionHistory\r\n      WHERE ProductID = p.ProductID\r\n      ORDER BY TransactionDate DESC\r\n    ) AS t\r\n    WHERE p.Name >= N'M' AND p.Name < N'S';\r\n\r\nRuntime results:\r\n\r\n![enter image description here][6]\r\n\r\nParallel CTE / `OVER()` plan:\r\n\r\n![enter image description here][7]\r\n\r\nSingle-threaded CTE / `OVER()` plan:\r\n\r\n![enter image description here][8]\r\n\r\n`CROSS APPLY` plan:\r\n\r\n![enter image description here][9]\r\n\r\n> 3. Same, for the special case where exactly one history line per product is required (the single most recent entry by `TransactionDate`, tie-break on `TransactionID`.\r\n\r\nAgain, minor changes here. In the CTE solution, we add `TransactionID` to the `OVER()` clause, and change the outer filter to `rn = 1`. For the `CROSS APPLY`, we change the `TOP` to `TOP (1)`, and add `TransactionID` to the inner `ORDER BY`.\r\n\r\n    -- CTE / OVER()\r\n\r\n    ;WITH History AS\r\n    (\r\n      SELECT p.ProductID, p.Name, t.TransactionID, t.TransactionDate,\r\n        rn = ROW_NUMBER() OVER \r\n        (PARTITION BY t.ProductID ORDER BY t.TransactionDate DESC, TransactionID DESC)\r\n      FROM Production.Product AS p\r\n      INNER JOIN Production.TransactionHistory AS t\r\n      ON p.ProductID = t.ProductID\r\n      WHERE p.Name >= N'M' AND p.Name < N'S'\r\n    )\r\n    SELECT ProductID, Name, TransactionID, TransactionDate\r\n    FROM History \r\n    WHERE rn = 1;\r\n    \r\n    -- CROSS APPLY\r\n\r\n    SELECT p.ProductID, p.Name, t.TransactionID, t.TransactionDate\r\n    FROM Production.Product AS p\r\n    CROSS APPLY\r\n    (\r\n      SELECT TOP (1) TransactionID, TransactionDate\r\n      FROM Production.TransactionHistory\r\n      WHERE ProductID = p.ProductID\r\n      ORDER BY TransactionDate DESC, TransactionID DESC\r\n    ) AS t\r\n    WHERE p.Name >= N'M' AND p.Name < N'S';\r\n\r\nRuntime results:\r\n\r\n![enter image description here][10]\r\n\r\nParallel CTE / `OVER()` plan:\r\n\r\n![enter image description here][11]\r\n\r\nSingle-threaded CTE / OVER() plan:\r\n\r\n![enter image description here][12]\r\n\r\n`CROSS APPLY` plan:\r\n\r\n![enter image description here][13]\r\n\r\nWindowing functions aren't always the best alternative (have a go at `COUNT(*) OVER()`), and these are not the only two approaches to solving the n rows per group problem, but in this specific case - given the schema, existing indexes, and data distribution - the CTE fared better by all meaningful accounts.\r\n\r\n----\r\n\r\n## AdventureWorks examples - with flexibility to add indexes\r\n\r\nHowever, if you add a supporting index, similar to [the one Paul mentioned in a comment](https://dba.stackexchange.com/questions/86415/retrieving-n-rows-per-group/86416#comment155628_86416) but with the 2nd and 3rd columns ordered `DESC`:\r\n\r\n    CREATE UNIQUE NONCLUSTERED INDEX UQ3 ON Production.TransactionHistory \r\n      (ProductID, TransactionDate DESC, TransactionID DESC);\r\n\r\nYou would actually get much more favorable plans all around, and the metrics would flip to favor the `CROSS APPLY` approach in all three cases:\r\n\r\n![enter image description here][14]\r\n\r\nIf this were my production environment, I'd probably be satisfied with the duration in this case, and wouldn't bother to optimize further.\r\n\r\n----\r\n*This was all much uglier in SQL Server 2000, which didn't support `APPLY` or the `OVER()` clause.*\r\n\r\n\r\n  [1]: http://i.stack.imgur.com/iOXqU.png\r\n  [2]: http://i.stack.imgur.com/KU3A8.png\r\n  [3]: http://i.stack.imgur.com/UQdCH.png\r\n  [4]: http://i.stack.imgur.com/IvBMZ.png\r\n  [5]: http://i.stack.imgur.com/Uj0hq.png\r\n  [6]: http://i.stack.imgur.com/3ttv7.png\r\n  [7]: http://i.stack.imgur.com/W7bbs.png\r\n  [8]: http://i.stack.imgur.com/lTgFD.png\r\n  [9]: http://i.stack.imgur.com/BnBka.png\r\n  [10]: http://i.stack.imgur.com/2lEnF.png\r\n  [11]: http://i.stack.imgur.com/X6Gyn.png\r\n  [12]: http://i.stack.imgur.com/0wDW5.png\r\n  [13]: http://i.stack.imgur.com/9UEip.png\r\n  [14]: http://i.stack.imgur.com/3tw5C.png	2019-11-27 15:06:45.681027+00	3	4	1	86416	0	0	0	2019-11-24 09:57:15.07507+00	\N	The typical way to do this in SQL Server 2005 and up is to use a CTE and windowing functions. For top n per group you can simply use `ROW_NUMBER()` with a `PARTITION` clause, and filter against that in the outer query. So, for example, the top 5 most recent orders per customer could be displayed this way:	f	f
69	91	90	2014-12-22 15:36:14+00	`APPLY TOP` or `ROW_NUMBER()`? What could there possibly be more to say on that matter?\r\n\r\nA short recap of the differences and to really keep it short I will only show the plans for option 2 and I have added the index on `Production.TransactionHistory`.\r\n\r\n    create index IX_TransactionHistoryX on \r\n      Production.TransactionHistory(ProductID, TransactionDate)\r\n\r\n The `row_number()` query:.\r\n\r\n    with C as\r\n    (\r\n      select T.TransactionID,\r\n             T.TransactionDate,\r\n             P.DaysToManufacture,\r\n             row_number() over(partition by P.ProductID order by T.TransactionDate desc) as rn\r\n      from Production.Product as P\r\n        inner join Production.TransactionHistory as T\r\n          on P.ProductID = T.ProductID\r\n      where P.Name >= N'M' and\r\n            P.Name < N'S'\r\n    )\r\n    select C.TransactionID,\r\n           C.TransactionDate\r\n    from C\r\n    where C.rn <= 5 * C.DaysToManufacture;\r\n\r\n![enter image description here][1]\r\n\r\nThe `apply top` version:\r\n\r\n    select T.TransactionID, \r\n           T.TransactionDate\r\n    from Production.Product as P\r\n      cross apply (\r\n                  select top(cast(5 * P.DaysToManufacture as bigint))\r\n                    T.TransactionID,\r\n                    T.TransactionDate\r\n                  from Production.TransactionHistory as T\r\n                  where P.ProductID = T.ProductID\r\n                  order by T.TransactionDate desc\r\n                  ) as T\r\n    where P.Name >= N'M' and\r\n          P.Name < N'S';\r\n\r\n\r\n![enter image description here][2]\r\n\r\n\r\nThe main difference between these are that `apply top` filters on the top expression below the nested loops join where `row_number` version filters after the join. That means there are more reads from `Production.TransactionHistory` than really is necessary.\r\n\r\nIf there only existed a way to push the operators responsible for enumerating rows down to the lower branch before the join then `row_number` version might do better.\r\n\r\nSo enter `apply row_number()` version.\r\n\r\n\r\n    select T.TransactionID, \r\n           T.TransactionDate\r\n    from Production.Product as P\r\n      cross apply (\r\n                  select T.TransactionID,\r\n                         T.TransactionDate\r\n                  from (\r\n                       select T.TransactionID,\r\n                              T.TransactionDate,\r\n                              row_number() over(order by T.TransactionDate desc) as rn\r\n                       from Production.TransactionHistory as T\r\n                       where P.ProductID = T.ProductID\r\n                       ) as T\r\n                  where T.rn <= cast(5 * P.DaysToManufacture as bigint)\r\n                  ) as T\r\n    where P.Name >= N'M' and\r\n          P.Name < N'S';\r\n\r\n![enter image description here][3]\r\n\r\n\r\nAs you can see `apply row_number()` is pretty much the same as `apply top` only slightly more complicated. Execution time is also about the same or bit slower. \r\n\r\nSo why did I bother to come up with an answer that is no better than what we already have? Well, you have one more thing to try out in the real world and there actually is a difference in reads. One that I don't have an explanation for[^1].\r\n\r\n    APPLY - ROW_NUMBER\r\n    (961 row(s) affected)\r\n    Table 'TransactionHistory'. Scan count 115, logical reads 230, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    Table 'Product'. Scan count 1, logical reads 15, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n    APPLY - TOP\r\n    (961 row(s) affected)\r\n    Table 'TransactionHistory'. Scan count 115, logical reads 268, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    Table 'Product'. Scan count 1, logical reads 15, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n\r\n\r\n\r\nWhile I'm at it i might as well throw in a second `row_number()` version that in certain cases might be the way to go. Those certain cases would be when you expect you actually need most of the rows from `Production.TransactionHistory` because here you get a merge join between `Production.Product` and the enumerated `Production.TransactionHistory`.\r\n\r\n    with C as\r\n    (\r\n      select T.TransactionID,\r\n             T.TransactionDate,\r\n    \t\t T.ProductID,\r\n             row_number() over(partition by T.ProductID order by T.TransactionDate desc) as rn\r\n      from Production.TransactionHistory as T\r\n    )\r\n    select C.TransactionID,\r\n           C.TransactionDate\r\n    from C\r\n     inner join Production.Product as P\r\n          on P.ProductID = C.ProductID\r\n    where P.Name >= N'M' and\r\n          P.Name < N'S' and\r\n          C.rn <= 5 * P.DaysToManufacture;\r\n\r\n![enter image description here][4]\r\n\r\n\r\nTo get the above shape without a sort operator you also have to change the supporting index to order by `TransactionDate` descending.\r\n\r\n    create index IX_TransactionHistoryX on \r\n      Production.TransactionHistory(ProductID, TransactionDate desc)\r\n\r\n---\r\n\r\n[^1]: The extra logical reads are due to the [nested loops prefetching][5] used with the apply-top. You can disable this with undoc'd TF 8744 (and/or 9115 on later versions) to get the same number of logical reads. Prefetching could be an advantage of the apply-top alternative in the right circumstances.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/Bdkw4.png\r\n  [2]: https://i.stack.imgur.com/ixKBI.png\r\n  [3]: https://i.stack.imgur.com/agcVw.png\r\n  [4]: https://i.stack.imgur.com/hCi6Z.png\r\n  [5]: https://sql.kiwi/2013/08/sql-server-internals-nested-loops-prefetching.html	2019-11-27 15:14:28.672113+00	1	4	1	86781	0	0	0	2019-11-24 10:03:56.36953+00	\N	`APPLY TOP` or `ROW_NUMBER()`? What could there possibly be more to say on that matter?	f	f
144	200	2	2011-12-24 15:40:35+00	The answer, at least on 11.2, is "It depends":\r\n\r\nThis `create table` is rolled back:\r\n\r\n<>https://dbfiddle.uk/?rdbms=oracle_11.2&fiddle=2dfdc24c2296a9014f6947c902b5ca46\r\n\r\nBut this `truncate` is not:\r\n\r\n<>https://dbfiddle.uk/?rdbms=oracle_11.2&fiddle=8f0e71b119b01e441e372e7c218858f7	2019-11-29 22:22:43.705477+00	7	4	1	9701	0	0	0	2019-11-29 17:53:06.933422+00	\N	The answer, at least on 11.2, is "It depends":	f	f
70	92	16	2019-11-24 10:09:44.764278+00	Dear [your name here]!\r\n\r\nOh no, I'm sorry to hear that! Let's start with some basics to get you fixed up in a jiffy.\r\n\r\n## The thing you're running into is called Parameter Sniffing\r\n\r\nIt's a way out wiggy weird problem. The name rolls right off the tongue. Like the German word for squirrel.\r\n\r\nAnd it's usually your friend. \r\n\r\nWhen a query hits your server, a plan has to be compiled. To save time and resources later, an execution plan is cached based on the estimated rows that parameter will cause your code to process and return. \r\n\r\nThe easiest way to picture this going bad is to imagine a stored procedure that needs to count things from two lopsided populations.\r\n\r\nFor example:\r\n\r\n - People wearing CrossFit shirts who aren't injured: Zero\r\n\r\n - People wearing CrossFit shirts who wince when they wince: All\r\n\r\nObviously, one execution of that code would have to do a lot more work than another, and the query plans you'd want to do totally different amounts of work would look totally different.\r\n\r\n## What am I up against?\r\n\r\nThis is a genuinely difficult problem to find, test, and fix. \r\n\r\n - It's hard to find because it doesn't happen consistently\r\n - It's hard to test because you need to know which parameters cause different plans\r\n - It's hard to fix because sometimes it requires query and index tuning\r\n - It's hard to fix because you may not be able to change queries or indexes\r\n - It's hard to fix because even if you change queries or indexes, it might still come back\r\n\r\n## Quick Fixes\r\nSometimes, all you need is a little clarity. Or rather, your plan cache does.\r\n\r\n### If it's a stored procedure\r\nTry running `EXEC sys.sp_recompile @objname = N'schema.procname'`. That'll cause the procedure to recompile a new plan the next time it runs.\r\n\r\nWhat this won't fix:\r\n\r\n - Processes currently running it.\r\n\r\nWhat this doesn't guarantee:\r\n\r\n - The next process that runs after recompiling will use a parameter that gives you a good plan.\r\n\r\nYou can also point `sp_recompile` at a table or view, but be forewarned that all code that touches that table or view will recompile. This could make the problem a whole lot harder.\r\n\r\n### If it's a parameterized query\r\n\r\nYour job is a little more difficult. You'll need to track down the SQL Handle. You don't want to free the entire plan cache -- just like using `sp_recompile` against a table or view, you could trigger (ha ha ha) a whole bunch of unintended consequences.\r\n\r\nThe easiest way to figure that command out is to run [sp_BlitzWho][1]*! There's a column called "fix parameter sniffing" that has a command to remove a single plan from the cache. This has the same drawbacks as recompile, though. \r\n\r\nWhat this won't fix:\r\n\r\n - Processes currently running it.\r\n\r\nWhat this doesn't guarantee:\r\n\r\n - The next process that runs after recompiling will use a parameter that gives you a good plan.\r\n\r\n\r\n## I still need help!\r\n\r\nWe're going to need the following things:\r\n\r\n - The good query plan, if possible\r\n - The bad query plan \r\n - The parameters used\r\n - The query in question\r\n - Table and index definitions\r\n\r\n### Getting the query plans and query\r\n\r\nIf the query is running, you can use [sp_BlitzWho][1]* or [sp_WhoIsActive][2] to capture currently executing queries.\r\n\r\n    EXEC sp_BlitzWho;\r\n    \r\n    EXEC sp_WhoIsActive @get_plans = 1;\r\n\r\n[![NUTS][3]][3]\r\n\r\nIf the query isn't currently executing, you can check for it in the plan cache, using [sp_BlitzCache][1]*. \r\n\r\nIf you're on SQL Server 2016+, and have Query Store turned on, you can use [sp_BlitzQueryStore][1]*.\r\n\r\n    EXEC dbo.sp_BlitzCache @StoredProcName = 'Your Mom';\r\n    \r\n    EXEC dbo.sp_BlitzQueryStore @StoredProcName = 'Your Mom';\r\n\r\nThese will help you track down cached version(s) of your Stored Procedure. If it's just parameterized code, your search is a little more difficult. This may help, though:\r\n\r\n    EXEC dbo.sp_BlitzCache @QueryFilter = 'statement';\r\n\r\nYou should see fairly similar output from any of those. Again, the query plan inviting cool blue clicky column is your friend.\r\n\r\n[![NUTS][4]][4]\r\n\r\nThe easiest way to share plans is using [Paste The Plan][5]*, or dumping the XML into pastebin. To get that, click on either one of those inviting blue clicky columns. Your query plan should appear in a new SSMS tab.\r\n\r\n[![NUTS][6]][6]\r\n\r\nIf you're touchy about sharing your company's code and query, you can use [Sentry One's free Plan Explorer tool][7] to anonymize your plan. Keep in mind, this makes getting help harder -- anonymized code is a lot harder to read and figure out.\r\n\r\nAll of these tools we talked about should return the Query Text. You don't need to do anything else here.\r\n\r\nGetting the parameter(s) is a little more difficult. If you're using [Plan Explorer][7], there's a tab at the bottom that lists them all out for you.\r\n\r\n[![NUTS][8]][8]\r\n\r\nIf you're using [sp_BlitzCache][1]*, there's a clickable column which gives you the execution statement for stored procedures.\r\n\r\n[![NUTS][9]][9]\r\n\r\n\r\n## Getting the table and index definitions\r\n\r\nYou can easily right click in SSMS to script things out.\r\n\r\n[![NUTS][10]][10]\r\n\r\nIf you want to get everything in one shot, [sp_BlitzIndex][1]* can help if you point it directly at a table.\r\n\r\n    EXEC dbo.sp_BlitzIndex @DatabaseName = 'StackOverflow2010',\r\n                           @SchemaName = 'dbo',\r\n                           @TableName = 'Users';\r\n\r\nThis will give you the table definition (though not as a create statement), and create statements for all your indexes. \r\n\r\nCollecting and adding this information to your question should get people enough information to help, or point you in the right direction.\r\n\r\n\r\n## I wanna do it myself!\r\n\r\nWell, cool. I'm happy for you. You crazy person.\r\n\r\nThere are a lot of ways people think they "fix" parameter sniffing:\r\n\r\n - [Recompile hints][11]\r\n - [Optimize for unknown][12]\r\n - Optimize for a value\r\n\r\nBut these really just disable parameter sniffing in different ways. That's not to say they can't solve the problem, they just don't really get to the root cause.\r\n\r\nThat's because getting to the root cause is usually kind of difficult. You have to look for those pesky "plan quality issues".\r\n\r\nStarting with the fast vs slow plans, look for differences like:\r\n\r\n - Indexes used\r\n - Join order\r\n - Serial vs Parallel\r\n\r\nAlso look for different operators that make your code sensitive to parameter sniffing:\r\n\r\n - Lookups\r\n - Sorts\r\n - Join type\r\n - Memory grants (and by extension, spills)\r\n - Spools\r\n\r\nDon't get too wrapped up in seek vs scan, index fragmentation, or any of the cargo cult-y stuff people hem and haw about. \r\n\r\nUsually, there's a pretty basic indexing problem. Sometimes the code needs a little rewriting. \r\n\r\nIf you want to learn more about parameter sniffing:\r\n\r\n - [Slow in the Application, Fast in SSMS?][13] - Erland Sommarskog\r\n\r\n - Troubleshooting Parameter Sniffing [1][14], [2][15], [3][16] - Tara Kizer\r\n\r\n - [Why You’re Tuning Stored Procedures Wrong (the Problem with Local Variables)][17] - Kendra Little\r\n\r\n - [How to Use Parameters Like a Pro and Boost Performance][18] - Guy Glantser\r\n\r\n - [Parameter Sniffing, Embedding, and the RECOMPILE Options][19] - Paul White\r\n\r\n**If you're reading this, and you think I missed a link or helpful tool, leave a comment. I'll do my best to keep this up to date.**\r\n\r\n---\r\n\r\n  [1]: http://firstresponderkit.org\r\n  [2]: http://whoisactive.com\r\n  [3]: https://i.stack.imgur.com/p1PZL.png\r\n  [4]: https://i.stack.imgur.com/6PXH4.png\r\n  [5]: https://www.brentozar.com/pastetheplan/\r\n  [6]: https://i.stack.imgur.com/nNUOP.png\r\n  [7]: https://www.sentryone.com/plan-explorer\r\n  [8]: https://i.stack.imgur.com/B5Et9.png\r\n  [9]: https://i.stack.imgur.com/i8Xgw.png\r\n  [10]: https://i.stack.imgur.com/BoXN8.png\r\n  [11]: https://www.brentozar.com/archive/2013/12/recompile-hints-and-execution-plan-caching/\r\n  [12]: https://www.brentozar.com/archive/2013/06/optimize-for-unknown-sql-server-parameter-sniffing/\r\n  [13]: http://www.sommarskog.se/query-plan-mysteries.html\r\n  [14]: https://www.brentozar.com/archive/2018/03/troubleshooting-parameter-sniffing-issues-the-right-way-part-1/\r\n  [15]: https://www.brentozar.com/archive/2018/03/troubleshooting-parameter-sniffing-issues-right-way-part-2/\r\n  [16]: https://www.brentozar.com/archive/2018/03/troubleshooting-parameter-sniffing-issues-the-right-way-part-3/\r\n  [17]: https://www.brentozar.com/archive/2014/06/tuning-stored-procedures-local-variables-problems/\r\n  [18]: https://groupby.org/conference-session-abstracts/how-to-use-parameters-like-a-pro-and-boost-performance/\r\n  [19]: https://sqlperformance.com/2013/08/t-sql-queries/parameter-sniffing-embedding-and-the-recompile-options	2019-11-25 22:46:11.686796+00	4	4	1	\N	0	0	0	\N	\N	Dear [your name here]!	f	f
274	301	16	2019-07-02 16:07:44+00	It looks like when you populate the `#runIds` table -- and I'm just taking a wild guess here -- you're using a string splitter function that outputs the values as `NVARCHAR(MAX)`.\r\n\r\n[![NUTS][1]][1]\r\n\r\nYou could try converting the values there to get rid of the implicit conversion warnings.\r\n\r\nAnother possible improvement would be to alter the `NonClustereIndex-Cardholder` index on RiskProductionStatistics to have `model_set_name` as a key column, and `model_name, run_id, value` as included columns. This would address the Key Lookup.\r\n\r\n[![NUTS][2]][2]\r\n\r\n[![NUTS][3]][3]\r\n\r\nYou may also want to check the datatype of `model_name`. It appears in a Filter operator, and my fear is that it's a MAX datatype, which may prevent the [predicate from being pushed down][4].\r\n\r\n[![NUTS][5]][5]\r\n\r\nSince this is an estimated plan, and you haven't included any metrics about the query, it's hard to say how much improvement these changes will have.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/8xt66.png\r\n  [2]: https://i.stack.imgur.com/9gK1n.png\r\n  [3]: https://i.stack.imgur.com/9TDNx.png\r\n  [4]: https://www.brentozar.com/archive/2016/10/max-data-types/\r\n  [5]: https://i.stack.imgur.com/bP1ZA.png	2019-12-04 22:56:12.041125+00	2	4	1	241905	0	0	0	2019-12-04 22:56:12.041125+00	\N	It looks like when you populate the `#runIds` table -- and I'm just taking a wild guess here -- you're using a string splitter function that outputs the values as `NVARCHAR(MAX)`.	f	f
239	278	16	2017-05-23 16:56:42+00	For values larger than the `INT` max (2,147,483,647), you'll want to use [COUNT_BIG][1](*).\r\n\r\n    SELECT COUNT_BIG(*) AS [Records], SUM(t.Amount) AS [Total]\r\n    FROM   dbo.t1 AS t\r\n    WHERE  t.Id > 0\r\n           AND t.Id < 101;\r\n\r\nIf it's happening in the `SUM`, you need to convert `Amount` to a `BIGINT`.\r\n\r\n    SELECT COUNT(*) AS [Records], SUM(CONVERT(BIGINT, t.Amount)) AS [Total]\r\n    FROM   dbo.t1 AS t\r\n    WHERE  t.Id > 0\r\n           AND t.Id < 101;\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/functions/count-big-transact-sql	2019-12-04 14:23:41.312752+00	1	4	1	174354	0	0	0	2019-12-04 14:23:41.312752+00	\N	For values larger than the `INT` max (2,147,483,647), you'll want to use [COUNT_BIG][1](*).	f	f
606	559	751	2011-12-01 10:53:11+00	> What is HOBT lock?\r\n\r\n[A lock protecting a B-tree (index) or the heap data pages in a table that does not have a clustered index.][1]\r\n\r\n> Why would I still get a S lock?\r\n\r\nThis happens on heaps. Example\r\n\r\n    SET NOCOUNT ON;\r\n    \r\n    DECLARE @Query nvarchar(max) = \r\n       N'DECLARE @C INT; \r\n         SELECT @C = COUNT(*) FROM master.dbo.MSreplication_options';\r\n    \r\n    /*Run once so compilation out of the way*/\r\n    EXEC(@Query);\r\n    \r\n    DBCC TRACEON(-1,3604,1200) WITH NO_INFOMSGS;\r\n    \r\n    PRINT 'READ UNCOMMITTED';\r\n    SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\r\n    EXEC(@Query);\r\n    \r\n    PRINT 'READ COMMITTED';\r\n    SET TRANSACTION ISOLATION LEVEL READ COMMITTED;\r\n    EXEC(@Query);\r\n    \r\n    DBCC TRACEOFF(-1,3604,1200) WITH NO_INFOMSGS;\r\n\r\n### Output `READ UNCOMMITTED`\r\n\r\n```none\r\nProcess 56 acquiring Sch-S lock on OBJECT: 1:1163151189:0  (class bit0 ref1) result: OK\r\n    \r\nProcess 56 acquiring S lock on HOBT: 1:72057594038910976 [BULK_OPERATION] (class bit0 ref1) result: OK\r\n    \r\nProcess 56 releasing lock on OBJECT: 1:1163151189:0 \r\n```\r\n\r\n### Output `READ COMMITTED`\r\n\r\n```none\r\nProcess 56 acquiring IS lock on OBJECT: 1:1163151189:0  (class bit0 ref1) result: OK\r\n    \r\nProcess 56 acquiring IS lock on PAGE: 1:1:169 (class bit0 ref1) result: OK\r\n    \r\nProcess 56 releasing lock on PAGE: 1:1:169\r\n    \r\nProcess 56 releasing lock on OBJECT: 1:1163151189:0 \r\n```\r\n\r\nAccording to [this article][2] referencing Paul Randal the reason for taking this `BULK_OPERATION` shared HOBT lock is to prevent reading of unformatted pages.\r\n\r\n\r\n  [1]: https://msdn.microsoft.com/en-us/library/ms189849.aspx\r\n  [2]: https://tenbulls.co.uk/2011/10/14/nolock-hits-mythbusters/	2020-01-12 14:17:30.173273+00	3	4	1	8643	0	0	0	2020-01-12 14:15:29.913352+00	\N	> What is HOBT lock?	f	f
264	294	16	2019-06-28 10:12:06+00	You need to be careful about [SQL injection attacks][2].\r\n\r\nWhile converting an `INT` to a `VARCHAR 11` is likely not going to cause any issues, [sysname][3] is the equivalent of an `NVARCHAR 128`, and you can jam a lot of extra code in there.\r\n\r\nTo make your code totally safe, you'd want to do this:\r\n\r\n    DECLARE @stringsvar NVARCHAR(1000) = '';\r\n    DECLARE @Emp_id INT = 1;\r\n    DECLARE @strvar sysname = N'Employee_test';\r\n    \r\n    SET @stringsvar = ( N'select * from ' \r\n                        + QUOTENAME(@strvar) \r\n    \t\t\t\t\t+ N' where emp_id' \r\n    \t\t\t\t\t+ N' = @iEmp_id' );\r\n    \r\n    PRINT @stringsvar;\r\n    EXEC sys.sp_executesql @stringsvar, \r\n                           N'@iEmp_id INT', \r\n    \t\t\t\t\t   @iEmp_id = @Emp_id;\r\n\r\nUsing [sp_executesql][4] to issue parameterized dynamic SQL, and [quotename][5] to make the table name non-executable code is a much safer choice.\r\n\r\nFor a little more reference, I'd suggest heading over here: [The Curse and Blessings of Dynamic SQL][6]\r\n\r\n\r\n  [2]: https://codecurmudgeon.com/wp/sql-injection-hall-of-shame/\r\n  [3]: https://stackoverflow.com/questions/5720212/what-is-sysname-data-type-in-sql-server\r\n  [4]: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql?view=sql-server-2017\r\n  [5]: https://docs.microsoft.com/en-us/sql/t-sql/functions/quotename-transact-sql?view=sql-server-2017\r\n  [6]: http://www.sommarskog.se/dynamic_sql.html	2019-12-04 22:42:36.399153+00	1	4	1	241610	0	0	0	2019-12-04 22:41:59.135018+00	\N	You need to be careful about [SQL injection attacks][2].	f	f
102	147	12	2016-04-11 09:59:14+00	>Should SQL Server not identify the highest value for that key and use that as the maximum `RANGE_HI_KEY`? Or is this just one of the limits of update without using `FULLSCAN`?\r\n\r\nIt's a limitation of the current implementation of sampled statistics. As it stands, sampled statistics collection uses `TABLESAMPLE SYSTEM`, which uses an allocation-order scan and chooses pages from the scan to sample. Only chosen pages contribute to the histogram.\r\n\r\nSince the scan is allocation-ordered (rather than index-ordered), there is no way to give preference to the first and last pages in key order.\r\n\r\nFor more information see this related question:\r\n\r\nhttps://dba.stackexchange.com/q/34883/1192\r\n\r\nand my article, [Allocation Order Scans][1]\r\n\r\nFor workarounds, see [Statistics on Ascending Columns][2] by [Fabiano Amorim][3]\r\n\r\n\r\n  [1]: http://sqlperformance.com/2015/01/t-sql-queries/allocation-order-scans\r\n  [2]: https://www.simple-talk.com/sql/database-administration/statistics-on-ascending-columns/\r\n  [3]: https://www.simple-talk.com/author/fabiano-amorim/	2019-11-27 10:05:15.118037+00	1	4	1	134953	0	0	0	2019-11-27 10:05:15.118037+00	\N	>Should SQL Server not identify the highest value for that key and use that as the maximum `RANGE_HI_KEY`? Or is this just one of the limits of update without using `FULLSCAN`?	f	f
559	507	167	2020-01-02 14:02:58.782153+00	The following search and replace terms will update the syntax\r\n\r\n### Circle:\r\n\r\n- search: `circle \\(([^\\)]*)\\)`\r\n- replace `circle[radius=\\1]`\r\n\r\n### Ellipse\r\n\r\n- search: `ellipse \\(([^\\)]*) and ([^\\)]*)\\)`\r\n- replace: `ellipse[x radius=\\1, y radius=\\2]`\r\n\r\n### Elliptical arc \r\n\r\n- search `arc\\(([^\\)]*):([^\\)]*):([^\\)]*) and ([^\\)]*)\\)`\r\n- replace `arc[start angle=\\1, end angle=\\2, x radius=\\3, y radius =\\4] `\r\n\r\n(if you have both elliptical and circular arcs in the same document, do the elliptical arcs first)\r\n\r\n### Circular arc\r\n\r\n- search `arc\\(([^\\)]*):([^\\)]*):([^\\)]*)\\)`\r\n- replace `arc[start angle=\\1, end angle=\\2, radius=\\3] `\r\n\r\n(Don't forget to activate the `Reg` and `\\1` buttons in texstudio to use this syntax)\r\n\r\n![Screen Shot 2020-01-02 at 22.13.45.png](/image?hash=f7ca2611f9a33ab3f4c35380d098912b2cb265534c94bba79bc0cf0306a6d293)	2020-01-04 13:46:25.194253+00	5	4	3	\N	0	0	0	\N	\N	The following search and replace terms will update the syntax	f	f
611	565	8	2017-11-15 10:33:17+00	This problem can be solved using [accent insensitive collations](https://msdn.microsoft.com/en-us/library/ms144250(v=sql.105).aspx).\r\n\r\nYour database is probably using a AS (Accent Sensitive) collation so by default it will search for the exact match including accents.\r\n\r\nYou could instruct the WHERE clause to use another collation than the database default by specifying a collation with the comparison.\r\n\r\nIn [this dbfiddle](http://dbfiddle.uk/?rdbms=sqlserver_2016&fiddle=e5f7fe8dd99179c44d19189187d6b9b9) I created an example using the LATIN1 collations but you could use the same approach with the collation you are using by just changing AS into AI for the collation your column is currently using.\r\n\r\nUse the Accent Insensitive collation that matches the collation the colummn is using. For example if the column is using `SQL_Latin1_General_CP1_CI_AS`, use `SQL_Latin1_General_CP1_CI_AI` and not `Latin1_General_CI_AS` or `Latin1_General_100_CI_AS` or any of the variations of those two since the behavior of the non-SQL_ collations will differ in more ways than just accent-insensitivity, and that might not be expected by users.\r\n\r\nYou can check the current collation in `sys.columns`.\r\n\r\n    CREATE TABLE testaccent (name nvarchar(50));\r\n    GO\r\n    INSERT INTO testaccent (name) VALUES ('Millière') , ('Milliere');\r\n    GO\r\n    -- returns Miliere\r\n    SELECT * FROM testaccent WHERE name = 'Milliere';\r\n    \r\n    -- returns both\r\n    SELECT * FROM testaccent WHERE name='Milliere' COLLATE Latin1_General_CI_AI\r\n    \r\n    --only returns Miliere\r\n    SELECT * FROM testaccent WHERE name='Milliere' COLLATE Latin1_General_CI_AS\r\n\r\nRead through [Using SQL Server Collations](https://msdn.microsoft.com/en-us/library/ms144260(v=sql.105).aspx) for more information.\r\n\r\nThen again you'd probably want sorting to use this collation (as [peufeu](https://dba.stackexchange.com/users/120712/) noted in the comments) to ensure that "é" sorts with "e". Otherwise, someone who paginates through results in alphabetical order would be surprised not to find the "é" where they expect them to be, but if you only want to touch this query you can add the `COLLATE` clause to the `ORDER BY` too.\r\n\r\nAs noted by [Solomon Rutzky](https://dba.stackexchange.com/users/30859/solomon-rutzky) in the comments, if this only affects 1 or a few columns, another option is to create a non-persisted computed column that simply repeats the "name" column and provides the accent insensitive collation, and then index the computed column. This avoids the scan caused by changing the collation within the query. Then the query needs to filter on the new column. \r\n\r\nSomething like: \r\n\r\n    ALTER TABLE \r\n    dbo.[table_name] ADD [SearchName] datatype_of_name_column \r\n    AS ([Name] COLLATE LATIN1_GENERAL_100_CI_AI)); \r\n    \r\n    CREATE INDEX [IX_table_name_SearchName] \r\n    ON dbo.[table_name] ([SearchName] ASC);\r\n\r\nOr you could also create a view instead of adding a computed column (as [jyao](https://dba.stackexchange.com/users/63896/jyao) prefers).	2020-01-14 10:09:55.797423+00	2	4	1	190975	0	0	0	2020-01-14 10:09:55.797423+00	\N	This problem can be solved using [accent insensitive collations](https://msdn.microsoft.com/en-us/library/ms144250(v=sql.105).aspx).	f	f
112	178	94	2014-05-27 19:19:14+00	    SELECT (ctid::text::point)[0]::bigint AS page_number FROM t;\r\n\r\n[Your **fiddle** with my solution.][1]\r\n\r\n@bma already hinted something similar in a comment. Here is a ...\r\n### Rationale for the type\r\n\r\n`ctid` is of type `tid` (tuple identifier), called `ItemPointer` in the C code. [Per documentation:][2]\r\n\r\n> This is the data type of the system column `ctid`. A tuple ID is a\r\n> pair (**block number**, **tuple index within block**) that identifies the\r\n> physical location of the row within its table.\r\n\r\nBold emphasis mine. [And:][3]\r\n\r\n> (`ItemPointer`, also known as `CTID`)\r\n\r\nA block is **8 KB** in standard installations. [Maximum Table Size is **32 TB**.][4] It follows logically that block numbers must accommodate *at least* a maximum of (calculation fixed according to comment by @Daniel):\r\n\r\n    SELECT (2^45 / 2^13)::int      -- = 2^32 = 4294967294\r\n\r\nWhich would fit into an unsigned `integer`. On further investigation I found [**in the source code**][5] that ...\r\n\r\n> blocks are numbered sequentially, **0 to 0xFFFFFFFE**.\r\n\r\nBold emphasis mine. Which confirms the first calculation:\r\n\r\n    SELECT 'xFFFFFFFE'::bit(32)::int8 -- max page number: 4294967294\r\n\r\nPostgres uses signed integer and is therefore one bit short. I couldn't pin down, yet, whether the text representation is shifted to accommodate signed integer. Until somebody can clear this up, I would fall back to **`bigint`**, which works in any case.\r\n\r\n### Cast\r\nThere is [no cast registered][6] for the `tid` type in Postgres 9.3:\r\n\r\n    SELECT *\r\n    FROM   pg_cast\r\n    WHERE  castsource = 'tid'::regtype\r\n    OR     casttarget = 'tid'::regtype;\r\n\r\n     castsource | casttarget | castfunc | castcontext | castmethod\r\n    ------------+------------+----------+-------------+------------\r\n    (0 rows)\r\n\r\nYou can still cast to `text`. There is a [text representation for everything in Postgres][7]:\r\n\r\n> Another important exception is that "automatic I/O conversion casts",\r\n> those performed using a data type's own I/O functions to convert to or\r\n> from text or other string types, are not explicitly represented in\r\n> `pg_cast`.\r\n\r\nThe text representation matches that of a point, which consists of two `float8` numbers, that cast is lossless.  \r\n\r\nYou can access the first number of a point with index 0. Cast to `bigint`. Voilá.\r\n\r\n## Performance\r\nI ran a quick test on a table with 30k rows (best of 5) on a couple of alternative expressions that came to mind, including your original:\r\n\r\n    SELECT (ctid::text::point)[0]::int                              --  25 ms\r\n          ,right(split_part(ctid::text, ',', 1), -1)::int           --  28 ms\r\n          ,ltrim(split_part(ctid::text, ',', 1), '(')::int          --  29 ms\r\n          ,(ctid::text::t_tid).page_number                          --  31 ms\r\n          ,(translate(ctid::text,'()', '{}')::int[])[1]             --  45 ms\r\n          ,(replace(replace(ctid::text,'(','{'),')','}')::int[])[1] --  51 ms\r\n          ,substring(right(ctid::text, -1), '^\\d+')::int            --  52 ms\r\n          ,substring(ctid::text, '^\\((\\d+),')::int                  -- 143 ms\r\n    FROM tbl;\r\n\r\n`int` instead of `bigint` here, mostly irrelevant for the purpose of the test. I didn't repeat for `bigint`.  \r\nThe cast to `t_tid` builds on a user-defined composite type, like @Jake commented.  \r\nThe gist of it: Casting tends to be faster than string manipulation. Regular expressions are expensive. The above solution is shortest and fastest.\r\n\r\n\r\n  [1]: http://sqlfiddle.com/#!15/9d249/4\r\n  [2]: http://www.postgresql.org/docs/current/interactive/datatype-oid.html\r\n  [3]: http://www.postgresql.org/docs/current/interactive/storage-page-layout.html\r\n  [4]: http://www.postgresql.org/about/\r\n  [5]: https://github.com/postgres/postgres/blob/master/src/include/storage/block.h\r\n  [6]: http://www.postgresql.org/docs/current/interactive/catalog-pg-cast.html\r\n  [7]: http://www.postgresql.org/docs/current/interactive/catalog-pg-cast.html	2019-11-27 23:11:31.145637+00	1	4	1	66007	0	0	0	2019-11-27 23:11:31.145637+00	\N	SELECT (ctid::text::point)[0]::bigint AS page_number FROM t;	f	f
219	267	87	2016-03-16 22:01:39+00	Risking ridicule from some of the biggest names in the SQL Server community, I'm going to stick my neck out and say, nope.\r\n\r\nIn order for your query to be SARGable, you'd have to basically construct a query that can pinpoint a starting row in a *range of consecutive rows* in an index. With the index `ix_dates`, the rows are not ordered by the date difference between `DateCol1` and `DateCol2`, so your target rows could be spread out anywhere in the index.\r\n\r\nSelf-joins, multiple passes, etc. all have in common that they include at least one Index Scan, although a (nested loop) join may well use an Index Seek. But I can't see how it would be possible to eliminate the Scan.\r\n\r\nAs for getting more accurate row estimates, there are no statistics on the date difference.\r\n\r\nThe following, fairly ugly recursive CTE construct does technically eliminate scanning the whole table, although it introduces a Nested Loop Join and a (potentially very large) number of Index Seeks.\r\n\r\n    DECLARE @from date, @count int;\r\n    SELECT TOP 1 @from=DateCol1 FROM #sargme ORDER BY DateCol1;\r\n    SELECT TOP 1 @count=DATEDIFF(day, @from, DateCol1) FROM #sargme WHERE DateCol1<=DATEADD(day, -48, {d '9999-12-31'}) ORDER BY DateCol1 DESC;\r\n    \r\n    WITH cte AS (\r\n        SELECT 0 AS i UNION ALL\r\n        SELECT i+1 FROM cte WHERE i<@count)\r\n    \r\n    SELECT b.*\r\n    FROM cte AS a\r\n    INNER JOIN #sargme AS b ON\r\n        b.DateCol1=DATEADD(day, a.i, @from) AND\r\n        b.DateCol2>=DATEADD(day, 48+a.i, @from)\r\n    OPTION (MAXRECURSION 0);\r\n\r\nIt creates an Index Spool containing every `DateCol1` in the table, then performs an Index Seek (range scan) for each of those `DateCol1` and `DateCol2` that are at least 48 days forward.\r\n\r\nMore IOs, slightly longer execution time, row estimate is still way off, and zero chance of parallelization because of the recursion: I'm guessing this query could possibly be useful if you have a very large number of values within relatively few distinct, consecutive `DateCol1` (keeping the number of Seeks down).\r\n\r\n[![Crazy recursive CTE query plan][1]][1]\r\n\r\n  [1]: https://i.stack.imgur.com/ReyHi.png	2019-12-04 14:11:29.994242+00	1	4	1	132445	0	0	0	2019-12-04 14:11:29.994242+00	\N	Risking ridicule from some of the biggest names in the SQL Server community, I'm going to stick my neck out and say, nope.	f	f
218	267	12	2016-03-18 03:08:31+00	I know this is not the answer you want, but an **indexed computed column** is usually the right solution for this type of problem.\r\n\r\nIt:\r\n\r\n* makes the predicate an indexable expression\r\n* allows automatic statistics to be created for better cardinality estimation\r\n* does not *need* to take any space in the base table\r\n\r\nTo be clear on that last point, the computed column is **not required to be persisted** in this case:\r\n\r\n    -- Note: not PERSISTED, metadata change only\r\n    ALTER TABLE #sargme\r\n    ADD DayDiff AS DATEDIFF(DAY, DateCol1, DateCol2);\r\n    \r\n    -- Index the expression\r\n    CREATE NONCLUSTERED INDEX index_name\r\n    ON #sargme (DayDiff)\r\n    INCLUDE (DateCol1, DateCol2);\r\n\r\nNow the query:\r\n\r\n    SELECT\r\n        S.ID,\r\n        S.DateCol1,\r\n        S.DateCol2,\r\n        DATEDIFF(DAY, S.DateCol1, S.DateCol2)\r\n    FROM\r\n        #sargme AS S\r\n    WHERE\r\n        DATEDIFF(DAY, S.DateCol1, S.DateCol2) >= 48;\r\n\r\n...gives the following *trivial* plan:\r\n\r\n[![Execution plan][1]][1]\r\n\r\nIf you have connections using the wrong set options, you could create a regular column and maintain the computed value using triggers.\r\n\r\nAll this only really matters (code challenge aside) if there's a real problem to solve, of course, as Aaron says in [his answer][2].\r\n\r\nThis is fun to think about, but I don't know any way to achieve what you want reasonably given the constraints in the question. It seems like any optimal solution would require a new data structure of some type; the closest we have being the 'function index' approximation provided by an index on a non-persisted computed column as above.\r\n\r\n  [1]: https://i.stack.imgur.com/z2rs7.png\r\n  [2]: https://topanswers.xyz/databases?q=267#a220	2019-12-04 14:14:21.6367+00	1	4	1	132612	0	0	0	2019-12-04 14:11:29.705837+00	\N	I know this is not the answer you want, but an **indexed computed column** is usually the right solution for this type of problem.	f	f
221	267	172	2016-05-02 22:03:53+00	*Answer originally added by the question author as an edit to the question*\r\n\r\n---\r\n\r\nAfter letting this sit for a bit, and some really smart people chiming in, my initial thought on this seems correct: there's no sane and SARGable way to write this query without adding a column, either computed, or maintained via some other mechanism, namely triggers.\r\n\r\nI did try a few other things, and I have some other observations that may or may not be interesting to anyone reading.\r\n\r\nFirst, re-running the setup using a regular table rather than a temp table\r\n\r\n - Even though I know their reputation, I wanted to try multi-column statistics out. They were useless.\r\n - I wanted to see which statistics were used\r\n\r\nHere's the new setup:\r\n\r\n    USE [tempdb]\r\n    SET NOCOUNT ON\t\r\n    \r\n    DBCC FREEPROCCACHE\r\n    \r\n    IF OBJECT_ID('tempdb..sargme') IS NOT NULL\r\n    BEGIN\r\n    DROP TABLE sargme\r\n    END\r\n    \r\n    SELECT TOP 1000\r\n    IDENTITY (BIGINT, 1,1) AS ID,\r\n    CAST(DATEADD(DAY, [m].[severity] * -1, GETDATE()) AS DATE) AS [DateCol1],\r\n    CAST(DATEADD(DAY, [m].[severity], GETDATE()) AS DATE) AS [DateCol2]\r\n    INTO sargme\r\n    FROM sys.[messages] AS [m]\r\n    \r\n    ALTER TABLE [sargme] ADD CONSTRAINT [pk_whatever] PRIMARY KEY CLUSTERED ([ID])\r\n    CREATE NONCLUSTERED INDEX [ix_dates] ON [sargme] ([DateCol1], [DateCol2])\r\n    \r\n    CREATE STATISTICS [s_sargme] ON [sargme] ([DateCol1], [DateCol2])\r\n\r\nThen, running the first query, it uses the ix_dates index, and scans, just like before. No change here. This seems redundant, but stick with me.\r\n\r\n    SELECT\r\n        * ,\r\n        DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2])\r\n    FROM\r\n        [sargme] AS [s]\r\n    WHERE\r\n        DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2]) >= 48\r\n\r\nRun the CTE query again, still the same...\r\n\r\n    WITH    [x] AS ( SELECT\r\n                    * ,\r\n                    DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2]) AS [ddif]\r\n                   FROM\r\n                    [sargme] AS [s])\r\n         SELECT\r\n            *\r\n         FROM\r\n            [x]\r\n         WHERE\r\n            [x].[ddif] >= 48;\r\n\r\nAlright! Run the not-even-half-sargable query again:\r\n\r\n    SELECT\r\n        * ,\r\n        DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2])\r\n    FROM\r\n        [sargme] AS [s]\r\n    WHERE\r\n        [s].[DateCol2] >= DATEADD(DAY, 48, [s].[DateCol1])\r\n\r\nNow add the computed column, and re-run all three, along with the query that hits the computed column:\r\n\r\n    ALTER TABLE [sargme] ADD [ddiff] AS \r\n    DATEDIFF(DAY, DateCol1, DateCol2) PERSISTED\r\n    \r\n    CREATE NONCLUSTERED INDEX [ix_dates2] ON [sargme] ([ddiff], [DateCol1], [DateCol2])\r\n    \r\n    SELECT [s].[ID] ,\r\n           [s].[DateCol1] ,\r\n           [s].[DateCol2]\r\n    FROM [sargme] AS [s]\r\n    WHERE [ddiff] >= 48\r\n\r\nIf you stuck with me to here, thanks. This is the interesting observation portion of the post.\r\n\r\nRunning a query with an undocumented trace flag by [Fabiano Amorim][3] to see which statistics each query used is pretty cool. Seeing that _no plan touched a statistics object_ until the computed column was created and indexed seemed odd.\r\n\r\n[![What the bloodclot][4]][4]\r\n\r\nHeck, even the query that hit the computed column ONLY didn't touch a statistics object until I ran it a few times and it got simple parameterization. So even though they all initially scanned the ix_dates index, they used hard-coded cardinality estimates (30% of the table) rather than any statistics object available to them. \r\n\r\nOne other point that raised an eyebrow over here is that when I added only the nonclustered index, the query plans all scanned the HEAP, rather than use the nonclustered index on both date columns.\r\n\r\nThanks to everyone who responded. You're all wonderful.\r\n\r\n  [1]: https://i.stack.imgur.com/vvmAp.jpg\r\n  [2]: https://i.stack.imgur.com/noBNR.jpg\r\n  [3]: http://blogfabiano.com/2012/07/03/statistics-used-in-a-cached-query-plan/\r\n  [4]: https://i.stack.imgur.com/80oNF.png	2019-12-04 14:16:06.742994+00	1	4	1	137292	0	0	0	2019-12-04 14:11:30.585552+00	\N	*Answer originally added by the question author as an edit to the question*	f	f
220	267	88	2016-03-17 03:18:15+00	I tried a bunch of wacky variations, but didn't find any version better than one of yours. The main problem is that your index looks like this in terms of how date1 and date2 are sorted together. The first column is going to be in a nice shelved line while the gap between them is going to be very jagged. You want this to look more like a funnel than the way it really will:\r\n\r\n    Date1    Date2\r\n    -----    -------\r\n    *             *\r\n    *             *\r\n    *              *\r\n     *       * \r\n     *        *\r\n     *         *\r\n      *      *\r\n      *           *\r\n\r\nThere's not really any way I can think of to make that seekable for a certain delta (or range of deltas) between the two points. And I mean a single seek that's executed once + a range scan, not a seek that's executed for every row. That will involve a scan and/or a sort at some point, and these are things you want to avoid obviously. It's too bad you can't use expressions like `DATEADD`/`DATEDIFF` in filtered indexes, or perform any possible schema modifications that would allow a sort on the product of the date diff (like calculating the delta at insert/update time). As is, this seems to be one of those cases where a scan is actually the optimal retrieval method.\r\n\r\nYou said that this query was no fun, but if you look closer, this is by far the best one (and would be even better if you left out the compute scalar output):\r\n\r\n    SELECT\r\n        * ,\r\n        DATEDIFF(DAY, [s].[DateCol1], [s].[DateCol2])\r\n    FROM\r\n        [#sargme] AS [s]\r\n    WHERE\r\n        [s].[DateCol2] >= DATEADD(DAY, 48, [s].[DateCol1])\r\n\r\nThe reason is that avoiding the `DATEDIFF` potentially shaves some CPU compared to a calculation against *only* the non-leading key column in the index, and also avoids some nasty implicit conversions to `datetimeoffset(7)` (don't ask me why those are there, but they are). Here is the `DATEDIFF` version:\r\n\r\n``` none\r\n<Predicate>\r\n<ScalarOperator ScalarString=\r\n"datediff(day,CONVERT_IMPLICIT(datetimeoffset(7),\r\n[splunge].[dbo].[sargme].[DateCol1] as [s].[DateCol1],0),\r\nCONVERT_IMPLICIT(datetimeoffset(7),[splunge].[dbo].[sargme].[DateCol2] \r\nas [s].[DateCol2],0))>=(48)">\r\n```\r\n\r\nAnd here's the one without `DATEDIFF`:\r\n\r\n``` none\r\n<Predicate>\r\n<ScalarOperator ScalarString=\r\n"[splunge].[dbo].[sargme].[DateCol2] as [s].[DateCol2]>=\r\ndateadd(day,(48),[splunge].[dbo].[sargme].[DateCol1] as [s].[DateCol1])">\r\n```\r\n\r\nAlso I found slightly better results in terms of duration when I changed the index to only *include* `DateCol2` (and when both indexes were present, SQL Server always chose the one with one key and one include column vs. multi-key). For this query, since we have to scan all rows to find the range anyway, there is no benefit to have the second date column as part of the key and sorted in any way. And while I know we can't get a seek here, there is something inherently good-feeling about *not* hindering the ability to get one by forcing calculations against the leading key column, and only performing them against secondary or included columns.\r\n\r\nIf it were me, and I gave up on finding the sargable solution, I know which one I would choose - the one that makes SQL Server do the least amount of work (even if the delta is almost nonexistent). Or better yet I would relax my restrictions about schema change and the like.\r\n\r\nAnd how much all of that matters? I don't know. I made the table 10 million rows and all of the above query variations still completed in under a second. And this is on a VM on a laptop (granted, with SSD).	2019-12-04 14:25:30.809842+00	1	4	1	132461	0	0	0	2019-12-04 14:11:30.303272+00	\N	I tried a bunch of wacky variations, but didn't find any version better than one of yours. The main problem is that your index looks like this in terms of how date1 and date2 are sorted together. The first column is going to be in a nice shelved line while the gap between them is going to be very jagged. You want this to look more like a funnel than the way it really will:	f	f
59	83	12	2019-11-23 08:10:56.291381+00	An initial look at the execution plans shows that the expression `1/0` is defined in the Compute Scalar operators:\r\n\r\n![Graphical plans][1]\r\n\r\nNow, even though execution plans do start executing at the far left, iteratively calling `Open` and `GetRow` methods on child iterators to return results, SQL Server 2005 and later contains an optimization whereby expressions are often only *defined* by a Compute Scalar, with [evaluation deferred until a subsequent operation requires the result][2]:\r\n\r\n![Compute Scalar operators that appear in Showplans generated by SET STATISTICS XML might not contain the RunTimeInformation element. In graphical Showplans, Actual Rows, Actual Rebinds, and Actual Rewinds might be absent from the Properties window when the Include Actual Execution Plan option is selected in SQL Server Management Studio. When this occurs, it means that although these operators were used in the compiled query plan, their work was performed by other operators in the run-time query plan. Also note that the number of executes in Showplan output generated by SET STATISTICS PROFILE is equivalent to the sum of rebinds and rewinds in Showplans generated by SET STATISTICS XML. From: MSDN Books Online][3]\r\n\r\nIn this case, the expression *result* is only needed when assembling the row for return to the client (which you can think of occurring at the green `SELECT` icon). By that logic, deferred evaluation would mean the expression is never evaluated because neither plan generates a return row. To labour the point a little, neither the Clustered Index Seek nor the Table Scan return a row, so there is no row to assemble for return to the client.\r\n\r\nHowever, there is a separate optimization whereby some expressions can be identified as [runtime constants][4] and so **evaluated once before query execution begins**. In this case, an indication this has occurred can be found in the showplan XML (Clustered Index Seek plan on the left, Table Scan plan on the right):\r\n\r\n![Showplan XML][5]\r\n\r\nI wrote more about the underlying mechanisms and how they can affect performance [in this blog post][6]. Using information provided there, we can modify the first query so both expressions are evaluated and cached before execution starts:\r\n\r\n```sql\r\nselect 1/0 * CONVERT(integer, @@DBTS)\r\nfrom #temp\r\nwhere id = 1\r\n    \r\nselect 1/0\r\nfrom #temp2\r\nwhere id = 1\r\n```\r\n\r\nNow, the first plan also contains a constant expression reference, and both queries produce the error message. The XML for the first query contains:\r\n\r\n![Constant Expression][7]\r\n\r\nMore information: [Compute Scalars, Expressions and Performance][8]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/Kz8Kv.png\r\n  [2]: https://technet.microsoft.com/en-us/library/ms178082.aspx\r\n  [3]: https://i.stack.imgur.com/7BoZq.png\r\n  [4]: https://www.sqlskills.com/blogs/conor/rand-and-other-runtime-constant-functions-redux/\r\n  [5]: https://i.stack.imgur.com/d2R6e.png\r\n  [6]: https://sql.kiwi/2012/09/compute-scalars-expressions-and-execution-plan-performance.html\r\n  [7]: https://i.stack.imgur.com/kcQkL.png\r\n  [8]: https://sql.kiwi/2012/09/compute-scalars-expressions-and-execution-plan-performance.html	2019-11-23 08:20:15.595557+00	2	4	2	\N	0	0	0	\N	\N	An initial look at the execution plans shows that the expression `1/0` is defined in the Compute Scalar operators:	f	f
616	571	758	2019-01-13 12:40:33+00	I had the same issue I think when doing some testing weeks ago.\r\nI have a query with a primary predicate that requires that results returned have a NULL closedatetime and I thought about using a filtered index as 25K of 2M+ records are NULL\r\nand this figure will decrease very soon. \r\n\r\nThe filtered index didn't get used - I assumed due to 'non-uniqueness' or commonality - until I found a [Microsoft support article][1] that says:\r\n\r\n> To resolve this issue, include the column that is tested as NULL in the returned columns. Or, add this column as include columns in the index.\r\n\r\n\r\nSo adding the column to the Index (or Include) seems to be the official MS response.\r\n\r\n\r\n  [1]: https://support.microsoft.com/en-gb/help/3051225/a-filtered-index-that-you-create-together-with-the-is-null-predicate-i	2020-01-14 15:20:29.492581+00	2	4	1	227033	0	0	0	2020-01-14 15:20:29.492581+00	\N	I had the same issue I think when doing some testing weeks ago.	f	f
615	571	160	2018-09-07 18:31:57+00	>Why this index is not "covering" for this query:\r\n\r\nNo good reason.  That is a covering index for that query.\r\n\r\nPlease vote for the feeback item here: https://feedback.azure.com/forums/908035-sql-server/suggestions/32896348-filtered-index-not-used-when-is-null-and-key-looku\r\n\r\nAnd as a [workaround][1] include the `WhereColumn` in the filtered index:\r\n\r\n    CREATE NONCLUSTERED INDEX IX_MyTab_GroupByColumn \r\n    ON MyTab (GroupByColumn) include (WhereColumn)\r\n    WHERE (WhereColumn IS NULL) \r\n\r\n\r\n  [1]: https://support.microsoft.com/en-gb/help/3051225/a-filtered-index-that-you-create-together-with-the-is-null-predicate-i	2020-01-14 15:20:29.219649+00	4	4	1	217051	0	0	0	2020-01-14 15:20:29.219649+00	\N	>Why this index is not "covering" for this query:	f	f
323	334	16	2018-10-16 12:05:54+00	The two reasons that I find the most compelling not to use `SELECT *` in SQL Server are\r\n\r\n   1. Memory Grants\r\n   2. Index usage\r\n\r\nMemory Grants\r\n--\r\n\r\nWhen queries need to Sort, Hash, or go Parallel, they ask for memory for those operations. The size of the memory grant is based on the size of the data, both row and column wise. \r\n\r\nString data especially has an impact on this, since the optimizer guesses half of the defined length as the 'fullness' of the column. So for a VARCHAR 100, it's 50 bytes * the number of rows. \r\n\r\nUsing Stack Overflow as an example, if I run these queries against the Users table:\r\n\r\n    SELECT TOP 1000 DisplayName\r\n    FROM dbo.Users AS u\r\n    ORDER BY u.Reputation;\r\n    \r\n    SELECT TOP 1000 DisplayName, u.Location\r\n    FROM dbo.Users AS u\r\n    ORDER BY u.Reputation;\r\n\r\nDisplayName is NVARCHAR 40, and Location is NVARCHAR 100.\r\n\r\nWithout an index on Reputation, SQL Server needs to sort the data on its own.\r\n\r\n[![NUTS][1]][1]\r\n\r\nBut the memory it nearly doubles.\r\n\r\n**DisplayName:**\r\n\r\n[![NUTS][2]][2]\r\n\r\n\r\n**DisplayName, Location:**\r\n\r\n[![NUTS][3]][3]\r\n\r\nThis gets much worse with `SELECT *`, asking for 8.2 GB of memory:\r\n\r\n[![NUTS][4]][4]\r\n\r\nIt does this to cope with the larger amount of data it needs to pass through the Sort operator, including the AboutMe column, which has a MAX length.\r\n\r\n[![NUTS][5]][5]\r\n\r\n\r\nIndex Usage\r\n--\r\n\r\nIf I have this index on the Users table:\r\n\r\n    CREATE NONCLUSTERED INDEX ix_Users\r\n        ON dbo.Users\r\n    (\r\n        CreationDate ASC,\r\n        Reputation ASC,\r\n        Id ASC \r\n     );\r\n\r\nAnd I have this query, with a WHERE clause that matches the index, but doesn't cover/include all the columns the query is selecting...\r\n\r\n\tSELECT u.*, \r\n           p.Id AS [PostId]\r\n\tFROM   dbo.Users AS u\r\n\tJOIN   dbo.Posts AS p\r\n\tON p.OwnerUserId = u.Id\r\n\tWHERE  u.CreationDate > '20171001'\r\n\t       AND u.Reputation > 100\r\n\t       AND p.PostTypeId = 1\r\n    ORDER BY u.Id\r\n\r\nThe optimizer may choose not to use the narrow index with a key lookup, in favor of just scanning the clustered index.\r\n\r\n[![NUTS][6]][6]\r\n\r\nYou would either have to create a very wide index, or [experiment with rewrites][7] to get the narrow index chosen, even though using the narrow index results in a much faster query.\r\n\r\n[![NUTS][8]][8]\r\n\r\nCX:\r\n\r\n     SQL Server Execution Times:\r\n       CPU time = 6374 ms,  elapsed time = 4165 ms.\r\n\r\nNC:\r\n\r\n     SQL Server Execution Times:\r\n       CPU time = 1623 ms,  elapsed time = 875 ms.\r\n\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/T8Gkh.jpg\r\n  [2]: https://i.stack.imgur.com/ZvHW5.jpg\r\n  [3]: https://i.stack.imgur.com/2hW99.jpg\r\n  [4]: https://i.stack.imgur.com/jTxvY.jpg\r\n  [5]: https://i.stack.imgur.com/LJfot.jpg\r\n  [6]: https://i.stack.imgur.com/8qBGD.jpg\r\n  [7]: https://groupby.org/conference-session-abstracts/improving-select-query-performance/\r\n  [8]: https://i.stack.imgur.com/6Z3X4.jpg	2019-12-05 23:57:33.085426+00	3	4	1	220233	0	0	0	2019-12-05 23:57:33.085426+00	\N	The two reasons that I find the most compelling not to use `SELECT *` in SQL Server are	f	f
333	345	96	2015-03-24 07:09:16+00	The LuaLaTeX engine makes some assumptions about file names that XeLaTeX does not. In particular it assumes that all files _should_ have a file extension segment following a period and that if such an extension is not otherwise specified that it _must_ be `.tex`. \r\n\r\nBased on this assumption it is munging the file name and the check `\\IfFileExists{XX-has-music}` is actually executing something like `\\IfFileExists{XX-has-music.tex}` and failing to find the file. The solution to make LuaLaTeX happy is to include at least one period in all file names. Even adjusting this example to use the filename `XX.has-music` will compile in either engine.	2019-12-06 06:43:24.666738+00	0	4	1	234751	0	0	0	2019-12-06 06:43:24.666738+00	\N	The LuaLaTeX engine makes some assumptions about file names that XeLaTeX does not. In particular it assumes that all files _should_ have a file extension segment following a period and that if such an extension is not otherwise specified that it _must_ be `.tex`.	f	f
103	148	12	2016-04-22 10:10:24+00	The semantics of the two statements are different:\r\n\r\n* The first does not set the value of the variable if no row is found.\r\n* The second always sets the variable, including to null if no row is found.\r\n\r\nThe Constant Scan produces an empty row (with no columns!) that will result in the variable being updated in case nothing matches from the base table. The left join ensures the empty row survives the join. Variable assignment can be thought of as happening at the root node of the execution plan.\r\n\r\n## Using `SELECT @result`\r\n\r\n    -- Set initial value\r\n    DECLARE @result uniqueidentifier = {guid 'FE2CA909-1162-4C6C-A7AC-33B257E28539'};\r\n\r\n    -- @result does not change\r\n    SELECT @result = AccountId \r\n    FROM Accounts \r\n    WHERE AccountId={guid '7AD4D33C-1ED7-4183-B7F3-48C33D666525'};\r\n\r\n    SELECT @result;\r\n\r\n[![Result 1][1]][1]\r\n\r\n## Using `SET @result`\r\n\r\n    -- Set initial value\r\n    DECLARE @result uniqueidentifier = {guid 'FE2CA909-1162-4C6C-A7AC-33B257E28539'};\r\n\r\n    -- @result set to null\r\n    SET @result = \r\n    (\r\n        SELECT AccountId \r\n        FROM Accounts \r\n        WHERE AccountId={guid '7AD4D33C-1ED7-4183-B7F3-48C33D666525'}\r\n    );\r\n\r\n    SELECT @result;\r\n\r\n[![Result 2][2]][2]\r\n\r\n### Execution plans\r\n\r\n[![SELECT assignment][3]][3]\r\n*No row arrives at the root node, so no assignment occurs.*\r\n\r\n[![SET assignment][4]][4]\r\n*A row always arrives at the root node, so variable assignment occurs.*\r\n\r\n---\r\n\r\nThe extra Constant Scan and Nested Loops Left Outer Join are nothing to be concerned about. The join in particular is cheap since it is guaranteed to encounter one row on its outer input, and at most one row (in your example) on the inner input.\r\n\r\nThere are other ways to ensure a row is generated from the subquery to ensure a variable assignment occurs. One is to use a redundant scalar aggregate (no group by clause):\r\n\r\n    -- Set initial value\r\n    DECLARE @result uniqueidentifier = {guid 'FE2CA909-1162-4C6C-A7AC-33B257E28539'};\r\n\r\n    -- @result set to null\r\n    SET @result = \r\n        (\r\n            SELECT MAX(AccountId)\r\n            FROM Accounts \r\n            WHERE AccountId={guid '7AD4D33C-1ED7-4183-B7F3-48C33D666525'} \r\n        );\r\n    SELECT @result;\r\n\r\n[![Result 3][5]][5]\r\n\r\n[![Scalar aggregate execution plan][6]][6]\r\n\r\nNotice the scalar aggregate produces a row even though it receives no input.\r\n\r\nDocumentation:\r\n\r\n* [SET @local_variable (Transact-SQL)][7]  \r\n* [SELECT @local_variable (Transact-SQL)][8]\r\n\r\n> If the SELECT statement returns no rows, the variable retains its present value. If expression is a scalar subquery that returns no value, the variable is set to NULL.\r\n\r\n> For assigning variables, we recommend that you use SET @local_variable instead of SELECT @local_variable.\r\n\r\nFurther reading:\r\n\r\n* [Fun with Scalar and Vector Aggregates][9]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/p1Qxy.png\r\n  [2]: https://i.stack.imgur.com/UaxFU.png\r\n  [3]: https://i.stack.imgur.com/apLml.png\r\n  [4]: https://i.stack.imgur.com/aSK8U.png\r\n  [5]: https://i.stack.imgur.com/v4HAW.png\r\n  [6]: https://i.stack.imgur.com/3PWRH.png\r\n  [7]: https://msdn.microsoft.com/en-NZ/library/ms189484.aspx\r\n  [8]: https://msdn.microsoft.com/en-us/library/ms187330.aspx\r\n  [9]: https://www.sql.kiwi/2012/03/fun-with-aggregates.html	2019-12-11 01:28:45.973982+00	2	4	1	136212	0	0	0	2019-11-27 10:14:50.349235+00	\N	The semantics of the two statements are different:	f	f
113	179	94	2012-08-15 03:36:39+00	### Setup\r\nI am building on **@Jack's setup** to make it easier for people to follow and compare. Tested with **PostgreSQL 9.1.4**.\r\n\r\n    CREATE TABLE lexikon (\r\n       lex_id    serial PRIMARY KEY\r\n     , word      text\r\n     , frequency int NOT NULL  -- we'd need to do more if NULL was allowed\r\n     , lset      int\r\n    );\r\n    \r\n    INSERT INTO lexikon(word, frequency, lset) \r\n    SELECT 'w' || g  -- shorter with just 'w'\r\n         , (1000000 / row_number() OVER (ORDER BY random()))::int\r\n         , g\r\n    FROM   generate_series(1,1000000) g\r\n\r\nFrom here on I take a different route:\r\n\r\n    ANALYZE lexikon;\r\n\r\n### Auxiliary table\r\nThis solution does not add columns to the original table, it just needs a tiny helper table. I placed it in the schema `public`, use any schema of your choice.\r\n\r\n    CREATE TABLE public.lex_freq AS\r\n    WITH x AS (\r\n       SELECT DISTINCT ON (f.row_min)\r\n              f.row_min, c.row_ct, c.frequency\r\n       FROM  (\r\n          SELECT frequency, sum(count(*)) OVER (ORDER BY frequency DESC) AS row_ct\r\n          FROM   lexikon\r\n          GROUP  BY 1\r\n          ) c\r\n       JOIN  (                                   -- list of steps in recursive search\r\n          VALUES (400),(1600),(6400),(25000),(100000),(200000),(400000),(600000),(800000)\r\n          ) f(row_min) ON c.row_ct >= f.row_min  -- match next greater number\r\n       ORDER  BY f.row_min, c.row_ct, c.frequency DESC\r\n       )\r\n    , y AS (   \r\n       SELECT DISTINCT ON (frequency)\r\n              row_min, row_ct, frequency AS freq_min\r\n            , lag(frequency) OVER (ORDER BY row_min) AS freq_max\r\n       FROM   x\r\n       ORDER  BY frequency, row_min\r\n       -- if one frequency spans multiple ranges, pick the lowest row_min\r\n       )\r\n    SELECT row_min, row_ct, freq_min\r\n         , CASE freq_min <= freq_max\r\n             WHEN TRUE  THEN 'frequency >= ' || freq_min || ' AND frequency < ' || freq_max\r\n             WHEN FALSE THEN 'frequency  = ' || freq_min\r\n             ELSE            'frequency >= ' || freq_min\r\n           END AS cond\r\n    FROM   y\r\n    ORDER  BY row_min;\r\n\r\nTable looks like this:\r\n\r\n    row_min | row_ct  | freq_min | cond\r\n    --------+---------+----------+-------------\r\n    400     | 400     | 2500     | frequency >= 2500\r\n    1600    | 1600    | 625      | frequency >= 625 AND frequency < 2500\r\n    6400    | 6410    | 156      | frequency >= 156 AND frequency < 625\r\n    25000   | 25000   | 40       | frequency >= 40 AND frequency < 156\r\n    100000  | 100000  | 10       | frequency >= 10 AND frequency < 40\r\n    200000  | 200000  | 5        | frequency >= 5 AND frequency < 10\r\n    400000  | 500000  | 2        | frequency >= 2 AND frequency < 5\r\n    600000  | 1000000 | 1        | frequency  = 1\r\n\r\nAs the column `cond` is going to be used in dynamic SQL further down, you have to make this table ***secure***. Always schema-qualify the table if you cannot be sure of an appropriate current `search_path`, and revoke write privileges from `public` (and any other untrusted role):\r\n\r\n    REVOKE ALL ON public.lex_freq FROM public;\r\n    GRANT SELECT ON public.lex_freq TO public;\r\n\r\nThe table `lex_freq` serves three purposes:\r\n\r\n- Create needed [partial indexes][1] automatically.\r\n- Provide steps for iterative function.\r\n- Meta information for tuning.\r\n\r\n### Indexes\r\nThis `DO` statement creates *all* needed indexes:\r\n\r\n    DO\r\n    $$\r\n    DECLARE\r\n       _cond text;\r\n    BEGIN\r\n       FOR _cond IN\r\n          SELECT cond FROM public.lex_freq\r\n       LOOP\r\n          IF _cond LIKE 'frequency =%' THEN\r\n             EXECUTE 'CREATE INDEX ON lexikon(lset) WHERE ' || _cond;\r\n          ELSE\r\n             EXECUTE 'CREATE INDEX ON lexikon(lset, frequency DESC) WHERE ' || _cond;\r\n          END IF;\r\n       END LOOP;\r\n    END\r\n    $$\r\n\r\nAll of these **partial** indexes together span the table once. They are about the same size as one basic index on the whole table:\r\n\r\n    SELECT pg_size_pretty(pg_relation_size('lexikon'));       -- 50 MB\r\n    SELECT pg_size_pretty(pg_total_relation_size('lexikon')); -- 71 MB\r\n\r\nOnly 21 MB of indexes for 50 MB table so far.\r\n\r\nI create most of the partial indexes on `(lset, frequency DESC)`. The second column only helps in special cases. But as both involved columns are of type `integer`, due to the specifics of data [alignment in combination with MAXALIGN][2] in PostgreSQL, the second column does not make the index any bigger. It's a small win for hardly any cost.\r\n\r\nThere is no point in doing that for partial indexes that only span a single frequency. Those are just on `(lset)`. Created indexes look like this:\r\n\r\n    CREATE INDEX ON lexikon(lset, frequency DESC) WHERE frequency >= 2500;\r\n    CREATE INDEX ON lexikon(lset, frequency DESC) WHERE frequency >= 625 AND frequency < 2500;\r\n    -- ...\r\n    CREATE INDEX ON lexikon(lset, frequency DESC) WHERE frequency >= 2 AND frequency < 5;\r\n    CREATE INDEX ON lexikon(lset) WHERE freqency = 1;\r\n\r\n### Function\r\nThe function is somewhat similar in style to @Jack's solution:\r\n\r\n    CREATE OR REPLACE FUNCTION f_search(_lset_min int, _lset_max int, _limit int)\r\n      RETURNS SETOF lexikon\r\n    $func$\r\n    DECLARE\r\n       _n      int;\r\n       _rest   int := _limit;   -- init with _limit param\r\n       _cond   text;\r\n    BEGIN \r\n       FOR _cond IN\r\n          SELECT l.cond FROM public.lex_freq l ORDER BY l.row_min\r\n       LOOP    \r\n          --  RAISE NOTICE '_cond: %, _limit: %', _cond, _rest; -- for debugging\r\n          RETURN QUERY EXECUTE '\r\n             SELECT * \r\n             FROM   public.lexikon \r\n             WHERE  ' || _cond || '\r\n             AND    lset >= $1\r\n             AND    lset <= $2\r\n             ORDER  BY frequency DESC\r\n             LIMIT  $3'\r\n          USING  _lset_min, _lset_max, _rest;\r\n        \r\n          GET DIAGNOSTICS _n = ROW_COUNT;\r\n          _rest := _rest - _n;\r\n          EXIT WHEN _rest < 1;\r\n       END LOOP;\r\n    END\r\n    $func$ LANGUAGE plpgsql STABLE;\r\n\r\nKey differences:\r\n\r\n- **dynamic SQL** with `RETURN QUERY EXECUTE`.  \r\nAs we loop through the steps, a different query plan may be beneficiary. The query plan for static SQL is generated once and then reused - which can save some overhead. But in this case the query is simple and the values are very different. Dynamic SQL will be a big win.\r\n\r\n- **Dynamic `LIMIT`** for every query step.  \r\nThis helps in multiple ways: First, rows are only fetched as needed. In combination with dynamic SQL this may also generate different query plans to begin with. Second: No need for an additional `LIMIT` in the function call to trim the surplus.\r\n\r\n## Benchmark\r\n### Setup\r\nI picked four examples and ran three different tests with each. I took the best of five to compare with warm cache:\r\n\r\n1. The raw SQL query of the form:\r\n\r\n        SELECT * \r\n        FROM   lexikon \r\n        WHERE  lset >= 20000\r\n        AND    lset <= 30000\r\n        ORDER  BY frequency DESC\r\n        LIMIT  5;\r\n\r\n2. The same after creating this index\r\n\r\n        CREATE INDEX ON lexikon(lset);\r\n\r\n Needs about the same space as all my partial indexes together:\r\n\r\n        SELECT pg_size_pretty(pg_total_relation_size('lexikon')) -- 93 MB\r\n\r\n3. The function\r\n\r\n        SELECT * FROM f_search(20000, 30000, 5);\r\n\r\n### Results\r\n\r\n```\r\nSELECT * FROM f_search(<b>20000, 30000, 5</b>);\r\n```\r\n```\r\n1: Total runtime: 315.458 ms  \r\n2: Total runtime: 36.458 ms  \r\n3: Total runtime: **0.330 ms**\r\n```\r\n\r\n```\r\nSELECT * FROM f_search(<b>60000, 65000, 100</b>);\r\n```\r\n```\r\n1: Total runtime: 294.819 ms  \r\n2: Total runtime: 18.915 ms  \r\n3: Total runtime: **1.414 ms**\r\n```\r\n\r\n```\r\nSELECT * FROM f_search(<b>10000, 70000, 100</b>);\r\n```\r\n```\r\n1: Total runtime: 426.831 ms  \r\n2: Total runtime: 217.874 ms  \r\n3: Total runtime: **1.611 ms**\r\n```\r\n\r\n```\r\nSELECT * FROM f_search(<b>1, 1000000, 5</b>);\r\n```\r\n```\r\n1: Total runtime: 2458.205 ms  \r\n2: Total runtime: 2458.205 ms  -- for large ranges of lset, seq scan is faster than index.  \r\n3: Total runtime: **0.266 ms**\r\n```\r\n\r\n### Conclusion\r\n\r\nAs expected, the benefit from the function grows with bigger ranges of `lset` and smaller `LIMIT`.  \r\n\r\nWith **very small ranges of `lset`**, the raw query in combination with the index is actually *faster*. You'll want to test and maybe branch: raw query for small ranges of `lset`, else function call. You could even just **build that into the function** for a "best of both worlds" - that's what I would do.\r\n\r\nDepending on your data distribution and typical queries, more steps in `lex_freq` may help performance. Test to find the sweet spot. With the tools presented here, it should be easy to test.\r\n\r\n\r\n  [1]: https://www.postgresql.org/docs/current/static/indexes-partial.html\r\n  [2]: https://stackoverflow.com/questions/2966524/calculating-and-saving-space-in-postgresql/7431468#7431468	2019-11-27 23:30:47.422147+00	1	4	1	22500	0	0	0	2019-11-27 23:25:25.956636+00	\N	Setup	f	f
114	179	2	2012-08-10 14:21:45+00	\r\nYou may be able to achieve better performance by searching first in rows with higher frequencies. This can be achieved by 'granulating' the frequencies and then stepping through them procedurally, for example as follows:\r\n\r\n--testbed and `lexikon` dummy data: \r\n\r\n    begin;\r\n    set role dba;\r\n    create role stack;\r\n    grant stack to dba;\r\n    create schema authorization stack;\r\n    set role stack;\r\n    --\r\n    create table lexikon( _id serial, \r\n                          word text, \r\n                          frequency integer, \r\n                          lset integer, \r\n                          width_granule integer);\r\n    --\r\n    insert into lexikon(word, frequency, lset) \r\n    select word, (1000000/row_number() over(order by random()))::integer as frequency, lset\r\n    from (select 'word'||generate_series(1,1000000) word, generate_series(1,1000000) lset) z;\r\n    --\r\n    update lexikon set width_granule=ln(frequency)::integer;\r\n    --\r\n    create index on lexikon(width_granule, lset);\r\n    create index on lexikon(lset);\r\n    -- the second index is not used with the function but is added to make the timings 'fair'\r\n\r\n`granule` analysis (mostly for information and tuning):\r\n\r\n    create table granule as \r\n    select width_granule, count(*) as freq, \r\n           min(frequency) as granule_start, max(frequency) as granule_end \r\n    from lexikon group by width_granule;\r\n    --\r\n    select * from granule order by 1;\r\n    /*\r\n     width_granule |  freq  | granule_start | granule_end\r\n    ---------------+--------+---------------+-------------\r\n                 0 | 500000 |             1 |           1\r\n                 1 | 300000 |             2 |           4\r\n                 2 | 123077 |             5 |          12\r\n                 3 |  47512 |            13 |          33\r\n                 4 |  18422 |            34 |          90\r\n                 5 |   6908 |            91 |         244\r\n                 6 |   2580 |           245 |         665\r\n                 7 |    949 |           666 |        1808\r\n                 8 |    349 |          1811 |        4901\r\n                 9 |    129 |          4926 |       13333\r\n                10 |     47 |         13513 |       35714\r\n                11 |     17 |         37037 |       90909\r\n                12 |      7 |        100000 |      250000\r\n                13 |      2 |        333333 |      500000\r\n                14 |      1 |       1000000 |     1000000\r\n    */\r\n    alter table granule drop column freq;\r\n    --\r\n\r\nfunction for scanning high frequencies first:\r\n\r\n    create function f(p_lset_low in integer, p_lset_high in integer, p_limit in integer)\r\n           returns setof lexikon language plpgsql set search_path to 'stack' as $$\r\n    declare\r\n      m integer;\r\n      n integer := 0;\r\n      r record;\r\n    begin \r\n      for r in (select width_granule from granule order by width_granule desc) loop\r\n        return query( select * \r\n                      from lexikon \r\n                      where width_granule=r.width_granule \r\n                            and lset>=p_lset_low and lset<=p_lset_high );\r\n        get diagnostics m = row_count;\r\n        n = n+m;\r\n        exit when n>=p_limit;\r\n      end loop;\r\n    end;$$;\r\n\r\nresults (timings should probably be taken with a pinch of salt but each query is run twice to counter any caching) \r\n\r\nfirst using the function we've written:\r\n\r\n    \\timing on\r\n    --\r\n    select * from f(20000, 30000, 5) order by frequency desc limit 5;\r\n    /*\r\n     _id |   word    | frequency | lset  | width_granule\r\n    -----+-----------+-----------+-------+---------------\r\n     141 | word23237 |      7092 | 23237 |             9\r\n     246 | word25112 |      4065 | 25112 |             8\r\n     275 | word23825 |      3636 | 23825 |             8\r\n     409 | word28660 |      2444 | 28660 |             8\r\n     418 | word29923 |      2392 | 29923 |             8\r\n    Time: 80.452 ms\r\n    */\r\n    select * from f(20000, 30000, 5) order by frequency desc limit 5;\r\n    /*\r\n     _id |   word    | frequency | lset  | width_granule\r\n    -----+-----------+-----------+-------+---------------\r\n     141 | word23237 |      7092 | 23237 |             9\r\n     246 | word25112 |      4065 | 25112 |             8\r\n     275 | word23825 |      3636 | 23825 |             8\r\n     409 | word28660 |      2444 | 28660 |             8\r\n     418 | word29923 |      2392 | 29923 |             8\r\n    Time: 0.510 ms\r\n    */\r\n\r\nand then with a simple index scan:\r\n\r\n    select * from lexikon where lset between 20000 and 30000 order by frequency desc limit 5;\r\n    /*\r\n     _id |   word    | frequency | lset  | width_granule\r\n    -----+-----------+-----------+-------+---------------\r\n     141 | word23237 |      7092 | 23237 |             9\r\n     246 | word25112 |      4065 | 25112 |             8\r\n     275 | word23825 |      3636 | 23825 |             8\r\n     409 | word28660 |      2444 | 28660 |             8\r\n     418 | word29923 |      2392 | 29923 |             8\r\n    Time: 218.897 ms\r\n    */\r\n    select * from lexikon where lset between 20000 and 30000 order by frequency desc limit 5;\r\n    /*\r\n     _id |   word    | frequency | lset  | width_granule\r\n    -----+-----------+-----------+-------+---------------\r\n     141 | word23237 |      7092 | 23237 |             9\r\n     246 | word25112 |      4065 | 25112 |             8\r\n     275 | word23825 |      3636 | 23825 |             8\r\n     409 | word28660 |      2444 | 28660 |             8\r\n     418 | word29923 |      2392 | 29923 |             8\r\n    Time: 51.250 ms\r\n    */\r\n    \\timing off\r\n    --\r\n    rollback;\r\n\r\nDepending on your real-world data, you will probably want to vary the number of granules and the function used for putting rows into them. The actual distribution of frequencies is key here, as is the expected values for the `limit` clause and size of `lset` ranges sought.\r\n	2019-11-27 23:25:26.219964+00	0	4	1	22295	0	0	0	2019-11-27 23:25:26.219964+00	\N	\nYou may be able to achieve better performance by searching first in rows with higher frequencies. This can be achieved by 'granulating' the frequencies and then stepping through them procedurally, for example as follows:	f	f
223	269	16	2019-03-28 23:15:23+00	Sup?\r\n--\r\nFor SQL Server queries that require additional memory, grants are derived for serial plans. If a parallel plan is explored and chosen, memory will be divided evenly among threads.\r\n\r\nMemory grant estimates are based on:\r\n\r\n - Number of rows (cardinality)\r\n - Size of rows (data size)\r\n - Number of concurrent memory consuming operators\r\n \r\nIf a parallel plan is chosen, there is some memory overhead to process parallel exchanges (distribute, redistribute, and gather streams), however their memory needs are still not calculated the same way.\r\n\r\nMemory Consuming Operators\r\n--\r\nThe most common operators that ask for memory are\r\n\r\n - Sorts\r\n - Hashes (joins, aggregates)\r\n - Optimized Nested Loops\r\n\r\nLess common operators that require memory are inserts to column store indexes. These also differ in that memory grants are currently multiplied by DOP for them.\r\n\r\nMemory needs for Sorts are typically much higher than for hashes. Sorts will ask for at least _estimated size of data_ for a memory grant, since they need to sort all result columns by the ordering element(s). Hashes need memory to build a hash table, which does not include all selected columns.\r\n\r\nExamples\r\n--\r\n\r\nIf I run this query, intentionally hinted to DOP 1, it will ask for 166 MB of memory.\r\n\r\n    SELECT *\r\n    FROM \r\n         (  \r\n    \t    SELECT TOP (1000) \r\n    \t           u.Id \r\n    \t\tFROM dbo.Users AS u\r\n    \t\tORDER BY u.Reputation\r\n    \t ) AS u\r\n    OPTION(MAXDOP 1);\r\n\r\n[![NUTS][1]][1]\r\n\r\nIf I run this query (again, DOP 1), the plan will change, and the memory grant will go up slightly.\r\n\r\n    SELECT *\r\n    FROM (  \r\n    \t    SELECT TOP (1000) \r\n    \t           u.Id\r\n    \t\tFROM dbo.Users AS u\r\n    \t\tORDER BY u.Reputation\r\n    \t ) AS u\r\n    JOIN (\r\n    \t\tSELECT TOP (1000) \r\n    \t\t       u.Id\r\n    \t\tFROM dbo.Users AS u\r\n    \t\tORDER BY u.Reputation\r\n    \t ) AS u2\r\n    ON u.Id = u2.Id\r\n    OPTION(MAXDOP 1);\r\n\r\n[![NUTS][2]][2]\r\n\r\nThere are two Sorts, and now a Hash Join. The memory grant bumps up a little bit to accommodate the hash build, but it does not double because the Sort operators cannot run concurrently. \r\n\r\nIf I change the query to force a nested loops join, the grant will double to deal with the concurrent Sorts.\r\n\r\n    SELECT *\r\n    FROM (  \r\n    \t    SELECT TOP (1000) \r\n    \t           u.Id\r\n    \t\tFROM dbo.Users AS u\r\n    \t\tORDER BY u.Reputation\r\n    \t ) AS u\r\n    INNER LOOP JOIN ( --Force the loop join\r\n    \t\tSELECT TOP (1000) \r\n    \t\t       u.Id\r\n    \t\tFROM dbo.Users AS u\r\n    \t\tORDER BY u.Reputation\r\n    \t ) AS u2\r\n    ON u.Id = u2.Id\r\n    OPTION(MAXDOP 1);\r\n\r\n\r\n[![NUTS][3]][3]\r\n\r\nThe memory grant doubles because Nested Loop is not a blocking operator, and Hash Join is.\r\n\r\nSize Of Data Matters\r\n--\r\nThis query selects string data of different combinations. Depending on which columns I select, the size of the memory grant will go up.\r\n\r\nThe way size of data is calculated for variable string data is rows * 50% of the column's declared length. This is true for VARCHAR and NVARCHAR, though NVARCHAR columns are doubled since they store double-byte characters. This does change in some cases with the new CE, but details aren't documented.\r\n\r\nSize of data also matters for hash operations, but not to the same degree that it does for Sorts.\r\n\r\n\r\n    SELECT *\r\n    FROM \r\n         (  \r\n    \t    SELECT TOP (1000) \r\n    \t             u.Id          -- 166MB (INT)\r\n                   , u.DisplayName -- 300MB (NVARCHAR 40)\r\n                   , u.WebsiteUrl  -- 900MB (NVARCHAR 200)\r\n                   , u.Location    -- 1.2GB (NVARCHAR 100)\r\n                   , u.AboutMe     -- 9GB   (NVARCHAR MAX)\r\n    \t\tFROM dbo.Users AS u\r\n    \t\tORDER BY u.Reputation\r\n    \t ) AS u\r\n    OPTION(MAXDOP 1);\r\n\r\nBut What About Parallelism?\r\n--\r\n\r\nIf I run this query at different DOPs, the memory grant is not multiplied by DOP.\r\n\r\n    SELECT *\r\n    FROM (  \r\n    \t    SELECT TOP (1000) \r\n    \t           u.Id\r\n    \t\tFROM dbo.Users AS u\r\n    \t\tORDER BY u.Reputation\r\n    \t ) AS u\r\n    INNER HASH JOIN (\r\n    \t\tSELECT TOP (1000) \r\n    \t\t       u.Id\r\n    \t\tFROM dbo.Users AS u\r\n    \t\tORDER BY u.Reputation\r\n    \t ) AS u2\r\n    ON u.Id = u2.Id\r\n    ORDER BY u.Id, u2.Id -- Add an ORDER BY\r\n    OPTION(MAXDOP ?);\r\n\r\n[![NUTS][4]][4]\r\n\r\nThere are slight increases to deal with more parallel buffers per exchange operator, and perhaps there are internal reasons that the Sort and Hash builds require extra memory to deal with higher DOP, but it's clearly not a multiplying factor.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/gOzLn.png\r\n  [2]: https://i.stack.imgur.com/07lPf.png\r\n  [3]: https://i.stack.imgur.com/kiBHV.png\r\n  [4]: https://i.stack.imgur.com/mf1jY.png	2019-12-04 14:13:58.358066+00	2	4	1	233446	0	0	0	2019-12-04 14:13:58.358066+00	\N	Sup?	f	f
72	94	12	2019-11-24 10:18:25.647067+00	The `MERGE` statement has a complex syntax and an even more complex implementation, but essentially the idea is to join two tables, filter down to rows that need to be changed (inserted, updated, or deleted), and then to perform the requested changes. Given the following sample data:\r\n\r\n```sql\r\nDECLARE @CategoryItem AS TABLE\r\n(\r\n    CategoryId  integer NOT NULL,\r\n    ItemId      integer NOT NULL,\r\n    \r\n    PRIMARY KEY (CategoryId, ItemId),\r\n    UNIQUE (ItemId, CategoryId)\r\n);\r\n    \r\nDECLARE @DataSource AS TABLE\r\n(\r\n    CategoryId  integer NOT NULL,\r\n    ItemId      integer NOT NULL\r\n    \r\n    PRIMARY KEY (CategoryId, ItemId)\r\n);\r\n    \r\nINSERT @CategoryItem\r\n    (CategoryId, ItemId)\r\nVALUES\r\n    (1, 1),\r\n    (1, 2),\r\n    (1, 3),\r\n    (2, 1),\r\n    (2, 3),\r\n    (3, 5),\r\n    (3, 6),\r\n    (4, 5);\r\n    \r\nINSERT @DataSource\r\n    (CategoryId, ItemId)\r\nVALUES\r\n    (2, 2);\r\n```\r\n\r\n### Target\r\n\r\n    ╔════════════╦════════╗\r\n    ║ CategoryId ║ ItemId ║\r\n    ╠════════════╬════════╣\r\n    ║          1 ║      1 ║\r\n    ║          2 ║      1 ║\r\n    ║          1 ║      2 ║\r\n    ║          1 ║      3 ║\r\n    ║          2 ║      3 ║\r\n    ║          3 ║      5 ║\r\n    ║          4 ║      5 ║\r\n    ║          3 ║      6 ║\r\n    ╚════════════╩════════╝\r\n\r\n### Source\r\n\r\n    ╔════════════╦════════╗\r\n    ║ CategoryId ║ ItemId ║\r\n    ╠════════════╬════════╣\r\n    ║          2 ║      2 ║\r\n    ╚════════════╩════════╝\r\n\r\nThe desired outcome is to replace data in the target with data from the source, but only for `CategoryId = 2`. Following the description of `MERGE` given above, we should write a query that joins the source and target on the keys only, and filter rows only in the `WHEN` clauses:\r\n\r\n```sql\r\nMERGE INTO @CategoryItem AS TARGET\r\nUSING @DataSource AS SOURCE ON \r\n    SOURCE.ItemId = TARGET.ItemId \r\n    AND SOURCE.CategoryId = TARGET.CategoryId\r\nWHEN NOT MATCHED BY SOURCE \r\n    AND TARGET.CategoryId = 2 \r\n    THEN DELETE\r\nWHEN NOT MATCHED BY TARGET \r\n    AND SOURCE.CategoryId = 2 \r\n    THEN INSERT (CategoryId, ItemId)\r\n        VALUES (CategoryId, ItemId)\r\nOUTPUT \r\n    $ACTION, \r\n    ISNULL(INSERTED.CategoryId, DELETED.CategoryId) AS CategoryId,\r\n    ISNULL(INSERTED.ItemId, DELETED.ItemId) AS ItemId\r\n;\r\n```\r\n\r\nThis gives the following results:\r\n\r\n    ╔═════════╦════════════╦════════╗\r\n    ║ $ACTION ║ CategoryId ║ ItemId ║\r\n    ╠═════════╬════════════╬════════╣\r\n    ║ DELETE  ║          2 ║      1 ║\r\n    ║ INSERT  ║          2 ║      2 ║\r\n    ║ DELETE  ║          2 ║      3 ║\r\n    ╚═════════╩════════════╩════════╝\r\n    ╔════════════╦════════╗\r\n    ║ CategoryId ║ ItemId ║\r\n    ╠════════════╬════════╣\r\n    ║          1 ║      1 ║\r\n    ║          1 ║      2 ║\r\n    ║          1 ║      3 ║\r\n    ║          2 ║      2 ║\r\n    ║          3 ║      5 ║\r\n    ║          3 ║      6 ║\r\n    ║          4 ║      5 ║\r\n    ╚════════════╩════════╝\r\n\r\nThe execution plan is:\r\n\r\n![Merge plan][1]\r\n\r\nNotice both tables are scanned fully. We might think this inefficient, because only rows where `CategoryId = 2` will be affected in the target table. This is where the warnings in Books Online come in. One misguided attempt to optimize to touch only necessary rows in the target is:\r\n\r\n```sql\r\nMERGE INTO @CategoryItem AS TARGET\r\nUSING \r\n(\r\n    SELECT CategoryId, ItemId\r\n    FROM @DataSource AS ds \r\n    WHERE CategoryId = 2\r\n) AS SOURCE ON\r\n    SOURCE.ItemId = TARGET.ItemId\r\n    AND TARGET.CategoryId = 2\r\nWHEN NOT MATCHED BY TARGET THEN\r\n    INSERT (CategoryId, ItemId)\r\n    VALUES (CategoryId, ItemId)\r\nWHEN NOT MATCHED BY SOURCE THEN\r\n    DELETE\r\nOUTPUT \r\n    $ACTION, \r\n    ISNULL(INSERTED.CategoryId, DELETED.CategoryId) AS CategoryId,\r\n    ISNULL(INSERTED.ItemId, DELETED.ItemId) AS ItemId\r\n;\r\n```\r\n\r\nThe logic in the `ON` clause is applied as part of the join. In this case, the join is a full outer join (see [this Books Online entry][2] for why). Applying the check for category 2 on the target rows as part of an outer join ultimately results in rows with a different value being deleted (because they do not match the source):\r\n\r\n    ╔═════════╦════════════╦════════╗\r\n    ║ $ACTION ║ CategoryId ║ ItemId ║\r\n    ╠═════════╬════════════╬════════╣\r\n    ║ DELETE  ║          1 ║      1 ║\r\n    ║ DELETE  ║          1 ║      2 ║\r\n    ║ DELETE  ║          1 ║      3 ║\r\n    ║ DELETE  ║          2 ║      1 ║\r\n    ║ INSERT  ║          2 ║      2 ║\r\n    ║ DELETE  ║          2 ║      3 ║\r\n    ║ DELETE  ║          3 ║      5 ║\r\n    ║ DELETE  ║          3 ║      6 ║\r\n    ║ DELETE  ║          4 ║      5 ║\r\n    ╚═════════╩════════════╩════════╝\r\n\r\n    ╔════════════╦════════╗\r\n    ║ CategoryId ║ ItemId ║\r\n    ╠════════════╬════════╣\r\n    ║          2 ║      2 ║\r\n    ╚════════════╩════════╝\r\n\r\nThe root cause is the same reason predicates behave differently in an outer join `ON` clause than they do if specified in the `WHERE` clause. The `MERGE` syntax (and the join implementation depending on the clauses specified) just make it harder to see that this is so.\r\n\r\nThe [guidance in Books Online][3] (expanded in the [Optimizing Performance][4] entry) offers guidance that will ensure the correct semantic is expressed using `MERGE` syntax, without the user necessarily having to understand all the implementation details, or account for the ways in which the optimizer might legitimately rearrange things for execution efficiency reasons.\r\n\r\nThe documentation offers three potential ways to implement early filtering:\r\n\r\n* **Specifying a filtering condition in the `WHEN` clause** guarantees correct results, but may mean that more rows are read and processed from the source and target tables than is strictly necessary (as seen in the first example).\r\n\r\n* **Updating through a view** that contains the filtering condition also guarantees correct results (since changed rows must be accessible for update through the view) but this does require a dedicated view, and one that follows the odd conditions for updating views.\r\n\r\n* **Using a common table expression** carries similar risks to adding predicates to the `ON` clause, but for slightly different reasons. In many cases it will be safe, but it requires expert analysis of the execution plan to confirm this (and extensive practical testing). For example:\r\n\r\n```sql\r\nWITH TARGET AS \r\n(\r\n    SELECT * \r\n    FROM @CategoryItem\r\n    WHERE CategoryId = 2\r\n)\r\nMERGE INTO TARGET\r\nUSING \r\n(\r\n    SELECT CategoryId, ItemId\r\n    FROM @DataSource\r\n    WHERE CategoryId = 2\r\n) AS SOURCE ON\r\n    SOURCE.ItemId = TARGET.ItemId\r\n    AND SOURCE.CategoryId = TARGET.CategoryId\r\nWHEN NOT MATCHED BY TARGET THEN\r\n    INSERT (CategoryId, ItemId)\r\n    VALUES (CategoryId, ItemId)\r\nWHEN NOT MATCHED BY SOURCE THEN\r\n    DELETE\r\nOUTPUT \r\n    $ACTION, \r\n    ISNULL(INSERTED.CategoryId, DELETED.CategoryId) AS CategoryId,\r\n    ISNULL(INSERTED.ItemId, DELETED.ItemId) AS ItemId\r\n;\r\n```\r\n\r\nThis produces correct results (not repeated) with a more optimal plan:\r\n\r\n![Merge plan 2][5]\r\n\r\nThe plan only reads rows for category 2 from the target table. This might be an important performance consideration if the target table is large, but it is all too easy to get this wrong using `MERGE` syntax.\r\n\r\nSometimes, it is easier to write the `MERGE` as separate DML operations. This approach can even **perform better** than a single `MERGE`, a fact which often surprises people.\r\n\r\n```sql\r\nDELETE ci\r\nFROM @CategoryItem AS ci\r\nWHERE ci.CategoryId = 2\r\nAND NOT EXISTS \r\n(\r\n    SELECT 1 \r\n    FROM @DataSource AS ds \r\n    WHERE \r\n        ds.ItemId = ci.ItemId\r\n        AND ds.CategoryId = ci.CategoryId\r\n);\r\n    \r\nINSERT @CategoryItem\r\nSELECT \r\n    ds.CategoryId, \r\n    ds.ItemId\r\nFROM @DataSource AS ds\r\nWHERE\r\n    ds.CategoryId = 2\r\n;\r\n```\r\n\r\n  [1]: https://i.stack.imgur.com/foVhU.jpg\r\n  [2]: https://technet.microsoft.com/en-us/library/bb522522%28v=sql.105%29.aspx\r\n  [3]: https://technet.microsoft.com/en-us/library/bb510625%28v=sql.105%29.aspx\r\n  [4]: https://technet.microsoft.com/en-us/library/cc879317%28v=sql.105%29.aspx\r\n  [5]: https://i.stack.imgur.com/taZmk.jpg\r\n\r\n\r\n	2019-11-24 10:23:36.179354+00	3	4	2	\N	0	0	0	\N	\N	The `MERGE` statement has a complex syntax and an even more complex implementation, but essentially the idea is to join two tables, filter down to rows that need to be changed (inserted, updated, or deleted), and then to perform the requested changes. Given the following sample data:	f	f
65	90	88	2013-08-18 01:13:01+00	Answer by [Aaron Bertrand](https://dba.stackexchange.com/users/1186/aaron-bertrand) from [dba.stackexchange.com](https://dba.stackexchange.com/a/48296) under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0):\r\n\r\n---\r\n\r\nSince I'm not really satisfied with any of the answers [over on Stack Overflow](https://stackoverflow.com/questions/56628/how-do-you-clear-the-sql-server-transaction-log/), including the most heavily up-voted suggestion, and because there are a few things I'd like to address that Mike's answer does not, I thought I would provide my input here too. I placed a copy of this answer there as well.\r\n\r\nMaking a log file smaller should really be reserved for scenarios where it encountered unexpected growth which you do not expect to happen again. If the log file will grow to the same size again, not very much is accomplished by shrinking it temporarily. Now, depending on the recovery goals of your database, these are the actions you should take.\r\n\r\n# First, take a full backup\r\n\r\nNever make any changes to your database without ensuring you can restore it should something go wrong.\r\n\r\n# If you care about point-in-time recovery\r\n\r\n(And by point-in-time recovery, I mean you care about being able to restore to anything other than a full or differential backup.)\r\n\r\nPresumably your database is in `FULL` recovery mode. If not, then make sure it is:\r\n\r\n    ALTER DATABASE yourdb SET RECOVERY FULL;\r\n\r\nEven if you are taking regular full backups, the log file will grow and grow until you perform a *log* backup - this is for your protection, not to needlessly eat away at your disk space. You should be performing these log backups quite frequently, according to your recovery objectives. For example, if you have a business rule that states you can afford to lose no less than 15 minutes of data in the event of a disaster, you should have a job that backs up the log every 15 minutes. Here is a script that will generate timestamped file names based on the current time (but you can also do this with maintenance plans etc., just don't choose any of the shrink options in maintenance plans, they're awful).\r\n\r\n    DECLARE @path NVARCHAR(255) = N'\\\\backup_share\\log\\yourdb_' \r\n      + CONVERT(CHAR(8), GETDATE(), 112) + '_'\r\n      + REPLACE(CONVERT(CHAR(8), GETDATE(), 108),':','')\r\n      + '.trn';\r\n    \r\n    BACKUP LOG foo TO DISK = @path WITH INIT, COMPRESSION;\r\n\r\nNote that `\\\\backup_share\\` should be on a different machine that represents a different underlying storage device. Backing these up to the same machine (or to a different machine that uses the same underlying disks, or a different VM that's on the same physical host) does not really help you, since if the machine blows up, you've lost your database *and* its backups. Depending on your network infrastructure it may make more sense to backup locally and then transfer them to a different location behind the scenes; in either case, you want to get them off the primary database machine as quickly as possible.\r\n\r\nNow, once you have regular log backups running, it should be reasonable to shrink the log file to something more reasonable than whatever it's blown up to now. This does *not* mean running `SHRINKFILE` over and over again until the log file is 1 MB - even if you are backing up the log frequently, it still needs to accommodate the sum of any concurrent transactions that can occur. Log file autogrow events are expensive, since SQL Server has to zero out the files (unlike data files when instant file initialization is enabled), and user transactions have to wait while this happens. You want to do this grow-shrink-grow-shrink routine as little as possible, and you certainly don't want to make your users pay for it.\r\n\r\nNote that you may need to back up the log twice before a shrink is possible (thanks Robert).\r\n\r\nSo, you need to come up with a practical size for your log file. Nobody here can tell you what that is without knowing a lot more about your system, but if you've been frequently shrinking the log file and it has been growing again, a good watermark is probably 10-50% higher than the largest it's been. Let's say that comes to 200 MB, and you want any subsequent autogrowth events to be 50 MB, then you can adjust the log file size this way:\r\n\r\n    USE [master];\r\n    GO\r\n    ALTER DATABASE Test1 \r\n      MODIFY FILE\r\n      (NAME = yourdb_log, SIZE = 200MB, FILEGROWTH = 50MB);\r\n    GO\r\n\r\nNote that if the log file is currently > 200 MB, you may need to run this first:\r\n\r\n    USE yourdb;\r\n    GO\r\n    DBCC SHRINKFILE(yourdb_log, 200);\r\n    GO\r\n\r\n# If you don't care about point-in-time recovery\r\n\r\nIf this is a test database, and you don't care about point-in-time recovery, then you should make sure that your database is in `SIMPLE` recovery mode.\r\n\r\n    ALTER DATABASE yourdb SET RECOVERY SIMPLE;\r\n\r\nPutting the database in `SIMPLE` recovery mode will make sure that SQL Server re-uses portions of the log file (essentially phasing out inactive transactions) instead of growing to keep a record of *all* transactions (like `FULL` recovery does until you back up the log). `CHECKPOINT` events will help control the log and make sure that it doesn't need to grow unless you generate a lot of t-log activity between `CHECKPOINT`s.\r\n\r\nNext, you should make absolute sure that this log growth was truly due to an abnormal event (say, an annual spring cleaning or rebuilding your biggest indexes), and not due to normal, everyday usage. If you shrink the log file to a ridiculously small size, and SQL Server just has to grow it again to accommodate your normal activity, what did you gain? Were you able to make use of that disk space you freed up only temporarily? If you need an immediate fix, then you can run the following:\r\n\r\n    USE yourdb;\r\n    GO\r\n    CHECKPOINT;\r\n    GO\r\n    CHECKPOINT; -- run twice to ensure file wrap-around\r\n    GO\r\n    -- 200 MB\r\n    DBCC SHRINKFILE(yourdb_log, 200);\r\n    GO\r\n\r\nOtherwise, set an appropriate size and growth rate. As per the example in the point-in-time recovery case, you can use the same code and logic to determine what file size is appropriate and set reasonable autogrowth parameters. \r\n\r\n# Some things you don't want to do\r\n\r\n- **Back up the log with `TRUNCATE_ONLY` option and then `SHRINKFILE`**. For one, this `TRUNCATE_ONLY` option has been deprecated and is no longer available in current versions of SQL Server. Second, if you are in `FULL` recovery model, this will destroy your log chain and require a new, full backup.\r\n\r\n- **Detach the database, delete the log file, and re-attach**. I can't emphasize how dangerous this can be. Your database may not come back up, it may come up as suspect, you may have to revert to a backup (if you have one), etc. etc.\r\n\r\n- **Use the "shrink database" option**. `DBCC SHRINKDATABASE` and the maintenance plan option to do the same are bad ideas, especially if you really only need to resolve a log problem issue. Target the file you want to adjust and adjust it independently, using `DBCC SHRINKFILE` or `ALTER DATABASE ... MODIFY FILE` (examples above).\r\n\r\n- **Shrink the log file to 1 MB**. This looks tempting because, hey, SQL Server will let me do it in certain scenarios, and look at all the space it frees! Unless your database is read only (and it is, you should mark it as such using `ALTER DATABASE`), this will absolutely just lead to many unnecessary growth events, as the log has to accommodate current transactions regardless of the recovery model. What is the point of freeing up that space temporarily, just so SQL Server can take it back slowly and painfully?\r\n\r\n- **Create a second log file**. This will provide temporarily relief for the drive that has filled your disk, but this is like trying to fix a punctured lung with a band-aid. You should deal with the problematic log file directly instead of just adding another potential problem. Other than redirecting some transaction log activity to a different drive, a second log file really does nothing for you (unlike a second data file), since only one of the files can ever be used at a time. [Paul Randal also explains why multiple log files can bite you later](http://www.sqlskills.com/blogs/paul/multiple-log-files-and-why-theyre-bad/).\r\n\r\n# Be proactive\r\n\r\nInstead of shrinking your log file to some small amount and letting it constantly autogrow at a small rate on its own, set it to some reasonably large size (one that will accommodate the sum of your largest set of concurrent transactions) and set a reasonable autogrow setting as a fallback, so that it doesn't have to grow multiple times to satisfy single transactions and so that it will be relatively rare for it to ever have to grow during normal business operations.\r\n\r\nThe worst possible settings here are 1 MB growth or 10% growth. Funny enough, these are the defaults for SQL Server (which I've complained about and [asked for changes to no avail](https://web.archive.org/web/20140108204835/http://connect.microsoft.com:80/SQLServer/feedback/details/415343)) - 1 MB for data files, and 10% for log files. The former is much too small in this day and age, and the latter leads to longer and longer events every time (say, your log file is 500 MB, first growth is 50 MB, next growth is 55 MB, next growth is 60.5 MB, etc. etc. - and on slow I/O, believe me, you will really notice this curve).\r\n\r\n# Further reading\r\n\r\nPlease don't stop here; while much of the advice you see out there about shrinking log files is inherently bad and even potentially disastrous, there are some people who care more about data integrity than freeing up disk space.\r\n\r\n- [A blog post I wrote in 2009, when I saw a few "here's how to shrink the log file" posts spring up](https://sqlblog.org/2009/07/27/oh-the-horror-please-stop-telling-people-they-should-shrink-their-log-files).\r\n\r\n- [A blog post Brent Ozar wrote four years ago, pointing to multiple resources, in response to a SQL Server Magazine article that should *not* have been published](https://www.brentozar.com/archive/2009/08/stop-shrinking-your-database-files-seriously-now/).\r\n\r\n- [A blog post by Paul Randal explaining why t-log maintenance is important](https://www.sqlskills.com/blogs/paul/importance-of-proper-transaction-log-size-management/) and [why you shouldn't shrink your data files, either](http://www.sqlskills.com/blogs/paul/why-you-should-not-shrink-your-data-files/).\r\n\r\n- [Mike Walsh has a great answer above, of course, covering some of these aspects too, including reasons why you might not be able to shrink your log file immediately](https://dba.stackexchange.com/a/29830).	2019-11-24 09:43:41.209696+00	2	4	1	48296	0	0	0	2019-11-24 09:43:41.209696+00	\N	Answer by [Aaron Bertrand](https://dba.stackexchange.com/users/1186/aaron-bertrand) from [dba.stackexchange.com](https://dba.stackexchange.com/a/48296) under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0):	f	f
64	90	70	2012-12-05 02:11:22+00	**A Shorter Answer:**\r\n\r\nYou probably either have a long running transaction running (Index maintenance? Big batch delete or update?) or you are in the "default" (more below on what is meant by default) recovery mode of `Full` and have not taken a *log backup* (or aren't taking them frequently enough). \r\n\r\nIf it is a recovery model issue, the simple answer could be Switch to `Simple` recovery mode if you do not need point in time recovery and regular log backups. Many people, though, make that their answer without understanding recovery models. Read on to understand why it matters and then decide what you do. You could also just start taking log backups and stay in `Full` recovery.\r\n\r\nThere could be other reasons but these are the most common. This answer begins to dive into the most common two reasons and gives you some background information on the why and how behind the reasons as well as explores some other reasons.\r\n\r\n---------------------\r\n\r\n**A Longer Answer:** \r\nWhat Scenarios can cause the log to keep Growing? There are many reasons, but usually these reasons are of the following two patterns: There is a misunderstanding about recovery models or there are long running transactions. Read on for details.\r\n\r\n# Top reason 1/2: Not Understanding Recovery Models\r\n(**Being in *Full Recovery Mode* and Not Taking *Log Backups* - This is the most common reason - the vast majority of those experiencing this issue are.**)\r\n\r\nWhile this answer is not a deep dive in SQL Server recovery models, the topic of recovery models is critical to this problem.\r\n\r\nIn SQL Server, there are three [recovery models][1]:\r\n\r\n* `Full`,\r\n* `Bulk-Logged` and \r\n* `Simple`. \r\n\r\nWe'll ignore `Bulk-Logged` for now we'll sort of say it is a hybrid model and most people who are in this model are there for a reason and understand recovery models.  \r\n\r\nThe two we care about and their confusion are the cause of the majority of the cases of people having this issue are `Simple` and `Full`.\r\n\r\n## Intermission: Recovery in General\r\nBefore we talk about Recovery Models: let's talk about recovery in general. If you want to go even deeper with this topic, just read [Paul Randal's blog][2] and as many posts on it as you want. For this question, though:\r\n\r\n 1. Crash/Restart Recovery  \r\n One purpose of the transaction log file is for *crash/restart recovery*. For the rolling forward and rolling back of work that was either done (rolling forward/redo) before a crash or restart and the work that was started but not finished after a crash or restart (rolling back/undo). It is the job of the transaction log to see that a transaction started but never finished (rolled back or crash/restart happened before the transaction committed). In that situation It is the log's job to say *"Hey.. this never really finished, let's roll it back"* during recovery. It is also the log's job to see that you did finish something and that your client application was told it was finished (even if it hadn't yet hardened to your data file) and say *"Hey.. this really happened, let's roll it forward, let's make it like the applications think it was"* after a restart. Now there is more but that is the main purpose.\r\n\r\n 2. Point in Time Recovery  \r\n The other purpose for a transaction log file is to be able to give us the ability to recover to a *point in time* due to an "oops" in a database or to guarantee a recovery point in the event of a hardware failure involving the data and/or log files of a database. If this transaction log contains the records of transactions that have been started and finished for recovery, SQL Server can and does then use this information to get a database to where it was before an issue happened. But that isn't always an available option for us. For that to work we have to have our database in the right *recovery model*, and we have to take *log backups*.\r\n\r\n## Recovery Models\r\nOnto the recovery models:\r\n\r\n* Simple Recovery Model  \r\n So with the above introduction, it is easiest to talk about `Simple Recovery` model first. In this model, you are telling SQL Server: *"I am fine with you using your transaction log file for crash and restart recovery..."* (You really have no choice there. Look up [ACID properties][3] and that should make sense quickly.) *"...but once you no longer need it for that crash/restart recovery purpose, go ahead and reuse the log file."* \r\n\r\n SQL Server listens to this request in Simple Recovery and it only keeps the information it needs to do crash/restart recovery. Once SQL Server is sure it can recover because data is hardened to the data file (more or less), the data that has been hardened is no longer necessary in the log and is marked for truncation - which means it gets re-used. \r\n\r\n* Full Recovery Model  \r\n With `Full Recovery`, you are telling SQL Server that you want to be able to recover to a specific point in time, as long as your log file is available or to a specific point in time that is covered by a log backup. In this case when SQL Server reaches the point where it would be safe to truncate the log file in Simple Recovery Model, it will not do that. Instead **It lets the log file continue to grow** and will allow it to keep growing, ***until you take a log backup*** (or run out of space on your log file drive) under normal circumstances.\r\n\r\n### Switching from Simple to Full has a Gotcha.\r\nThere are rules and exceptions here. We'll talk about long running transactions in depth below.  \r\n\r\nBut one caveat to keep in mind for Full Recovery Mode is this: If you just switch into `Full Recovery` mode, but never take an initial Full Backup, SQL Server will **not** honor your request to be in `Full Recovery` model. Your transaction log will continue to operate as it has in `Simple`until you switch to Full Recovery Model AND take your first `Full Backup`.\r\n\r\n## Full Recovery Model without log backups is bad.\r\nSo, that's the most common reason for uncontrolled log growth? Answer: Being in Full Recovery mode without having any log backups.  \r\n\r\nThis happens **all** the time to people.\r\n\r\n### Why is this such a common mistake?\r\nWhy does it happen all the time? Because each new database gets its initial recovery model setting by looking at the model database.\r\n\r\nModel's initial recovery model setting is always `Full Recovery Model` - until and unless someone changes that. So you could say the "default Recovery Model" is `Full`. Many people are not aware of this and have their databases running in `Full Recovery Model` with no log backups, and therefore a transaction log file much larger than necessary. This is why it is important to change defaults when they don't work for your organization and its needs)\r\n\r\n\r\n## Full Recovery Model with too few log backups is bad.\r\nYou can also get yourself in trouble here by not taking log backups frequently enough.  \r\nTaking a log backup a day may sound fine, it makes a restore require less restore commands, but keeping in mind the discussion above, that log file will continue to grow and grow until you take log backups. \r\n\r\n### How do I find out what log backup frequency I need?\r\nYou need to consider your log backup frequency with two things in mind:\r\n  \r\n 1. **Recovery Needs** - This should hopefully be first. In the event that the drive housing your transaction log goes bad or you get serious corruption that affects your log backup, how much data can be lost? If that number is no more than 10-15 minutes, then you need to be taking the log backup every 10-15 minute, end of discussion. \r\n 2. **Log Growth** - If your organization is fine to lose more data because of the ability to easily recreate that day you may be fine to have a log backup much less frequently than 15 minutes. Maybe your organization is fine with every 4 hours. But you have to look at how many transactions you generate in 4 hours. Will allowing the log to keep growing in those four hours make too large of a log file? Will that mean your log backups take too long?\r\n\r\n-----\r\n\r\n# Top reason 2/2: Long Running Transactions\r\n\r\n(**"My recovery model is fine! The log is still growing!**)\r\n\r\nThis can also be a cause of uncontrolled and unrestrained log growth. No matter the recovery model, but it often comes up as *"But I'm in Simple Recovery Model - why is my log still growing?!"*\r\n\r\nThe reason here is simple: if SQL is using this transaction log for recovery purposes as I described above, then it has to see back to the start of a transaction.\r\n\r\nIf you have a transaction that takes a long time or does a lot of changes, the log cannot truncate on checkpoint for any of the changes that are still in open transactions or that have started since that transaction started.\r\n\r\nThis means that a big delete, deleting millions of rows in one delete statement is one transaction and the log cannot do any truncating until that whole delete is done. In `Full Recovery Model`, this delete is logged and that could be a lot of log records. Same thing with Index optimization work during maintenance windows. It also means that poor transaction management and not watching for and closing open transactions can really hurt you and your log file.\r\n\r\n## What can I do about these long running transactions?\r\nYou can save yourself here by:\r\n\r\n - Properly sizing your log file to account for the worst case scenario - like your maintenance or known large operations. And when you grow your log file you should look to this [guidance][4] (and the two links she sends you to) by Kimberly Tripp. Right sizing is super critical here.\r\n - Watching your usage of transactions. Don't start a transaction in your application server and start having long conversations with SQL Server and risk leaving one open too long. \r\n - Watching the *implied transactions* in your DML statements. For example: `UPDATE TableName Set Col1 = 'New Value'` is a transaction. I didn't put a `BEGIN TRAN` there and I don't have to, it is still one transaction that just automatically commits when done. So if doing operations on large numbers of rows, consider batching those operations up into more manageable chunks and giving the log time to recover. Or consider the right size to deal with that. Or perhaps look into changing recovery models during a bulk load window.\r\n\r\n----\r\n\r\n\r\n# Do these two reasons also apply to Log Shipping?\r\n\r\nShort answer: yes. Longer answer below.\r\n\r\nQuestion: *"I'm using log shipping, so my log backups are automated... Why am I still seeing transaction log growth?"*\r\n\r\nAnswer: read on.\r\n\r\n## What is Log Shipping?\r\nLog shipping is just what it sounds like - you are shipping your transaction log backups to another server for DR purposes. There is some initialization but after that the process is fairly simple: \r\n\r\n* A job to backup the log on one server, \r\n* a job to copy that log backup and \r\n* a job to restore it without recovery (either `NORECOVERY` or `STANDBY`) on the destination server.\r\n\r\nThere are also some jobs to monitor and alert if things don't go as you have them planned.\r\n\r\nIn some cases, you may only want to do the log shipping restore once a day or every third day or once a week. That is fine. But if you make this change on all of the jobs (including the log backup and copy jobs) that means you are waiting all that time to take a log backup. That means you will have a lot of log growth -- because you are *in full recovery mode without log backups* -- and it probably also means a large log file to copy across. You should only modify the restore job's schedule and let the log backups and copies happen on a more frequent basis, otherwise you will suffer from the first issue described in this answer.\r\n\r\n\r\n-----\r\n\r\n# General troubleshooting via status codes\r\n\r\nThere are reasons other than these two, but these are the most common. Regardless of the cause: there is a way you can analyze your reason for this unexplained log growth/lack of truncation and see what they are.\r\n\r\nBy querying the [`sys.databases`][5] catalog view you can see information describing the reason your log file may be waiting on truncate/reuse.\r\n  \r\nThere is a column called `log_reuse_wait` with a lookup ID of the reason code and a `log_reuse_wait_desc` column with a description of the wait reason. From the referenced books online article are the majority of the reasons (the ones you are likely to see and the ones we can explain reasons for. The missing ones are either out of use or for internal use) with a few notes about the wait in *italics*:\r\n\r\n - 0 = Nothing  \r\n *What it sounds like.. Shouldn't be waiting*\r\n\r\n - 1 = Checkpoint  \r\n *Waiting for a checkpoint to occur. This should happen and you should be fine - but there are some cases to look for here for later answers or edits.*\r\n\r\n - 2 = Log backup  \r\n *You are waiting for a log backup to occur. Either you have them scheduled and it will happen soon, or you have the first problem described here and you now know how to fix it*\r\n\r\n - 3 = Active backup or restore  \r\n *A backup or restore operation is running on the database*\r\n\r\n - 4 = Active transaction  \r\n There is an active transaction that needs to complete (either way - `ROLLBACK` or `COMMIT`) before the log can be backed up. This is the second reason described in this answer. \r\n\r\n - 5 = Database mirroring  \r\n *Either a mirror is getting behind or under some latency in a high performance mirroring situation or mirroring is paused for some reason*\r\n\r\n - 6 = Replication  \r\n *There can be issues with replication that would cause this - like a log reader agent not running, a database thinking it is marked for replication that no longer is and various other reasons. You can also see this reason and it is perfectly normal because you are looking at just the right time, just as transactions are being consumed by the log reader*\r\n\r\n - 7 = Database snapshot creation  \r\n *You are creating a database snapshot, you'll see this if you look at just the right moment as a snapshot is being created*\r\n\r\n - 8 = Log Scan  \r\n *I have yet to encounter an issue with this running along forever. If you look long enough and frequently enough you can see this happen, but it shouldn't be a cause of excessive transaction log growth, that I've seen.*\r\n\r\n - 9 = An AlwaysOn Availability Groups secondary replica is applying transaction log records of this database to a corresponding secondary database.\r\n *About the clearest description yet..*\r\n\r\n\r\n  [1]: https://msdn.microsoft.com/en-us/library/ms189275.aspx\r\n  [2]: https://www.sqlskills.com/blogs/paul/category/transaction-log.aspx\r\n  [3]: https://stackoverflow.com/questions/6633808/sql-server-and-acid-property-of-database\r\n  [4]: https://www.sqlskills.com/blogs/kimberly/post/8-steps-to-better-transaction-log-throughput.aspx\r\n  [5]: https://msdn.microsoft.com/en-us/library/ms178534.aspx	2019-11-27 13:25:29.569005+00	3	4	1	29830	0	0	0	2019-11-24 09:31:18.425769+00	\N	**A Shorter Answer:**	f	f
618	573	12	2015-04-21 03:13:02+00	>*Are we doing something wrong or is it SQL Server error?*\r\n\r\nIt is a wrong-results bug, which you should report via your usual support channel. If you do not have a support agreement, it may help to know that [paid incidents][1] are normally refunded if Microsoft confirms the behaviour as a bug.\r\n\r\nThe bug requires three ingredients:\r\n\r\n1. Nested loops with an outer reference (an apply)\r\n2. An inner-side lazy index spool that seeks on the outer reference\r\n3. An inner-side Concatenation operator\r\n\r\nFor example, the query in the question produces a plan like the following:\r\n\r\n![Annotated plan][2]\r\n\r\nThere are many ways to remove one of these elements, so the bug no longer reproduces.\r\n\r\nFor example, one could create indexes or statistics that happen to mean the optimizer chooses not to utilize a Lazy Index Spool. Or, one could use hints to force a hash or merge union instead of using Concatenation. One could also rewrite the query to express the same semantics, but which results in a different plan shape where one or more of the required elements are missing.\r\n\r\n### More details\r\n\r\nA Lazy Index Spool lazily caches inner side result rows, in a work table indexed by outer reference (correlated parameter) values. If a Lazy Index Spool is asked for an outer reference it has seen before, it fetches the cached result row from its work table (a "rewind"). If the spool is asked for an outer reference value it has not seen before, it runs its subtree with the current outer reference value and caches the result (a "rebind"). The seek predicate on the Lazy Index Spool indicates the key(s) for its work table.\r\n\r\nThe problem occurs in this specific plan shape when the spool checks to see if a new outer reference is the same as one it has seen before. The Nested Loops Join updates its outer references correctly, and notifies operators on its inner input via their `PrepRecompute` interface methods. At the start of this check, inner side operators read the `CParamBounds:FNeedToReload` property to see if the outer reference has changed from last time. An example stack trace is shown below:\r\n\r\n![CParamBounds:FNeedToReload][3]\r\n\r\nWhen the subtree shown above exists, specifically where Concatenation is used, something goes wrong (perhaps a ByVal/ByRef/Copy problem) with the bindings such that `CParamBounds:FNeedToReload` always returns false, regardless of whether the outer reference actually changed or not.\r\n\r\nWhen the same subtree exists, but a Merge Union or Hash Union is used, this essential property is set correctly on each iteration, and the Lazy Index Spool rewinds or rebinds each time as appropriate. The Distinct Sort and Stream Aggregate are blameless, by the way. My suspicion is that Merge and Hash Union make a copy of the previous value, whereas Concatenation uses a reference. It is just about impossible to verify this without access to the SQL Server source code, unfortunately.\r\n\r\nThe net result is that the Lazy Index Spool in the problematic plan shape always thinks it has already seen the current outer reference, rewinds by seeking into its work table, generally finds nothing, so no row is returned for that outer reference. Stepping through the execution in a debugger, the spool only ever executes its `RewindHelper` method, and never its `ReloadHelper` method (reload = rebind in this context). This is evident in the execution plan because operators under the spool all have 'Number of Executions = 1'.\r\n\r\n![RewindHelper][4]\r\n\r\nThe exception, of course, is for the first outer reference the Lazy Index Spool is given. This always executes the subtree and caches a result row in the work table. All subsequent iterations result in a rewind, which will only produce a row (the single cached row) when the current iteration has the same value for the outer reference as the first time around.\r\n\r\nSo, for any given input set on the outer side of the Nested Loops Join, the query will return as many rows as there are duplicates of the first row processed (plus one for the first row itself of course).\r\n\r\n### Demo\r\n\r\nTable and sample data:\r\n\r\n    CREATE TABLE #T1 \r\n    (\r\n        pk integer IDENTITY NOT NULL,\r\n        c1 integer NOT NULL,\r\n    \r\n        CONSTRAINT PK_T1\r\n        PRIMARY KEY CLUSTERED (pk)\r\n    );\r\n    GO\r\n    INSERT #T1 (c1)\r\n    VALUES\r\n        (1), (2), (3), (4), (5), (6),\r\n        (1), (2), (3), (4), (5), (6),\r\n        (1), (2), (3), (4), (5), (6);\r\n\r\nThe following (trivial) query produces a correct count of two for each row (18 in total) using a Merge Union:\r\n\r\n    SELECT T1.c1, C.c1\r\n    FROM #T1 AS T1\r\n    CROSS APPLY \r\n    (\r\n        SELECT COUNT_BIG(*) AS c1\r\n        FROM\r\n        (\r\n            SELECT T1.c1\r\n            UNION\r\n            SELECT NULL\r\n        ) AS U\r\n    ) AS C;\r\n\r\n![Merge Union Plan][5]\r\n\r\nIf we now add a query hint to force a Concatenation:\r\n\r\n    SELECT T1.c1, C.c1\r\n    FROM #T1 AS T1\r\n    CROSS APPLY \r\n    (\r\n        SELECT COUNT_BIG(*) AS c1\r\n        FROM\r\n        (\r\n            SELECT T1.c1\r\n            UNION\r\n            SELECT NULL\r\n        ) AS U\r\n    ) AS C\r\n    OPTION (CONCAT UNION);\r\n\r\nThe execution plan has the problematic shape:\r\n\r\n![Concatenation Plan][6]\r\n\r\nAnd the result is now incorrect, just three rows:\r\n\r\n![Three row result][7]\r\n\r\nThough this behaviour is not guaranteed, the first row from the Clustered Index Scan has a `c1` value of 1. There are two other rows with this value, so three rows are produced in total.\r\n\r\nNow truncate the data table and load it with more duplicates of the 'first' row:\r\n\r\n    TRUNCATE TABLE #T1;\r\n    \r\n    INSERT #T1 (c1)\r\n    VALUES\r\n        (1), (2), (3), (4), (5), (6),\r\n        (1), (2), (3), (4), (5), (6),\r\n        (1), (1), (1), (1), (1), (1);\r\n\r\nNow the Concatenation plan is:\r\n\r\n![8 row Concatenation Plan][8]\r\n\r\nAnd, as indicated, 8 rows are produced, all with `c1 = 1` of course:\r\n\r\n![8 row result][9]\r\n\r\nI notice you have opened a [Connect item for this bug][10] but really that is not the place to report issues that are having a production impact. If that is the case, you really ought to contact Microsoft Support.\r\n\r\n---\r\n\r\nThis wrong-results bug was fixed at some stage. It no longer reproduces for me on any version of SQL Server from 2012 onward. It does repro on SQL Server 2008 R2 SP3-GDR build 10.50.6560.0 (X64).\r\n\r\n  [1]: http://support.microsoft.com/en-us/gp/microsoft-support-options\r\n  [2]: https://i.stack.imgur.com/aA6dj.png\r\n  [3]: https://i.stack.imgur.com/N2kjO.png\r\n  [4]: https://i.stack.imgur.com/NVK8R.png\r\n  [5]: https://i.stack.imgur.com/pZU8T.png\r\n  [6]: https://i.stack.imgur.com/CQRtJ.png\r\n  [7]: https://i.stack.imgur.com/QQVzM.png\r\n  [8]: https://i.stack.imgur.com/FhTMt.png\r\n  [9]: https://i.stack.imgur.com/qHZp8.png\r\n  [10]: https://feedback.azure.com/forums/908035-sql-server/suggestions/32904934-unpredictable-select-results-index-spool-bug	2020-01-15 08:30:17.664172+00	5	4	1	98906	0	0	0	2020-01-15 08:30:17.664172+00	\N	>*Are we doing something wrong or is it SQL Server error?*	f	f
45	75	12	2019-11-20 12:21:35.244926+00	> *How does LIKE '%123456789%' benefit from indexing?*\r\n\r\nOnly a little bit. The query processor can **scan** the whole nonclustered index looking for matches instead of the entire table (the clustered index). Nonclustered indexes are generally smaller than the table they are built on, so scanning the nonclustered index may be faster.\r\n\r\nThe downside, is that any columns needed by the query that are not included in the nonclustered index definition must be looked up in the base table, per row.\r\n\r\nThe optimizer makes a decision between scanning the table (clustered index) and scanning the nonclustered index with lookups, based on cost estimates. The estimated costs depend to a great extent on how many rows the optimizer **expects** your `LIKE` or `CHARINDEX` predicate to select.\r\n\r\n> *Why do the listed articles state that it will not improve performance?*\r\n\r\nFor a `LIKE` condition that does **not** start with a wildcard, SQL Server can perform a **partial scan** of the index instead of scanning the whole thing. For example, `LIKE 'A%` can be correctly evaluated by testing only index records `>= 'A'` and `< 'B'` (the exact boundary values depend on collation).\r\n\r\nThis sort of query can use the seeking ability of b-tree indexes: we can go straight to the first record `>= 'A'` using the b-tree, then scan forward in index key order until we reach a record that fails the `< 'B'` test. Since we only need to apply the `LIKE` test to a smaller number of rows, performance is generally better.\r\n\r\nBy contrast, `LIKE '%A` cannot be turned into a partial scan because we don't know where to start or end; any record could end in `'A'`, so we cannot improve on scanning the whole index and testing every row individually.\r\n\r\n> *I tried rewriting the query to use `CHARINDEX`, but performance is still slow. Why does `CHARINDEX` not benefit from the indexing as it appears the LIKE query does?*\r\n\r\nThe query optimizer has the **same choice** between scanning the table (clustered index) and scanning the nonclustered index (with lookups) in both cases.\r\n\r\nThe choice is made between the two based on **cost estimation**. It so happens that SQL Server may produce a different estimate for the two methods. For the `LIKE` form of the query, the estimate may be able to use special string statistics to produce a reasonably accurate estimate. The `CHARINDEX > 0` form produces an estimate based on a guess.\r\n\r\nThe different estimates are enough to make the optimizer choose a Clustered Index Scan for `CHARINDEX` and a NonClustered Index Scan with Lookups for the `LIKE`. If you force the `CHARINDEX` query to use the nonclustered index with a hint, you will get the same plan as for `LIKE`, and performance will be about the same:\r\n\r\n    SELECT\r\n        [Customer name],\r\n        [Sl_No],\r\n        [Id]\r\n    FROM dbo.customer WITH (INDEX (f))\r\n    WHERE \r\n        CHARINDEX('9000413237', [Phone no]) >0;\r\n\r\nThe number of rows processed at runtime will be the same for both methods, it's just that the `LIKE` form produces a more accurate estimation in this case, so the query optimizer chooses a better plan.\r\n\r\nIf you find yourself needing `LIKE %thing%` searches often, you might want to consider a technique I wrote about in [Trigram Wildcard String Search in SQL Server][1].\r\n\r\n\r\n  [1]: https://sqlperformance.com/2017/09/sql-performance/sql-server-trigram-wildcard-search	2019-11-20 12:21:35.244926+00	3	4	2	\N	0	0	0	\N	\N	> *How does LIKE '%123456789%' benefit from indexing?*	f	f
57	79	2	2019-11-22 12:55:08.334782+00	Thanks Colin, you are right of course and apostrophes are now accepted in names :)	2019-11-22 12:55:08.334782+00	4	1	1	\N	0	0	0	\N	\N	Thanks Colin, you are right of course and apostrophes are now accepted in names :)	f	f
71	93	12	2019-11-24 10:13:45.488628+00	**During parsing**, SQL Server calls `sqllang!DecodeCompOp` to determine the type of comparison operator present:\r\n\r\n[![Call stack][1]][1]\r\n\r\nThis occurs well before anything in the optimizer gets involved.\r\n\r\n>From [Comparison Operators (Transact-SQL)][2]\r\n\r\n>[![Comparison operators and meanings][3]][3]\r\n\r\nTracing the code using a debugger and public symbols[^symbols], `sqllang!DecodeCompOp` returns a value in register `eax`[^eax] as follows:\r\n\r\n~~~\r\n╔════╦══════╗\r\n║ Op ║ Code ║\r\n╠════╬══════╣\r\n║ <  ║    1 ║\r\n║ =  ║    2 ║\r\n║ <= ║    3 ║\r\n║ !> ║    3 ║\r\n║ >  ║    4 ║\r\n║ <> ║    5 ║\r\n║ != ║    5 ║\r\n║ >= ║    6 ║\r\n║ !< ║    6 ║\r\n╚════╩══════╝\r\n~~~\r\n\r\n`!=` and `<>` both return 5, so are **indistinguishable** in all later operations (including compilation & optimization).\r\n\r\n---\r\n\r\nThough secondary to the above point, it is also possible (e.g. using undocumented trace flag 8605) to look at the logical tree passed to the optimizer to confirm that both `!=` and `<>` map to `ScaOp_Comp x_cmpNe` (not equal scalar operator comparison).\r\n\r\nFor example:\r\n\r\n```sql\r\nSELECT P.ProductID FROM Production.Product AS P\r\nWHERE P.ProductID != 4\r\nOPTION (QUERYTRACEON 3604, QUERYTRACEON 8605);\r\n    \r\nSELECT P.ProductID FROM Production.Product AS P\r\nWHERE P.ProductID <> 4\r\nOPTION (QUERYTRACEON 3604, QUERYTRACEON 8605);\r\n```\r\n\r\nboth produce:\r\n\r\n~~~\r\nLogOp_Project QCOL: [P].ProductID\r\n    LogOp_Select\r\n        LogOp_Get TBL: Production.Product(alias TBL: P)\r\n          ScaOp_Comp x_cmpNe\r\n            ScaOp_Identifier QCOL: [P].ProductID\r\n            ScaOp_Const TI(int,ML=4) XVAR(int,Not Owned,Value=4)\r\n    AncOp_PrjList \r\n~~~\r\n\r\n---\r\n\r\n### Footnotes\r\n\r\n[^symbols]: I use [WinDbg][4]; other debuggers are available. Public symbols are available via the usual Microsoft symbol server. For more information, see [Looking deeper into SQL Server using Minidumps][5] by the SQL Server Customer Advisory Team and [SQL Server Debugging with WinDbg – an Introduction][6] by Klaus Aschenbrenner.\r\n\r\n[^eax]: Using EAX on 32-bit Intel derivatives for return values from a function is common. Certainly the Win32 ABI does it that way, and I'm pretty sure it inherits that practice from back in the old MS-DOS days, where AX was used for the same purpose - [Michael Kjörling][7]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/4qRy4.png\r\n  [2]: https://msdn.microsoft.com/en-us/library/ms188074.aspx\r\n  [3]: https://i.stack.imgur.com/ThPRY.png\r\n  [4]: http://www.windbg.org/\r\n  [5]: https://blogs.msdn.microsoft.com/sqlcat/2009/09/11/looking-deeper-into-sql-server-using-minidumps\r\n  [6]: http://www.sqlpassion.at/archive/2014/05/05/sql-server-debugging-with-windbg-an-introduction\r\n  [7]: https://dba.stackexchange.com/users/3984/michael-kj%C3%B6rling	2019-11-25 22:46:34.261798+00	2	4	2	\N	0	0	0	\N	\N	**During parsing**, SQL Server calls `sqllang!DecodeCompOp` to determine the type of comparison operator present:	f	f
252	288	16	2019-08-28 14:43:52+00	What you're looking for is an optimistic isolation level, like Snapshot Isolation, or Read Committed Snapshot Isolation.\r\n\r\n\r\nCode example:\r\n--\r\n    USE Crap;\r\n    \r\n    CREATE TABLE dbo.users (id INT, username NVARCHAR(40));\r\n    \r\n    INSERT dbo.users ( id, username )\r\n    VALUES ( 1, N'Jimbo' )\r\n    \r\n    /*To turn on Snapshot*/\r\n    ALTER DATABASE Crap SET ALLOW_SNAPSHOT_ISOLATION ON;\r\n    \r\n    /*To turn on RCSI*/\r\n    ALTER DATABASE Crap SET READ_COMMITTED_SNAPSHOT ON;\r\n    \r\n    UPDATE dbo.users \r\n    SET username = 'Dimbo'\r\n    WHERE id = 1;\r\n    \r\n    /*Snapshot needs this, RCSI doesn't*/\r\n    SET TRANSACTION ISOLATION LEVEL SNAPSHOT\r\n    SELECT *\r\n    FROM dbo.users AS u\r\n    WHERE u.id = 1;\r\n\r\nThings to be aware of:\r\n--\r\n\r\n - Row versioning [uses space in tempdb][1] (except SQL Server 2019 when [Accelerated Database Recovery][2] is configured - versions are stored with the user database, either in-row or in the Persisted Version Store)\r\n - There can be [race conditions][3] where you depend on locking for queueing\r\n\r\nDifferences\r\n--\r\nOne important difference between Snapshot Isolation and RCSI is inside transactions:\r\n\r\n - Under Snapshot Isolation, BEGIN TRAN marks the point when all queries inside the transaction will read from the version store. \r\n\r\n - Under RCSI, each statement after BEGIN TRAN will read the version store as of when the statement executes.\r\n\r\nAnother difference is that Snapshot Isolation can be applied to modification queries, where RCSI can't. More precisely, SI detects write conflicts and rolls one of the conflicting transactions back automatically. [Updates under RCSI][4] do not use row versions when locating data to update, but this only applies to the target table. Other tables in the same delete or update statement, including additional references to the target table, will continue to use row versions.\r\n\r\n\r\n  [1]: https://www.sqlshack.com/snapshot-isolation-in-sql-server/\r\n  [2]: https://docs.microsoft.com/en-us/azure/sql-database/sql-database-accelerated-database-recovery#adr-recovery-components\r\n  [3]: https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/optimistic-concurrency\r\n  [4]: https://sqlperformance.com/2014/05/t-sql-queries/data-modifications-under-rcsi	2019-12-04 14:31:23.393393+00	2	4	1	246456	0	0	0	2019-12-04 14:31:23.393393+00	\N	What you're looking for is an optimistic isolation level, like Snapshot Isolation, or Read Committed Snapshot Isolation.	f	f
61	85	12	2019-11-23 08:21:17.954825+00	This is an efficient way to convert a nonclustered primary key to clustered, and rename it:\r\n\r\n```sql\r\n-- How the table looks now\r\nCREATE TABLE dbo.Example\r\n(\r\n    pk integer NOT NULL,\r\n    some_data integer NOT NULL,\r\n    \r\n    CONSTRAINT PK_UnusualName\r\n        PRIMARY KEY NONCLUSTERED (pk)\r\n);\r\n    \r\n-- Some data\r\nINSERT dbo.Example (pk, some_data)\r\nVALUES (1, 100), (2, 200), (3, 300);\r\n    \r\n-- Change the nonclustered PK to clustered\r\nCREATE UNIQUE CLUSTERED INDEX PK_UnusualName\r\nON dbo.Example (pk)\r\nWITH (DROP_EXISTING = ON);\r\n    \r\n-- Rename\r\nEXECUTE sys.sp_rename \r\n    @objname = N'dbo.Example.PK_UnusualName',\r\n    @newname = N'PK__dbo_Example_pk',\r\n    @objtype = 'INDEX';\r\n```\r\n\r\n	2019-11-23 08:23:33.092015+00	1	4	2	\N	0	0	0	\N	\N	This is an efficient way to convert a nonclustered primary key to clustered, and rename it:	f	f
73	95	12	2019-11-24 10:28:03.860301+00	### Yes.\r\n\r\nFailing to specify `WITH SCHEMABINDING` means SQL Server skips the detailed checks it normally makes on the function body. It simply marks the function as accessing data (as mentioned in the link given in the question).\r\n\r\nThis is a performance optimization. If it did not make this assumption, SQL Server would have to perform the detailed checks on every function invocation (since the unbound function could change at any time).\r\n\r\n## Function properties\r\n\r\nThere are **five** important function properties:\r\n\r\n* Determinism\r\n* Precision\r\n* Data Access\r\n* System Data Access\r\n* System Verification\r\n\r\nFor example, take the following unbound scalar function:\r\n\r\n```sql\r\nCREATE FUNCTION dbo.F\r\n(\r\n    @i integer\r\n)\r\nRETURNS datetime\r\nAS\r\nBEGIN\r\n    RETURN '19000101';\r\nEND;\r\n```\r\n\r\nWe can look at the five properties using a metadata function:\r\n\r\n```sql\r\nSELECT \r\n    IsDeterministic = OBJECTPROPERTYEX(Func.ID, 'IsDeterministic'),\r\n    IsPrecise = OBJECTPROPERTYEX(Func.ID, 'IsPrecise'),\r\n    IsSystemVerified = OBJECTPROPERTYEX(Func.ID, 'IsSystemVerified'),\r\n    UserDataAccess = OBJECTPROPERTYEX(Func.ID, 'UserDataAccess'),\r\n    SystemDataAccess = OBJECTPROPERTYEX(Func.ID, 'SystemDataAccess')\r\nFROM \r\n(\r\n    VALUES (OBJECT_ID(N'dbo.F', N'FN'))\r\n) AS Func (ID);\r\n```\r\n\r\n[![Result][1]][1]\r\n\r\nThe two data access properties have been set true, and **the other three are set false**.\r\n\r\nThis has implications beyond those that might be expected (use in indexed views or indexed computed columns, for example).\r\n\r\n## Effects on the query optimizer\r\n\r\nThe **Determinism** property in particular affects the query optimizer. It has detailed rules concerning the types of rewrites and manipulations it is allowed to perform, and these are **very much restricted** for non-deterministic elements. The side-effects can be quite subtle.\r\n\r\nFor example, consider the following two tables:\r\n\r\n```sql\r\nCREATE TABLE dbo.T1\r\n(\r\n    SomeInteger integer PRIMARY KEY\r\n);\r\nGO\r\nCREATE TABLE dbo.T2\r\n(\r\n    SomeDate datetime PRIMARY KEY\r\n);\r\n```\r\n\r\n...and a query that uses the function (as defined previously):\r\n\r\n```sql\r\nSELECT * \r\nFROM dbo.T1 AS T1\r\nJOIN dbo.T2 AS T2\r\n    ON T2.SomeDate = dbo.F(T1.SomeInteger);\r\n```\r\n\r\nThe query plan is as expected, featuring a seek into table T2:\r\n\r\n[![Seek plan][2]][2]\r\n\r\nHowever, if the same logical query is written using a derived table or common table expression:\r\n\r\n```sql\r\nWITH CTE AS\r\n(\r\n    SELECT *, dt = dbo.F(T1.SomeInteger) \r\n    FROM dbo.T1 AS T1\r\n)\r\nSELECT * \r\nFROM CTE\r\nJOIN dbo.T2 AS T2\r\n    ON T2.SomeDate = CTE.dt;\r\n    \r\n-- Derived table\r\nSELECT\r\n    *\r\nFROM \r\n(\r\n    SELECT *, dt = dbo.F(T1.SomeInteger)\r\n    FROM dbo.T1 AS T1\r\n) AS T1\r\nJOIN dbo.T2 AS T2\r\n    ON T2.SomeDate = T1.dt;\r\n```\r\n\r\nThe execution plan now features a scan, with the predicate involving the function stuck in a Filter:\r\n\r\n[![Scan plan][3]][3]\r\n\r\nThis also happens if the derived table or common table expression is replaced by a view or in-line function. A `FORCESEEK` hint (and other similar attempts) will not succeed:\r\n\r\n[![Error message][4]][4]\r\n\r\nThe fundamental issue is that the query optimizer **cannot reorder nondeterministic query elements as freely**.\r\n\r\nTo produce a seek, the Filter predicate would need to be moved down the plan to the T2 data access. This movement is prevented when the function is non-deterministic.\r\n\r\n### Fix\r\n\r\nThe fix for this example involves two steps:\r\n\r\n1. Add `WITH SCHEMABINDING`\r\n2. Make the function deterministic\r\n\r\nThe first step is trivial. The second involves removing the non-deterministic implicit cast from string to `datetime`; replacing it with a deterministic `CONVERT`. **Neither is sufficient on its own**.\r\n\r\n```sql\r\nALTER FUNCTION dbo.F\r\n(\r\n    @i integer\r\n)\r\nRETURNS datetime\r\nWITH SCHEMABINDING\r\nAS\r\nBEGIN\r\n    -- Convert with a deterministic style\r\n    RETURN CONVERT(datetime, '19000101', 112);\r\nEND;\r\n```\r\n\r\nThe function properties are now:\r\n\r\n[![New properties][5]][5]\r\n\r\nWith the optimizer freed up, all examples now produce the **desired seek plan**.\r\n\r\n### `CAST` limitation\r\n\r\nNote that using a `CAST` to `datetime` in the function would not work, because it is not possible to specify a conversion style in that syntax:\r\n\r\n```sql\r\nALTER FUNCTION dbo.F\r\n(\r\n    @i integer\r\n)\r\nRETURNS datetime\r\nWITH SCHEMABINDING\r\nAS\r\nBEGIN\r\n    -- Convert with a deterministic style\r\n    RETURN CAST('19000101' AS datetime);\r\nEND;\r\n```\r\n\r\nThis function definition produces the scan plan, and the properties show it remains non-deterministic:\r\n\r\n[![CAST function properties][6]][6]\r\n\r\n  [1]: https://i.stack.imgur.com/L2c9Z.png\r\n  [2]: https://i.stack.imgur.com/CpR1H.png\r\n  [3]: https://i.stack.imgur.com/VdlJH.png\r\n  [4]: https://i.stack.imgur.com/ZLMrR.png\r\n  [5]: https://i.stack.imgur.com/cL0du.png\r\n  [6]: https://i.stack.imgur.com/g88Xg.png	2019-11-24 10:34:46.459126+00	2	4	2	\N	0	0	0	\N	\N	Yes.	f	f
196	250	12	2019-02-25 10:45:03+00	SQL Server chooses to scan the heap tables on the inner side of the loops joins using row-level locks. A full scan would normally choose page-level locking, but a combination of the size of the table and the predicate means the storage engine chooses row locks, since that appears to be the cheapest strategy.\r\n\r\nThe cardinality misestimation deliberately introduced by the `OPTIMIZE FOR` means that the heaps are scanned *many* more times than the optimizer expects, and it does not introduce a spool as it normally would.\r\n\r\nThis combination of factors means that performance is very sensitive to the number of locks required at runtime.\r\n\r\nThe `SELECT` statement benefits from an optimization that allows [row-level shared locks to be skipped][1] (taking only intent-shared page-level locks) when there is no danger of reading uncommitted data, and there is no off-row data.\r\n\r\nThe `INSERT...SELECT` statement does not benefit from this optimization, so millions of RID locks are taken and released each second in the second case, along with the intent-shared page-level locks.\r\n\r\nThe enormous amount of locking activity accounts for the extra CPU and elapsed time.\r\n\r\nThe most natural workaround is to ensure the optimizer (and storage engine) get decent cardinality estimates so they can make good choices.\r\n\r\nIf that is not practical in the real use case, the `INSERT` and `SELECT` statements could be separated, with the result of the `SELECT` held in a variable. This will allow the `SELECT` statement to benefit from the lock-skipping optimization.\r\n\r\nChanging the isolation level can also be made to work, either by not taking shared locks, or by ensuring that lock escalation takes places quickly.\r\n\r\nAs a final point of interest, the query can be made to run even faster than the optimized `SELECT` case by forcing the use of spools using undocumented trace flag 8691.\r\n\r\n  [1]: https://www.sql.kiwi/2010/11/read-committed-shared-locks-and-rollbacks.html	2019-12-03 12:34:28.609937+00	1	4	1	230653	0	0	0	2019-12-03 12:34:28.609937+00	\N	SQL Server chooses to scan the heap tables on the inner side of the loops joins using row-level locks. A full scan would normally choose page-level locking, but a combination of the size of the table and the predicate means the storage engine chooses row locks, since that appears to be the cheapest strategy.	f	f
189	246	1	2019-12-02 20:36:30.317878+00	> Fortunately they mostly have tooltips now (thanks for adding those!), though there's one (I think "..."?) that doesn't.\r\n\r\nI have added a tooltip to the '…' too, thanks for pointing out the omission. I also added tooltips to all the action buttons in the notification area.\r\n\r\n> Messages are preceeded by very tiny text that includes the author, timestamp, and who it's replying to (if applicable). I can't read that.\r\n\r\nTooltips have also been added to these, and now to all the identicons too.\r\n\r\nWe have also bumped up the size of all the smallest text by a notch or two.	2019-12-09 19:40:30.07016+00	7	1	1	\N	0	0	0	\N	\N	> Fortunately they mostly have tooltips now (thanks for adding those!), though there's one (I think "..."?) that doesn't.	f	f
628	580	168	2020-01-16 21:26:20.512676+00	Currently you can mouse-over any avatar and see how many stars the corresponding user has in a tool tip.\r\n\r\nFor example, at the time of writing this you have 29 stars in Meta.\r\n\r\nScreenshot (cursor not visible):\r\n\r\n![stars-tooltip.png](/image?hash=5865d7253d93b6790d45eb167557abd3434f95615d7d06aa5037935f912b4787)	2020-01-16 21:29:18.844202+00	8	6	3	\N	0	0	0	\N	\N	Currently you can mouse-over any avatar and see how many stars the corresponding user has in a tool tip.	f	f
500	457	2	2019-12-18 23:09:48.537671+00	The original dates are now imported and shown, and the date of the import itself is visible in the history.\r\n\r\nWe've gone back over all previously imported posts and re-imported the dates from SE. All imported posts should have a 'history' link even if they haven't been edited after import, and this shows that they were imported, and when, and (for new imports only)[^1] by who.\r\n\r\nIf you spot anything that looks wrong, please let us know in the comments here.\r\n\r\n[^1]: plus ~50% of TeX imports where we were able to positively identify who did the import	2020-01-04 21:53:21.607519+00	6	1	1	\N	0	0	0	\N	\N	The original dates are now imported and shown, and the date of the import itself is visible in the history.	f	f
743	669	16	2017-10-04 01:22:12+00	**What's Memory Grant Feedback All About?**\r\n\r\nBatch Mode Memory Grant Feedback attempts to right-size memory grants for queries, correcting for both over- and under-estimates. \r\n\r\nWhen a query runs that requires a memory grant, the optimizer will ask for a grant of a size that it thinks will keep all row operations in memory. \r\n\r\nThings that commonly require memory grants:\r\n\r\n - Sorts\r\n - Hash Joins\r\n\r\nWhen too much memory is asked for and granted, concurrency may suffer as a result. Memory is a finite resource, and not everything can use all of it all the time. There's no such thing as unlimited access to such resources.\r\n\r\nWhen too little memory is asked for, queries may spill to disk. Look, no one wants their queries spilling anywhere. Disk is dreadful.\r\n\r\n**How Does The Magic Work?**\r\n\r\nWhen a plan that requires a memory grant executes and is cached, actual memory needed to run the plan is recalculated, and the plan information is updated accordingly. Right now, it requires the presence of a ColumnStore index to achieve Batch execution mode.\r\n\r\n**What If The Magic Isn't So Magical?**\r\n\r\nThis feature does have a terminating point in which it will fall back to the original memory grant. If queries run that need constant recalculating, our magical feature will give up. Eventually. As of this writing, I don't have all of the implementation details on when it will quit.\r\n\r\n**How Do I Know If It's Working?**\r\n\r\nYou can use Extended Events:\r\n\r\n[![NUTS][1]][1]\r\n\r\nYou may also observe it during regular query tuning over multiple runs in the actual execution plan.\r\n\r\n**Can You Show Me An Example?**\r\n\r\nOf course! Here's a stored procedure. Under the right circumstances, it will ask for an incorrect memory grant.\r\n\r\n\tCREATE OR ALTER PROCEDURE dbo.LargeUnusedGrant (@OwnerUserId INT)\r\n\tAS \r\n\tBEGIN\r\n\t\r\n\t\tSELECT TOP 200 *\r\n\t\tFROM dbo.Posts_cx AS p\r\n\t\tWHERE p.OwnerUserId = @OwnerUserId\r\n\t\tAND p.PostTypeId = 1\r\n\t\tORDER BY p.Score DESC, p.Id DESC;\r\n\t\r\n\tEND;\r\n\tGO \r\n\r\nThe query plan has a warning on the select operator.\r\n\r\n[![NUTS][2]][2]\r\n\r\nSadness indeed! Our query asked for too much memory.\r\n\r\n[![NUTS][3]][3]\r\n\r\nOn second execution, the warning will disappear.\r\n\r\n**I've Heard About Some Sneaky Tricks...**\r\n\r\n[Itzik Ben-Gan][4] and [Niko Neugebauer][5] have both come up with workarounds to features and operators that require Batch Mode execution.\r\n\r\n1. Create a filtered, non-clustered ColumnStore index with a `WHERE` clause that contains 0 rows\r\n2. Join your table to a temp table with a Clustered ColumnStore index on it with no rows using a superfluous Left Join\r\n\r\nBoth methods are valid workaround to get this to work!\r\n\r\nCreating an empty nonclustered ColumnStore index:\r\n\r\n    /*Itzik's Trizik*/\r\n\tCREATE NONCLUSTERED COLUMNSTORE INDEX ncci_helper\r\n\t    ON dbo.Posts\r\n\t(\r\n\t    Id,\r\n\t    AcceptedAnswerId,\r\n\t    AnswerCount,\r\n\t    ClosedDate,\r\n\t    CommentCount,\r\n\t    CommunityOwnedDate,\r\n\t    CreationDate,\r\n\t    FavoriteCount,\r\n\t    LastActivityDate,\r\n\t    LastEditDate,\r\n\t    LastEditorDisplayName,\r\n\t    LastEditorUserId,\r\n\t    OwnerUserId,\r\n\t    ParentId,\r\n\t    PostTypeId,\r\n\t    Score,\r\n\t    ViewCount,\r\n\t    IsHot )\r\n\t    WHERE ( Id = -2147483647 AND Id = 2147483647) -- eez impossible!\r\n\r\nRunning a different version of the proc against the table with the empty index:\r\n\r\n\tCREATE OR ALTER PROCEDURE dbo.LargeUnusedGrant_alt1 (@OwnerUserId INT)\r\n\tAS \r\n\tBEGIN\r\n\t\r\n\t\tSELECT TOP 200 *\r\n\t\tFROM dbo.Posts AS p\r\n\t\tWHERE p.OwnerUserId = @OwnerUserId\r\n\t\tAND p.PostTypeId = 1\r\n\t\tORDER BY p.Score DESC, p.Id DESC;\r\n\t\r\n\tEND;\r\n\tGO \r\n\r\nUsing the same value:\r\n\r\n\tEXEC dbo.LargeUnusedGrant_alt1 @OwnerUserId = 8672;\r\n\tGO \r\n\t\r\nFirst run: sad memory grant\r\n\r\nSecond run: happy memory grant\r\n\r\nSuperfluous left join:\r\n\r\n\t/*Niko's Triko*/\r\n\tCREATE OR ALTER PROCEDURE dbo.LargeUnusedGrant_alt2 (@OwnerUserId INT)\r\n\tAS \r\n\tBEGIN\r\n\t\r\n\t\tCREATE TABLE #t1 (Id INT, INDEX cx_whatever CLUSTERED COLUMNSTORE);\r\n\t\r\n\t\tSELECT TOP 200 *\r\n\t\tFROM dbo.Posts AS p\r\n\t\tLEFT JOIN #t1 ON 1 = 1\r\n\t\tWHERE p.OwnerUserId = @OwnerUserId\r\n\t\tAND p.PostTypeId = 1\r\n\t\tORDER BY p.Score DESC, p.Id DESC;\r\n\t\r\n\tEND;\r\n\tGO \r\n\t\r\n\tEXEC dbo.LargeUnusedGrant_alt2 @OwnerUserId = 8672;\r\n\tGO \r\n\r\nFirst run: sad memory grant\r\n\r\nSecond run: happy memory grant\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/LoyMN.jpg\r\n  [2]: https://i.stack.imgur.com/ynI4M.jpg\r\n  [3]: https://i.stack.imgur.com/tijL9.jpg\r\n  [4]: http://tsql.solidq.com/\r\n  [5]: http://www.nikoport.com/	2020-02-03 03:08:53.553994+00	3	4	1	187583	0	0	0	2020-02-03 03:08:53.553994+00	\N	**What's Memory Grant Feedback All About?**	f	f
110	158	12	2019-11-27 19:21:55.667514+00	The choice of isolation level is driven by the guarantees your application requires from the database. Full ACID isolation is only provided by serializable isolation; all other isolation levels provide a lesser degree of protection for transactions.\r\n\r\n> Read Committed admits inconsistent results in repeated reads\r\n\r\nThe only guarantee provided by read committed isolation is that only committed data will be read (dirty reads are disallowed). Anything beyond that is implementation specific.\r\n\r\nFor example, in SQL Server, read committed *using row versioning* (RCSI) also guarantees *statement-level* repeatable reads and a *statement-level* point in time view of the database. To be clear, neither of those are guaranteed across multiple statements within the same transaction under RCSI (this is provided by Snapshot Isolation).\r\n\r\nSQL Server's implementation of read committed using *locking* allows repeated reads *within the same statement* to encounter different values, and allows the database to return data from different points in time.\r\n\r\nTo emphasise: a single statement that touches the same row more than once may encounter different values. The multiple reads may not be apparent from inspecting the SQL alone, since the query optimizer has considerable freedom in constructing a physical execution strategy.\r\n\r\n> I can simply cache the first read result in my transaction and let other transactions to update...\r\n\r\nThis is a valid approach, if it meets the isolation requirements of whatever it is you are doing with (or based on) the data.\r\n\r\n> Nothing bad can happen and I can feel as safe as Repeated Read isolation, right?\r\n\r\nNot generally, no. Repeatable read (RR) isolation provides more guarantees than read committed (RC) isolation. Specifically, RR guarantees that data *will not change for the life of the transaction once the data item has been read for the first time*. RR does not guarantee that reads will in fact be entirely repeatable because new data may appear in subsequent reads (phantoms).\r\n\r\nNote that implementations are free to exceed the base requirements of each isolation level, so some products may in fact provide equal guarantees under RC as RR (this is not the case for SQL Server). Indeed, an implementation could make all isolation levels equal to serializable, and still be in compliance with the standard.\r\n\r\n> No, I guess that holding a read lock for the data I read helps to prevent some undesirable scenarios...\r\n\r\nLocking is an implementation detail. It is better to think about the *degree of isolation* your use of the data requires to behave correctly in all circumstances (when multiple concurrent data changes are possible).\r\n\r\n> ...to prevent some undesirable scenarios in booking reservations, probably. Which ones?\r\n\r\nThe ways in which multiple concurrent database transactions can interact unhelpfully are limited only by one's imagination.\r\n\r\nFor example, using RC and caching the value read could result in a lost update, where another transaction updates based on the stored value, before your transaction does the same thing. In terms of histories for two transactions running `x:= x + 1`:\r\n\r\n```none\r\ntransaction 1 reads x (=1)\r\ntransaction 2 reads x (=1) \r\ntransaction 2 writes [x:=x + 1] (=2)\r\ntransaction 2 commits\r\ntransaction 1 writes [x:=x + 1] (=2)\r\ntransaction 1 commits\r\n```\r\n\r\nOnly serializable isolation guarantees that if a transaction that can be shown to produce correct results with no concurrent activity, it will continue to do so when competing with any combination of concurrent transactions.\r\n\r\nNote that this does not mean that concurrent serializable transactions actually execute sequentially, just that they will have the same effects as if they had (in some unspecified order).\r\n\r\nSpecifying any other isolation level involves a trade-off: potentially higher performance and/or concurrency with fewer isolation guarantees. Exactly what the trade-off is varies significantly between implementations.\r\n\r\nChoosing the appropriate isolation level requires comparing the needs of your application with the guarantees available. In addition, you may need to consider other factors, for example whether the transaction requires a point-in-time view of the data or not.\r\n\r\nFor more information and some examples from a SQL Server point of view, please refer to my articles in [SQL Server Isolation Levels : A Series][1]\r\n\r\n  [1]: https://sqlperformance.com/2014/07/t-sql-queries/isolation-levels	2019-11-27 19:21:55.667514+00	2	4	1	\N	0	0	0	\N	\N	The choice of isolation level is driven by the guarantees your application requires from the database. Full ACID isolation is only provided by serializable isolation; all other isolation levels provide a lesser degree of protection for transactions.	f	f
203	257	12	2019-06-19 00:33:24+00	You've written an accidental cross join:\r\n\r\n```\r\nUPDATE Staging.[TdDailyPerformance]\r\nSET [SYS_OPERATION] = 'U'\r\nFROM (\r\n    SELECT [HashCode]           \r\n    FROM Staging.[TdDailyPerformance]\r\n    INTERSECT\r\n    SELECT [HashCode]\r\n    FROM [IdMatch].[TdDailyPerformance]\r\n) AS A\r\n```\r\n\r\nThat is quite a [common error][1] when using the `FROM` extension of T-SQL `UPDATE`.\r\n\r\nThe expectation is that both references to `Staging.[TdDailyPerformance]` identify the same instance of the object, but that is not how it works.\r\n\r\nThe statement above actually specifies that **all** rows of the target should be updated if the derived table `A` produces **any** rows at all. The two instances of `Staging.[TdDailyPerformance]` are bound separately.\r\n\r\nThe query appears to hang for the reasons I discuss separately at the end of this answer.\r\n\r\nThe safest way to write this sort of update is to alias the tables and **always** use an alias as the target. (You should also write the query deterministically such that each target row can only be updated at most once.)\r\n\r\nIf we try to follow the alias rule with the statement above:\r\n\r\n```\r\nUPDATE S_TDP\r\nSET SYS_OPERATION = 'U'\r\nFROM\r\n(\r\n    SELECT S_TDP.HashCode\r\n    FROM Staging.TdDailyPerformance AS S_TDP\r\n    INTERSECT\r\n    SELECT I_TDP.HashCode\r\n    FROM IdMatch.TdDailyPerformance AS I_TDP\r\n) AS A;\r\n```\r\n\r\nWe get a binding error, which alerts us to the mistake:\r\n\r\n>Msg 208, Level 16, State 1, Line xxx  \r\nInvalid object name 'S_TDP'.\r\n\r\nYou're already aware of the working alternatives, so I won't labour that point, except to mention that you should probably look at combining the two updates.\r\n\r\n## Plan Analysis\r\n\r\nI don't know how interested you are in the execution plan for the incorrect update statement, but just in case, here is a brief analysis of the serial version of the plan.\r\n\r\nThe portion of the plan below the *Top* is concerned with finding the first row (if any) resulting from the `INTERSECT`:\r\n\r\n[![Top Subtree][2]][2]\r\n\r\nThe [*Flow Distinct*][3] is a row-goal optimization that aims to produce the first distinct hash code value quickly. The *Nested Loops Join* is chosen because the optimizer only expects the scan the inner side heap table once to find a matching hash code.\r\n\r\nThis strategy is exposed when there is **no match** on hash code. In that case, the inner side will be fully scanned for every row on the outer side - 70,000 full scans in total. This might take a while. You can test the effect of removing the [row goal][4] by using [documented trace flag 4138][5] e.g. via a query hint `OPTION (QUERYTRACEON 4138)`. The update statement will still be incorrect, but at least it won't appear to hang.\r\n\r\nYou don't see this problem when running the `INTERSECT` on its own because the row goal is introduced (with the Top) by the optimizer as it searches for a reasonable plan. You can simulate it with a query like:\r\n\r\n```\r\nSELECT TOP (1) 1\r\nFROM \r\n(\r\n    SELECT S_TDP.HashCode\r\n    FROM Staging.TdDailyPerformance AS S_TDP\r\n    INTERSECT\r\n    SELECT I_TDP.HashCode\r\n    FROM IdMatch.TdDailyPerformance AS I_TDP\r\n) AS A\r\n```\r\n\r\nOr:\r\n\r\n```\r\nSELECT DISTINCT \r\n    TDP.HashCode\r\nFROM Staging.TdDailyPerformance AS TDP\r\nCROSS JOIN \r\n(\r\n    SELECT [HashCode]           \r\n    FROM Staging.[TdDailyPerformance]\r\n    INTERSECT\r\n    SELECT [HashCode]\r\n    FROM [IdMatch].[TdDailyPerformance]\r\n) AS A;\r\n\r\n```\r\n \r\nThe remainder of the plan updates the whole target table if a row was found:\r\n\r\n[![Update][6]][6]\r\n\r\nThe *Nested Loops Join* has no join predicate. The *Sort* and *Stream Aggregate* group records by heap RID. This is pointless but it is part of the general logic used to collapse plans that might update the same target row multiple times to a single (non-deterministic) update per-row.\r\n\r\n\r\n  [1]: https://sqlserverfast.com/blog/hugo/2008/03/lets-deprecate-update-from/\r\n  [2]: https://i.stack.imgur.com/IKx4J.png\r\n  [3]: https://www.sql.kiwi/2010/08/row-goals-and-grouping.html\r\n  [4]: https://www.sql.kiwi/2010/08/inside-the-optimiser-row-goals-in-depth.html\r\n  [5]: https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-traceon-trace-flags-transact-sql\r\n  [6]: https://i.stack.imgur.com/VE92E.png	2019-12-04 01:23:30.352255+00	0	4	1	240864	0	0	0	2019-12-04 01:22:26.090238+00	\N	You've written an accidental cross join:	f	f
204	258	12	2019-06-20 05:32:45+00	This is documented in [UPDATE (Transact-SQL)][1]:\r\n\r\n> SET @variable = column = expression sets the variable to the same value as the column. This differs from SET @variable = column, column = expression, which sets the variable to the pre-update value of the column.\r\n\r\nIn your code example, `sum` is the (unwise) name of a column, not an aggregate.\r\n\r\nDemo:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=8aa0603ba344816390cb287b842385de\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/queries/update-transact-sql\r\n  [2]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=8aa0603ba344816390cb287b842385de	2019-12-04 01:26:19.26907+00	2	4	1	240996	0	0	0	2019-12-04 01:25:12.885374+00	\N	This is documented in [UPDATE (Transact-SQL)][1]:	f	f
125	186	12	2017-04-06 22:16:11+00	The only difficulty is in deciding how to handle the histogram step(s) **partially covered** by the query predicate interval. Whole histogram steps covered by the predicate range are trivial as noted in the question.\r\n\r\n## Legacy Cardinality Estimator\r\n\r\n`F` = fraction (between 0 and 1) of the step range covered by the query predicate.\r\n\r\nThe basic idea is to use `F` (linear interpolation) to determine how many of the intra-step distinct values are covered by the predicate. Multiplying this result by the average number of rows per distinct value (assuming uniformity), and adding the step equal rows gives the cardinality estimate:\r\n\r\n``` none\r\nCardinality = EQ_ROWS + (AVG_RANGE_ROWS * F * DISTINCT_RANGE_ROWS)\r\n```\r\n\r\nThe same formula is used for `>` and `>=` in the legacy CE.\r\n\r\n## New Cardinality Estimator\r\n\r\nThe new CE modifies the previous algorithm slightly to differentiate between `>` and `>=`.\r\n\r\nTaking `>` first, the formula is:\r\n\r\n``` none\r\nCardinality = EQ_ROWS + (AVG_RANGE_ROWS * (F * (DISTINCT_RANGE_ROWS - 1)))\r\n```\r\n\r\nFor `>=` it is:\r\n\r\n``` none\r\nCardinality = EQ_ROWS + (AVG_RANGE_ROWS * ((F * (DISTINCT_RANGE_ROWS - 1)) + 1))\r\n```\r\n\r\nThe `+ 1` reflects that when the comparison involves equality, a match is assumed (the inclusion assumption).\r\n\r\nIn the question example, `F` can be calculated as:\r\n\r\n    DECLARE \r\n        @Q datetime = '1999-10-13T10:48:38.550',\r\n        @K1 datetime = '1999-10-13T10:47:38.550',\r\n        @K2 datetime = '1999-10-13T10:51:19.317';\r\n    \r\n    DECLARE\r\n        @QR float = DATEDIFF(MILLISECOND, @Q, @K2), -- predicate range\r\n        @SR float = DATEDIFF(MILLISECOND, @K1, @K2) -- whole step range\r\n    \r\n    SELECT\r\n        F = @QR / @SR;\r\n\r\nThe result is **0.728219019233034**. Plugging that into the formula for `>=` with the other known values:\r\n\r\n``` none\r\nCardinality = EQ_ROWS + (AVG_RANGE_ROWS * ((F * (DISTINCT_RANGE_ROWS - 1)) + 1))\r\n            = 16 + (16.1956 * ((0.728219019233034 * (409 - 1)) + 1))\r\n            = 16 + (16.1956 * ((0.728219019233034 * 408) + 1))\r\n            = 16 + (16.1956 * (297.113359847077872 + 1))\r\n            = 16 + (16.1956 * 298.113359847077872)\r\n            = 16 + 4828.1247307393343837632\r\n            = 4844.1247307393343837632\r\n            = <b>4844.12473073933</b> (to float precision)\r\n```\r\n\r\nThis result agrees with the estimate of 4844.13 shown in the question.\r\n\r\nThe same query using the legacy CE (e.g. using trace flag 9481) should produce an estimate of:\r\n\r\n``` none\r\nCardinality = EQ_ROWS + (AVG_RANGE_ROWS * F * DISTINCT_RANGE_ROWS)\r\n            = 16 + (16.1956 * 0.728219019233034 * 409)\r\n            = 16 + 4823.72307468722\r\n            = <b>4839.72307468722</b>\r\n```\r\n\r\nNote the estimate would be the same for `>` and `>=` using the legacy CE.	2019-11-28 13:57:27.284766+00	2	4	1	169384	0	0	0	2019-11-28 13:56:15.80056+00	\N	The only difficulty is in deciding how to handle the histogram step(s) **partially covered** by the query predicate interval. Whole histogram steps covered by the predicate range are trivial as noted in the question.	f	f
817	707	905	2017-01-11 22:33:52+00	I wanted to share my experience with trace flag 4199.\r\n\r\nI just finished diagnosing a performance issue on a customer system running SQL Server 2012 SP3.  The customer was moving a reporting database away from their production OLTP server onto a new server.  The customer's goal was to remove competition for resources with the OLTP queries.  Unfortunately, the customer said the new reporting server was very slow.\r\n\r\nA sample query run on the OLTP system completed in 1.6 seconds.  The query plan did an index seek on a ~200 million row table that was part of a view.\r\n\r\nOn the new server, the same query completed in 10 minutes 44 seconds.  It performed an index scan on the same ~200 million row table.\r\n\r\nThe reporting server data was a copy of the OLTP data, so it did not appear to be a difference in the data.\r\n\r\nI was stumped until I recalled that our software (which runs their OLTP system) enabled some trace flags on startup.  One of them, 4199, I recalled was a query optimizer fix.\r\n\r\nI tested enabling trace flag 4199 on the customer's new reporting server, and the reporting query completed in 0.6 seconds.  (Wow!)  I disabled the trace flag, and the query was back to completing in 10 min 44 sec.  Enabled the flag: back to 0.6 seconds.  Apparently, enabling the trace flag enabled the optimizer to use an index seek into the view on the 200 million row table.\r\n\r\nIn this case, the query optimizations enabled by trace flag 4199 made an enormous difference.  Your experiences may vary.  However, based on this experience, it definitely seems worth enabling to me.\r\n	2020-02-13 01:41:12.964+00	2	4	1	160793	0	0	0	2020-02-13 01:41:12.964+00	\N	I wanted to share my experience with trace flag 4199.	f	f
816	707	904	2015-06-03 20:05:57+00	My search on the topic brought me here, so I'd just like to share my recent experience on the topic.\r\n\r\nI was running SQL 2014, so I figured that I would be safe from having to care about 4199 for a little bit... but it just wasn't true...\r\n\r\n**How to Diagnose if you need 4199**\r\n\r\nIf your **query** appears to run **poorly**, particularly when you feel it shouldn't, then try adding the following to the end of it too see if it fixes all your problems, as you might need **4199** **("Enable all Query Optimizer fixes."**)\r\n\r\n    SELECT SomeColumn\r\n    FROM SomeTable    \r\n    OPTION(QUERYTRACEON 4199)\r\n\r\nIn my situation, I had a top 10 clause blowing up a query that ran fine without, which is what made me think something fishy was happening, and that 4199 might help.\r\n\r\n**About 4199**\r\n\r\nAny SQL Server Query Optimizer bug/performance fixes that are created after the new major version release actually get hidden and blocked.  This is in case they might actually harm some other theoretically perfectly optimized program.  So, install updates as you might, the actual query optimizer changes are not enabled by default.  Therefore, once a single fix or enhancement has been done, 4199 becomes a necessity if you want to take advantage of it.  As many fixes show up, you'll eventually find yourself turning this on when one of them affects you.  These fixes usually are tied to their own trace flags, but 4199 is used as the master "Turn every fix on."\r\n\r\nIf you know which fixes you need, you could enable them piece-meal instead of using 4199.  If you want to enable all fixes, use 4199.\r\n\r\n**Ok, So you want 4199 Globally...**\r\n\r\nJust Create a SQL Agent Job which runs every morning with the following line to enable the trace flag globally.  This ensures if anyone turned them off or etc, that they get turned back on.  This job step has pretty simple sql:\r\n\r\n    DBCC TRACEON (4199, -1);\r\n\r\nWhere -1 specifies the Global part in DBCC TRACEON.  For more info see:\r\n\r\nhttps://msdn.microsoft.com/en-us/library/ms187329.aspx?f=255&MSPPError=-2147217396\r\n\r\n**"Recompiling" Query Plans**\r\n\r\nIn my most recent attempt I had to enable 4199 globally, **and then also remove existing cached query plans**:\r\n\r\n    sp_recompile 'dbo.SomeTable'\r\n\r\nhttps://msdn.microsoft.com/en-us/library/ms181647.aspx?f=255&MSPPError=-2147217396\r\n\r\nWhere the recompile stored procedure finds any query plans relating to the database object (such as a table) and deletes those query plans, requiring the next attempt to run a similar query to compile them.\r\n\r\nSo, in my case 4199 kept the bad query plans from being created, but I also had to remove those that were still cached via sp_recompile.  Pick any table from the known query affected and you should be good to try that query again, assuming you have now enabled 4199 globally and cleared the offending cached query plan.\r\n\r\n**In Conclusion on 4199**\r\n\r\nAs you utilize indexes, a smart query plan optimization becomes important to actually using those indexes intelligently, and assuming that over time some fix to the query optimization process will be released, you're generally in safe water to just run with 4199 globally enabled, as long as you realize that some new fix might not actually play as nicely with a highly optimized database that was such optimized in the prior environment before said fix.  But what does 4199 do?  It just enables all fixes.	2020-02-13 01:41:12.685633+00	2	4	1	103194	0	0	0	2020-02-13 01:41:12.685633+00	\N	My search on the topic brought me here, so I'd just like to share my recent experience on the topic.	f	f
815	707	903	2015-05-22 16:11:31+00	Personally, whenever I build a new server for a new project I always enable TF4199 globally. The same applies when I upgrade existing instances to newer versions. \r\n\r\nThe TF enables new fixes that would affect the behaviour of the application, but for new projects the risk of regression is not an issue. For instances upgraded from previous versions, the differences between old and new version are a concern on their own and having to deal with plan regression is expected anyway, so I prefer fighting with it with TF4199 enabled.\r\n\r\nAS far as existing databases is concerned, there is only one way to know: test it. You can capture a workload on the existing setup and replay it after enabling the flag. RML Utilities can help you automate the process, as described in [this answer][1].\r\n\r\nObviously, the flag affects the whole instance, so you'll have to test all the databases sitting there.\r\n\r\n  [1]: https://dba.stackexchange.com/questions/101374/capture-and-replay-workload/101392#101392	2020-02-13 01:41:12.395903+00	3	4	1	102295	0	0	0	2020-02-13 01:41:12.395903+00	\N	Personally, whenever I build a new server for a new project I always enable TF4199 globally. The same applies when I upgrade existing instances to newer versions.	f	f
148	206	12	2018-02-27 13:44:43+00	The SQL Server database is ready to accept queries as soon as:\r\n\r\n    SELECT DATABASEPROPERTYEX(N'database name', 'Collation')\r\n\r\ndoes not return `NULL`.\r\n\r\nFrom the documentation for [`DATABASEPROPERTYEX` (Transact-SQL)][1]:\r\n\r\n>Note: The `ONLINE` status may be returned while the database is being opened and is not yet recovered. To identify when a database can accept connections, query the Collation property of `DATABASEPROPERTYEX`. The database can accept connections when the database collation returns a non-null value. For Always On databases, query the `database_state` or `database_state_desc` columns of `sys.dm_hadr_database_replica_states`.\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/functions/databasepropertyex-transact-sql	2019-11-30 12:51:50.609757+00	2	4	1	198912	0	0	0	2019-11-30 12:51:50.609757+00	\N	The SQL Server database is ready to accept queries as soon as:	f	f
197	251	12	2019-02-26 04:10:10+00	This is what I call *Manual Halloween Protection*.\r\n\r\nYou can find an example of it being used with an update statement in my article [Optimizing Update Queries][1]. One has to be a bit careful to preserve the same semantics, for example by locking the target table against all concurrent modifications while the separate queries execute, if that is relevant in your scenario.\r\n\r\n>Why is the plan with the temp table more efficient? Isn't an eager spool mostly just an internal temp table anyway?\r\n\r\nA spool has some of the characteristics of a temporary table, but the two are not exact equivalents. In particular, a spool is essentially a row-by-row [unordered insert to a b-tree structure][2]. It does benefit from locking and logging optimizations, but does not support *bulk load optimizations*.\r\n\r\nConsequently, one can often get better performance by splitting the query in a natural way: Bulk loading the new rows into a temporary table or variable, then performing an optimized insert (without explicit Halloween Protection) from the temporary object.\r\n\r\nMaking this separation also allows you extra freedom to tune the read and write portions of the original statement separately.\r\n\r\nAs a side note, it is interesting to think about how the Halloween Problem might be addressed using row versions. Perhaps a future version of SQL Server will provide that feature in suitable circumstances.\r\n\r\n---\r\n\r\nAs Michael Kutz alluded to in a comment, you could also explore the possibility of exploiting the [hole-filling optimization][3] to avoid explicit HP. One way to achieve this for the demo is to create a unique index (clustered if you like) on the `ID` column of `A_HEAP_OF_MOSTLY_NEW_ROWS`.\r\n\r\n```\r\nCREATE UNIQUE INDEX i ON dbo.A_HEAP_OF_MOSTLY_NEW_ROWS (ID);\r\n```\r\n\r\nWith that guarantee in place the optimizer can use hole-filling and rowset sharing:\r\n\r\n```\r\nMERGE dbo.HALLOWEEN_IS_COMING_EARLY_THIS_YEAR WITH (SERIALIZABLE) AS HICETY\r\nUSING dbo.A_HEAP_OF_MOSTLY_NEW_ROWS AS AHOMNR\r\n    ON AHOMNR.ID = HICETY.ID\r\nWHEN NOT MATCHED BY TARGET\r\nTHEN INSERT (ID) VALUES (AHOMNR.ID);\r\n```\r\n\r\n[![MERGE plan][4]][4]\r\n\r\nWhile interesting, you will still be able to achieve better performance in many cases by employing carefully-implemented Manual Halloween Protection.\r\n\r\n\r\n  [1]: https://sqlperformance.com/2015/12/sql-plan/optimizing-update-queries\r\n  [2]: https://sqlperformance.com/2019/11/sql-performance/eager-index-spool-optimizer\r\n  [3]: https://sqlperformance.com/2013/02/sql-plan/halloween-problem-part-3\r\n  [4]: https://i.stack.imgur.com/1aHPS.png	2019-12-03 12:36:10.881367+00	3	4	1	230729	0	0	0	2019-12-03 12:36:10.881367+00	\N	This is what I call *Manual Halloween Protection*.	f	f
272	300	751	2012-02-19 16:26:30+00	The output of `SET STATISTICS IO ON` for both looks similar\r\n\r\n    SET STATISTICS IO ON;\r\n    PRINT 'V2'\r\n    EXEC dbo.V2 10\r\n    PRINT 'T2'\r\n    EXEC dbo.T2 10\r\n\r\nGives\r\n\r\n    V2\r\n    Table '#58B62A60'. Scan count 0, logical reads 20\r\n    Table 'NUM'. Scan count 1, logical reads 3\r\n    \r\n    Table '#58B62A60'. Scan count 10, logical reads 20\r\n    Table 'NUM'. Scan count 1, logical reads 3\r\n    \r\n    T2\r\n    Table '#T__ ... __00000000E2FE'. Scan count 0, logical reads 20\r\n    Table 'NUM'. Scan count 1, logical reads 3\r\n    \r\n    Table '#T__ ... __00000000E2FE'. Scan count 0, logical reads 20\r\n    Table 'NUM'. Scan count 1, logical reads 3\r\n\r\nAnd as Aaron points out in the comments the plan for the table variable version is actually less efficient as whilst both have a nested loops plan driven by an index seek on `dbo.NUM` the `#temp` table version performs a seek into the index on `[#T].n = [dbo].[NUM].[n]` with residual predicate `[#T].[n]<=[@total]` whereas the table variable version performs an index seek on `@V.n <= [@total]` with residual predicate `@V.[n]=[dbo].[NUM].[n]` and so processes more rows (which is why this plan performs so poorly for larger number of rows)\r\n\r\nUsing [Extended Events][1] to look at the wait types for the specific spid gives these results for 10,000 executions of `EXEC dbo.T2 10`\r\n\r\n    +---------------------+------------+----------------+----------------+----------------+\r\n    |                     |            |     Total      | Total Resource |  Total Signal  |\r\n    | Wait Type           | Wait Count | Wait Time (ms) | Wait Time (ms) | Wait Time (ms) |\r\n    +---------------------+------------+----------------+----------------+----------------+\r\n    | SOS_SCHEDULER_YIELD | 16         | 19             | 19             | 0              |\r\n    | PAGELATCH_SH        | 39998      | 14             | 0              | 14             |\r\n    | PAGELATCH_EX        | 1          | 0              | 0              | 0              |\r\n    +---------------------+------------+----------------+----------------+----------------+\r\n\r\nand these results for 10,000 executions of `EXEC dbo.V2 10`\r\n\r\n    +---------------------+------------+----------------+----------------+----------------+\r\n    |                     |            |     Total      | Total Resource |  Total Signal  |\r\n    | Wait Type           | Wait Count | Wait Time (ms) | Wait Time (ms) | Wait Time (ms) |\r\n    +---------------------+------------+----------------+----------------+----------------+\r\n    | PAGELATCH_EX        | 2          | 0              | 0              | 0              |\r\n    | PAGELATCH_SH        | 1          | 0              | 0              | 0              |\r\n    | SOS_SCHEDULER_YIELD | 676        | 0              | 0              | 0              |\r\n    +---------------------+------------+----------------+----------------+----------------+\r\n\r\nSo it is clear that the number of `PAGELATCH_SH` waits is much higher in the `#temp` table case. I'm not aware of any way of adding the wait resource to the extended events trace so to investigate this further I ran \r\n\r\n    WHILE 1=1\r\n    EXEC dbo.T2 10\r\n\r\nWhilst in another connection polling `sys.dm_os_waiting_tasks`\r\n\r\n    CREATE TABLE #T(resource_description NVARCHAR(2048))\r\n    \r\n    WHILE 1=1\r\n    INSERT INTO #T\r\n    SELECT resource_description\r\n    FROM sys.dm_os_waiting_tasks\r\n    WHERE session_id=<spid_of_other_session> and wait_type='PAGELATCH_SH'\r\n\r\nAfter leaving that running for about 15 seconds it had gathered the following results\r\n\r\n    +-------+----------------------+\r\n    | Count | resource_description |\r\n    +-------+----------------------+\r\n    |  1098 | 2:1:150              |\r\n    |  1689 | 2:1:146              |\r\n    +-------+----------------------+\r\n\r\nBoth of these pages being latched belong to (different) non clustered indexes on the `tempdb.sys.sysschobjs` base table named `'nc1'` and `'nc2'`.\r\n\r\nQuerying `tempdb.sys.fn_dblog` during the runs indicates that the number of log records added by the first execution of each stored procedure was somewhat variable but for subsequent executions the number added by each iteration was very consistent and predictable. Once the procedure plans are cached the number of log entries are about half those needed for the `#temp` version.\r\n\r\n    +-----------------+----------------+------------+\r\n    |                 | Table Variable | Temp Table |\r\n    +-----------------+----------------+------------+\r\n    | First Run       |            126 | 72 or 136  |\r\n    | Subsequent Runs |             17 | 32         |\r\n    +-----------------+----------------+------------+\r\n\r\nLooking at the transaction log entries in more detail for the `#temp` table version of the SP each subsequent invocation of the stored procedure creates three transactions and the table variable one only two.\r\n\r\n    +---------------------------------+----+---------------------------------+----+\r\n    |           #Temp Table                |         @Table Variable              |\r\n    +---------------------------------+----+---------------------------------+----+\r\n    | CREATE TABLE                    |  9 |                                 |    |\r\n    | INSERT                          | 12 | TVQuery                         | 12 |\r\n    | FCheckAndCleanupCachedTempTable | 11 | FCheckAndCleanupCachedTempTable |  5 |\r\n    +---------------------------------+----+---------------------------------+----+\r\n\r\nThe `INSERT`/`TVQUERY` transactions are identical except for the name. This contains the log records for each of the 10 rows inserted to the temporary table or table variable plus the `LOP_BEGIN_XACT`/ `LOP_COMMIT_XACT` entries.\r\n\r\nThe `CREATE TABLE` transaction only appears in the `#Temp` version and looks as follows.\r\n\r\n    +-----------------+-------------------+---------------------+\r\n    |    Operation    |      Context      |    AllocUnitName    |\r\n    +-----------------+-------------------+---------------------+\r\n    | LOP_BEGIN_XACT  | LCX_NULL          |                     |\r\n    | LOP_SHRINK_NOOP | LCX_NULL          |                     |\r\n    | LOP_MODIFY_ROW  | LCX_CLUSTERED     | sys.sysschobjs.clst |\r\n    | LOP_DELETE_ROWS | LCX_MARK_AS_GHOST | sys.sysschobjs.nc1  |\r\n    | LOP_INSERT_ROWS | LCX_INDEX_LEAF    | sys.sysschobjs.nc1  |\r\n    | LOP_DELETE_ROWS | LCX_MARK_AS_GHOST | sys.sysschobjs.nc2  |\r\n    | LOP_INSERT_ROWS | LCX_INDEX_LEAF    | sys.sysschobjs.nc2  |\r\n    | LOP_MODIFY_ROW  | LCX_CLUSTERED     | sys.sysschobjs.clst |\r\n    | LOP_COMMIT_XACT | LCX_NULL          |                     |\r\n    +-----------------+-------------------+---------------------+\r\n\r\nThe `FCheckAndCleanupCachedTempTable` transaction appears in both but has 6 additional entries in the `#temp` version. These are the 6 rows referring to `sys.sysschobjs` and they have exactly the same pattern as above.\r\n\r\n    +-----------------+-------------------+----------------------------------------------+\r\n    |    Operation    |      Context      |                AllocUnitName                 |\r\n    +-----------------+-------------------+----------------------------------------------+\r\n    | LOP_BEGIN_XACT  | LCX_NULL          |                                              |\r\n    | LOP_DELETE_ROWS | LCX_NONSYS_SPLIT  | dbo.#7240F239.PK__#T________3BD0199374293AAB |\r\n    | LOP_HOBT_DELTA  | LCX_NULL          |                                              |\r\n    | LOP_HOBT_DELTA  | LCX_NULL          |                                              |\r\n    | LOP_MODIFY_ROW  | LCX_CLUSTERED     | sys.sysschobjs.clst                          |\r\n    | LOP_DELETE_ROWS | LCX_MARK_AS_GHOST | sys.sysschobjs.nc1                           |\r\n    | LOP_INSERT_ROWS | LCX_INDEX_LEAF    | sys.sysschobjs.nc1                           |\r\n    | LOP_DELETE_ROWS | LCX_MARK_AS_GHOST | sys.sysschobjs.nc2                           |\r\n    | LOP_INSERT_ROWS | LCX_INDEX_LEAF    | sys.sysschobjs.nc2                           |\r\n    | LOP_MODIFY_ROW  | LCX_CLUSTERED     | sys.sysschobjs.clst                          |\r\n    | LOP_COMMIT_XACT | LCX_NULL          |                                              |\r\n    +-----------------+-------------------+----------------------------------------------+\r\n\r\nLooking at these 6 rows in both transactions they correspond to the same operations. The first `LOP_MODIFY_ROW, LCX_CLUSTERED` is an update to the `modify_date` column in `sys.objects`. The remaining five rows are all concerned with object renaming. Because `name` is a key column of both affected NCIs (`nc1` and `nc2`) this is carried out as a delete/insert for those then it goes back to the clustered index and updates that too.\r\n\r\nIt appears that for the `#temp` table version when the stored procedure ends part of the clean up carried out by the `FCheckAndCleanupCachedTempTable` transaction is to rename the temp table from something like `#T__________________________________________________________________________________________________________________00000000E316` to a different internal name such as `#2F4A0079` and when it is entered the `CREATE TABLE` transaction renames it back. This flip flopping name can be seen by in one connection executing `dbo.T2` in a loop whilst in another \r\n\r\n    WHILE 1=1\r\n    SELECT name, object_id, create_date, modify_date\r\n    FROM tempdb.sys.objects \r\n    WHERE name LIKE '#%'\r\n\r\nExample Results\r\n\r\n![Screenshot][2]\r\n\r\nSo one potential explanation for the observed performance differential as alluded to by Alex is that it is this additional work maintaining the system tables in `tempdb` that is responsible.\r\n\r\n-----\r\n\r\nRunning both procedures in a loop the Visual Studio Code profiler reveals the following\r\n\r\n    +-------------------------------+--------------------+-------+-----------+\r\n    |           Function            |    Explanation     | Temp  | Table Var |\r\n    +-------------------------------+--------------------+-------+-----------+\r\n    | CXStmtDML::XretExecute        | Insert ... Select  | 16.93 | 37.31     |\r\n    | CXStmtQuery::ErsqExecuteQuery | Select Max         | 8.77  | 23.19     |\r\n    +-------------------------------+--------------------+-------+-----------+\r\n    | Total                         |                    | 25.7  | 60.5      |\r\n    +-------------------------------+--------------------+-------+-----------+\r\n\r\nThe table variable version spends about 60% of the time performing the insert statement and the subsequent select whereas the temporary table is less than half that. This is inline with the timings shown in the OP and with the conclusion above that the difference in performance is down to time spent performing ancillary work not due to time spent in the query execution itself.\r\n\r\nThe most important functions contributing towards the "missing" 75% in the temporary table version are\r\n\r\n    +------------------------------------+-------------------+\r\n    |              Function              | Inclusive Samples |\r\n    +------------------------------------+-------------------+\r\n    | CXStmtCreateTableDDL::XretExecute  | 26.26%            |\r\n    | CXStmtDDL::FinishNormalImp         | 4.17%             |\r\n    | TmpObject::Release                 | 27.77%            |\r\n    +------------------------------------+-------------------+\r\n    | Total                              | 58.20%            |\r\n    +------------------------------------+-------------------+\r\n\r\nUnder both the create and release functions the function `CMEDProxyObject::SetName` is shown with an inclusive sample value of `19.6%`. From which I infer that 39.2% of the time in the temporary table case is taken up with the renaming described earlier.\r\n\r\nAnd the largest ones in the table variable version contributing to the other 40% are\r\n\r\n    +-----------------------------------+-------------------+\r\n    |             Function              | Inclusive Samples |\r\n    +-----------------------------------+-------------------+\r\n    | CTableCreate::LCreate             | 7.41%             |\r\n    | TmpObject::Release                | 12.87%            |\r\n    +-----------------------------------+-------------------+\r\n    | Total                             | 20.28%            |\r\n    +-----------------------------------+-------------------+\r\n\r\n## Temporary Table Profile\r\n\r\n[![enter image description here][3]][3]\r\n\r\n\r\n\r\n\r\n## Table Variable Profile\r\n\r\n[![enter image description here][4]][4]\r\n \r\n\r\n\r\n  [1]: http://sqlskills.com/BLOGS/PAUL/post/Capturing-wait-stats-for-a-single-operation.aspx\r\n  [2]: https://i.stack.imgur.com/yXNRJ.png\r\n  [3]: https://i.stack.imgur.com/FE4TL.png\r\n  [4]: https://i.stack.imgur.com/kJFDv.png	2019-12-04 22:55:12.481079+00	5	4	1	13412	0	0	0	2019-12-04 22:55:12.481079+00	\N	The output of `SET STATISTICS IO ON` for both looks similar	f	f
273	300	16	2019-09-13 21:08:47+00	Disco Inferno\r\n--\r\nSince this is an older question, I decided to revisit the issue on newer versions of SQL Server to see if the same performance profile still exists, or if the characteristics have changed at all. \r\n\r\nSpecifically, the addition of [in-memory system tables for SQL Server 2019][1] seems a worthwhile occasion to re-test.\r\n\r\nI'm using a slightly different test harness, since I ran into this issue while working on something else.\r\n\r\nTesting, testing\r\n--\r\nUsing the [2013 version of Stack Overflow][2], I have this index and these two procedures:\r\n\r\nIndex:\r\n\r\n    CREATE INDEX ix_whatever \r\n        ON dbo.Posts(OwnerUserId) INCLUDE(Score);\r\n    GO\r\n\r\n \r\n\r\n\r\nTemp table:\r\n    \r\n        CREATE OR ALTER PROCEDURE dbo.TempTableTest(@Id INT)\r\n        AS\r\n        BEGIN\r\n        SET NOCOUNT ON;\r\n            \r\n        \tCREATE TABLE #t(i INT NOT NULL);\r\n        \tDECLARE @i INT;\r\n        \r\n        \tINSERT #t ( i )\r\n            SELECT p.Score\r\n        \tFROM dbo.Posts AS p\r\n        \tWHERE p.OwnerUserId = @Id;\r\n        \r\n        \tSELECT @i = AVG(t.i)\r\n        \tFROM #t AS t;\r\n        \r\n        END;\r\n        GO \r\n\r\n\r\nTable variable:\r\n        \r\n        CREATE OR ALTER PROCEDURE dbo.TableVariableTest(@Id INT)\r\n        AS\r\n        BEGIN\r\n        SET NOCOUNT ON;\r\n            \r\n        \tDECLARE @t TABLE (i INT NOT NULL);\r\n        \tDECLARE @i INT;\r\n        \r\n        \tINSERT @t ( i )\r\n            SELECT p.Score\r\n        \tFROM dbo.Posts AS p\r\n        \tWHERE p.OwnerUserId = @Id;\r\n        \r\n        \tSELECT @i = AVG(t.i)\r\n        \tFROM @t AS t;\r\n        \r\n        END;\r\n        GO \r\n\r\nTo prevent any potential [ASYNC_NETWORK_IO waits][3], I'm using wrapper procedures.\r\n\r\n    CREATE PROCEDURE #TT AS\r\n    SET NOCOUNT ON;\r\n        DECLARE @i INT = 1;\r\n        DECLARE @StartDate DATETIME2(7) = SYSDATETIME();\r\n        \r\n        WHILE @i <= 50000\r\n            BEGIN\r\n                EXEC dbo.TempTableTest @Id = @i;\r\n                SET @i += 1;\r\n            END;\r\n        SELECT DATEDIFF(MILLISECOND, @StartDate, SYSDATETIME()) AS [ElapsedTimeMilliseconds];\r\n    GO\r\n       \r\n    CREATE PROCEDURE #TV AS\r\n    SET NOCOUNT ON;\r\n        DECLARE @i INT = 1;\r\n        DECLARE @StartDate DATETIME2(7) = SYSDATETIME();\r\n        \r\n        WHILE @i <= 50000\r\n            BEGIN\r\n                EXEC dbo.TableVariableTest @Id = @i;\r\n                SET @i += 1;\r\n            END;\r\n        SELECT DATEDIFF(MILLISECOND, @StartDate, SYSDATETIME()) AS [ElapsedTimeMilliseconds];\r\n    GO\r\n\r\n\r\nSQL Server 2017\r\n--\r\nSince 2014 and 2016 are basically RELICS at this point, I'm starting my testing with 2017. Also, for brevity, I'm jumping right to profiling the code with [Perfview][4]. In real life, I looked at waits, latches, spinlocks, crazy trace flags, and other stuff. \r\n\r\nProfiling the code is the only thing that revealed anything of interest.\r\n\r\n**Time difference:**\r\n\r\n- Temp Table: 17891 ms\r\n- Table Variable: 5891 ms\r\n\r\nStill a very clear difference, eh? But what's SQL Server hitting now?\r\n\r\n[![NUTS][5]][5]\r\n\r\nLooking at the top two increases in the diffed samples, we see `sqlmin` and `sqlsqllang!TCacheStore<CacheClockAlgorithm>::GetNextUserDataInHashBucket` are the two biggest offenders.\r\n\r\n[![NUTS][6]][6]\r\n\r\nJudging by the names in the call stacks, cleaning up and internally renaming temp tables seems to be the biggest time sucks in the temp table call vs. the table variable call.\r\n\r\nEven though table variables are internally backed by temp tables, this doesn't seem to be an issue.\r\n\r\n    SET STATISTICS IO ON;\r\n    DECLARE @t TABLE(id INT);\r\n    SELECT * FROM @t AS t;\r\n\r\n> Table '#B98CE339'. Scan count 1\r\n\r\nLooking through the call stacks for the table variable test doesn't show either of the main offenders at all:\r\n\r\n[![NUTS][7]][7]\r\n\r\nSQL Server 2019 (Vanilla)\r\n--\r\nAlright, so this is still an issue in SQL Server 2017, is anything different in 2019 out of the box?\r\n\r\nFirst, to show there's nothing up my sleeve:\r\n\r\n    SELECT c.name,\r\n           c.value_in_use,\r\n           c.description\r\n    FROM sys.configurations AS c\r\n    WHERE c.name = 'tempdb metadata memory-optimized';\r\n\r\n[![NUTS][8]][8]\r\n\r\n**Time difference:**\r\n\r\n- Temp table: 15765 ms\r\n- Table Variable: 7250 ms\r\n\r\nBoth procedures were different. The temp table call was a couple seconds faster, and the table variable call was about 1.5 seconds slower. The table variable slow down may be partially explained by [table variable deferred compilation][9], a new optimizer choice in 2019.\r\n\r\nLooking at the diff in Perfview, it has changed a bit -- sqlmin is no longer there -- but `sqllang!TCacheStore<CacheClockAlgorithm>::GetNextUserDataInHashBucket` is.\r\n\r\n[![NUTS][10]][10]\r\n\r\nSQL Server 2019 (In-Memory Tempdb system tables)\r\n--\r\nWhat about this new in memory system table thing? Hm? Sup with that?\r\n\r\nLet's turn it on!\r\n\r\n    EXEC sys.sp_configure @configname = 'advanced', \r\n                          @configvalue = 1  \r\n    RECONFIGURE;\r\n    \r\n    EXEC sys.sp_configure @configname = 'tempdb metadata memory-optimized', \r\n                          @configvalue = 1 \r\n    RECONFIGURE;\r\n\r\nNote that this requires a SQL Server restart to kick in, so pardon me while I reboot SQL on this lovely Friday afternoon.\r\n\r\nNow things look different:\r\n\r\n    SELECT c.name,\r\n           c.value_in_use,\r\n           c.description\r\n    FROM sys.configurations AS c\r\n    WHERE c.name = 'tempdb metadata memory-optimized';\r\n    \r\n    SELECT *, \r\n           OBJECT_NAME(object_id) AS object_name, \r\n    \t   @@VERSION AS sql_server_version\r\n    FROM tempdb.sys.memory_optimized_tables_internal_attributes;\r\n\r\n\r\n[![NUTS][11]][11]\r\n\r\n\r\n**Time difference:**\r\n\r\n - Temp table: 11638 ms\r\n - Table variable: 7403 ms\r\n\r\nThe temp tables did about 4 seconds better! That's something.\r\n\r\nI like something.\r\n\r\nThis time, the Perfview diff isn't very interesting. Side by side, it's interesting to note how close the times are across the board:\r\n\r\n[![NUTS][12]][12]\r\n\r\nOne interesting point in the diff are the calls to `hkengine!`, which may seem obvious since hekaton-ish features are now in use.\r\n\r\n[![NUTS][13]][13]\r\n\r\nAs far as the top two items in the diff, I can't make much of `ntoskrnl!?`:\r\n\r\n[![NUTS][14]][14]\r\n\r\nOr `sqltses!CSqlSortManager_80::GetSortKey`, but they're here for Smrtr Ppl™ to look at:\r\n\r\n[![NUTS][15]][15]\r\n\r\nNote that there is an undocumented and definitely not safe for production so please don't use it [startup trace flag][16] you can use to have additional temp table system objects (sysrowsets, sysallocunits, and sysseobjvalues) included in the in-memory feature, but it didn't make a noticeable difference in execution times in this case.\r\n\r\nRoundup\r\n--\r\nEven in newer versions of SQL server, high frequency calls to table variables are much faster than high frequency calls to temp tables. \r\n\r\nThough it's tempting to blame compilations, recompilations, auto stats, latches, spinlocks, caching, or other issues, the issue is clearly still around managing temp table cleanup. \r\n\r\nIt's a closer call in SQL Server 2019 with in-memory system tables enabled, but table variables still perform better when call frequency is high. \r\n\r\nOf course, as a vaping sage once mused: "use table variables when plan choice isn't an issue".\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/databases/tempdb-database?view=sql-server-2017#memory-optimized-tempdb-metadata\r\n  [2]: http://bit.ly/Stack2013\r\n  [3]: https://dba.stackexchange.com/q/232816/32281\r\n  [4]: https://github.com/microsoft/perfview\r\n  [5]: https://i.stack.imgur.com/e3mI2.jpg\r\n  [6]: https://i.stack.imgur.com/BaoJv.jpg\r\n  [7]: https://i.stack.imgur.com/mkeXm.jpg\r\n  [8]: https://i.stack.imgur.com/X7Ggr.jpg\r\n  [9]: https://blogs.msdn.microsoft.com/sqlserverstorageengine/2018/07/16/public-preview-of-table-variable-deferred-compilation-in-azure-sql-database/\r\n  [10]: https://i.stack.imgur.com/ZTyNy.jpg\r\n  [11]: https://i.stack.imgur.com/cXrkW.jpg\r\n  [12]: https://i.stack.imgur.com/rrCGz.jpg\r\n  [13]: https://i.stack.imgur.com/LT3E3.jpg\r\n  [14]: https://i.stack.imgur.com/l2vDJ.jpg\r\n  [15]: https://i.stack.imgur.com/ORdLd.jpg\r\n  [16]: https://dba.stackexchange.com/users/3899/chris-betti	2019-12-04 22:55:12.751185+00	4	4	1	248756	0	0	0	2019-12-04 22:55:12.751185+00	\N	Disco Inferno	f	f
637	515	2	2020-01-18 23:56:22.489545+00	Here is the meta post where you can request a new community, along with an example (TeX, currently in private beta):\r\n\r\n@@@ answer 284	2020-01-18 23:56:40.641659+00	2	1	1	\N	0	0	0	\N	\N	Here is the meta post where you can request a new community, along with an example (TeX, currently in private beta):	f	f
793	483	2	2020-02-10 17:22:43.355093+00	How would you feel about a weaker connection, for example just adding the text: "@Person, re [your answer](), " by default when the 'comment' link is clicked?	2020-02-10 17:22:43.355093+00	3	1	1	\N	0	0	0	\N	\N	How would you feel about a weaker connection, for example just adding the text: "@Person, re [your answer](), " by default when the 'comment' link is clicked?	t	f
215	266	45	2017-03-28 01:56:30+00	I wasn't game to restore a 110 GB database for just one table so [I created my own data][1]. The age distributions should match what's on Stack Overflow but obviously the table itself won't match. I don't think that it's too much of an issue because the queries are going to hit indexes anyway. I'm testing on a 4 CPU computer with SQL Server 2016 SP1. One thing to note is that for queries that finish this quickly it's important not to include the actual execution plan. That can slow things down quite a bit.\r\n\r\nI started by going through some of the solutions in Erik's excellent answer. For this one:\r\n\r\n    SELECT SUM(Records)\r\n    FROM \r\n    (\r\n        SELECT COUNT(Id)\r\n        FROM dbo.Users AS u\r\n        WHERE u.Age < 18\r\n    \r\n        UNION ALL\r\n    \r\n        SELECT COUNT(Id)\r\n        FROM dbo.Users AS u\r\n        WHERE u.Age IS NULL\r\n    ) x (Records);\r\n\r\nI got the following results from [sys.dm_exec_sessions][2] over 10 trials (the query naturally went parallel for me):\r\n\r\n    ╔══════════╦════════════════════╦═══════════════╗\r\n    ║ cpu_time ║ total_elapsed_time ║ logical_reads ║\r\n    ╠══════════╬════════════════════╬═══════════════╣\r\n    ║     3532 ║                975 ║         60830 ║\r\n    ╚══════════╩════════════════════╩═══════════════╝\r\n\r\nThe query that worked better for Erik actually performed worse on my machine:\r\n\r\n    SELECT SUM(Records)\r\n    FROM \r\n    (\r\n        SELECT 1\r\n        FROM dbo.Users AS u\r\n        WHERE u.Age < 18\r\n    \r\n        UNION ALL\r\n    \r\n        SELECT 1\r\n        FROM dbo.Users AS u\r\n        WHERE u.Age IS NULL\r\n    ) x (Records)   \r\n    OPTION(QUERYTRACEON 8649);\r\n\r\nResults from 10 trials:\r\n\r\n    ╔══════════╦════════════════════╦═══════════════╗\r\n    ║ cpu_time ║ total_elapsed_time ║ logical_reads ║\r\n    ╠══════════╬════════════════════╬═══════════════╣\r\n    ║     5704 ║               1636 ║         60850 ║\r\n    ╚══════════╩════════════════════╩═══════════════╝\r\n\r\nI'm not immediately able to explain why it's that bad, but it's not clear why we want to force nearly every operator in the query plan to go parallel. In the original plan we have a serial zone that finds all rows with `AGE < 18`. There are only a few thousand rows. On my machine I get 9 logical reads for that part of the query and 9 ms of reported CPU time and elapsed time. There's also a serial zone for the global aggregate for the rows with `AGE IS NULL` but that only processes one row per DOP. On my machine this is just four rows.\r\n\r\nMy takeaway is that it's most important to optimize the part of the query that finds rows with a `NULL` for `Age` because there are millions of those rows. I wasn't able to create an index with less pages that covered the data than a simple page-compressed one on the column. I assume that there's a minimum index size per row or that a lot of the index space cannot be avoided with the tricks that I tried. So if we're stuck with about the same number of logical reads to get the data then the only way to make it faster is to make the query more parallel, but this needs to be done in a different way than Erik's query that used TF 8649. In the query above we have a ratio of 3.62 for CPU time to elapsed time which is pretty good. The ideal would be a ratio of 4.0 on my machine.\r\n\r\nOne possible area of improvement is to divide the work more evenly among threads. In the screenshot below we can see that one of my CPUs decided to take a little break:\r\n\r\n[![lazy thread][3]][3]\r\n\r\nIndex scan is one of the few operators that can be implemented in parallel and we can't do anything about how the rows are distributed to threads. There's an element of chance to it as well but pretty consistently I saw one underworked thread. One way to work around this is to do parallelism the hard way: on the inner part of a nested loop join. Anything on the inner part of a nested loop will be implemented in a serial way but many serial threads can run concurrently. As long as we get a favorable parallel distribution method (such as round robin), we can control exactly how many rows are sent to each thread.\r\n\r\nI'm running queries with DOP 4 so I need to evenly divide the `NULL` rows in the table into four buckets. One way to do this is to create a bunch of indexes on computed columns:\r\n\r\n    ALTER TABLE dbo.Users\r\n    ADD Compute_bucket_0 AS (CASE WHEN Age IS NULL AND Id % 4 = 0 THEN 1 ELSE NULL END),\r\n    Compute_bucket_1 AS (CASE WHEN Age IS NULL AND Id % 4 = 1 THEN 1 ELSE NULL END),\r\n    Compute_bucket_2 AS (CASE WHEN Age IS NULL AND Id % 4 = 2 THEN 1 ELSE NULL END),\r\n    Compute_bucket_3 AS (CASE WHEN Age IS NULL AND Id % 4 = 3 THEN 1 ELSE NULL END);\r\n    \r\n    CREATE INDEX IX_Compute_bucket_0 ON dbo.Users (Compute_bucket_0) WITH (DATA_COMPRESSION = PAGE);\r\n    CREATE INDEX IX_Compute_bucket_1 ON dbo.Users (Compute_bucket_1) WITH (DATA_COMPRESSION = PAGE);\r\n    CREATE INDEX IX_Compute_bucket_2 ON dbo.Users (Compute_bucket_2) WITH (DATA_COMPRESSION = PAGE);\r\n    CREATE INDEX IX_Compute_bucket_3 ON dbo.Users (Compute_bucket_3) WITH (DATA_COMPRESSION = PAGE);\r\n\r\nI'm not quite sure why four separate indexes is a little faster than one index but that's one what I found in my testing.\r\n\r\nTo get a parallel nested loop plan I'm going to use the undocumented [trace flag 8649][4]. I'm also going to write the code a little strangely to encourage the optimizer not to process more rows than necessary. Below is one implementation which appears to work well:\r\n\r\n    SELECT SUM(t.cnt) + (SELECT COUNT(*) FROM dbo.Users AS u WHERE u.Age < 18)\r\n    FROM \r\n    (VALUES (0), (1), (2), (3)) v(x)\r\n    CROSS APPLY \r\n    (\r\n    \tSELECT COUNT(*) cnt \r\n    \tFROM dbo.Users \r\n    \tWHERE Compute_bucket_0 = CASE WHEN v.x = 0 THEN 1 ELSE NULL END\r\n    \r\n    \tUNION ALL\r\n    \r\n    \tSELECT COUNT(*) cnt \r\n    \tFROM dbo.Users \r\n    \tWHERE Compute_bucket_1 = CASE WHEN v.x = 1 THEN 1 ELSE NULL END\r\n    \t\t\r\n    \tUNION ALL\r\n    \r\n    \tSELECT COUNT(*) cnt \r\n    \tFROM dbo.Users \r\n    \tWHERE Compute_bucket_2 = CASE WHEN v.x = 2 THEN 1 ELSE NULL END\r\n    \r\n    \tUNION ALL\r\n    \r\n    \tSELECT COUNT(*) cnt \r\n    \tFROM dbo.Users \r\n    \tWHERE Compute_bucket_3 = CASE WHEN v.x = 3 THEN 1 ELSE NULL END\r\n    ) t\r\n    OPTION (QUERYTRACEON 8649);\r\n\r\nThe results from ten trials:\r\n\r\n    ╔══════════╦════════════════════╦═══════════════╗\r\n    ║ cpu_time ║ total_elapsed_time ║ logical_reads ║\r\n    ╠══════════╬════════════════════╬═══════════════╣\r\n    ║     3093 ║                803 ║         62008 ║\r\n    ╚══════════╩════════════════════╩═══════════════╝\r\n\r\nWith that query we have a CPU to elapsed time ratio of 3.85! We shaved off 17 ms from the runtime and it only took 4 computed columns and indexes to do it! Each thread processes very close to the same number of rows overall because each index has very close to the same number of rows and each thread only scans one index:\r\n\r\n[![well divided work][5]][5]\r\n\r\nOn a final note we can also hit the easy button and add a nonclustered CCI to the `Age` column:\r\n\r\n    CREATE NONCLUSTERED COLUMNSTORE INDEX X_NCCI ON dbo.Users (Age);\r\n\r\nThe following query finishes in 3 ms on my machine:\r\n\r\n    SELECT COUNT(*)\r\n    FROM dbo.Users AS u\r\n    WHERE u.Age < 18 OR u.Age IS NULL;\r\n\r\nThat's going to be tough to beat.\r\n\r\n\r\n  [1]: https://pastebin.com/gfs3HkSm\r\n  [2]: https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-sessions-transact-sql\r\n  [3]: https://i.stack.imgur.com/5xNa1.png\r\n  [4]: http://web.archive.org/web/20180404164406/http://sqlblog.com/blogs/paul_white/archive/2011/12/23/forcing-a-parallel-query-execution-plan.aspx\r\n  [5]: https://i.stack.imgur.com/WuE9l.png	2019-12-04 14:01:11.469134+00	2	4	1	168342	0	0	0	2019-12-04 14:01:11.469134+00	\N	I wasn't game to restore a 110 GB database for just one table so [I created my own data][1]. The age distributions should match what's on Stack Overflow but obviously the table itself won't match. I don't think that it's too much of an issue because the queries are going to hit indexes anyway. I'm testing on a 4 CPU computer with SQL Server 2016 SP1. One thing to note is that for queries that finish this quickly it's important not to include the actual execution plan. That can slow things down quite a bit.	f	f
216	266	171	2017-03-27 19:27:50+00	Although I don't have a local copy of the Stack Overflow database, I was able to try out a couple of queries. My thought was to get a count of users from a system catalog view (as opposed to directly getting a count of rows from the underlying table). Then get a count of rows that do (or maybe do not) match Erik's criteria, and do some simple math.\r\n\r\nI used the [Stack Exchange Data Explorer][1] (Along with `SET STATISTICS TIME ON;` and `SET STATISTICS IO ON;`) to test the queries. For a point of reference, here are some queries and the CPU/IO statistics:\r\n\r\nQUERY 1\r\n\r\n    --Erik's query From initial question.\r\n    SELECT COUNT(*)\r\n    FROM dbo.Users AS u\r\n    WHERE ISNULL(u.Age, 17) < 18;\r\n\r\n> SQL Server Execution Times:    CPU time = 0 ms,  elapsed time = 0 ms.\r\n> (1 row(s) returned)\r\n> \r\n> Table 'Users'. Scan count 17, logical reads 201567, physical reads 0,\r\n> read-ahead reads 2740, lob logical reads 0, lob physical reads 0, lob\r\n> read-ahead reads 0.\r\n> \r\n>  SQL Server Execution Times:    CPU time = 1829 ms,  elapsed time =\r\n> 296 ms.\r\n\r\n\r\nQUERY 2\r\n\r\n    --Erik's "OR" query.\r\n    SELECT COUNT(*)\r\n    FROM dbo.Users AS u\r\n    WHERE u.Age < 18\r\n    OR u.Age IS NULL;\r\n\r\n>  SQL Server Execution Times:    CPU time = 0 ms,  elapsed time = 0 ms.\r\n> (1 row(s) returned)\r\n> \r\n> Table 'Users'. Scan count 17, logical reads 201567, physical reads 0,\r\n> read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob\r\n> read-ahead reads 0.\r\n> \r\n>  SQL Server Execution Times:    CPU time = 2500 ms,  elapsed time =\r\n> 147 ms.\r\n\r\n\r\nQUERY 3\r\n\r\n    --Erik's derived tables/UNION ALL query.\r\n    SELECT SUM(Records)\r\n    FROM \r\n    (\r\n        SELECT COUNT(Id)\r\n        FROM dbo.Users AS u\r\n        WHERE u.Age < 18\r\n    \r\n        UNION ALL\r\n    \r\n        SELECT COUNT(Id)\r\n        FROM dbo.Users AS u\r\n        WHERE u.Age IS NULL\r\n    ) x (Records);\r\n\r\n>  SQL Server Execution Times:    CPU time = 0 ms,  elapsed time = 0 ms.\r\n> (1 row(s) returned)\r\n> \r\n> Table 'Users'. Scan count 34, logical reads 403134, physical reads 0,\r\n> read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob\r\n> read-ahead reads 0.\r\n> \r\n>  SQL Server Execution Times:    CPU time = 3156 ms,  elapsed time =\r\n> 215 ms.\r\n\r\n\r\n**1st Attempt**\r\n\r\nThis was slower than all of Erik's queries I listed here...at least in terms of elapsed time.\r\n\r\n    SELECT SUM(p.Rows)  -\r\n      (\r\n        SELECT COUNT(*)\r\n        FROM dbo.Users AS u\r\n        WHERE u.Age >= 18\r\n      ) \r\n    FROM sys.objects o\r\n    JOIN sys.partitions p\r\n    \tON p.object_id = o.object_id\r\n    WHERE p.index_id < 2\r\n    AND o.name = 'Users'\r\n    AND SCHEMA_NAME(o.schema_id) = 'dbo'\r\n    GROUP BY o.schema_id, o.name\r\n\r\n>  SQL Server Execution Times:    CPU time = 0 ms,  elapsed time = 0 ms.\r\n> (1 row(s) returned)\r\n> \r\n> Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0,\r\n> read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob\r\n> read-ahead reads 0. Table 'sysrowsets'. Scan count 2, logical reads\r\n> 10, physical reads 0, read-ahead reads 0, lob logical reads 0, lob\r\n> physical reads 0, lob read-ahead reads 0. Table 'sysschobjs'. Scan\r\n> count 1, logical reads 4, physical reads 0, read-ahead reads 0, lob\r\n> logical reads 0, lob physical reads 0, lob read-ahead reads 0. Table\r\n> 'Users'. Scan count 1, logical reads 201567, physical reads 0,\r\n> read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob\r\n> read-ahead reads 0.\r\n> \r\n>  SQL Server Execution Times:    CPU time = 593 ms,  elapsed time = 598\r\n> ms.\r\n\r\n\r\n**2nd Attempt**\r\n\r\nHere I opted for a variable to store the total number of users (instead of a sub-query). The scan count increased from 1 to 17 compared to the 1st attempt. Logical reads stayed the same. However, elapsed time dropped considerably.\r\n \r\n    DECLARE @Total INT;\r\n    \r\n    SELECT @Total = SUM(p.Rows)\r\n    FROM sys.objects o\r\n    JOIN sys.partitions p\r\n    \tON p.object_id = o.object_id\r\n    WHERE p.index_id < 2\r\n    AND o.name = 'Users'\r\n    AND SCHEMA_NAME(o.schema_id) = 'dbo'\r\n    GROUP BY o.schema_id, o.name\r\n    \r\n    SELECT @Total - COUNT(*)\r\n    FROM dbo.Users AS u\r\n    WHERE u.Age >= 18\r\n\r\n>  SQL Server Execution Times:    CPU time = 0 ms,  elapsed time = 0 ms.\r\n> Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0,\r\n> read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob\r\n> read-ahead reads 0. Table 'sysrowsets'. Scan count 2, logical reads\r\n> 10, physical reads 0, read-ahead reads 0, lob logical reads 0, lob\r\n> physical reads 0, lob read-ahead reads 0. Table 'sysschobjs'. Scan\r\n> count 1, logical reads 4, physical reads 0, read-ahead reads 0, lob\r\n> logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n> \r\n>  SQL Server Execution Times:    CPU time = 0 ms,  elapsed time = 1 ms.\r\n> (1 row(s) returned)\r\n> \r\n> Table 'Users'. Scan count 17, logical reads 201567, physical reads 0,\r\n> read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob\r\n> read-ahead reads 0.\r\n> \r\n>  SQL Server Execution Times:    CPU time = 1471 ms,  elapsed time = 98\r\n> ms.\r\n\r\n\r\n**Other Notes:**\r\nDBCC TRACEON is not permitted on Stack Exchange Data Explorer, as noted below:\r\n\r\n> User 'STACKEXCHANGE\\svc_sede' does not have permission to run DBCC\r\n> TRACEON.\r\n\r\n  [1]: https://data.stackexchange.com/stackoverflow/query/new	2019-12-04 14:01:11.759581+00	0	4	1	168308	0	0	0	2019-12-04 14:01:11.759581+00	\N	Although I don't have a local copy of the Stack Overflow database, I was able to try out a couple of queries. My thought was to get a count of users from a system catalog view (as opposed to directly getting a count of rows from the underlying table). Then get a count of rows that do (or maybe do not) match Erik's criteria, and do some simple math.	f	f
217	266	16	2017-03-27 15:31:24+00	**Answer section**\r\n\r\nThere are various ways to rewrite this using different T-SQL constructs. We'll look at the pros and cons and do an overall comparison below.\r\n\r\n**First up**: Using `OR`\r\n\r\n\tSELECT COUNT(*)\r\n\tFROM dbo.Users AS u\r\n\tWHERE u.Age < 18\r\n\tOR u.Age IS NULL;\r\n\r\nUsing `OR` gives us a more efficient Seek plan, which reads the exact number of rows we need, however it adds what the technical world calls `a whole mess of malarkey` to the query plan.\r\n\r\n[![Nuts][5]][5] \r\n\r\nAlso note that the Seek is executed twice here, which really should be more obvious from the graphical operator:\r\n\r\n[![Nuts][6]][6]\r\n\r\n\r\n    Table 'Users'. Scan count 2, logical reads 8233, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 469 ms,  elapsed time = 473 ms.\r\n\r\n**Second up**: Using derived tables with `UNION ALL`\r\nOur query can also be rewritten like this\r\n\r\n\tSELECT SUM(Records)\r\n\tFROM \r\n\t(\r\n\t\tSELECT COUNT(Id)\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age < 18\r\n\t\r\n\t\tUNION ALL\r\n\t\r\n\t\tSELECT COUNT(Id)\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age IS NULL\r\n\t) x (Records);\r\n\r\nThis yields the same type of plan, with far less malarkey, and a more apparent degree of honesty about how many times the index was seeked (sought?) into.\r\n\r\n[![Nuts][7]][7]\r\n\r\nIt does the same amount of reads (8233) as the `OR` query, but shaves about 100ms of CPU time off.\r\n\r\n    CPU time = 313 ms,  elapsed time = 315 ms.\r\n\r\nHowever, you have to be *really* careful here, because if this plan attempts to go parallel, the two separate `COUNT` operations will be serialized, because they're each considered a global scalar aggregate. If we force a parallel plan using Trace Flag 8649, the problem becomes obvious.\r\n\r\n\tSELECT SUM(Records)\r\n\tFROM \r\n\t(\r\n\t\tSELECT COUNT(Id)\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age < 18\r\n\t\r\n\t\tUNION ALL\r\n\t\r\n\t\tSELECT COUNT(Id)\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age IS NULL\r\n\t) x (Records)\r\n\tOPTION(QUERYTRACEON 8649);\r\n\r\n[![Nuts][8]][8]\r\n\r\nThis can be avoided by changing our query slightly.\r\n\r\n\tSELECT SUM(Records)\r\n\tFROM \r\n\t(\r\n\t\tSELECT 1\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age < 18\r\n\t\r\n\t\tUNION ALL\r\n\t\r\n\t\tSELECT 1\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age IS NULL\r\n\t) x (Records)\t\r\n\tOPTION(QUERYTRACEON 8649);\r\n\r\nNow both nodes performing a Seek are fully parallelized until we hit the concatenation operator.\r\n\r\n[![Nuts][9]][9]\r\n\r\nFor what it's worth, the fully parallel version has some good benefit. At the cost of about 100 more reads, and about 90ms of additional CPU time, the elapsed time shrinks to 93ms.\r\n\r\n    Table 'Users'. Scan count 12, logical reads 8317, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 500 ms,  elapsed time = 93 ms.\r\n\r\n**What about CROSS APPLY?**\r\nNo answer is complete without the magic of `CROSS APPLY`!\r\n\r\nUnfortunately, we run into more problems with `COUNT`.\r\n\r\n\tSELECT SUM(Records)\r\n\tFROM dbo.Users AS u \r\n\tCROSS APPLY \r\n\t(\r\n\t\tSELECT COUNT(Id)\r\n\t\tFROM dbo.Users AS u2 \r\n\t\tWHERE u2.Id = u.Id\r\n\t\tAND\tu2.Age < 18\r\n\t\r\n\t\tUNION ALL\r\n\t\r\n\t\tSELECT COUNT(Id)\r\n\t\tFROM dbo.Users AS u2 \r\n\t\tWHERE u2.Id = u.Id \r\n\t\tAND u2.Age IS NULL\r\n\t) x (Records);\r\n\r\nThis plan is horrible. This is the kind of plan you end up with when you show up last to St. Patrick's Day. Though nicely parallel, for some reason it's scanning the PK/CX. Ew. The plan has a cost of 2198 query bucks.\r\n\r\n[![Nuts][10]][10]\r\n\r\n    Table 'Users'. Scan count 7, logical reads 31676233, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 29532 ms,  elapsed time = 5828 ms.\r\n\r\n\r\nWhich is a weird choice, because if we force it to use the nonclustered index, the cost drops rather significantly to 1798 query bucks.\r\n\r\n\tSELECT SUM(Records)\r\n\tFROM dbo.Users AS u \r\n\tCROSS APPLY \r\n\t(\r\n\t\tSELECT COUNT(Id)\r\n\t\tFROM dbo.Users AS u2 WITH (INDEX(ix_Id_Age))\r\n\t\tWHERE u2.Id = u.Id\r\n\t\tAND\tu2.Age < 18\r\n\t\r\n\t\tUNION ALL\r\n\t\r\n\t\tSELECT COUNT(Id)\r\n\t\tFROM dbo.Users AS u2 WITH (INDEX(ix_Id_Age))\r\n\t\tWHERE u2.Id = u.Id \r\n\t\tAND u2.Age IS NULL\r\n\t) x (Records);\r\n\r\nHey, seeks! Check you out over there. Also note that with the magic of `CROSS APPLY`, we don't need to do anything goofy to have a mostly fully parallel plan.\r\n\r\n[![Nuts][11]][11]\r\n\r\n\r\n    Table 'Users'. Scan count 5277838, logical reads 31685303, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 27625 ms,  elapsed time = 4909 ms.\r\n\r\nCross apply does end up faring better without the `COUNT` stuff in there.\r\n\r\n\tSELECT SUM(Records)\r\n\tFROM dbo.Users AS u\r\n\tCROSS APPLY \r\n\t(\r\n\t\tSELECT 1\r\n\t\tFROM dbo.Users AS u2\r\n\t\tWHERE u2.Id = u.Id\r\n\t\tAND\tu2.Age < 18\r\n\t\r\n\t\tUNION ALL\r\n\t\r\n\t\tSELECT 1\r\n\t\tFROM dbo.Users AS u2\r\n\t\tWHERE u2.Id = u.Id \r\n\t\tAND u2.Age IS NULL\r\n\t) x (Records);\r\n\r\nThe plan looks good, but the reads and CPU aren't an improvement.\r\n\r\n[![Nuts][12]][12]\r\n\r\n\r\n    Table 'Users'. Scan count 20, logical reads 17564, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    Table 'Workfile'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 4844 ms,  elapsed time = 863 ms.\r\n\r\n\r\nRewriting the cross apply to be a derived join results in the exact same everything. I'm not going to re-post the query plan and stats info -- they really didn't change. \r\n\r\n\tSELECT COUNT(u.Id)\r\n\tFROM dbo.Users AS u\r\n\tJOIN \r\n\t(\r\n\t\tSELECT u.Id\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age < 18\r\n\t\r\n\t\tUNION ALL\r\n\t\r\n\t\tSELECT u.Id\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age IS NULL\r\n\t) x ON x.Id = u.Id;\r\n\r\n\r\n**Relational Algebra**:\r\nTo be thorough, and to keep Joe Celko from haunting my dreams, we need to at least try some weird relational stuff. Here goes nothin'!\r\n\r\nAn attempt with `INTERSECT`\r\n\r\n    SELECT COUNT(*)\r\n\tFROM dbo.Users AS u\r\n\tWHERE NOT EXISTS ( SELECT u.Age WHERE u.Age >= 18\r\n\t\t\t\t\t   INTERSECT\r\n\t\t\t\t\t   SELECT u.Age WHERE u.Age IS NOT NULL );\r\n\r\n\r\n[![Nuts][13]][13]\r\n\r\n    Table 'Users'. Scan count 1, logical reads 9157, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 1094 ms,  elapsed time = 1090 ms.\r\n\r\nAnd here's an attempt with `EXCEPT`\r\n\r\n    SELECT COUNT(*)\r\n\tFROM dbo.Users AS u\r\n\tWHERE NOT EXISTS ( SELECT u.Age WHERE u.Age >= 18\r\n\t\t\t\t\t   EXCEPT\r\n\t\t\t\t\t   SELECT u.Age WHERE u.Age IS NULL);\r\n\r\n[![Nuts][14]][14]\r\n\r\n\r\n    Table 'Users'. Scan count 7, logical reads 9247, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 2126 ms,  elapsed time = 376 ms.\r\n\r\nThere may be other ways to write these, but I'll leave that up to people who perhaps use `EXCEPT` and `INTERSECT` more often than I do.\r\n\r\n**If you really just need a count**\r\nI use `COUNT` in my queries as a bit of shorthand (read: I'm too lazy to come up with more involved scenarios sometimes). If you just need a count, you can use a `CASE` expression to do just about the same thing.\r\n\r\n\tSELECT SUM(CASE WHEN u.Age < 18 THEN 1\r\n\t\t\t\t\tWHEN u.Age IS NULL THEN 1\r\n\t\t\t\t\tELSE 0 END) \r\n\tFROM dbo.Users AS u\r\n\t\r\n\tSELECT SUM(CASE WHEN u.Age < 18 OR u.Age IS NULL THEN 1\r\n\t\t\t\t\tELSE 0 END) \r\n\tFROM dbo.Users AS u\r\n\r\nThese both get the same plan and have the same CPU and read characteristics.\r\n\r\n[![Nuts][15]][15]\r\n\r\n    Table 'Users'. Scan count 1, logical reads 9157, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 719 ms,  elapsed time = 719 ms.\r\n\r\n**The winner?**\r\nIn my tests, the forced parallel plan with SUM over a derived table performed the best. And yeah, many of these queries could have been assisted by adding a couple filtered indexes to account for both predicates, but I wanted to leave some experimentation to others.\r\n\r\n\tSELECT SUM(Records)\r\n\tFROM \r\n\t(\r\n\t\tSELECT 1\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age < 18\r\n\t\r\n\t\tUNION ALL\r\n\t\r\n\t\tSELECT 1\r\n\t\tFROM dbo.Users AS u\r\n\t\tWHERE u.Age IS NULL\r\n\t) x (Records)\t\r\n\tOPTION(QUERYTRACEON 8649);\r\n\r\nThanks!\r\n\r\n\r\n  [1]: http://sqlinthewild.co.za/index.php/2009/03/19/catch-all-queries/\r\n  [2]: https://en.wikipedia.org/wiki/Sargable\r\n  [3]: https://i.stack.imgur.com/WNwbT.jpg\r\n  [4]: https://i.stack.imgur.com/x1IPP.jpg\r\n  [5]: https://i.stack.imgur.com/rBPws.jpg\r\n  [6]: https://i.stack.imgur.com/N7EZc.jpg\r\n  [7]: https://i.stack.imgur.com/YfmVM.jpg\r\n  [8]: https://i.stack.imgur.com/AR9Td.jpg\r\n  [9]: https://i.stack.imgur.com/xoudR.jpg\r\n  [10]: https://i.stack.imgur.com/NTVIG.jpg\r\n  [11]: https://i.stack.imgur.com/h0mGX.jpg\r\n  [12]: https://i.stack.imgur.com/VZU0E.jpg\r\n  [13]: https://i.stack.imgur.com/mAxVC.jpg\r\n  [14]: https://i.stack.imgur.com/LnBVV.jpg\r\n  [15]: https://i.stack.imgur.com/JlKx1.jpg	2019-12-04 14:01:12.063382+00	3	4	1	168277	0	0	0	2019-12-04 14:01:12.063382+00	\N	**Answer section**	f	f
717	647	2	2020-01-30 23:28:55.537625+00	There should be a way to delete chat messages, and we will add this and update this answer when we do.\r\n\r\nThere will ultimately be a 'comments/chat cleanup crew' to complement the '[post cleanup crew](/meta?q=182#a402)', and they will be able to see deleted messages as they may be necessary to interpret flags.\r\n\r\nI think perhaps deleted messages should remain visible (but be clearly marked as deleted) for those currently in the chat room — they can disappear properly on refresh or navigation.	2020-01-30 23:28:55.537625+00	6	1	1	\N	0	0	0	\N	\N	There should be a way to delete chat messages, and we will add this and update this answer when we do.	f	f
711	647	811	2020-01-30 11:17:28.027508+00	It sure doesn't look like it.\r\n\r\nMaybe the same red "close" button as used to dismiss messages can appear in the same location on one's own messages until edit time runs out?\r\n\r\nI also propose that if one edits a chat message before time runs out, removing all of its content, it should disappear. Currently, one just cannot submit it if it is all whitespace.	2020-01-30 11:32:28.969029+00	3	1	1	\N	0	0	0	\N	\N	It sure doesn't look like it.	f	f
599	551	202	2020-01-12 09:36:57.661139+00	My opinions are as follows:\r\n\r\n1. Moderators must be elected through a democratic election. More importantly, it should follow a equal consideration to all approach.\r\n\r\n2. The term must be fixed. My personal personal preference would be 3-5 years.\r\n\r\n3. They should be able to participate in the election as many time as they want. \r\n\r\n4. in the beginning, atleast 2-3 would be nice-to-have. \r\n\r\n5. There should be a transparent process to report a moderator. From there on, the process forward should be public, in the sense that "why, what, and so what, etc.," should be made public. \r\n\r\nthese are my initial thoughts, I'll add things as I think more.	2020-01-12 09:37:13.367131+00	6	4	3	\N	0	0	0	\N	\N	My opinions are as follows:	f	f
488	458	167	2019-12-17 15:13:46.548712+00	For "normal" blocks (`block`, `alertblock`, `exampleblock`) that's easy, one can set their dedicated beamer colours: \r\n\r\n```\r\n% block\r\n\\setbeamercolor{block title}{fg=blue,bg=blue!20!bg}\r\n\\setbeamercolor{block body}{bg=block title.bg!30!bg}\r\n\r\n% alertblock\r\n\\setbeamercolor{block title alerted}{fg=white,bg=red!75!black}\r\n\\setbeamercolor{block body alerted}{bg=block title alerted.bg!10!bg}\r\n\r\n% exampleblock\r\n\\setbeamercolor{block title example}{fg=white,bg=green!75!black}\r\n\\setbeamercolor{block body example}{bg=block title example.bg!10!bg}\r\n```    \r\n\r\nFor blocks like `theorem`, it is more difficult. Normally they all share their colour with the normal `block` or `exampleblock`, but with a few tricks one can modify their colour individually:\r\n\r\n```\r\n\\documentclass{beamer}\r\n\r\n% block\r\n\\setbeamercolor{block title}{fg=black,bg=green!50!black}\r\n\\setbeamercolor{block body}{fg=green!50!black,bg=green!50!black!30!white}\r\n\r\n% alertblock\r\n\\setbeamercolor{block title alerted}{fg=black,bg=green!50!white}\r\n\\setbeamercolor{block body alerted}{fg=green,bg=green!30!white}\r\n\r\n% exampleblock\r\n\\setbeamercolor{block title example}{fg=black,bg=lime!50!white}\r\n\\setbeamercolor{block body example}{fg=lime,bg=lime!30!white}\r\n\r\n% theorem\r\n\\BeforeBeginEnvironment{theorem}{\r\n  \\setbeamercolor{block title}{fg=black,bg=yellow!50!white}\r\n  \\setbeamercolor{block body}{fg=yellow,bg=yellow!30!white}\r\n}\r\n\\AfterEndEnvironment{theorem}{\r\n    \\setbeamercolor{block title}{fg=blocktitlefgsave,bg=blocktitlebgsave}\r\n    \\setbeamercolor{block body}{fg=blockbodyfgsave,bg=blockbodybgsave}\r\n}\r\n\r\n% corollary\r\n\\BeforeBeginEnvironment{corollary}{\r\n  \\setbeamercolor{block title}{fg=black,bg=orange!50!white}\r\n  \\setbeamercolor{block body}{fg=orange, bg=orange!30!white}\r\n}\r\n\\AfterEndEnvironment{corollary}{\r\n    \\setbeamercolor{block title}{fg=blocktitlefgsave,bg=blocktitlebgsave}\r\n    \\setbeamercolor{block body}{fg=blockbodyfgsave,bg=blockbodybgsave}\r\n}\r\n\r\n% definition\r\n\\BeforeBeginEnvironment{definition}{\r\n  \\setbeamercolor{block title}{fg=black,bg=red!50!white}\r\n  \\setbeamercolor{block body}{fg=red, bg=red!30!white}\r\n}\r\n\\AfterEndEnvironment{definition}{\r\n    \\setbeamercolor{block title}{fg=blocktitlefgsave,bg=blocktitlebgsave}\r\n    \\setbeamercolor{block body}{fg=blockbodyfgsave,bg=blockbodybgsave}\r\n}\r\n\r\n% definitions\r\n\\BeforeBeginEnvironment{definitions}{\r\n  \\setbeamercolor{block title}{fg=black,bg=violet!50!white}\r\n  \\setbeamercolor{block body}{fg=violet, bg=violet!30!white}\r\n}\r\n\\AfterEndEnvironment{definitions}{\r\n    \\setbeamercolor{block title}{fg=blocktitlefgsave,bg=blocktitlebgsave}\r\n    \\setbeamercolor{block body}{fg=blockbodyfgsave,bg=blockbodybgsave}\r\n}\r\n\r\n% fact\r\n\\BeforeBeginEnvironment{fact}{\r\n  \\setbeamercolor{block title}{fg=black,bg=blue!50!white}\r\n  \\setbeamercolor{block body}{fg=blue, bg=blue!30!white}\r\n}\r\n\\AfterEndEnvironment{fact}{\r\n    \\setbeamercolor{block title}{fg=blocktitlefgsave,bg=blocktitlebgsave}\r\n    \\setbeamercolor{block body}{fg=blockbodyfgsave,bg=blockbodybgsave}\r\n}\r\n\r\n% example\r\n\\BeforeBeginEnvironment{example}{\r\n  \\setbeamercolor{block title example}{fg=black,bg=cyan!50!white}\r\n  \\setbeamercolor{block body example}{fg=cyan, bg=cyan!30!white}\r\n}\r\n\\AfterEndEnvironment{example}{\r\n    \\setbeamercolor{block title example}{fg=exblocktitlefgsave,bg=exblocktitlebgsave}\r\n    \\setbeamercolor{block body example}{fg=exblockbodyfgsave,bg=exblockbodybgsave}\r\n}\r\n\r\n% examples\r\n\\BeforeBeginEnvironment{examples}{\r\n  \\setbeamercolor{block title example}{fg=black,bg=teal!50!white}\r\n  \\setbeamercolor{block body example}{fg=teal, bg=teal!30!white}\r\n}\r\n\\AfterEndEnvironment{examples}{\r\n    \\setbeamercolor{block title example}{fg=exblocktitlefgsave,bg=exblocktitlebgsave}\r\n    \\setbeamercolor{block body example}{fg=exblockbodyfgsave,bg=exblockbodybgsave}\r\n}\r\n\r\n% proof\r\n\\addtobeamertemplate{proof begin}{%\r\n    \\setbeamercolor{block title}{fg=black,bg=gray!50!white}\r\n    \\setbeamercolor{block body}{fg=gray, bg=gray!30!white}\r\n}{}\r\n\r\n\\begin{document}\r\n\r\n\\usebeamercolor{block title}\r\n\\colorlet{blocktitlefgsave}{block title.fg} \r\n\\colorlet{blocktitlebgsave}{block title.bg}\r\n\\usebeamercolor{normal text}\r\n\\usebeamercolor{block body}\r\n\\colorlet{blockbodyfgsave}{block body.fg}\r\n\\colorlet{blockbodybgsave}{block body.bg}   \r\n\\usebeamercolor{normal text}\r\n\r\n\\usebeamercolor{block title example}\r\n\\colorlet{exblocktitlefgsave}{block title example.fg}   \r\n\\colorlet{exblocktitlebgsave}{block title example.bg}\r\n\\usebeamercolor{normal text}\r\n\\usebeamercolor{block body example}\r\n\\colorlet{exblockbodyfgsave}{block body example.fg}\r\n\\colorlet{exblockbodybgsave}{block body example.bg} \r\n\\usebeamercolor{normal text}\r\n\r\n\\begin{frame}[allowframebreaks]\r\n\r\n% normal blocks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\r\n\r\n    \\begin{block}{My block}\r\n        A block.\r\n    \\end{block}\r\n\r\n    \\begin{alertblock}{my alertblock}\r\n            An alertblock\r\n    \\end{alertblock}\r\n\r\n    \\begin{exampleblock}{my exampleblock}\r\n            An exampleblock\r\n    \\end{exampleblock}\r\n\r\n% theorem blocks %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\r\n\r\n    \\begin{theorem}[My theorem]\r\n        A theorem.\r\n    \\end{theorem}\r\n\r\n    \\begin{corollary}[My corollary]\r\n        A corollary.\r\n    \\end{corollary}\r\n\r\n    \\begin{definition}[My definition]\r\n        A definition.\r\n    \\end{definition}\r\n\r\n    \\begin{definitions}[My definitions]\r\n        Some definitions.\r\n    \\end{definitions}\r\n\r\n    \\begin{fact}[My fact]\r\n        A fact.\r\n    \\end{fact}\r\n\r\n    \\begin{example}[My example]\r\n        An example.\r\n    \\end{example}\r\n\r\n    \\begin{examples}[My examples]\r\n        Some examples.\r\n    \\end{examples}\r\n\r\n    \\begin{proof}[My proof]\r\n        A proof.\r\n    \\end{proof}\r\n\r\n% testing normal blocks again\r\n\r\n    \\begin{block}{My block}\r\n        A block.\r\n    \\end{block}\r\n\r\n    \\begin{exampleblock}{my exampleblock}\r\n            An exampleblock\r\n    \\end{exampleblock}\r\n\r\n\\end{frame}\r\n\\end{document}\r\n```\r\n![Screen_Shot_2019-04-18_at_16.31.08.png](/image?hash=7d2334d256348d28121d193711f07ced765807b20b65884c823c95fe23bb4722)	2019-12-17 15:14:16.315095+00	2	4	3	\N	0	0	0	\N	\N	For "normal" blocks (`block`, `alertblock`, `exampleblock`) that's easy, one can set their dedicated beamer colours:	f	f
728	646	2	2020-01-31 16:48:43.073888+00	I think I'm convinced that 'monospace by default' is probably only suitable for databases.ta, so we ~~are planning to make~~ have made this a community theme setting like the colour scheme.\r\n\r\nThe default for meta.ta, tex.ta, codegolf.ta, and any new communities, is now proportional rather than monospace.	2020-02-02 23:40:59.288963+00	7	1	1	\N	0	0	0	\N	\N	I think I'm convinced that 'monospace by default' is probably only suitable for databases.ta, so we ~~are planning to make~~ have made this a community theme setting like the colour scheme.	f	f
750	646	32	2020-02-03 13:06:49.721189+00	I guess it depends on the amount of data that is inserted/displayed in the table. And if I'm using lots of similar data:\r\n\r\n0 O l 1 I L o\r\n\r\n...then I prefer mono-spaced:\r\n\r\n`0 O l 1 I L o`\r\n\r\n# Mono-spaced Font\r\n\r\n## Small Table\r\n\r\n|`Col1`|`Col2`|`Col3`|`Numeric Column`|  \r\n|-|-|-|-|\r\n|`Value 1`|`Value 2`|`123`|`10.0`|  \r\n|`This is a row with only one cell`|` `|` `|` `|\r\n\r\n## Big Table\r\n\r\n|`Column1`|`Server 1`|`Server 2`|`Server 3`|\r\n|---------------------|----------------------|----------------------|----------------------|\r\n|`Server`|`SQLSERVERP09`|`SQLSERVERP10`|`SQLSERVERP11`|\r\n|`IP Adresse`|`10.58.231.169`|`10.58.231.171`|`10.58.231.173`|\r\n|`-`|` `|` `|` `|\r\n|`SQL Server Instance`|`SQLINSTANCEP09`|`SQLINSTANCEP10`|`SQLINSTANCEP11`|\r\n|`SQL Server Alias`|`SQLSERVERP09-i01`|`SQLSERVERP10-i01`|`SQLSERVERP11-i01`|\r\n|`SQL Server Alias IP`|`10.58.231.170`|`10.58.231.172`|`10.58.231.174`|\r\n|`SQL Server Port`|`1433`|`1433`|`1433`|\r\n|`SQL Server Protocol`|`TCP`|`TCP`|`TCP`|\r\n|`-`|` `|` `|` `|\r\n|`SQL Server Login`|`SQL\\ProgramAdmin`|`SQL\\ProgramAdmin`|`SQL\\ProgramAdmin`|\r\n|`SQL Server Roles`|`public,sysadmin`|`public,sysdadmin`|`public,sysadmin`|\r\n|`-`|` `|` `|` `|\r\n|`SQL Server Login`|`SQL\\SQLPRGDDCP03$`|`SQL\\SQLPRGDDCP03$`|`SQL\\SQLPRGDDCP03$`|\r\n|`SQL Server Roles`|`public`|`public`|`public`|\r\n|`-`|` `|` `|` `|\r\n|`SQL Server Login`|`SQL\\SQLPRGDDCP04$    `|`SQL\\SQLPRGDDCP04$`|`SQL\\SQLPRGDDCP04$`|\r\n|`SQL Server Roles`|`public`|`public`|`public`|\r\n|`-`|` `|` `|` `|\r\n\r\n\r\n\r\n# Proportional Font\r\n\r\n## Small Table\r\n\r\n|Col1|Col2|Col3|Numeric Column|  \r\n|-|-|-|-|\r\n|Value 1|Value 2|123|10.0|  \r\n|This is a row with only one cell| | | |\r\n\r\n## Big Table\r\n\r\n|  Column1 Server 1   |       Server 2       |       Server 3       |                      |\r\n|---------------------|----------------------|----------------------|----------------------|\r\n| Server              | SQLSERVERP09         | SQLSERVERP10         | SQLSERVERP11         |\r\n| IP Adresse          | 10.58.231.169        | 10.58.231.171        | 10.58.231.173        |\r\n|-| | | |\r\n| SQL Server Instance | SQLINSTANCEP09       | SQLINSTANCEP10       | SQLINSTANCEP11       |\r\n| SQL Server Alias    | SQLSERVERP09-i01     | SQLSERVERP10-i01     | SQLSERVERP11-i01     |\r\n| SQL Server Alias IP | 10.58.231.170        | 10.58.231.172        | 10.58.231.174        |\r\n| SQL Server Port     | 1433                 | 1433                 | 1433                 |\r\n| SQL Server Protocol | TCP                  | TCP                  | TCP                  |\r\n|-| | | |\r\n| SQL Server Login    | SQL\\ProgramAdmin     | SQL\\ProgramAdmin     | SQL\\ProgramAdmin     |\r\n| SQL Server Roles    | public,sysadmin      | public,sysdadmin     | public,sysadmin      |\r\n|-| | | |\r\n| SQL Server Login    | SQL\\SQLPRGDDCP03$    | SQL\\SQLPRGDDCP03$    | SQL\\SQLPRGDDCP03$    |\r\n| SQL Server Roles    | public               | public               | public               |\r\n|-| | | |\r\n| SQL Server Login    | SQL\\SQLPRGDDCP04$    | SQL\\SQLPRGDDCP04$    | SQL\\SQLPRGDDCP04$    |\r\n| SQL Server Roles    | public               | public               | public               |\r\n|-| | | |\r\n\r\n	2020-02-03 13:06:49.721189+00	4	4	1	\N	0	0	0	\N	\N	I guess it depends on the amount of data that is inserted/displayed in the table. And if I'm using lots of similar data:	f	f
818	708	158	2017-04-06 13:54:08+00	I can't test this theory at the moment, but based on the [most recent capture data posted to GitHub][1], I would say that the reason that thee `<process>` node is empty is that it requires a currently running request (many of the attributes are found in `sys.dm_exec_requests` and not in `sys.dm_exec_sessions`) and without a currently running request, it can't report any details, similar to how doing an `INNER JOIN` between  `sys.dm_exec_requests` and `sys.dm_exec_sessions` will exclude rows where a Session is active but is idle due to no current request.\r\n\r\n\r\nLooking at the top set of data (`monitorLoop` values: 1748823, 1748824, 1748825, and 1748827) we can see the following:\r\n\r\n* the `id` of the `blocked-process` is the same in each case: **process2552c1fc28**, and the only attribute that is different is the `waittime` (understandably).\r\n* the attributes of the `blocking-process` nodes show differences in both `lastbatchstarted` and `lastbatchcompleted`\r\n* the attributes of the `blocking-process` nodes show identical values for `spid` and `xactid`\r\n\r\nSo, how can the SessionID and TransactionID of the blocking process be the same across 4 different query batches? Easy, an explicit transaction was started and then these batches were executed. And because these are seperate batches, there is time between them being submitted, at which point there is no current request, hence no process info to show (but the session and the transaction are still there).\r\n\r\nIn order to do additional research into this, you can capture helpful information from `sys.dm_exec_requests` and `sys.dm_tran_locks` by placing the following T-SQL in a SQL Server Agent "Transaction-SQL script (T-SQL)" Job Step, setting the "Database" to be the one you are researching (in this case it is the one with an ID of 6), and scheduling this job to run every 10 seconds. The T-SQL below will create the two tables in that same DB if they don't exist and then will populate the "Requests" table if any request is either blocking itself, or if it is a Delete or Update operation that is being blocked. If any requests are found, it will try to capture:\r\n\r\n* Session and Request info on the blocking process (this part does not assume that there is an active Request, hence the `RIGHT JOIN` to at least get the Session info)\r\n* Connection info for the blocked and (hopefully) blocking processes.\r\n* <del>the current locks for those same session_id's (just keep in mind that the lock info isn't _guaranteed_ to be 100% accurate as that info can change in the time between those two statements executing; still, the info is good enough often enough to be worth capturing).</del> This section is currently commented out.\r\n\r\n**SQL Server Agent T-SQL Job Step:**\r\n\r\n    -- !! Remember to set the "Database" for the T-SQL Job Step to\r\n    --    the DB that has database_id = 6 !!\r\n    SET NOCOUNT ON;\r\n    IF (OBJECT_ID(N'dbo.tmpBlockingResearch_Requests') IS NULL)\r\n    BEGIN\r\n      -- Create requests capture table\r\n      SELECT SYSDATETIME() AS [CaptureTime], req.*,\r\n             ses.login_time, ses.[host_name], ses.[program_name], ses.host_process_id,\r\n             ses.client_version, ses.client_interface_name, ses.security_id,\r\n             ses.login_name, ses.nt_domain, ses.nt_user_name, ses.memory_usage,\r\n             ses.total_scheduled_time, ses.endpoint_id, ses.last_request_start_time,\r\n             ses.last_request_end_time, ses.is_user_process, ses.original_security_id,\r\n             ses.original_login_name, ses.last_successful_logon, ses.last_unsuccessful_logon,\r\n             ses.unsuccessful_logons, ses.authenticating_database_id\r\n      INTO   dbo.tmpBlockingResearch_Requests\r\n      FROM   sys.dm_exec_requests req\r\n      INNER JOIN sys.dm_exec_sessions ses\r\n              ON ses.[session_id] = req.[session_id]\r\n      WHERE  1 = 0;\r\n    END;\r\n\r\n    IF (OBJECT_ID(N'dbo.tmpBlockingResearch_Connections') IS NULL)\r\n    BEGIN\r\n      -- Create connections capture table\r\n      SELECT SYSDATETIME() AS [CaptureTime], con.*\r\n      INTO   dbo.tmpBlockingResearch_Connections\r\n      FROM   sys.dm_exec_connections con\r\n      WHERE  1 = 0;\r\n    END;\r\n    \r\n    IF (OBJECT_ID(N'dbo.tmpBlockingResearch_Locks') IS NULL)\r\n    BEGIN\r\n      -- Create locks capture table\r\n      SELECT SYSDATETIME() AS [CaptureTime], loc.*\r\n      INTO   dbo.tmpBlockingResearch_Locks\r\n      FROM   sys.dm_tran_locks loc\r\n      WHERE  1 = 0;\r\n    END;\r\n    ---------------------------------\r\n    DECLARE @SessionIDs TABLE (SessionID SMALLINT NOT NULL,\r\n                               BlockingSessionID SMALLINT NOT NULL);\r\n    \r\n    INSERT INTO dbo.tmpBlockingResearch_Requests\r\n    OUTPUT inserted.[session_id], inserted.[blocking_session_id]\r\n    INTO   @SessionIDs ([SessionID], [BlockingSessionID])\r\n      SELECT SYSDATETIME() AS [CaptureTime], req.*,\r\n             ses.login_time, ses.[host_name], ses.[program_name], ses.host_process_id,\r\n             ses.client_version, ses.client_interface_name, ses.security_id,\r\n             ses.login_name, ses.nt_domain, ses.nt_user_name, ses.memory_usage,\r\n             ses.total_scheduled_time, ses.endpoint_id, ses.last_request_start_time,\r\n             ses.last_request_end_time, ses.is_user_process, ses.original_security_id,\r\n             ses.original_login_name, ses.last_successful_logon, ses.last_unsuccessful_logon,\r\n             ses.unsuccessful_logons, ses.authenticating_database_id\r\n      FROM   sys.dm_exec_requests req\r\n      INNER JOIN sys.dm_exec_sessions ses\r\n              ON ses.[session_id] = req.[session_id]\r\n      WHERE ses.[is_user_process] = 1\r\n      AND   req.[database_id] = DB_ID()\r\n      AND   (\r\n              req.blocking_session_id IN (req.[session_id], -2, -3, -4)\r\n        OR   (req.[command] IN (N'DELETE', N'UPDATE') AND req.[blocking_session_id] > 0)\r\n            );\r\n\r\n    -- Get at least session info, if not also request info, on blocking process\r\n    INSERT INTO dbo.tmpBlockingResearch_Requests\r\n      SELECT SYSDATETIME() AS [CaptureTime], req.*,\r\n             ses.login_time, ses.[host_name], ses.[program_name], ses.host_process_id,\r\n             ses.client_version, ses.client_interface_name, ses.security_id,\r\n             ses.login_name, ses.nt_domain, ses.nt_user_name, ses.memory_usage,\r\n             ses.total_scheduled_time, ses.endpoint_id, ses.last_request_start_time,\r\n             ses.last_request_end_time, ses.is_user_process, ses.original_security_id,\r\n             ses.original_login_name, ses.last_successful_logon, ses.last_unsuccessful_logon,\r\n             ses.unsuccessful_logons, ses.authenticating_database_id\r\n      FROM   sys.dm_exec_requests req\r\n      RIGHT JOIN sys.dm_exec_sessions ses\r\n              ON ses.[session_id] = req.[session_id]\r\n      WHERE ses.[session_id] IN (SELECT DISTINCT [BlockingSessionID] FROM @SessionIDs);\r\n\r\n    -- If any rows are captured this time, try to capture their connection info\r\n    INSERT INTO dbo.tmpBlockingResearch_Connections\r\n      SELECT SYSDATETIME() AS [CaptureTime], con.*\r\n      FROM   sys.dm_exec_connections con\r\n      WHERE  con.[session_id] IN (\r\n                                  SELECT [SessionID]\r\n                                  FROM @SessionIDs\r\n                                  UNION -- No "ALL" so it does DISTINCT\r\n                                  SELECT [BlockingSessionID]\r\n                                  FROM @SessionIDs\r\n                                 );\r\n\r\n    /*\r\n    -- If any rows are captured this time, try to capture their lock info\r\n    INSERT INTO dbo.tmpBlockingResearch_Locks\r\n      SELECT SYSDATETIME() AS [CaptureTime], loc.*\r\n      FROM   sys.dm_tran_locks loc\r\n      WHERE  loc.[request_session_id] IN (\r\n                                          SELECT [SessionID]\r\n                                          FROM @SessionIDs\r\n                                          UNION -- No "ALL" so it does DISTINCT\r\n                                          SELECT [BlockingSessionID]\r\n                                          FROM @SessionIDs\r\n                                         );\r\n     */\r\n\r\nI think you should be able to reproduce this by opening one query tab and executing the following:\r\n\r\n    CREATE TABLE dbo.tmp (Col1 INT);\r\n    BEGIN TRAN;\r\n    INSERT INTO dbo.tmp (Col1) VALUES (1);\r\n\r\nThen, open a second query tab and execute the following:\r\n\r\n    UPDATE dbo.tmp\r\n    SET    Col1 = 2\r\n    WHERE  Col1 = 1;\r\n\r\nP.S. Just to have it stated, the only thing that does not make sense is that the request &amp; session info &ndash; `dbo.tmpBlockingResearch_Requests` &ndash; still never contains rows for the blocking session. Yet I know that the table variable has the blocking session id in it as it did pull in the locks for both SessionIDs. This could point to a scenario in which a Transaction is allowed to stay open after the "connection" from the client is closed but the connection is still maintained due to Connection Pooling.\r\n\r\n  [1]: https://gist.github.com/anonymous/4c8df0a69cf5e2bd0a0741aed8b10019	2020-02-13 13:48:13.34798+00	2	4	1	169309	0	0	0	2020-02-13 13:48:13.34798+00	\N	I can't test this theory at the moment, but based on the [most recent capture data posted to GitHub][1], I would say that the reason that thee `<process>` node is empty is that it requires a currently running request (many of the attributes are found in `sys.dm_exec_requests` and not in `sys.dm_exec_sessions`) and without a currently running request, it can't report any details, similar to how doing an `INNER JOIN` between  `sys.dm_exec_requests` and `sys.dm_exec_sessions` will exclude rows where a Session is active but is idle due to no current request.	f	f
819	708	32	2017-04-03 15:16:16+00	Blocked transactions can occur because of lock escalations.\r\n\r\nThis is explained in the Microsoft Support article:\r\n\r\n[How to resolve blocking problems that are caused by lock escalation in SQL Server][L1]\r\n\r\n> ...  \r\n> Lock escalation does not cause most blocking problems. To determine whether lock escalation is occurring around the time when you experience blocking issues, start a SQL Profiler trace that includes the Lock:Escalation event. If you do not see any Lock:Escalation events, lock escalation is not occurring on your server and the information in this article does not apply to your situation. \r\n> \r\n> If lock escalation is occurring, verify that the escalated table lock is blocking other users  \r\n> ...  \r\n\r\nCheck the Extended Events (physical file) for **lock escalation** events that occurred before the **blocked process** event.\r\n\r\n# Explaining  \r\n\r\nThere is a Microsoft Blog article that goes into further detail:\r\n\r\n[SQL Server Lock Escalation and Blocking][L2]\r\n\r\n> ...  \r\n> Step 2:  Collect Lock Escalation and Blocked Process Report Events.\r\n> \r\n> Lock escalation and blocked process report events are not automatically captured by SQL Server. In order to know if these events are happening, we need to tell SQL Server to record them. Our team uses the Performance Analyzer for Microsoft Dynamics tool to gather that information. Check out this post by Rod Hansen for more information on the tool and how to collect blocking details with it. If you just want to use SQL Server Profiler, the events you would need to collect are shown below:\r\n> ...  \r\n\r\nAfter you have captured lock escalations and blocked processes you have to determine if the lock escalations are the root cause of the blocked processes:\r\n\r\n> ...  \r\n> Step 3:  Review the Trace in SQL Server Profiler.\r\n> \r\n> There are two main indicators that will tell you if the blocking is related to lock escalation.\r\n> \r\n> First, you see a series of lock escalation events immediately preceding the blocked process report events. Below is an example taken from a trace produced by the Performance Analyzer for Microsoft Dynamics tool. This is one thing to look for in the trace, but this alone doesn’t mean lock escalation is causing the blocking.\r\n> ...  \r\n\r\nand further\r\n\r\n> To verify that the blocking is in fact related to lock escalation, you need to look at the blocked process report details. In the TextData section look for waitresource (see the screenshot below). If waitresource starts with OBJECT, we know the blocked statement is waiting on a table level lock to be released before it can proceed. If waitresource starts with **KEY** or **PAG** instead of OBJECT, then **lock escalation isn’t involved in that specific block**. Lock escalation will always increase the scope of a lock to OJBECT regardless of where it starts\r\n\r\n# Solution \r\n_(only if the above mentioned matches)_\r\n\r\nThe solution is apparently to turn on the trace flag 1224 which will turn off lock escalation:\r\n\r\n[SQL Server Lock Escalation and Blocking][L2]\r\n\r\n> If you see these two things together, it’s a pretty good bet that lock escalation is causing the blocking and you would probably benefit from implementing SQL Server trace flag 1224.\r\n\r\n[SQL Server Trace Flags for Dynamics AX][L3]\r\n\r\n> Trace flag 1224 disables lock escalation based on number of locks. Enabling this trace flag can reduce the likelihood of blocking due to lock escalation- something I’ve seen with a number of AX implementations. The most common scenario where this becomes an issue is when there’s a requirement for Master Planning to run during the day\r\n\r\n# Answer\r\n\r\nIn the end it could be that lock escalation is the root cause of blocked processes.\r\n____\r\n# Alternate Solution (process node empty)\r\n\r\n<em>After further investigation of some blocked_process_reports the following alternate explanation can be made.</em>\r\n\r\nThe Extended Events are capturing blocked_process_reports which are unrelated to any other processes at the time.\r\n\r\nErgo: They must be blocked for a different reason\r\n\r\nI would suggest you capture a time frame of wait types from the sys.dm_os_wait_stats view on your SQL Server and correlate the numbers with the blocked_process_reports happening during your measurements. Paul Randall has a good script:  [Send me your wait stats and get my advice and 30 days of free Pluralsight in return][L5]\r\n\r\nThe scripts captures the current counters, waits for 23hours (can be modified), recaptures the current counters again and compares them to give you the top 95% of wait types. You could try this out for say 1 hour and have the XEL file handy.\r\n\r\nYou might find a wait type (e.g. LCK_M_SH, …) that is telling you that your storage is slow in writing. Or that you have some other overhead (e.g. CX_PACKET_WAITS, ….).  Something is slowing down your Updates. You can then see if the sys.dm_os_wait_stats relate to the blocked_process_reports with the empty nodes.\r\n\r\nThere are cases when a blocked SPID is being blocked by the same SPID:\r\n\r\n[The blocked column in the sysprocesses table is populated for latch waits after you install SQL Server 2000 SP4][L4]\r\n\r\n> When an SPID is waiting for an I/O page latch, you may notice that the blocked column briefly reports that the SPID is blocking itself. This behavior is a side effect of the way that latches are used for I/O operations on data pages. When a thread issues an I/O request, the SPID that issues the I/O request acquires a latch on the page. All SQL Server 2000 I/O operations are asynchronous. Therefore, the SPID will try to acquire another latch on the same page if the SPID that issued the I/O request must wait for the request to finish. This second latch is blocked by the first latch. Therefore, the blocked column reports that the SPID is blocking itself. When the I/O request finishes, the first latch is released. Then, the second latch request is granted. \r\n\r\n# Alternate Answer \r\n\r\nThis is a further indication that you might be having IO issues. These issues result in "blocked processes" but without a related foreign SPID. Extended Events might not report the process/SPID in a separate node.\r\n\r\n[L1]: https://support.microsoft.com/en-us/help/323630/how-to-resolve-blocking-problems-that-are-caused-by-lock-escalation-in-sql-server\r\n\r\n[L2]: https://blogs.msdn.microsoft.com/axinthefield/sql-server-lock-escalation-and-blocking/\r\n\r\n[L3]: https://blogs.msdn.microsoft.com/axinthefield/sql-server-trace-flags-for-dynamics-ax/\r\n\r\n[L4]: https://support.microsoft.com/en-us/help/906344/the-blocked-column-in-the-sysprocesses-table-is-populated-for-latch-waits-after-you-install-sql-server-2000-sp4\r\n\r\n[L5]: http://www.sqlskills.com/blogs/paul/send-wait-stats-get-advice-30-days-free-pluralsight-return/	2020-02-13 13:48:13.768122+00	0	4	1	168964	0	0	0	2020-02-13 13:48:13.768122+00	\N	Blocked transactions can occur because of lock escalations.	f	f
227	272	16	2018-04-13 14:34:13+00	The answer, as usual (alright, most of the time), lies in the execution plan.\r\n\r\nThere are certain operators that require all rows to arrive at them before they can start processing those rows, and passing them downstream, for example:\r\n\r\n - Hash Join (on building the hash table)\r\n - Hash Match\r\n - Sort (Except Hash Flow Distinct)\r\n\r\nThey're either called blocking, or stop and go operators because of this, and they're often chosen when the optimizer thinks it'll have to process a whole lot of data to find your data.\r\n\r\nThere are other operators that are able to begin streaming, or passing any found rows along immediately\r\n\r\n - Nested Loops\r\n - Index supported Merge Joins\r\n - Stream Aggregates\r\n\r\nWhen queries start returning data immediately, but don't finish immediately, it's usually a sign that the the optimizer chose a plan to locate and return some rows quickly using operators that have a lower start up cost. \r\n\r\nThis can happen because of row goals introduced either by you, or by the optimizer. \r\n\r\nIt can also happen if a bad plan is chosen for some reason (lack of SARGability, parameter sniffing, insufficient statistics, etc.), but that takes more digging to figure out.\r\n\r\nFor more information, check out Rob Farley's blog [here][1]\r\n\r\nAnd Paul White's series on row goals [here][2], [here][3], [here][4], and [here][5].\r\n\r\nIt should also be noted that, if you're talking about SSMS, rows only appear once an entire buffer has been filled, not just willy-nilly.\r\n\r\n  [1]: http://blogs.lobsterpot.com.au/2011/02/17/the-ssis-tuning-tip-that-everyone-misses/\r\n  [2]: https://sqlperformance.com/2018/02/sql-plan/setting-and-identifying-row-goals\r\n  [3]: https://sqlperformance.com/2018/02/sql-plan/row-goals-part-2-semi-joins\r\n  [4]: https://sqlperformance.com/2018/03/sql-plan/row-goals-part-3-anti-joins\r\n  [5]: https://sqlperformance.com/2018/03/sql-performance/row-goals-part-4-anti-join-anti-pattern	2019-12-04 14:17:10.242468+00	3	4	1	203880	0	0	0	2019-12-04 14:17:10.242468+00	\N	The answer, as usual (alright, most of the time), lies in the execution plan.	f	f
228	272	88	2018-04-13 14:35:35+00	If I understand what you're observing, this is how Management Studio *renders* rows, and has little to do with how SQL Server *returns* rows. In fact often when you are returning large results to SSMS and attempting to render them in a grid, SSMS can't keep up and SQL Server ends up waiting for the app to process more rows. In this case you'll see SQL Server accumulating `ASYNC_NETWORK_IO` waits.\r\n\r\nYou can control it somewhat by using Results to Text instead of Results to Grid, since SSMS can draw text faster than it can draw grids, but you'll likely find this can affect readability depending on the number of columns and the data types involved. Both are impacted by when SSMS decides to actually write results out to that pane, which depends on how full the output buffer is.\r\n\r\nWhen you have *multiple* statements, and you want to force the buffer to render output results to the messages pane, you can use a little printing trick in between statements:\r\n\r\n    RAISERROR('', 0, 1) WITH NOWAIT;\r\n\r\nBut this won't help when you're trying to get SSMS to render rows more quickly when all the output is coming from a single statement.\r\n\r\nMore directly, you can control it by limiting how many results you are rendering in SSMS. I often see people complain about how long it takes to return a million rows to the grid. What on earth anyone is going to do with a million rows in an SSMS grid, I have no idea.\r\n\r\nThere are some hacks like [**`OPTION (FAST 100)`**](https://dba.stackexchange.com/questions/135455/what-does-option-fast-in-select-statement-do), which will optimize for retrieving those first 100 rows (or any 100 rows if there is no outer `ORDER BY`), but this can come at the cost of much slower retrieval for the remainder of the rows and a plan that is more inefficient overall, so isn't really a go-to option IMHO.	2019-12-04 14:17:10.53161+00	1	4	1	203881	0	0	0	2019-12-04 14:17:10.53161+00	\N	If I understand what you're observing, this is how Management Studio *renders* rows, and has little to do with how SQL Server *returns* rows. In fact often when you are returning large results to SSMS and attempting to render them in a grid, SSMS can't keep up and SQL Server ends up waiting for the app to process more rows. In this case you'll see SQL Server accumulating `ASYNC_NETWORK_IO` waits.	f	f
473	347	2	2019-12-15 09:16:17.189367+00	in brief: we need the raw errors (for now), and we've fixed the importing issue\r\n\r\n> The site should probably catch and display these more gracefully in general, and with friendlier messages for cases like this where we know what happened and expect users to make the mistake.\r\n\r\nFor now, we are [planning to keep the error dumps](https://topanswers.xyz/meta?q=428#a472) as they are really very useful at this stage of development. Later on we'll tidy that up and do without them.\r\n\r\n> I would suggest using that as a chance to add any answers that were not imported last time. Don't muck with any that were, but if new answer ids are in the list, use the re-import of a known question to add answers.\r\n\r\n> As an addendum about how I think this should work, importing anything that is an answer post should never post it as a question ... it should always look for and use the question post (and import that too if not found). This would allow answers to be added one by one to existing questions as appropriate (similar to re-importing).\r\n\r\nWe've followed those suggestions to the letter and added in a few extras:\r\n\r\n* re-importing will simply add any additional answers that weren't previously imported (if any) but will not touch the ones that were\r\n* importing with just an answer id will no longer import that answer *as a question* (which was always a bug of course), but will instead import the answer *along with the question*.\r\n* you can now use either urls (all the SE formats I know about) or ids interchangeably\r\n\r\n\r\n	2019-12-15 14:50:56.728273+00	7	1	1	\N	0	0	0	\N	\N	in brief: we need the raw errors (for now), and we've fixed the importing issue	f	f
389	347	96	2019-12-09 08:07:25.130829+00	As an addendum about how I think this should work, importing anything that is an answer post should never post it as a question ... it should always look for and use the question post (and import that too if not found). This would allow answers to be added one by one to existing questions as appropriate (similar to re-importing).	2019-12-09 08:07:25.130829+00	5	4	2	\N	0	0	0	\N	\N	As an addendum about how I think this should work, importing anything that is an answer post should never post it as a question ... it should always look for and use the question post (and import that too if not found). This would allow answers to be added one by one to existing questions as appropriate (similar to re-importing).	f	f
209	260	12	2019-07-01 15:59:28+00	```\r\nVALUES (convert(varchar,convert(datetime,{D '2019-06-30'}),102));\r\n```\r\n\r\nStarting from the inside, the ODBC escape sequence `{D '2019-06-30'}` returns a `datetime`.\r\n\r\n(ignoring the redundant convert to `datetime`)\r\n\r\nYou're then converting that to a string with 102 style (rather than 105 for Italian).\r\n\r\nYou're then relying on an implicit conversion back to `datetime` to match the type of the target column.\r\n\r\nThe implicit conversion has a default style of 0 as you can see in the execution plan:\r\n\r\n```none\r\n[Expr1003] = Scalar Operator(CONVERT_IMPLICIT(datetime,CONVERT(varchar(30),[@1],102),0))\r\n```\r\n\r\n(note: you should always specify the maximum length when using `varchar`)\r\n\r\nWhen you use style 102 `yyyy.mm.dd` you must also set `DATEFORMAT` to `YMD` so SQL Server can parse the format under style 0.\r\n\r\nWhen you use style 105 `dd-mm-yyyy`, you must set `DATEFORMAT` to `DMY` for the same reason.\r\n\r\nThe reason it works on one and not the other is the default `DATEFORMAT` for the language in each case.\r\n\r\nSee [datetime][1], [`SET LANGUAGE`][2], and [Write International Transact-SQL Statements][3] in the documentation.\r\n\r\n>Applications that use other APIs, or Transact-SQL scripts, stored procedures, and triggers should use the [CONVERT][4] statement with an explicit style parameter for all conversions between the **time**, **date**, **smalldate**, **datetime**, **datetime2**, and **datetimeoffset** data types and character string data types.\r\n\r\nAlso [SQL Server DateTime Best Practices][5] by Aaron Bertrand.\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/data-types/datetime-transact-sql#supported-string-literal-formats-for-datetime\r\n  [2]: https://docs.microsoft.com/en-us/sql/t-sql/statements/set-language-transact-sql\r\n  [3]: https://docs.microsoft.com/en-us/sql/relational-databases/collations/write-international-transact-sql-statements\r\n  [4]: https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql\r\n  [5]: https://www.mssqltips.com/sqlservertip/5206/sql-server-datetime-best-practices/	2019-12-04 03:30:27.580781+00	1	4	1	241803	0	0	0	2019-12-04 03:30:27.580781+00	\N	```	f	f
222	268	45	2019-09-05 01:45:29+00	This is a little broad but I think I understand the True Question and will answer accordingly. Just going to talk about table vs index spool though. I don't think it's quite correct to view there as being a choice between table and index spools. As you know, it's possible in a single subtree to get an index spool, a table spool, or both an index spool and a table spool. I believe it's generally correct to say that you get an index spool under the following conditions:\r\n\r\n 1. The query optimizer has a reason to transform a join into an apply\r\n 2. The query optimizer actually performs the transform to the apply\r\n 3. The query optimizer uses the rule to add an index spool (at minimum the index spool must be safe to use)\r\n 4. The plan with the index spool is selected\r\n\r\nYou can see most of these with simple demos. Start by creating a pair of heaps:\r\n\r\n    DROP TABLE IF EXISTS dbo.X_10000_VARCHAR_901;\r\n    CREATE TABLE dbo.X_10000_VARCHAR_901 (ID VARCHAR(901) NOT NULL);\r\n    \r\n    INSERT INTO dbo.X_10000_VARCHAR_901 WITH (TABLOCK)\r\n    SELECT TOP (10000) ROW_NUMBER() OVER (ORDER BY (SELECT NULL))\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2;\r\n    \r\n    \r\n    DROP TABLE IF EXISTS dbo.X_10000_VARCHAR_800;\r\n    CREATE TABLE dbo.X_10000_VARCHAR_800 (ID VARCHAR(800) NOT NULL);\r\n    \r\n    INSERT INTO dbo.X_10000_VARCHAR_800 WITH (TABLOCK)\r\n    SELECT TOP (10000) ROW_NUMBER() OVER (ORDER BY (SELECT NULL))\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2;\r\n\r\nFor the first query, there's nothing to seek on:\r\n\r\n    SELECT *\r\n    FROM dbo.X_10000_VARCHAR_901 a\r\n    CROSS JOIN dbo.X_10000_VARCHAR_901 b\r\n    OPTION (MAXDOP 1);\r\n\r\nSo there's no reason for the optimizer to transform the join into an apply. You end up with a table spool due to costing reasons. So this query fails the first test.\r\n\r\n[![enter image description here][1]][1]\r\n\r\nFor the next query, it's fair to expect that the optimizer has a reason to consider an apply:\r\n\r\n    SELECT *\r\n    FROM dbo.X_10000_VARCHAR_901 a\r\n    INNER JOIN dbo.X_10000_VARCHAR_901 b ON a.ID = b.ID \r\n    OPTION (LOOP JOIN, MAXDOP 1);\r\n\r\nBut it's not meant to be:\r\n\r\n[![enter image description here][2]][2]\r\n\r\nThis query fails the second test. A complete explanation is [here][3]. Quoting the most relevant part:\r\n\r\n> The optimizer does not consider building an index on the fly to enable\r\n> an apply; rather the sequence of events is usually the reverse:\r\n> transform to apply because a good index exists.\r\n\r\nI can rewrite the query to encourage the optimizer consider an apply:\r\n\r\n    SELECT *\r\n    FROM dbo.X_10000_VARCHAR_901 a\r\n    INNER JOIN dbo.X_10000_VARCHAR_901 b ON a.ID >= b.ID AND a.ID <= b.ID\r\n    OPTION (MAXDOP 1);\r\n\r\nBut there's still no index spool:\r\n\r\n[![enter image description here][4]][4]\r\n\r\nThis query fails the third test. In SQL Server 2014 there was an index key length limit of 900 bytes. This was extended in SQL Server 2016 but only for nonclustered indexes. [The index for a spool is a clustered index so the limit remains at 900 bytes][5]. In any case, the index spool rule can't be applied because it could lead to an error during query execution.\r\n\r\nReducing the data type length to 800 finally provides a plan with an index spool:\r\n\r\n[![enter image description here][6]][6]\r\n\r\nThe index spool plan, not surprisingly, is costed significantly cheaper than a plan with no spool: 89.7603 units vs 598.832 units. You can see the difference with the undocumented `QUERYRULEOFF BuildSpool` query hint:\r\n\r\n[![enter image description here][7]][7]\r\n\r\nThis isn't a complete answer, but hopefully it's some of what you were looking for.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/rtZSm.png\r\n  [2]: https://i.stack.imgur.com/5x6nk.png\r\n  [3]: https://dba.stackexchange.com/a/173944/104717\r\n  [4]: https://i.stack.imgur.com/BwYBZ.png\r\n  [5]: https://sqlperformance.com/2019/09/sql-performance/nested-loops-joins-performance-spools\r\n  [6]: https://i.stack.imgur.com/U8pZQ.png\r\n  [7]: https://i.stack.imgur.com/Bfpf6.png	2019-12-04 14:13:23.289666+00	2	4	1	247004	0	0	0	2019-12-04 14:13:23.289666+00	\N	This is a little broad but I think I understand the True Question and will answer accordingly. Just going to talk about table vs index spool though. I don't think it's quite correct to view there as being a choice between table and index spools. As you know, it's possible in a single subtree to get an index spool, a table spool, or both an index spool and a table spool. I believe it's generally correct to say that you get an index spool under the following conditions:	f	f
168	229	12	2018-09-01 14:22:32+00	The documentation is a little misleading. The DMV is a non-materialized view, and does not have a primary key as such. The underlying definitions are a little complex but a simplified definition of `sys.query_store_plan` is:\r\n\r\n    CREATE VIEW sys.query_store_plan AS\r\n    SELECT\r\n        PPM.plan_id\r\n        -- various other attributes\r\n    FROM sys.plan_persist_plan_merged AS PPM\r\n    LEFT JOIN sys.syspalvalues AS P\r\n        ON P.class = 'PFT' \r\n        AND P.[value] = plan_forcing_type;\r\n\r\nFurther, `sys.plan_persist_plan_merged` is also a view, though one needs to connect via the Dedicated Administrator Connection to see its definition. Again, simplified:\r\n\r\n    CREATE VIEW sys.plan_persist_plan_merged AS   \r\n    SELECT     \r\n        P.plan_id as plan_id,    \r\n        -- various other attributes\r\n    FROM sys.plan_persist_plan P \r\n        -- NOTE - in order to prevent potential deadlock\r\n        -- between QDS_STATEMENT_STABILITY LOCK and index locks   \r\n        WITH (NOLOCK) \r\n    LEFT JOIN sys.plan_persist_plan_in_memory PM\r\n        ON P.plan_id = PM.plan_id;\r\n\r\nThe indexes on `sys.plan_persist_plan` are:\r\n\r\n``` none\r\n╔════════════════════════╦══════════════════════════════════════╦═════════════╗\r\n║       index_name       ║          index_description           ║ index_keys  ║\r\n╠════════════════════════╬══════════════════════════════════════╬═════════════╣\r\n║ plan_persist_plan_cidx ║ clustered, unique located on PRIMARY ║ plan_id     ║\r\n║ plan_persist_plan_idx1 ║ nonclustered located on PRIMARY      ║ query_id(-) ║\r\n╚════════════════════════╩══════════════════════════════════════╩═════════════╝\r\n```\r\n\r\nSo `plan_id` is constrained to be unique on `sys.plan_persist_plan`.\r\n\r\nNow, `sys.plan_persist_plan_in_memory` is a streaming table-valued function, presenting a tabular view of data only held in internal memory structures. As such, it does not have any unique constraints.\r\n\r\nAt heart, the query being executed is therefore equivalent to:\r\n\r\n    DECLARE @t1 table (plan_id integer NOT NULL);\r\n    DECLARE @t2 table (plan_id integer NOT NULL UNIQUE CLUSTERED);\r\n    DECLARE @t3 table (plan_id integer NULL);\r\n    \r\n    SELECT \r\n        T1.plan_id\r\n    FROM @t1 AS T1 \r\n    LEFT JOIN\r\n    (\r\n        SELECT \r\n            T2.plan_id\r\n        FROM @t2 AS T2\r\n        LEFT JOIN @t3 AS T3 \r\n            ON T3.plan_id = T2.plan_id\r\n    ) AS Q1\r\n        ON Q1.plan_id = T1.plan_id;\r\n\r\n...which does not produce join elimination:\r\n\r\n[![no join elimination][1]][1]\r\n\r\nGetting right to the core of the issue, the problem is the inner query:\r\n\r\n    DECLARE @t2 table (plan_id integer NOT NULL UNIQUE CLUSTERED);\r\n    DECLARE @t3 table (plan_id integer NULL);\r\n    \r\n    SELECT \r\n        T2.plan_id\r\n    FROM @t2 AS T2\r\n    LEFT JOIN @t3 AS T3 \r\n        ON T3.plan_id = T2.plan_id;\r\n\r\n...clearly the left join might result in rows from `@t2` being duplicated because `@t3` has no uniqueness constraint on `plan_id`. Therefore, the join cannot be eliminated:\r\n\r\n[![inner query][2]][2]\r\n\r\nTo workaround this, we can explicitly tell the optimizer that we do not require any duplicate `plan_id` values:\r\n\r\n```\r\nDECLARE @t2 table (plan_id integer NOT NULL UNIQUE CLUSTERED);\r\nDECLARE @t3 table (plan_id integer NULL);\r\n\r\nSELECT DISTINCT -- added\r\n    T2.plan_id\r\nFROM @t2 AS T2\r\nLEFT JOIN @t3 AS T3 \r\n    ON T3.plan_id = T2.plan_id;\r\n```\r\n\r\nThe outer join to `@t3` can now be eliminated:\r\n\r\n[![join eliminated][3]][3]\r\n\r\nApplying that to the real query:\r\n\r\n```\r\nSELECT DISTINCT\r\n    T.plan_id\r\nFROM #tears AS T\r\nLEFT JOIN sys.query_store_plan AS QSP\r\n    ON QSP.plan_id = T.plan_id;\r\n```\r\n\r\nEqually, we could add `GROUP BY T.plan_id` instead of the `DISTINCT`. Anyway, the optimizer can now correctly reason about the `plan_id` attribute all the way down through the nested views, and eliminate both outer joins as desired:\r\n\r\n[![Both joins eliminated][4]][4]\r\n\r\nNote that making `plan_id` unique in the temporary table would not be sufficient to obtain join elimination, since it would not preclude incorrect results. We must explicitly reject duplicate `plan_id` values from the final result to allow the optimizer to work its magic here.\r\n\r\n  [1]: https://i.stack.imgur.com/iVQdv.png\r\n  [2]: https://i.stack.imgur.com/19qlT.png\r\n  [3]: https://i.stack.imgur.com/EyM0S.png\r\n  [4]: https://i.stack.imgur.com/b0yx2.png	2019-12-01 18:07:23.155535+00	2	4	1	216486	0	0	0	2019-12-01 18:04:57.750485+00	\N	The documentation is a little misleading. The DMV is a non-materialized view, and does not have a primary key as such. The underlying definitions are a little complex but a simplified definition of `sys.query_store_plan` is:	f	f
177	241	12	2019-01-28 12:15:34+00	To produce this warning:\r\n\r\n1. The **maximum used** memory must be **less than 5%** of the granted memory; ***AND***\r\n2. The query must use the **regular** (not small) resource semaphore\r\n\r\nTo use the [regular resource semaphore][1] the query must:\r\n\r\n* Have **granted memory over 5MB** (5120 KB, 640 x 8KB pages); ***OR***\r\n* Have a total estimated plan cost of **over 3 units** and not be a *trivial* plan\r\n\r\nServer [version requirements][2]:\r\n\r\n* SQL Server 2014 SP2 (12.0.5000) or later\r\n* SQL Server 2016 SP1 (13.0.4001) or later\r\n* SQL Server 2017 RTM (14.0.1000) or later\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-query-resource-semaphores-transact-sql\r\n  [2]: https://support.microsoft.com/en-us/help/3172997	2019-12-01 21:44:04.076444+00	3	4	1	228238	0	0	0	2019-12-01 21:44:04.076444+00	\N	To produce this warning:	f	f
286	313	16	2017-03-16 23:23:00+00	Rather than search all of your stored procedure text for those wildcards, you'd probably be better off looking for cursors that are open, and their associated text. That would likely make it easier to locate cursor names, stored procs, etc. written by forgetful developers.\r\n\r\nTo test, run this in one window:\r\n\r\n    SET NOCOUNT ON; \r\n    \r\n    IF OBJECT_ID('tempdb..#commands') IS NOT NULL\r\n       BEGIN\r\n             DROP TABLE #commands;\r\n       END;\r\n    \r\n    CREATE TABLE #commands\r\n    (\r\n      ID INT IDENTITY(1, 1)\r\n             PRIMARY KEY CLUSTERED,\r\n      Command NVARCHAR(2000)\r\n    );\r\n    DECLARE @CurrentCommand NVARCHAR(2000);\r\n    \r\n    INSERT INTO #commands ( Command )\r\n        SELECT 'SELECT 1';\r\n    \r\n    DECLARE result_cursor CURSOR\r\n    FOR\r\n            SELECT Command\r\n                FROM #commands;\r\n    \r\n    OPEN result_cursor;\r\n    FETCH NEXT FROM result_cursor INTO @CurrentCommand;\r\n    WHILE @@FETCH_STATUS = 0\r\n          BEGIN \r\n    \r\n                EXEC (@CurrentCommand);\r\n    \r\n                FETCH NEXT FROM result_cursor INTO @CurrentCommand;\r\n          END;\r\n    \r\n    --CLOSE result_cursor;\r\n    --DEALLOCATE result_cursor;\r\n\r\nThen in another window, run this:\r\n\r\n    SELECT dec.session_id, dec.cursor_id, dec.name, dec.properties, dec.creation_time, dec.is_open, dec.is_close_on_commit,\r\n            dec.fetch_status, dec.worker_time, dec.reads, dec.writes, dec.dormant_duration, dest.text\r\n        FROM sys.dm_exec_cursors (0) AS dec\r\n        CROSS APPLY sys.dm_exec_sql_text(dec.sql_handle) AS dest\r\n        WHERE dec.is_open = 1;\r\n\r\n[![Nuts][1]][1]\r\n\r\nYou should see the cursor opened and left hanging by the other session here, along with any other open cursors. \r\n\r\nIf you need to monitor for these (and you don't have a monitoring tool), you can use the dormant_duration column and poll that DMV with an agent job that will fire off an alert or email when an open cursor is dormant for a certain amount of time. That's in milliseconds.\r\n\r\n  [1]: https://i.stack.imgur.com/irkfP.jpg	2019-12-05 21:22:44.32372+00	3	4	1	167407	0	0	0	2019-12-05 16:44:07.353882+00	\N	Rather than search all of your stored procedure text for those wildcards, you'd probably be better off looking for cursors that are open, and their associated text. That would likely make it easier to locate cursor names, stored procs, etc. written by forgetful developers.	f	f
130	189	12	2017-05-18 14:09:12+00	As you know, the optimizer's search is not exhaustive. It tries things that make sense in context, and which frequently pay dividends on real queries. Forcing a loop join between two single-column unindexed heap tables is not such a scenario. That said, here are some details:\r\n\r\nSQL Server likes to transform applies to joins early, because it knows more tricks with joins. Later on, it may explore converting the join back to an apply. The [difference between the two][1] being correlated parameters (outer references). Applies make sense when there is a suitable index on the inner side. Your example has no indexes, so the optimizer is not persuaded to explore translation to an apply.\r\n\r\nA simple (non-apply) join has the join predicate on the join operator instead of outer references. The spool optimization for a non-apply is typically a lazy table spool, since there is no predicate on the inner side, only at the join.\r\n\r\nThe optimizer does not consider building an index on the fly to enable an apply; rather the sequence of events is usually the reverse: transform to apply because a good index exists.\r\n\r\nYou can sometimes encourage an apply rather than a join by using `APPLY` syntax in your query. The undocumented trace flag 9114 can assist in this by dissuading the optimizer from translating a logical apply to a join up front. For example:\r\n\r\n    SELECT * \r\n    FROM dbo.X_1000 AS a\r\n    CROSS APPLY (SELECT * FROM dbo.X_1000 AS b WHERE b.ID = a.ID) AS b\r\n    OPTION (QUERYTRACEON 9114);\r\n\r\n[![Spool plan][2]][2]\r\n\r\nAn index spool is favoured for apply because the outer reference means the selection is applied on the inner side of the join. You will often see this via `SelToIndexOnTheFly` but other paths exist. See my article [The Eager Index Spool and The Optimizer](https://sqlperformance.com/2019/11/sql-performance/eager-index-spool-optimizer).\r\n\r\n\r\n  [1]: https://www.sql.kiwi/2019/06/apply-versus-nested-loops-join.html\r\n  [2]: https://i.stack.imgur.com/AYIC4.png	2019-11-28 16:37:38.880549+00	2	4	1	173944	0	0	0	2019-11-28 16:36:19.891667+00	\N	As you know, the optimizer's search is not exhaustive. It tries things that make sense in context, and which frequently pay dividends on real queries. Forcing a loop join between two single-column unindexed heap tables is not such a scenario. That said, here are some details:	f	f
501	466	751	2013-03-13 10:46:52+00	To insert a single row\r\n\r\n    INSERT INTO RR DEFAULT VALUES;\r\n\r\nIt is possible to insert multiple rows of default values by (ab)using `MERGE`\r\n\r\n    MERGE INTO RR\r\n    USING (SELECT TOP 1000 *\r\n           FROM   master..spt_values) T\r\n    ON 1 = 0\r\n    WHEN NOT MATCHED THEN\r\n      INSERT\r\n      DEFAULT VALUES; 	2019-12-19 13:05:18.710396+00	4	4	1	36557	0	0	0	2019-12-19 13:05:18.710396+00	\N	To insert a single row	f	f
268	297	16	2018-09-13 13:29:22+00	I know that this has a long-standing Great Answer® from Martin, but I wanted to add in some changes to the behavior here in newer versions of SQL Server. This appears only to have been tested up to 2008R2.\r\n\r\nWith the new [USE HINTs][1] that make doing some cardinality estimation time travel possible, we can see when things changed.\r\n\r\nUsing the same setup as in the SQL Fiddle.\r\n\r\n    CREATE TABLE T ( ID INT IDENTITY PRIMARY KEY, DateTimeCol DATETIME, Filler CHAR(8000) NULL );\r\n    \r\n    CREATE INDEX IX_T_DateTimeCol ON T ( DateTimeCol );\r\n    \r\n    \r\n    WITH E00(N) AS (SELECT 1 UNION ALL SELECT 1),\r\n         E02(N) AS (SELECT 1 FROM E00 a, E00 b),\r\n         E04(N) AS (SELECT 1 FROM E02 a, E02 b),\r\n         E08(N) AS (SELECT 1 FROM E04 a, E04 b),\r\n         Num(N) AS (SELECT ROW_NUMBER() OVER (ORDER BY E08.N) FROM E08)\r\n    INSERT INTO T(DateTimeCol)\r\n    SELECT TOP 100 DATEADD(MINUTE, Num.N, '20130101')\r\n    FROM Num;\r\n\r\nWe can test the different levels like so:\r\n\r\n    SELECT *\r\n    FROM   T\r\n    WHERE  CAST(DateTimeCol AS DATE) = '20130101'\r\n    OPTION ( USE HINT ( 'QUERY_OPTIMIZER_COMPATIBILITY_LEVEL_100' ));\r\n    GO\r\n    \r\n    SELECT *\r\n    FROM   T\r\n    WHERE  CAST(DateTimeCol AS DATE) = '20130101'\r\n    OPTION ( USE HINT ( 'QUERY_OPTIMIZER_COMPATIBILITY_LEVEL_110' ));\r\n    GO \r\n    \r\n    SELECT *\r\n    FROM   T\r\n    WHERE  CAST(DateTimeCol AS DATE) = '20130101'\r\n    OPTION ( USE HINT ( 'QUERY_OPTIMIZER_COMPATIBILITY_LEVEL_120' ));\r\n    GO \r\n    \r\n    SELECT *\r\n    FROM   T\r\n    WHERE  CAST(DateTimeCol AS DATE) = '20130101'\r\n    OPTION ( USE HINT ( 'QUERY_OPTIMIZER_COMPATIBILITY_LEVEL_130' ));\r\n    GO \r\n    \r\n    SELECT *\r\n    FROM   T\r\n    WHERE  CAST(DateTimeCol AS DATE) = '20130101'\r\n    OPTION ( USE HINT ( 'QUERY_OPTIMIZER_COMPATIBILITY_LEVEL_140' ));\r\n    GO \r\n\r\nThe plans for all of these are [available here][2]. Compat levels 100 and 110 both give the key lookup plan, but starting with compat level 120, we start getting the same scan plan with 100 row estimates. This is true up to compat level 140.\r\n\r\n[![NUTS][3]][3]\r\n\r\n[![NUTS][4]][4]\r\n\r\n[![NUTS][5]][5]\r\n\r\nCardinality estimation for the `>= '20130101', < '20130102'` plans remain at 100, which was expected.\r\n\r\n\r\n  [1]: https://support.microsoft.com/en-us/help/4342424\r\n  [2]: https://www.brentozar.com/pastetheplan/?id=H14WB1u_Q\r\n  [3]: https://i.stack.imgur.com/LgfFa.jpg\r\n  [4]: https://i.stack.imgur.com/7WVm1.jpg\r\n  [5]: https://i.stack.imgur.com/fHz00.jpg	2019-12-04 22:47:40.194792+00	3	4	1	217503	0	0	0	2019-12-04 22:47:40.194792+00	\N	I know that this has a long-standing Great Answer® from Martin, but I wanted to add in some changes to the behavior here in newer versions of SQL Server. This appears only to have been tested up to 2008R2.	f	f
267	297	751	2013-02-04 10:39:53+00	The mechanism behind the sargability of casting to date is called [dynamic seek][1]. \r\n\r\nSQL Server calls an internal function `GetRangeThroughConvert` to get the start and end of the range.\r\n\r\nSomewhat surprisingly this is **not** the same range as your literal values.\r\n\r\nCreating a table with a row per page and 1440 rows per day\r\n\r\n    CREATE TABLE T\r\n      (\r\n         DateTimeCol DATETIME PRIMARY KEY,\r\n         Filler      CHAR(8000) DEFAULT 'X'\r\n      );\r\n    \r\n    WITH Nums(Num)\r\n         AS (SELECT number\r\n             FROM   spt_values\r\n             WHERE  type = 'P'\r\n                    AND number BETWEEN 1 AND 1440),\r\n         Dates(Date)\r\n         AS (SELECT {d '2012-12-30'} UNION ALL\r\n             SELECT {d '2012-12-31'} UNION ALL\r\n             SELECT {d '2013-01-01'} UNION ALL\r\n             SELECT {d '2013-01-02'} UNION ALL\r\n             SELECT {d '2013-01-03'})\r\n    INSERT INTO T\r\n                (DateTimeCol)\r\n    SELECT DISTINCT DATEADD(MINUTE, Num, Date)\r\n    FROM   Nums,\r\n           Dates \r\n\r\nThen running\r\n\r\n    SET STATISTICS IO ON;\r\n    SET STATISTICS TIME ON;\r\n    \r\n    SELECT *\r\n    FROM   T\r\n    WHERE  DateTimeCol >= '20130101'\r\n           AND DateTimeCol < '20130102'\r\n    \r\n    SELECT *\r\n    FROM   T\r\n    WHERE  CAST(DateTimeCol AS DATE) = '20130101'; \r\n\r\nThe first query has `1443` reads and the second `2883` so it is reading an entire additional day then discarding it against a residual predicate.\r\n\r\nThe plan shows the seek predicate is \r\n\r\n    Seek Keys[1]: Start: DateTimeCol > Scalar Operator([Expr1006]), \r\n                   End: DateTimeCol < Scalar Operator([Expr1007])\r\n\r\n\r\nSo instead of `>= '20130101' ... < '20130102'` it reads `> '20121231' ... < '20130102'` then discards all the `2012-12-31` rows.\r\n\r\nAnother disadvantage of relying on it is that the cardinality estimates may not be as accurate as with the traditional range query. This can be seen in an amended version of your [SQL Fiddle][2].\r\n\r\nAll 100 rows in the table now match the predicate (with datetimes 1 minute apart all on the same day). \r\n\r\nThe second (range) query correctly estimates that 100 will match and uses a clustered index scan. The `CAST( AS DATE)` query incorrectly estimates that only one row will match and produces a plan with key lookups. \r\n\r\nThe statistics aren't ignored completely. If all rows in the table have the same `datetime` and it matches the predicate (e.g. `20130101 00:00:00` or `20130101 01:00:00`) then the plan shows a clustered index scan with an estimated 31.6228 rows. \r\n\r\n    100 ^ 0.75 = 31.6228\r\n\r\nSo in that case it appears the estimate is derived from [this formula][3]:\r\n\r\n>The following table shows the number of conjuncts guessed and the resultant selectivity as a function of input table cardinality of N:\r\n\r\n>```none\r\n| Conjuncts | Cardinality | Selectivity |\r\n|-----------|-------------|-------------|\r\n| 1         | N^(3/4)     | N^(-1/4)    |\r\n| 2         | N^(11/16)   | N^(-5/16)   |\r\n| 3         | N^(43/64)   | N^(-21/64)  |\r\n| 4         | N^(171/256) | N^(-85/256) |\r\n| 5         | N^(170/256) | N^(-86/256) |\r\n| 6         | N^(169/256) | N^(-87/256) |\r\n| 7         | N^(168/256) | N^(-88/256) |\r\n| ...       |             |             |\r\n| 175       | N^(0/256)   | N^(-1)      |\r\n>```\r\n\r\n\r\nIf all rows in the table have the same `datetime` and it doesn't match the predicate (e.g. `20130102 01:00:00`) then it falls back to the estimated row count of 1 and the plan with lookups. \r\n\r\nFor the cases where the table has more than one `DISTINCT` value the estimated rows seems to be the same as if the query was looking for exactly `20130101 00:00:00`. \r\n\r\nIf the statistics histogram happens to have a step at `2013-01-01 00:00:00.000` then the estimate will be based on the `EQ_ROWS` (i.e. not taking into account other times on that date). Otherwise if there is no step it looks as though it uses the `AVG_RANGE_ROWS` from the surrounding steps.\r\n\r\nAs `datetime` has a precision of approx 3ms in many systems there will be very few actual duplicate values and this number will be 1.\r\n\r\n  [1]: https://sql.kiwi/2012/01/dynamic-seeks-and-hidden-implicit-conversions.html\r\n  [2]: http://sqlfiddle.com/#!3/0c907c/1\r\n  [3]: https://blogs.msdn.microsoft.com/ianjo/2006/03/28/disabling-constant-constant-comparison-estimation/	2019-12-04 22:47:39.754246+00	3	4	1	34052	0	0	0	2019-12-04 22:47:39.754246+00	\N	The mechanism behind the sargability of casting to date is called [dynamic seek][1].	f	f
253	289	16	2019-08-27 11:46:57+00	Index Hint\r\n--\r\nYou can use an [index hint][1] to do that:\r\n\r\n    SELECT *\r\n    FROM dbo.Users AS u WITH (INDEX = ix_definitely_an_index)\r\n    WHERE u.Reputation = 2;\r\n\r\nThe downsides are:\r\n\r\n - Potentially changing a lot of code\r\n - If you rename an index, this breaks\r\n - If you change an index definition, it might not be the best index to use anymore\r\n\r\nPlan Guides\r\n--\r\nYou can also use a [Plan Guide][2], which sets the plan you want in place.\r\n\r\nThe downsides are:\r\n\r\n - It's fickle; if your query isn't exactly the same, your plan might not get used\r\n - It's tough to set up; you have to get everything exactly right\r\n - Also the same stuff as an index hint, so uh... That's rough.\r\n\r\nQuery Store?\r\n--\r\nSince you're on SQL Server 2017, you may be able to use [Query Store to force plans][3]. This is much easier than using a Plan Guide, but you may end up with an [unexpected plan][4].\r\n\r\nDownsides:\r\n\r\n - Same stuff as above\r\n - Having to enable Query Store, if you're not already using it\r\n - Having to query Query Store to see if forced plans are failing\r\n\r\nAlternatives\r\n--\r\nWithout knowing more, I'd probably be more keen on investigating if the index use changes for other reasons, like [parameter sniffing][5].\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-query?view=sql-server-2017\r\n  [2]: https://docs.microsoft.com/en-us/sql/relational-databases/performance/plan-guides?view=sql-server-2017\r\n  [3]: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-query-store-force-plan-transact-sql?view=sql-server-2017\r\n  [4]: https://sqlworkbooks.com/2018/03/what-is-a-morally-equivalent-execution-plan-and-why-is-it-good/\r\n  [5]: https://dba.stackexchange.com/q/204565/32281	2019-12-04 14:31:41.615967+00	2	4	1	246356	0	0	0	2019-12-04 14:31:41.615967+00	\N	Index Hint	f	f
248	286	16	2018-09-05 18:16:48+00	If you're on SQL Server 2012+, and you want to do it (without decimal places):\r\n\r\n    SELECT FORMAT(2036150, N'N0')\r\n\r\nIf you're on an earlier version, you have to jump through some hoops:\r\n\r\n    SELECT REPLACE(CONVERT(NVARCHAR(30), CAST((2036150) AS MONEY), 1), N'.00', N'')	2019-12-04 14:29:34.177495+00	1	4	1	216821	0	0	0	2019-12-04 14:29:34.177495+00	\N	If you're on SQL Server 2012+, and you want to do it (without decimal places):	f	f
244	282	16	2019-09-28 13:01:48+00	Escalation, though\r\n--\r\n[Lock escalation][1] under serializable isolation level may occur the same as it does with other isolation levels. \r\n\r\n- Correct indexes can help to avoid lock escalation up to a point\r\n- Locking many indexes will increase the likelihood of lock escalation; the count is cumulative across objects for a single statement\r\n\r\nSome quick examples using a single table with a single index. Id is the primary key and clustered index on the table.\r\n\r\nOne Row\r\n--\r\n\r\n    SET TRANSACTION ISOLATION LEVEL SERIALIZABLE\r\n    BEGIN TRAN\r\n    \r\n    UPDATE c\r\n    SET c.Score = 2147483647 \r\n    FROM dbo.Comments AS c\r\n    WHERE c.Id = 138; --One value\r\n    \r\n    ROLLBACK\r\n\r\nFor a single Id value, locking is minimal. \r\n\r\n    +--------------+---------------+---------------+-------------+\r\n    | request_mode | locked_object | resource_type | total_locks |\r\n    +--------------+---------------+---------------+-------------+\r\n    | RangeX-X     | Comments      | KEY           |           1 |\r\n    | IX           | Comments      | OBJECT        |           1 |\r\n    | IX           | Comments      | PAGE          |           1 |\r\n    +--------------+---------------+---------------+-------------+\r\n\r\nMultiple Rows\r\n--\r\n\r\nBut locks will go up if we start working in ranges:\r\n\r\n    SET TRANSACTION ISOLATION LEVEL SERIALIZABLE\r\n    BEGIN TRAN\r\n    \r\n    UPDATE c\r\n    SET c.Score = 2147483647 \r\n    FROM dbo.Comments AS c\r\n    WHERE c.Id BETWEEN 1 AND 5000; -- Small range\r\n    \r\n    ROLLBACK\r\n\r\nNow we have more exclusive locks on more keys:\r\n\r\n    +--------------+---------------+---------------+-------------+\r\n    | request_mode | locked_object | resource_type | total_locks |\r\n    +--------------+---------------+---------------+-------------+\r\n    | RangeX-X     | Comments      | KEY           |        2429 |\r\n    | IX           | Comments      | OBJECT        |           1 |\r\n    | IX           | Comments      | PAGE          |          97 |\r\n    +--------------+---------------+---------------+-------------+\r\n\r\nWay More Rows\r\n--\r\n\r\nThis will carry on until we hit a tipping point:\r\n\r\n    SET TRANSACTION ISOLATION LEVEL SERIALIZABLE\r\n    BEGIN TRAN\r\n    \r\n    UPDATE c\r\n    SET c.Score = 2147483647 \r\n    FROM dbo.Comments AS c\r\n    WHERE c.Id BETWEEN 1 AND 11655; --Larger range\r\n    \r\n    ROLLBACK\r\n\r\nLock escalation is attempted and is successful:\r\n\r\n    +--------------+---------------+---------------+-------------+\r\n    | request_mode | locked_object | resource_type | total_locks |\r\n    +--------------+---------------+---------------+-------------+\r\n    | X            | Comments      | OBJECT        |           1 |\r\n    +--------------+---------------+---------------+-------------+\r\n\r\nPay Attention\r\n--\r\nIt's important to separate two concepts here: the isolation level will be serializable no matter what kind of locks are taken. The query chooses the isolation level, and the storage engine chooses the locks. Serializable won't always result in range locks -- the storage engine can pick whichever kind of locks still honor the isolation level.\r\n\r\n\r\n  [1]: https://dba.stackexchange.com/questions/12864/what-is-lock-escalation	2019-12-04 14:25:34.597582+00	2	4	1	249846	0	0	0	2019-12-04 14:25:34.597582+00	\N	Escalation, though	f	f
727	648	1	2020-01-31 11:32:35.566253+00	This is now fixed, it's now working the way it was originally intended:\r\n\r\n* Question text is cached locally in your browser (per community)\r\n* Answer text is cached locally in your browser (per question)\r\n* The cached text is removed when you post the question/answer	2020-01-31 11:32:35.566253+00	9	1	1	\N	0	0	0	\N	\N	This is now fixed, it's now working the way it was originally intended:	f	f
718	648	2	2020-01-30 23:30:19.93289+00	Thanks, this is a regression — [now fixed](/meta?q=648#a727).	2020-01-31 11:34:04.793524+00	5	1	1	\N	0	0	0	\N	\N	Thanks, this is a regression — [now fixed](/meta?q=648#a727).	f	f
418	415	167	2016-09-05 15:52:46+00	The following will show the current section +-2 sections. If you want more or less sections to be displayed, change the value of `\\mymin=3` and `\\mymax=3` [and add corresponding exception for first, second, second to last and last section].\r\n\r\n    \\documentclass{beamer}\r\n    \r\n    \\usetheme{Darmstadt}\r\n    \r\n    % based on the original definitions in beamerbasenavigation.sty\r\n    \\makeatletter\r\n    \\def\\sectionentry#1#2#3#4#5{% section number, section title, page\r\n    %\r\n    \\newcount\\mymin%\r\n    \\mymin=3\r\n    \\ifnum\\c@section=1%\r\n        \\mymin=5\r\n    \\fi%\r\n    \\ifnum\\c@section=2%\r\n        \\mymin=4\r\n    \\fi%\r\n    %\r\n    \\newcount\\mymax%\r\n    \\mymax=3\r\n    \\ifnum\\c@section=\\beamer@sectionmax%\r\n        \\mymax=5\r\n    \\fi%\r\n    \\ifnum\\c@section=\\numexpr\\beamer@sectionmax-1%\r\n        \\mymax=4\r\n    \\fi%\r\n    %\r\n        \\ifnum\\numexpr\\c@section-#1<\\mymax%\r\n            \\ifnum\\numexpr#1-\\c@section<\\mymin%\r\n                \\ifnum#5=\\c@part%\r\n                    \\beamer@section@set@min@width\r\n                    \\box\\beamer@sectionbox\\hskip1.875ex plus 1fill%\r\n                    \\beamer@xpos=0\\relax%\r\n                    \\beamer@ypos=1\\relax%\r\n                    \\setbox\\beamer@sectionbox=\r\n                    \\hbox{\r\n                        \\def\\insertsectionhead{#2}%\r\n                        \\def\\insertsectionheadnumber{#1}%\r\n                        \\def\\insertpartheadnumber{#5}%\r\n    \r\n                        {%\r\n                            \\usebeamerfont{section in head/foot}\\usebeamercolor[fg]{section in head/foot}%\r\n                            \\ifnum\\c@section=#1%\r\n                                \\hyperlink{Navigation#3}{{\\usebeamertemplate{section in head/foot}}}%\r\n                            \\else%\r\n                                \\hyperlink{Navigation#3}{{\\usebeamertemplate{section in head/foot shaded}}}%\r\n                            \\fi%    \r\n                        }%\r\n                    }%\r\n                    \\ht\\beamer@sectionbox=1.875ex%\r\n                    \\dp\\beamer@sectionbox=0.75ex%\r\n                \\fi%\r\n            \\fi%\r\n        \\fi%\r\n        \\ignorespaces%\r\n    }\r\n    \r\n    \\def\\slideentry#1#2#3#4#5#6{%\r\n    \t%section number, subsection number, slide number, first/last frame, page number, part number\r\n    \t%\r\n    \t\\newcount\\mymin%\r\n    \t\\mymin=3\r\n    \t\\ifnum\\c@section=1%\r\n    \t\t\\mymin=5\r\n    \t\\fi%\r\n    \t\\ifnum\\c@section=2%\r\n    \t\t\\mymin=4\r\n    \t\\fi%\r\n    \t\t%\r\n    \t\\newcount\\mymax%\r\n    \t\\mymax=3\r\n    \t\\ifnum\\c@section=\\beamer@sectionmax%\r\n    \t\t\\mymax=5\r\n    \t\\fi%\r\n    \t\\ifnum\\c@section=\\numexpr\\beamer@sectionmax-1%\r\n    \t\t\\mymax=4\r\n    \t\\fi%\r\n    \t%\r\n    \t\\ifnum\\numexpr\\c@section-#1<\\mymax%\r\n    \t\t\\ifnum\\numexpr#1-\\c@section<\\mymin%\r\n    \t\t  \\ifnum#6=\\c@part\\ifnum#2>0\\ifnum#3>0%\r\n    \t\t    \\ifbeamer@compress%\r\n    \t\t      \\advance\\beamer@xpos by1\\relax%\r\n    \t\t    \\else%\r\n    \t\t      \\beamer@xpos=#3\\relax%\r\n    \t\t      \\beamer@ypos=#2\\relax%\r\n    \t\t    \\fi%\r\n    \t\t  \\hbox to 0pt{%\r\n    \t\t    \\beamer@tempdim=-\\beamer@vboxoffset%\r\n    \t\t    \\advance\\beamer@tempdim by-\\beamer@boxsize%\r\n    \t\t    \\multiply\\beamer@tempdim by\\beamer@ypos%\r\n    \t\t    \\advance\\beamer@tempdim by -.05cm%\r\n    \t\t    \\raise\\beamer@tempdim\\hbox{%\r\n    \t\t      \\beamer@tempdim=\\beamer@boxsize%\r\n    \t\t      \\multiply\\beamer@tempdim by\\beamer@xpos%\r\n    \t\t      \\advance\\beamer@tempdim by -\\beamer@boxsize%\r\n    \t\t      \\advance\\beamer@tempdim by 1pt%\r\n    \t\t      \\kern\\beamer@tempdim\r\n    \t\t      \\global\\beamer@section@min@dim\\beamer@tempdim\r\n    \t\t      \\hbox{\\beamer@link(#4){%\r\n    \t\t          \\usebeamerfont{mini frame}%\r\n    \t\t          \\ifnum\\c@section=#1%\r\n    \t\t            \\ifnum\\c@subsection=#2%\r\n    \t\t              \\usebeamercolor[fg]{mini frame}%\r\n    \t\t              \\ifnum\\c@subsectionslide=#3%\r\n    \t\t                \\usebeamertemplate{mini frame}%\\beamer@minislidehilight%\r\n    \t\t              \\else%\r\n    \t\t                \\usebeamertemplate{mini frame in current subsection}%\\beamer@minisliderowhilight%\r\n    \t\t              \\fi%\r\n    \t\t            \\else%\r\n    \t\t              \\usebeamercolor{mini frame}%\r\n    \t\t              %\\color{fg!50!bg}%\r\n    \t\t              \\usebeamertemplate{mini frame in other subsection}%\\beamer@minislide%\r\n    \t\t            \\fi%\r\n    \t\t          \\else%\r\n    \t\t            \\usebeamercolor{mini frame}%\r\n    \t\t            %\\color{fg!50!bg}%\r\n    \t\t            \\usebeamertemplate{mini frame in other subsection}%\\beamer@minislide%\r\n    \t\t          \\fi%\r\n    \t\t        }}}\\hskip-10cm plus 1fil%\r\n    \t\t  }\\fi\\fi%\r\n    \t\t  \\else%\r\n    \t\t  \\fakeslideentry{#1}{#2}{#3}{#4}{#5}{#6}%\r\n    \t\t \\fi%\r\n    \t\t\\fi%\r\n    \t\\fi%\r\n    \t\\ignorespaces%\r\n    }\r\n    \\makeatother\r\n    \r\n    \r\n    \\begin{document}\r\n    \r\n    \\section{Section 1}\r\n    \\subsection{Subsection 1.1}\r\n    \\begin{frame}{Frame 1.11}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 1.12}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 1.13}\r\n    ...\r\n    \\end{frame}\r\n    \\subsection{Subsection 1.2}\r\n    \\begin{frame}{Frame 1.21}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 1.22}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 1.23}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 2}\r\n    \\begin{frame}{Frame 2}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 3}\r\n    \\begin{frame}{Frame 3}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 4}\r\n    \\begin{frame}{Frame 4}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 5}\r\n    \\begin{frame}{Frame 5}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 6}\r\n    \\begin{frame}{Frame 6}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 7}\r\n    \\begin{frame}{Frame 7}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 8}\r\n    \\begin{frame}{Frame 8}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 9}\r\n    \\subsection{Subsection 9.1}\r\n    \\begin{frame}{Frame 9.11}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 9.12}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 9.13}\r\n    ...\r\n    \\end{frame}\r\n    \\subsection{Subsection 9.2}\r\n    \\begin{frame}{Frame 9.21}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 9.22}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 9.23}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \r\n    \\section{Section 10}\r\n    \\begin{frame}{Frame 10}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 11}\r\n    \\begin{frame}{Frame 11}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 12}\r\n    \\begin{frame}{Frame 12}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 13}\r\n    \\begin{frame}{Frame 13}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 14}\r\n    \\begin{frame}{Frame 14}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 15}\r\n    \\begin{frame}{Frame 15}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 16}\r\n    \\subsection{Subsection 16.1}\r\n    \\begin{frame}{Frame 16.11}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 16.12}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 16.13}\r\n    ...\r\n    \\end{frame}\r\n    \\subsection{Subsection 16.2}\r\n    \\begin{frame}{Frame 16.21}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 16.22}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 16.23}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 17}\r\n    \\begin{frame}{Frame 17}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 18}\r\n    \\begin{frame}{Frame 18}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 19}\r\n    \\subsection{Subsection 19.1}\r\n    \\begin{frame}{Frame 19.11}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 19.12}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 19.13}\r\n    ...\r\n    \\end{frame}\r\n    \\subsection{Subsection 19.2}\r\n    \\begin{frame}{Frame 19.21}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 19.22}\r\n    ...\r\n    \\end{frame}\r\n    \\begin{frame}{Frame 19.23}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\section{Section 20}\r\n    \\begin{frame}{Frame 20}\r\n    ...\r\n    \\end{frame}\r\n    \r\n    \\end{document}\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/keg5v.png	2019-12-11 15:54:47.584978+00	4	4	1	328123	0	0	0	2019-12-11 15:54:47.584978+00	\N	The following will show the current section +-2 sections. If you want more or less sections to be displayed, change the value of `\\mymin=3` and `\\mymax=3` [and add corresponding exception for first, second, second to last and last section].	f	f
820	709	12	2019-11-20 05:47:22+00	### Cause\r\n\r\nSQL Server is trying to [inline the function][1] but failing due to the complexity.\r\n\r\nUsing so much memory while doing so is unexpected and almost certainly a bug.\r\n\r\nA definition for the nested function `dbo.IstFeiertag` would be needed for a full repro.\r\n\r\n### Workaround\r\n\r\nAdd `WITH INLINE = OFF` to the function(s) definition. Once this issue is resolved, you should be able to remove that option to reap the performance benefits of function inlining.\r\n\r\n### Reporting and Status\r\n\r\nYou should [report][2] this issue to Microsoft. If you have a support agreement, go that route. Alternatively, post a bug report on [User Voice][3], and email the Intelligent Query Processing team at intelligentqp@microsoft.com.\r\n\r\n[Joe Sack][4] (Principal Program Manager, Microsoft SQL Server product team) commented:\r\n\r\n> Thank you for reporting.  Paul White gave me a heads-up and I've reported to our team for investigation.\r\n\r\n---\r\n\r\n### Resolution\r\n\r\nA [fix for this issue][5] was released as part of [Cumulative Update 2 for SQL Server 2019][6].\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/user-defined-functions/scalar-udf-inlining\r\n  [2]: https://docs.microsoft.com/en-us/sql/sql-server/sql-server-get-help\r\n  [3]: https://aka.ms/sqlfeedback\r\n  [4]: https://dba.stackexchange.com/users/110891/joe-sack\r\n  [5]: https://support.microsoft.com/en-us/help/4538581/fix-scalar-udf-inlining-issues-in-sql-server-2019\r\n  [6]: https://support.microsoft.com/en-us/help/4536075/cumulative-update-2-for-sql-server-2019	2020-02-14 04:19:39.742384+00	8	4	1	253683	0	0	0	2020-02-14 04:19:39.742384+00	\N	Cause	f	f
80	103	12	2019-11-25 13:54:52.878225+00	>1. Am I right that it will guarantee the order in this case without an order by clause?\r\n\r\n**No.** A *Flow Distinct* that preserves order (allowing `ORDER BY` without a sort) is not implemented in SQL Server today. It is possible to do in principle, but then many things are possible if we are allowed to change the SQL Server source code. If you can make a good case for this development work, you could [suggest it to Microsoft][1].\r\n\r\n>2. If not, is there another method to force a plan that is as fast as Solution A, preferably one that avoids sorts?\r\n\r\n**Yes.** (Table & query hints only required when using the pre-2014 cardinality estimator):\r\n\r\n    -- Additional index\r\n    CREATE UNIQUE NONCLUSTERED INDEX i \r\n    ON #Orders (StoreID, CustID, Amount, OrderID);\r\n\r\n    -- Query\r\n    SELECT TOP (500) \r\n    \tO.CustID, \r\n    \tO.Amount\r\n    FROM #Orders AS O\r\n        WITH (FORCESEEK(IX (StoreID)))\r\n    WHERE O.StoreID = 1\r\n    AND NOT EXISTS\r\n    (\r\n    \tSELECT NULL\r\n    \tFROM #Orders AS O2\r\n            WITH (FORCESEEK(i (StoreID, CustID, Amount)))\r\n    \tWHERE \r\n    \t\tO2.StoreID = O.StoreID\r\n    \t\tAND O2.CustID = O.CustID\r\n    \t\tAND O2.Amount >= O.Amount\r\n    \t\tAND\r\n    \t\t(\r\n    \t\t\tO2.Amount > O.Amount\r\n    \t\t\tOR\r\n    \t\t\t(\r\n    \t\t\t\tO2.Amount = O.Amount\r\n    \t\t\t\tAND O2.OrderID > O.OrderID\r\n    \t\t\t)\r\n    \t\t)\r\n    )\r\n    ORDER BY\r\n    \tO.Amount DESC\r\n    OPTION (MAXDOP 1);\r\n\r\n![Actual Execution Plan][2]\r\n\r\n    (500 row(s) affected)\r\n    \r\n     SQL Server Execution Times:\r\n       CPU time = 0 ms,  elapsed time = 4 ms.\r\n\r\n### SQL CLR solution\r\n\r\nThe following script shows using a SQL CLR table-valued function to meet the stated requirements. I am not a C# expert, so the code may bear improvement:\r\n\r\n    USE Sandpit;\r\n    GO\r\n    -- Ensure SQLCLR is enabled\r\n    EXECUTE sys.sp_configure\r\n    \t@configname = 'clr enabled',\r\n        @configvalue = 1;\r\n    RECONFIGURE;\r\n    GO\r\n    -- Lazy, but effective to allow EXTERNAL_ACCESS\r\n    ALTER DATABASE Sandpit\r\n    SET TRUSTWORTHY ON;\r\n    GO\r\n    -- The CLR assembly\r\n    CREATE ASSEMBLY FlowDistinctOrder\r\n    AUTHORIZATION dbo\r\n    FROM 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r\n    WITH PERMISSION_SET = EXTERNAL_ACCESS;\r\n    GO\r\n    -- The CLR TVF with order guarantee\r\n    CREATE FUNCTION dbo.FlowDistinctOrder \r\n    (\r\n    \t@ServerName nvarchar(128), \r\n    \t@DatabaseName nvarchar(128), \r\n    \t@MaxRows bigint\r\n    )\r\n    RETURNS TABLE \r\n    (\r\n    \tCustID integer NULL, \r\n    \tAmount float NULL\r\n    )\r\n    ORDER (Amount DESC)\r\n    AS EXTERNAL NAME FlowDistinctOrder.UserDefinedFunctions.FlowDistinctOrder;\r\n    \r\nTest table and sample data from the question:\r\n\r\n    -- Test table\r\n    CREATE TABLE dbo.Orders\r\n    (  \r\n    \tOrderID    integer\tNOT NULL IDENTITY(1,1),\r\n    \tCustID     integer\tNOT NULL,\r\n    \tStoreID    integer\tNOT NULL,\r\n    \tAmount     float\tNOT NULL\r\n    );\r\n    GO\r\n    -- Sample data\r\n    WITH  \r\n    \tCte0 AS (SELECT 1 AS C UNION ALL SELECT 1), --2 rows  \r\n    \tCte1 AS (SELECT 1 AS C FROM Cte0 AS A, Cte0 AS B),--4 rows  \r\n    \tCte2 AS (SELECT 1 AS C FROM Cte1 AS A ,Cte1 AS B),--16 rows \r\n    \tCte3 AS (SELECT 1 AS C FROM Cte2 AS A ,Cte2 AS B),--256 rows \r\n    \tCte4 AS (SELECT 1 AS C FROM Cte3 AS A ,Cte3 AS B),--65536 rows \r\n    \tCte5 AS (SELECT 1 AS C FROM Cte4 AS A ,Cte2 AS B),--1048576 rows \r\n    \tFinalCte AS (SELECT  ROW_NUMBER() OVER (ORDER BY C) AS Number FROM   Cte5)\r\n    INSERT dbo.Orders \r\n    \t(CustID, StoreID, Amount)\r\n    SELECT \r\n    \tCustID\t= Number / 10,\r\n    \tStoreID = Number % 4,\r\n    \tAmount  = 1000 * RAND(Number)\r\n    FROM FinalCte\r\n    WHERE \r\n    \tNumber <= 1000000;\r\n    GO\r\n    -- Index\r\n    CREATE CLUSTERED INDEX IX \r\n    ON dbo.Orders \r\n    \t(StoreID ASC, Amount DESC, CustID ASC);\r\n\r\nFunction test:\r\n\r\n    -- Test the function\r\n    -- Run several times to ensure connection is cached\r\n    -- and CLR code fully compiled\r\n    DECLARE @Start datetime2 = SYSUTCDATETIME();\r\n\r\n    SELECT TOP (500) \r\n    \tFDO.CustID\r\n    FROM dbo.FlowDistinctOrder\r\n    (\r\n    \t@@SERVERNAME,\t-- For external connection\r\n    \tDB_NAME(),\t\t-- For external connection\r\n    \t500\t\t\t\t-- Number of rows to return\r\n    ) AS FDO \r\n    ORDER BY \r\n    \tFDO.Amount DESC;\r\n    \r\n    SELECT DATEDIFF(MILLISECOND, @Start, SYSUTCDATETIME());\r\n\r\nExecution plan (note the validation of the `ORDER` guarantee):\r\n\r\n![CLR function execution plan][3]\r\n\r\nOn my laptop, this typically executes in 80-100ms. This is nowhere near as fast as the T-SQL rewrite above, but it should show good performance stability in the face of different data distributions.\r\n\r\nSource code:\r\n\r\n```cs\r\nusing Microsoft.SqlServer.Server;\r\nusing System.Collections;\r\nusing System.Collections.Generic;\r\nusing System.Data.SqlClient;\r\n\r\npublic partial class UserDefinedFunctions\r\n{\r\n    private sealed class ReverseComparer<T> : IComparer<T>\r\n    {\r\n        private readonly IComparer<T> original;\r\n\r\n        public ReverseComparer(IComparer<T> original)\r\n        {\r\n            this.original = original;\r\n        }\r\n\r\n        public int Compare(T left, T right)\r\n        {\r\n            return original.Compare(right, left);\r\n        }\r\n    }\r\n\r\n    [SqlFunction\r\n        (\r\n        DataAccess = DataAccessKind.Read,\r\n        SystemDataAccess = SystemDataAccessKind.None,\r\n        FillRowMethodName = "FillRow",\r\n        TableDefinition = "CustID integer NULL, Amount float NULL"\r\n        )\r\n    ]\r\n    public static IEnumerable FlowDistinctOrder\r\n        (\r\n        [SqlFacet (MaxSize=128)]string ServerName, \r\n        [SqlFacet (MaxSize=128)]string DatabaseName,\r\n        long MaxRows\r\n        )\r\n    {\r\n        var list = new SortedDictionary<double, int>\r\n            (new ReverseComparer<double>(Comparer<double>.Default));\r\n\r\n        var csb = new SqlConnectionStringBuilder();\r\n        csb.ConnectTimeout = 10;\r\n        csb.DataSource = ServerName;\r\n        csb.Enlist = false;\r\n        csb.InitialCatalog = DatabaseName;\r\n        csb.IntegratedSecurity = true;\r\n\r\n        using (var conn = new SqlConnection(csb.ConnectionString))\r\n        {\r\n            conn.Open();\r\n            using (var cmd = conn.CreateCommand())\r\n            {\r\n                cmd.CommandText =\r\n                    @"\r\n                    SELECT\r\n                        O.CustID, \r\n                        O.Amount\r\n                    FROM dbo.Orders AS O\r\n                    WHERE \r\n                        O.StoreID = 1 \r\n                    ORDER BY \r\n                        O.Amount DESC";\r\n\r\n                int custid;\r\n                double amount;\r\n\r\n                using (var rdr = cmd.ExecuteReader())\r\n                {\r\n                    while (rdr.Read())\r\n                    {\r\n                        custid = rdr.GetInt32(0);\r\n                        amount = rdr.GetDouble(1);\r\n\r\n                        if (!list.ContainsKey(amount))\r\n                        {\r\n                            list.Add(amount, custid);\r\n                            if (list.Count == MaxRows)\r\n                            {\r\n                                break;\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        return list;\r\n    }\r\n\r\n    public static void FillRow(object obj, out int CustID, out double Amount)\r\n    {\r\n        var v = (KeyValuePair<double, int>)obj;\r\n        CustID = v.Value;\r\n        Amount = v.Key;\r\n    }\r\n}\r\n```\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/sql-server/sql-server-get-help\r\n  [2]: https://i.stack.imgur.com/kMIm2.png\r\n  [3]: https://i.stack.imgur.com/WDkuQ.png\r\n	2019-11-25 17:57:44.850608+00	1	4	2	\N	0	0	0	\N	\N	>1. Am I right that it will guarantee the order in this case without an order by clause?	f	f
266	296	16	2017-08-10 20:29:25+00	It looks to me like just updating the columns to `NULL` will release pages for reuse. Here's a Very Scottish® demo, to celebrate it being almost 5PM, EST.\r\n\r\n    USE tempdb;\r\n    \r\n    DROP TABLE IF EXISTS dbo.RobertBurns;\r\n    \r\n    CREATE TABLE dbo.RobertBurns\r\n    (\r\n        Id INT IDENTITY(1, 1) PRIMARY KEY CLUSTERED,\r\n        Scotch VARCHAR(50),\r\n        HaggisAddress VARBINARY(MAX)\r\n    );\r\n    \r\n    DECLARE @AddressToAVarbinaryHaggis VARBINARY(MAX); \r\n    DECLARE @AddressToAHaggis NVARCHAR(MAX) = N'\r\n    Good luck to you and your honest, plump face,\r\n    Great chieftain of the pudding race!\r\n    Above them all you take your place,\r\n            gut, stomach-lining, or intestine,\r\n    You''re well worth a grace\r\n            as long as my arm.\r\n    \r\n    The overloaded serving tray there you fill,\r\n    Your buttocks shaped like a distant hilltop,\r\n    Your wooden skewer could be used to fix a mill\r\n             if need be,\r\n    While through your pores your juices drip\r\n             like liquid gold.\r\n    \r\n    His knife see the serving-man clean,\r\n    And then cut you up with great skill,\r\n    Making a trench in your bright, gushing guts\r\n            To form a ditch,\r\n    And then, 0h! What a glorious sight!\r\n            Warm, steaming, and rich!\r\n    \r\n    Then, spoonful after spoonful, they eagerly eat,\r\n    The devil will get the last bit, on they go,\r\n    Until all their well-stretched stomachs, by-and-by,\r\n            are bent like drums,\r\n    Then the head of the family, about to burst,\r\n            murmurs “Thank the Lord".\r\n    \r\n    Is there a pretentious soul who, over his French ragout,\r\n    Or Italian cuisine that would make a pig sick,\r\n    Or French stew that would make that same pig ill\r\n            with complete and utter disgust,\r\n    Looks down with a sneering, scornful attitude,\r\n            on such a meal? (as Haggis)\r\n    \r\n    Poor devil! See him over his trash!\r\n    As feeble as a withered bullrush,\r\n    His skinny leg no thicker than a thin rope,\r\n            His fist the size of a nut,\r\n    Through a river or field to travel,\r\n            Completely unfit!\r\n    \r\n    But look at the healthy, Haggis-fed person!\r\n    The trembling earth respects him as a man!\r\n    Put a knife in his fist,\r\n            He''ll make it work!\r\n    And legs, and arms, and heads will come off,\r\n            Like the tops of thistle.\r\n    \r\n    You Powers who look after mankind,\r\n    And dish out his bill of fare,\r\n    Old Scotland wants no watery, wimpy stuff\r\n            That splashes about in little wooden bowls!\r\n    But, if You will grant her a grateful prayer,\r\n            Give her a Haggis!';\r\n    \r\n    \r\n    INSERT dbo.RobertBurns (Scotch, HaggisAddress )\r\n    SELECT TOP 1000 \r\n    CASE WHEN x.c % 15 = 0 THEN 'Laphroaig'\r\n    \t WHEN x.c % 5 = 0 THEN 'Lagavulin'\r\n    \t WHEN x.c % 3 = 0 THEN 'Port Ellen'\r\n    \t ELSE 'Ardbeg'\r\n    END AS Scotch, \r\n    CONVERT(VARBINARY(MAX), REPLICATE(@AddressToAHaggis, x.c % 20 + 1))\r\n    FROM (\r\n    SELECT ROW_NUMBER() OVER (ORDER BY @@ROWCOUNT) AS c\r\n    FROM sys.messages AS m\r\n    ) AS x;\r\n    \r\n    CREATE INDEX ix_novarbinary\tON\tdbo.RobertBurns (Scotch, Id);\r\n    CREATE INDEX ix_yesvarbinary ON\tdbo.RobertBurns (Scotch, Id) INCLUDE (HaggisAddress);\r\n    \r\n\r\nWith rows inserted, let's check on our index pages.\r\n\r\n    SELECT   OBJECT_NAME(i.object_id) AS table_name,\r\n             i.name AS index_name,\r\n             MAX(a.used_pages) AS leaf_me_alone\r\n    FROM     sys.indexes AS i\r\n    JOIN     sys.partitions AS p\r\n    ON p.object_id = i.object_id\r\n       AND p.index_id = i.index_id\r\n    JOIN     sys.allocation_units AS a\r\n    ON a.container_id = p.partition_id\r\n    WHERE OBJECT_NAME(i.object_id) = 'RobertBurns'\r\n    GROUP BY i.object_id, i.index_id, i.name\r\n    ORDER BY OBJECT_NAME(i.object_id), i.index_id;\r\n    \r\n\r\nAfter the insert, I get this. Actual pages may vary for you.\r\n\r\n    table_name\tindex_name\t                    leaf_me_alone\r\n    RobertBurns\tPK__RobertBu__3214EC074BE633A2\t5587\r\n    RobertBurns\tix_novarbinary\t                10\r\n    RobertBurns\tix_yesvarbinary\t                5581\r\n\r\nLet's `NULL` out some rows!\r\n\r\n    UPDATE rb\r\n    \tSET rb.HaggisAddress = NULL\r\n    FROM dbo.RobertBurns AS rb\r\n    WHERE rb.Id % 15 = 0;\r\n\r\nAnd check back in on our pages:\r\n    \r\n\r\n    table_name\tindex_name\t                    leaf_me_alone\r\n    RobertBurns\tPK__RobertBu__3214EC074BE633A2\t5300\r\n    RobertBurns\tix_novarbinary\t                10\r\n    RobertBurns\tix_yesvarbinary\t                5273\r\n\r\n    \r\nSo page count was reduced. Huzzah! For the two indexes that touch our `VARBINARY` data, they lost a buncha pages. That means they're back in circulation for other objects to use. Since I'm in tempdb, they probably get gobbled up pretty quickly by all the garbage things that go on here.\r\n\r\nNow let's put some data back in:\r\n\r\n    INSERT dbo.RobertBurns (Scotch, HaggisAddress )\r\n    SELECT TOP 10 rb.Scotch, rb.HaggisAddress\r\n    FROM dbo.RobertBurns AS rb;\r\n\r\nAnd checking back in:\r\n\r\n    table_name\tindex_name\t                    leaf_me_alone\r\n    RobertBurns\tPK__RobertBu__3214EC074BE633A2\t5330\r\n    RobertBurns\tix_novarbinary\t                11\r\n    RobertBurns\tix_yesvarbinary\t                5305\r\n\r\nPage counts inched up a bit.\r\n\r\nSo, looks like you don't have to do anything too crazy, or even shrink your database to get space reuse. I think you were conflating the behavior of dropping columns and needing to run `DBCC CLEANTABLE` with what you're actually doing.\r\n\r\nHope this helps!\r\n	2019-12-04 22:46:02.191238+00	3	4	1	183220	0	0	0	2019-12-04 22:46:02.191238+00	\N	It looks to me like just updating the columns to `NULL` will release pages for reuse. Here's a Very Scottish® demo, to celebrate it being almost 5PM, EST.	f	f
260	293	33	2019-12-04 11:06:25+00	As of writing this, there is no way to insert multiple rows into just an `IDENTITY` column using an `INSERT` statement. The `DEFAULT VALUES` placeholder stands for just one row. And the `INSERT ... SELECT` syntax has no extension to support the same functionality as the `DEFAULT VALUES` clause.\r\n\r\nInstead, you can use a `MERGE` statement to achieve the goal:\r\n\r\n```\r\nMERGE INTO\r\n  dbo.Groups AS tgt\r\nUSING\r\n  dbo.MySource AS src  -- << use your source row set here\r\nON\r\n  1 = 0\r\nWHEN NOT MATCHED THEN\r\n  INSERT DEFAULT VALUES\r\nOUTPUT\r\n  INTO @output (ID)\r\n;\r\n```\r\n\r\nThe `ON 1 = 0` clause basically turns the `MERGE` into a pure `INSERT`, because the explicitly false condition causes all the source rows not to be matched and thus trigger the `WHEN NOT MATCHED THEN` branch of the statement. Now, the `WHEN NOT MATCHED THEN` branch expects a single-row insert definition, where the familiar `DEFAULT VALUES` is perfectly valid. As the result, you effectively get an `INSERT ... SELECT` statement with the functionality of `INSERT ... DEFAULT VALUES` for an arbitrary number of rows.	2019-12-04 17:41:37.634316+00	3	4	1	254772	0	0	0	2019-12-04 17:41:37.634316+00	\N	As of writing this, there is no way to insert multiple rows into just an `IDENTITY` column using an `INSERT` statement. The `DEFAULT VALUES` placeholder stands for just one row. And the `INSERT ... SELECT` syntax has no extension to support the same functionality as the `DEFAULT VALUES` clause.	f	f
712	650	811	2020-01-30 12:57:33.69865+00	[This fails] ₋ [This works](#) — This answer is just to demonstrate the bug.\r\n\r\n\r\n[This fails]: #	2020-01-30 12:57:33.69865+00	3	1	1	\N	0	0	0	\N	\N	[This fails](#) ₋ [This works](#) — This answer is just to demonstrate the bug.	f	f
715	650	2	2020-01-30 22:56:47.822785+00	This is an unfortunate consequence of a performance feature of markdown-it, which provides a `renderInline` function to do *almost* exactly what we want for answer summaries.\r\n\r\nI did raise it as [an issue on GitHub](https://github.com/markdown-it/markdown-it/issues/615) but it isn't going to change:\r\n\r\n> References are not "inline" objects. So, works as expected.\r\n\r\nWe are left with three choices:\r\n\r\n1. Edit answers so the first line only has inline links.\r\n\r\n   *Downside:* we'll probably be forever editing posts just for this tweak, and it makes posts that have neat links less neat.\r\n\r\n\r\n1. Use the full-fat renderer for generating the answer summaries.\r\n\r\n    *Downside:* we've tried this, and apart from meaning a lot of unnecessary bandwidth downloading full answers to the client, it also appears to be hard to get right. \r\n\r\n1. Pre-parse reference links in markdown on the server.\r\n\r\n    *Downside:* this will get horribly complicated and bug-prone if we try to do too much — we can probably get away with just replacing reference links.\r\n    \r\nWe've gone for option (3) for now at least. Reference links were the last *common* oddity showing up in the answer summaries, and now they work, along with all the genuine inline markdown. Performance and bandwidth aren't affected because we generate the amended markdown one-liner when answers are posted or amended, and only send that snippet to the browser where it needs to render answer summaries.	2020-01-30 22:56:47.822785+00	3	1	1	\N	0	0	0	\N	\N	This is an unfortunate consequence of a performance feature of markdown-it, which provides a `renderInline` function to do *almost* exactly what we want for answer summaries.	f	f
76	99	12	2019-11-25 10:12:07.953847+00	>*According to official Microsoft BOL DENSE_RANK is nondeterministic (RANK()). But according to Ranking Functions by Itzik Ben-Gan "... the RANK() and DENSE_RANK() functions are always deterministic". Who is right?*\r\n\r\nThey are both right, because they are using different senses of the word "deterministic".\r\n\r\nFrom the SQL Server optimizer's point of view, "deterministic" has a very precise meaning; a meaning that existed before window and ranking functions were added to the product. To the optimizer, the "deterministic" property defines whether a function can be freely duplicated within its internal tree structures during optimization. This is not legal for a non-deterministic function.\r\n\r\n*Deterministic* here means: the exact instance of the function always returns the same output for the same input, no matter how many times it is called. This is never true for windowing functions, by definition, because as a (single-row) scalar function, they do not return the same result within a row or across rows. To state it simply, using `ROW_NUMBER` as an example:\r\n\r\n> The `ROW_NUMBER` function returns different values for different rows (by definition!), so for *optimization purposes* it is nondeterministic\r\n\r\nThis is the sense BOL is using.\r\n\r\nItzik is making a different point about the determinism of the result as a whole. Over an ordered input set (with suitable tie-breaking) the output is a "deterministic" sequence. That is a valid observation, but it is not the "deterministic" quality that is important during query optimization. 	2019-11-25 10:12:07.953847+00	2	4	2	\N	0	0	0	\N	\N	>*According to official Microsoft BOL DENSE_RANK is nondeterministic (RANK()). But according to Ranking Functions by Itzik Ben-Gan "... the RANK() and DENSE_RANK() functions are always deterministic". Who is right?*	f	f
539	332	8	2018-06-06 11:33:08+00	What you refer to is the "tipping point" at which the SQL Server optimizer decides to go with a table scan instead of an index seek and key lookup.\r\n\r\nThere are some things to note about that, the tipping point obviously only affects non-clustered indexes (as the key lookup doesn't have to occur when using a clustered index) and the tipping point also doesn't come into play when your non-clustered index is covering (either all selected columns are in the key columns or in the included columns of the index).\r\n\r\nThat being said, the number of rows is *not* 30% of the total rows. It's not a fixed value.\r\n\r\nThe number of rows is somewhere between 25% and 33% percent of the number of pages, so unless you have 1 row per page the percentage of rows is much much smaller.\r\n\r\nSee the examples by Kimberly Tripp in [The Tipping Point Query Answers](https://www.sqlskills.com/blogs/kimberly/the-tipping-point-query-answers/)\r\n\r\n>  - If a table has 500,000 pages then 25% = 125,000 and 33% = 166,000. So, somewhere between 125,000 and 166,000 ROWS the query will tip.\r\n> Turning that into a percentage 125,000/1million = 12.5% and\r\n> 166,000/1million = 16.6%. So, if a table has 500,000 pages (and 1\r\n> million rows) then queries that return less than 12.5% of the data are\r\n> likely to USE the nonclustered index to lookup the data and queries\r\n> over 16.6% of the data are LIKELY to use a table scan.\r\n> \r\n>  - If a table has 10,000 pages then 25% = 2,500 and 33% = 3,333. So, somewhere between 2,500 and 3,333 ROWS the query will tip. Turning\r\n> that into a percentage 2,500/1million = .25% and 3,333/1million = .33%\r\n> (not even 1%). So, if a table has only 10,000 pages (and 1 million\r\n> rows) then queries that return less than a quarter of 1% of the data\r\n> are likely to USE the nonclustered index to lookup the data and\r\n> queries over one third of one percent are LIKELY to use a table scan. \r\n> \r\n>  - If a table has 50,000 pages then 25% = 12,500 and 33% = 16,666. So,\r\n> somewhere between 12,500 and 16,666 ROWS the query will tip. Turning\r\n> that into a percentage 12,500/1million = 1.25% and 16,666/1million =\r\n> 1.66% (under 2%). So, if a table has 50,000 pages (and 1 million rows) then queries that return less than 1.25% of the data are likely to USE\r\n> the nonclustered index to lookup the data and queries over 1.66% are\r\n> LIKELY to use a table scan.\r\n\r\nNow to answer your question, how selective should it be before an index is used?\r\n\r\nDepending on your row size and number of pages it could be *very* selective. If you want to make sure your index is used you want your index to be covering.	2019-12-25 09:34:34.233526+00	0	4	1	208888	0	0	0	2019-12-25 09:34:34.233526+00	\N	What you refer to is the "tipping point" at which the SQL Server optimizer decides to go with a table scan instead of an index seek and key lookup.	f	f
317	332	16	2018-06-06 14:23:13+00	Considering column selectivity only when deciding which columns to index ignores quite a bit of what indexes can do, and what they're generally good for.\r\n\r\nFor instance, you may have an identity or guid column that's incredibly selective -- unique, even -- but never gets used. In that case, who cares? Why index columns that queries don't touch?\r\n\r\nMuch less selective indexes, even `BIT` columns, can make useful, or useful parts of indexes. In some scenarios, very un-selective columns on large tables can benefit quite a bit from indexing when they need to be sorted on, or grouped by.\r\n\r\n**Joins**\r\n\r\nTake this query:\r\n\r\n    SELECT COUNT(*) AS records\r\n    FROM dbo.Users AS u\r\n    JOIN dbo.Posts AS p\r\n    ON u.Id = p.OwnerUserId;\r\n\r\nWithout a helpful index on `OwnerUserId`, this is our plan with a Hash Join -- which spills -- but that's secondary to the point.\r\n\r\n[![NUTS][1]][1]\r\n \r\nWith a helpful index -- `CREATE INDEX ix_yourmom ON dbo.Posts (OwnerUserId);` -- our plan changes.\r\n\r\n[![NUTS][2]][2]\r\n\r\n**Aggregates**\r\n\r\nLikewise, grouping operations can benefit from indexing.\r\n\r\n    SELECT   p.OwnerUserId, COUNT(*) AS records\r\n    FROM     dbo.Posts AS p\r\n    GROUP BY p.OwnerUserId;\r\n\r\nWithout an index:\r\n\r\n[![NUTS][3]][3]\r\n\r\nWith an index:\r\n\r\n[![NUTS][4]][4]\r\n\r\n**Sorts**\r\n\r\nSorting data can be another sticking point in queries that indexes can help.\r\n\r\nWithout an index:\r\n\r\n[![NUTS][5]][5]\r\n\r\nWith our index:\r\n\r\n[![NUTS][6]][6]\r\n\r\n**Blocking**\r\n\r\nIndexes can also help avoid blocking pile-ups. \r\n\r\nIf we try to run this update:\r\n\r\n    UPDATE p\r\n    SET p.Score += 100\r\n    FROM dbo.Posts AS p\r\n    WHERE p.OwnerUserId = 22656;\r\n\r\nAnd concurrently run this select:\r\n\r\n    SELECT *\r\n    FROM   dbo.Posts AS p\r\n    WHERE  p.OwnerUserId = 8;\r\n\r\nThey'll end up blocking:\r\n\r\n[![NUTS][7]][7]\r\n\r\nWith our index in place, the select finishes instantly without being blocked. SQL Server has a way to access the data it needs efficiently.\r\n\r\nIn case you're wondering (using the equation Kumar provided) the OwnerUserId column's selectivity is `0.0701539878296839478`\r\n\r\n**Wrap it up**\r\n\r\nDon't just blindly index columns based on how selective they are. Design indexes that help your workload run efficiently. Using more selective columns as leading key columns is generally a good idea when you're searching for equality predicates, but can be less helpful when searching on ranges. \r\n\r\n  [1]: https://i.stack.imgur.com/09wP6.jpg\r\n  [2]: https://i.stack.imgur.com/MrkF7.jpg\r\n  [3]: https://i.stack.imgur.com/Puo9c.jpg\r\n  [4]: https://i.stack.imgur.com/Hs9oA.jpg\r\n  [5]: https://i.stack.imgur.com/chlXI.jpg\r\n  [6]: https://i.stack.imgur.com/YvXT8.jpg\r\n  [7]: https://i.stack.imgur.com/VI6nw.jpg	2019-12-05 18:20:34.974193+00	3	4	1	208908	0	0	0	2019-12-05 18:20:34.974193+00	\N	Considering column selectivity only when deciding which columns to index ignores quite a bit of what indexes can do, and what they're generally good for.	f	f
127	187	45	2017-04-21 15:33:41+00	This definitely seems like unintended behavior. It is true that cardinality estimates do not need to be consistent at each step of a plan but this is a relatively simple query plan and the final cardinality estimate is inconsistent with what the query is doing. Such a low cardinality estimate could result in poor choices for join types and access methods for other tables downstream in a more complicated plan.\r\n\r\nThrough trial and error we can come up with a few similar queries for which the issue does not appear:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID <> 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tELSE (SELECT -1) \r\n      END AS ID2\r\n    FROM dbo.X_HEAP;\r\n    \r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID < 500 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tWHEN ID >= 500 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n      END AS ID2\r\n    FROM dbo.X_HEAP;\r\n    \r\nWe can also come up with more queries for which the issue appears:\r\n    \r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID < 500 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tWHEN ID >= 500 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n    \tELSE (SELECT TOP 1 ID FROM X_OTHER_TABLE) \r\n      END AS ID2\r\n    FROM dbo.X_HEAP;\r\n    \r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID = 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tELSE (SELECT -1) \r\n      END AS ID2\r\n    FROM dbo.X_HEAP;\r\n    \r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID = 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tELSE (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n      END AS ID2\r\n    FROM dbo.X_HEAP;\r\n\r\nThere appears to be a pattern: if there is an expression within the `CASE` that is not expected to be executed and the result expression is a subquery against a table then the row estimate falls to 1 after that expression.\r\n\r\nIf I write the query against a table with a clustered index the rules change somewhat. We can use the same data:\r\n\r\n    CREATE TABLE dbo.X_CI (ID INT NOT NULL, PRIMARY KEY (ID))\r\n    \r\n    INSERT INTO dbo.X_CI WITH (TABLOCK)\r\n    SELECT * FROM dbo.X_HEAP;\r\n    \r\n    UPDATE STATISTICS X_CI WITH FULLSCAN;\r\n\r\nThis query has a 1000 row final estimate:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID = 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n    \tELSE (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n      END\r\n    FROM dbo.X_CI;\r\n\r\nBut this query has a 1 row final estimate:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID <> 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tELSE (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n      END\r\n    FROM dbo.X_CI;\r\n\r\n\r\nTo dig into this further we can use the undocumented [trace flag 2363][1] to get information about how the query optimizer performed selectivity calculations. I found it helpful to pair that trace flag with the undocumented [trace flag 8606][2]. TF 2363 seems to give selectivity computations for both the simplified tree and the tree after project normalization. Having both trace flags enabled makes it clear which calculations apply to which tree.\r\n\r\nLet's try it for the original query posted in the question:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID <> 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM X_OTHER_TABLE) \r\n    \tELSE (SELECT TOP 1 ID FROM X_OTHER_TABLE_2) \r\n      END AS ID2\r\n    FROM X_HEAP\r\n    OPTION (QUERYTRACEON 3604, QUERYTRACEON 2363, QUERYTRACEON 8606);\r\n\r\nHere is part of the part of output which I think is relevant along with some comments:\r\n\r\n    Plan for computation:\r\n    \r\n      CSelCalcColumnInInterval -- this is the type of calculator used\r\n    \r\n          Column: QCOL: [SE_DB].[dbo].[X_HEAP].ID -- this is the column used for the calculation\r\n    \r\n    Pass-through selectivity: 0 -- all rows are expected to have a true value for the case expression\r\n    \r\n    Stats collection generated: \r\n    \r\n      CStCollOuterJoin(ID=8, CARD=1000 x_jtLeftOuter) -- the row estimate after the join will still be 1000\r\n    \r\n          CStCollBaseTable(ID=1, CARD=1000 TBL: X_HEAP)\r\n    \r\n          CStCollBaseTable(ID=2, CARD=1 TBL: X_OTHER_TABLE)\r\n    \r\n    ...\r\n    \r\n    Plan for computation:\r\n    \r\n      CSelCalcColumnInInterval\r\n    \r\n          Column: QCOL: [SE_DB].[dbo].[X_HEAP].ID\r\n    \r\n    Pass-through selectivity: 1 -- no rows are expected to have a true value for the case expression\r\n    \r\n    Stats collection generated: \r\n    \r\n      CStCollOuterJoin(ID=9, CARD=1 x_jtLeftOuter) -- the row estimate after the join will still be 1\r\n    \r\n          CStCollOuterJoin(ID=8, CARD=1000 x_jtLeftOuter) -- here is the row estimate after the previous join\r\n    \r\n              CStCollBaseTable(ID=1, CARD=1000 TBL: X_HEAP)\r\n    \r\n              CStCollBaseTable(ID=2, CARD=1 TBL: X_OTHER_TABLE)\r\n    \r\n          CStCollBaseTable(ID=3, CARD=1 TBL: X_OTHER_TABLE_2)\r\n\r\nNow let's try it for a similar query that doesn't have the issue. I'm going to use this one:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID <> 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tELSE (SELECT -1) \r\n      END AS ID2\r\n    FROM dbo.X_HEAP\r\n    OPTION (QUERYTRACEON 3604, QUERYTRACEON 2363, QUERYTRACEON 8606);\r\n\r\n\r\nDebug output at the very end:\r\n\r\n    Plan for computation:\r\n    \r\n      CSelCalcColumnInInterval\r\n    \r\n          Column: QCOL: [SE_DB].[dbo].[X_HEAP].ID\r\n    \r\n    Pass-through selectivity: 1\r\n    \r\n    Stats collection generated: \r\n    \r\n      CStCollOuterJoin(ID=9, CARD=1000 x_jtLeftOuter)\r\n    \r\n          CStCollOuterJoin(ID=8, CARD=1000 x_jtLeftOuter)\r\n    \r\n              CStCollBaseTable(ID=1, CARD=1000 TBL: dbo.X_HEAP)\r\n    \r\n              CStCollBaseTable(ID=2, CARD=1 TBL: dbo.X_OTHER_TABLE)\r\n    \r\n          CStCollConstTable(ID=4, CARD=1) -- this is different than before because we select a constant instead of from a table\r\n\r\n\r\nLet's try another query for which the bad row estimate is present:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID < 500 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tWHEN ID >= 500 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n    \tELSE (SELECT TOP 1 ID FROM X_OTHER_TABLE) \r\n      END AS ID2\r\n    FROM dbo.X_HEAP\r\n    OPTION (QUERYTRACEON 3604, QUERYTRACEON 2363, QUERYTRACEON 8606);\r\n\r\nAt the very end the cardinality estimate drops to 1 row, again after Pass-through selectivity = 1. The cardinality estimate is preserved after a selectivity of 0.501 and 0.499.\r\n\r\n\r\n    Plan for computation:\r\n    \r\n     CSelCalcColumnInInterval\r\n    \r\n          Column: QCOL: [SE_DB].[dbo].[X_HEAP].ID\r\n    \r\n    Pass-through selectivity: 0.501\r\n    \r\n    ...\r\n    \r\n    Plan for computation:\r\n    \r\n      CSelCalcColumnInInterval\r\n    \r\n          Column: QCOL: [SE_DB].[dbo].[X_HEAP].ID\r\n    \r\n    Pass-through selectivity: 0.499\r\n    \r\n    ...\r\n    \r\n    Plan for computation:\r\n    \r\n      CSelCalcColumnInInterval\r\n    \r\n          Column: QCOL: [SE_DB].[dbo].[X_HEAP].ID\r\n    \r\n    Pass-through selectivity: 1\r\n    \r\n    Stats collection generated: \r\n    \r\n      CStCollOuterJoin(ID=12, CARD=1 x_jtLeftOuter) -- this is associated with the ELSE expression\r\n    \r\n          CStCollOuterJoin(ID=11, CARD=1000 x_jtLeftOuter)\r\n    \r\n              CStCollOuterJoin(ID=10, CARD=1000 x_jtLeftOuter)\r\n    \r\n                  CStCollBaseTable(ID=1, CARD=1000 TBL: dbo.X_HEAP)\r\n    \r\n                  CStCollBaseTable(ID=2, CARD=1 TBL: dbo.X_OTHER_TABLE)\r\n    \r\n              CStCollBaseTable(ID=3, CARD=1 TBL: dbo.X_OTHER_TABLE_2)\r\n    \r\n          CStCollBaseTable(ID=4, CARD=1 TBL: X_OTHER_TABLE)\r\n\r\n\r\nLet's again switch to another similiar query that does not have the issue. I'm going to use this one:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID < 500 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tWHEN ID >= 500 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n      END AS ID2\r\n    FROM dbo.X_HEAP\r\n    OPTION (QUERYTRACEON 3604, QUERYTRACEON 2363, QUERYTRACEON 8606);\r\n\r\nIn the debug output there is never a step which has a pass-through selectivity of 1. The cardinality estimate stays at 1000 rows.\r\n\r\n    Plan for computation:\r\n    \r\n      CSelCalcColumnInInterval\r\n    \r\n          Column: QCOL: [SE_DB].[dbo].[X_HEAP].ID\r\n    \r\n    Pass-through selectivity: 0.499\r\n    \r\n    Stats collection generated: \r\n    \r\n      CStCollOuterJoin(ID=9, CARD=1000 x_jtLeftOuter)\r\n    \r\n          CStCollOuterJoin(ID=8, CARD=1000 x_jtLeftOuter)\r\n    \r\n              CStCollBaseTable(ID=1, CARD=1000 TBL: dbo.X_HEAP)\r\n    \r\n              CStCollBaseTable(ID=2, CARD=1 TBL: dbo.X_OTHER_TABLE)\r\n    \r\n          CStCollBaseTable(ID=3, CARD=1 TBL: dbo.X_OTHER_TABLE_2)\r\n    \r\n    End selectivity computation\r\n\r\n\r\nWhat about the query when it involves a table with a clustered index? Consider the following query with the row estimate issue:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID <> 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n    \tELSE (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n      END\r\n    FROM dbo.X_CI\r\n    OPTION (QUERYTRACEON 3604, QUERYTRACEON 2363, QUERYTRACEON 8606);\r\n\r\nThe end of the debug output is similar to what we already have seen:\r\n\r\n\r\n    Plan for computation:\r\n    \r\n      CSelCalcColumnInInterval\r\n    \r\n          Column: QCOL: [SE_DB].[dbo].[X_CI].ID\r\n    \r\n    Pass-through selectivity: 1\r\n    \r\n    Stats collection generated: \r\n    \r\n      CStCollOuterJoin(ID=9, CARD=1 x_jtLeftOuter)\r\n    \r\n          CStCollOuterJoin(ID=8, CARD=1000 x_jtLeftOuter)\r\n    \r\n              CStCollBaseTable(ID=1, CARD=1000 TBL: dbo.X_CI)\r\n    \r\n              CStCollBaseTable(ID=2, CARD=1 TBL: dbo.X_OTHER_TABLE)\r\n    \r\n          CStCollBaseTable(ID=3, CARD=1 TBL: dbo.X_OTHER_TABLE_2)\r\n\r\nHowever, the query against the CI without the issue has different output. Using this query:\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID = 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE_2) \r\n    \tELSE (SELECT TOP 1 ID FROM dbo.X_OTHER_TABLE) \r\n      END\r\n    FROM dbo.X_CI\r\n    OPTION (QUERYTRACEON 3604, QUERYTRACEON 2363, QUERYTRACEON 8606);\r\n\r\nResults in different calculators being used. `CSelCalcColumnInInterval` no longer appears:\r\n\r\n\r\n    Plan for computation:\r\n    \r\n      CSelCalcFixedFilter (0.559)\r\n    \r\n    Pass-through selectivity: 0.559\r\n    \r\n    Stats collection generated: \r\n    \r\n      CStCollOuterJoin(ID=8, CARD=1000 x_jtLeftOuter)\r\n    \r\n          CStCollBaseTable(ID=1, CARD=1000 TBL: dbo.X_CI)\r\n    \r\n          CStCollBaseTable(ID=2, CARD=1 TBL: dbo.X_OTHER_TABLE_2)\r\n    \r\n    ...\r\n    \r\n    Plan for computation:\r\n    \r\n      CSelCalcUniqueKeyFilter\r\n    \r\n    Pass-through selectivity: 0.001\r\n    \r\n    Stats collection generated: \r\n    \r\n      CStCollOuterJoin(ID=9, CARD=1000 x_jtLeftOuter)\r\n    \r\n          CStCollOuterJoin(ID=8, CARD=1000 x_jtLeftOuter)\r\n    \r\n              CStCollBaseTable(ID=1, CARD=1000 TBL: dbo.X_CI)\r\n    \r\n              CStCollBaseTable(ID=2, CARD=1 TBL: dbo.X_OTHER_TABLE_2)\r\n    \r\n          CStCollBaseTable(ID=3, CARD=1 TBL: dbo.X_OTHER_TABLE)\r\n\r\n\r\nIn conclusion, we appear to get a bad row estimate after the subquery under the following conditions:\r\n\r\n 1. The `CSelCalcColumnInInterval` selectivity calculator is used. I don't know exactly when this is used but it seems to show up much more often when the base table is a heap.\r\n\r\n 2. Pass-through selectivity = 1. In other words, one of the `CASE` expressions is expected to evaluated to false for all of the rows. It does not matter if the first `CASE` expression evaluates to true for all rows.\r\n\r\n 3. There is an outer join to `CStCollBaseTable`. In other words, the `CASE` result expression is a subquery against a table. A constant value will not work.\r\n\r\nPerhaps under those conditions the query optimizer is unintentionally applying the pass-through selectivity to the row estimate of the outer table instead of to the work done on the inner part of the nested loop. That would reduce the row estimate to 1.\r\n\r\nI was able to find two workarounds. I was not able to reproduce the issue when using `APPLY` instead of a subquery. The output of trace flag 2363 was very different with `APPLY`. Here's one way to rewrite the original query in the question:\r\n\r\n    SELECT \r\n      h.ID\r\n    , a.ID2\r\n    FROM X_HEAP h\r\n    OUTER APPLY\r\n    (\r\n    SELECT CASE\r\n    \tWHEN ID <> 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM X_OTHER_TABLE) \r\n    \tELSE (SELECT TOP 1 ID FROM X_OTHER_TABLE_2) \r\n      END\r\n    ) a(ID2);\r\n\r\n[![good query 1][3]][3]\r\n\r\nThe legacy CE appears to avoid the issue as well.\r\n\r\n    SELECT \r\n      ID\r\n    , CASE\r\n    \tWHEN ID <> 0 \r\n    \tTHEN (SELECT TOP 1 ID FROM X_OTHER_TABLE) \r\n    \tELSE (SELECT TOP 1 ID FROM X_OTHER_TABLE_2) \r\n      END AS ID2\r\n    FROM X_HEAP\r\n    OPTION (USE HINT('FORCE_LEGACY_CARDINALITY_ESTIMATION'));\r\n\r\n[![good query 2][4]][4]\r\n\r\nA [connect item][5] was submitted for this issue (with some of the details that Paul White provided in his answer).\r\n\r\n  [1]: https://sqlperformance.com/2014/01/sql-plan/cardinality-estimation-for-multiple-predicates\r\n  [2]: http://sqlblog.com/blogs/paul_white/archive/2012/05/01/query-optimizer-deep-dive-part-4.aspx\r\n  [3]: https://i.stack.imgur.com/DfAN1.png\r\n  [4]: https://i.stack.imgur.com/5eGyF.png\r\n  [5]: https://connect.microsoft.com/SQLServer/feedback/details/3133394/subquery-in-select-reduces-row-estimate-to-1	2019-11-28 14:42:16.481602+00	2	4	1	171655	0	0	0	2019-11-28 14:42:16.481602+00	\N	This definitely seems like unintended behavior. It is true that cardinality estimates do not need to be consistent at each step of a plan but this is a relatively simple query plan and the final cardinality estimate is inconsistent with what the query is doing. Such a low cardinality estimate could result in poor choices for join types and access methods for other tables downstream in a more complicated plan.	f	f
128	187	12	2017-04-23 16:55:09+00	## Summary\r\n\r\nThis cardinality estimation (CE) issue surfaces when:\r\n\r\n1. The join is an **outer** join with a **pass-through** predicate\r\n2. The **selectivity** of the pass-through predicate is estimated to be **exactly 1**.\r\n\r\nNote: The particular calculator used to determine the selectivity is not important.\r\n\r\n---\r\n\r\n## Details\r\n\r\nThe CE computes the selectivity of the outer join as the **sum** of:\r\n\r\n* The **inner join** selectivity with the same predicate\r\n* The **anti join** selectivity with the same predicate\r\n\r\nThe only difference between an outer and inner join is that an outer join also returns rows that do not match on the join predicate. The anti join provides exactly this difference. Cardinality estimation for inner and anti join is easier than for outer join directly.\r\n\r\nThe join selectivity estimation process is very straightforward:\r\n\r\n* First, the selectivity **S~PT~** of the pass-through predicate is assessed.  \r\n - This is done using whichever calculator is appropriate to the circumstances.  \r\n - The predicate is the whole thing, including any negating `IsFalseOrNull` component.\r\n* Inner join selectivity := **1 - S~PT~**\r\n* Anti join selectivity := **S~PT~**\r\n\r\nThe anti join represents rows that will 'pass through' the join. The inner join represents rows that will not 'pass through'. Note that 'pass through' means rows that flow through the join without running the inner side at all. To emphasise: all rows will be returned by the join, the distinction is between rows that run the inner side of the join before emerging, and those that do not.\r\n\r\nClearly, adding **1 - S~PT~** to **S~PT~** should always give a total selectivity of 1, meaning all rows are returned by the join, as expected.\r\n\r\nThe above calculation works exactly as described for all values of S~PT~ **except 1**.\r\n\r\nWhen **S~PT~ = 1**, both inner join and anti join selectivities are **estimated to be zero**, resulting in a cardinality estimate (for the join as a whole) of one row. As far as I can tell, this is unintentional, and should be reported as a bug.\r\n\r\n---\r\n\r\n### A related issue\r\n\r\nThis bug is more likely to manifest than one might think, due to a separate CE limitation. This arises when the `CASE` expression uses an `EXISTS` clause (as is common). For example the following modified query from the question does **not** encounter the unexpected cardinality estimate:\r\n\r\n    -- This is fine\r\n    SELECT \r\n        CASE\r\n            WHEN XH.ID = 1\r\n            THEN (SELECT TOP (1) XOT.ID FROM dbo.X_OTHER_TABLE AS XOT) \r\n        END\r\n    FROM dbo.X_HEAP AS XH;\r\n\r\nIntroducing a trivial `EXISTS` does cause the issue to surface:\r\n\r\n    -- This is not fine\r\n    SELECT \r\n        CASE\r\n            WHEN EXISTS (SELECT 1 WHERE XH.ID = 1)\r\n            THEN (SELECT TOP (1) XOT.ID FROM dbo.X_OTHER_TABLE AS XOT) \r\n        END\r\n    FROM dbo.X_HEAP AS XH;\r\n\r\nUsing `EXISTS` introduces a semi join (highlighted) to the execution plan:\r\n\r\n[![Semi join plan][1]][1]\r\n\r\nThe estimate for the semi join is fine. The problem is that the CE treats the associated probe column as a simple projection, with a **fixed selectivity of 1**:\r\n\r\n```none\r\nSemijoin with probe column treated as a Project.\r\n\r\nSelectivity of probe column = 1\r\n```\r\n\r\nThis automatically meets one of the conditions required for this CE issue to manifest, regardless of the contents of the `EXISTS` clause.\r\n\r\n---\r\n\r\nFor important background information, see [Subqueries in `CASE` Expressions][2] by Craig Freedman.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/0VvC3.png\r\n  [2]: https://blogs.msdn.microsoft.com/craigfr/2006/08/23/subqueries-in-case-expressions/	2019-11-28 14:46:55.485535+00	2	4	1	171767	0	0	0	2019-11-28 14:42:16.736882+00	\N	Summary	f	f
150	208	12	2018-05-05 10:39:20+00	>He says that in a parallel plan, every operator gets DOP threads.\r\n\r\n**No. This is at best misleading, but closer to being simply wrong.**\r\n\r\nIn a serial plan, every operator 'gets' one thread, but that thread is the *same thread* for all operators. The principle is similar for parallel plans.\r\n\r\nEach parallel operator is *run by* `DOP` threads, but those threads are *not exclusive* to a particular operator, they are *shared* among operators within the same **parallel branch**.\r\n\r\nBranch boundaries are delimited by Parallelism operators (Demand, Repartition, and Gather Streams). The diagram below shows a parallel plan with three branches:\r\n\r\n[![enter image description here][1]][1]  \r\n*Reproduced from the article referenced at the end of this answer*\r\n\r\n---\r\n\r\n>So if you have `MAXDOP 8` and your query has 4 parallel operators it'll use 32 threads at once.\r\n\r\n**No**. You can't just multiply `DOP` by the number of operators to get the number of threads. The number of threads reserved for parallel branches is the *number of parallel branches* (not operators) multiplied by `DOP`.\r\n\r\nThe number of threads that can be active at the same time for a single parallel query is limited to `DOP` in [SQL Server 2005 and later][2]. SQL Server achieves this by allocating threads to `DOP` schedulers.\r\n\r\n---\r\n\r\n>I also read that it might just be 8 for the whole query, which seems like too few.\r\n\r\nAt `DOP = 8` this would be correct for a plan with a single parallel branch. There can be multiple parallel plan branches in a parallel plan. For a plan with `n` parallel branches, the thread reservation for parallel workers is `n * DOP`.\r\n\r\nSee [Parallel Execution Plans – Branches and Threads](https://sqlperformance.com/2013/10/sql-plan/parallel-plans-branches-threads) by Paul White.\r\n\r\nNote: The number of branches reported in execution plans is normally the number of branches that may possibly execute concurrently (due to blocking operators, threads in one branch may sometimes be safely recycled for a later branch).\r\n\r\n  [1]: https://i.stack.imgur.com/05MZo.png\r\n  [2]: https://dba.stackexchange.com/q/205643/150519	2019-11-30 15:30:53.446699+00	3	4	1	205920	0	0	0	2019-11-30 12:58:33.687532+00	\N	>He says that in a parallel plan, every operator gets DOP threads.	f	f
732	651	769	2020-02-01 17:43:57.869218+00	You are correct.\r\n\r\nThe CASE conditional expression can evaluate to either 1 or 0, never to a NULL.\r\n\r\nThe encapsulating ISNULL function will always see a concrete value, `1` or `0`, and so will never return `-1`.\r\n\r\nIf you explain what you are trying to achieve with this code that doesn't seem to work for you, we may be able to offer some advice.	2020-02-02 14:28:19.170973+00	2	4	1	\N	0	0	0	\N	\N	You are correct.	f	f
765	678	45	2020-02-07 01:31:34.574322+00	For full disclosure, I'm lazy and leave writing safe locking code under concurrency to the professionals. So perhaps this is a Non-Answer but I'm writing it up anyway because you're a good guy and I want to help you.\r\n\r\nI think that you currently have a query performance problem instead of a concurrency problem. The query that you use to find the next work to do runs longer as time passes. Rows with non-NULL values for EndDT slow down query performance. For clarity, I mean this query:\r\n\r\n    WITH dequeue AS\r\n    (\r\n    \tSELECT TOP 1 WITH TIES\r\n    \t\tMain, Sub, Thread\r\n    \tFROM\tmyQueue\r\n    \tWHERE\tEndDT IS NULL\r\n    \t\tAND\t(Thread IS NULL OR Thread = @CurrentThread)\r\n    \tORDER BY Main, Sub\r\n    )\r\n    UPDATE\tdequeue\r\n    SET\tThread = @CurrentThread\r\n    OUTPUT\tDELETED.Main,\r\n    \t\tDELETED.Sub\r\n    INTO @updOUT;\t\r\n\t\t\t\r\nThe graph below measures the approximate amount of execution time in microseconds for each 100 batches of application lock acquires and releases:\r\n\r\n![Capture2.PNG](/image?hash=726b4815932fbaae34063d6704a4bace53cd9f18db8fbc6177e247b148fea0d4)\r\n\r\nThat's why you see so much lock waiting time. The other sessions wait a while to get the lock because the query that updates the table gets slower over time. The query gets slower over time because the nonclustered scan reads over many completed rows. In the example below, we had to scan past 30k rows just to get 3 relevant rows:\r\n\r\n![Capture3.PNG](/image?hash=0f6a15eac96ed99a212f0335dad9db9082af4c9de05e2c2944e39f46fb27d96f)\r\n\r\nThe most important change you can make is to make your dequeue code run in constant time regardless of the size of the myQueue table. One possible tweak is to change your index:\r\n\r\n    CREATE NONCLUSTERED INDEX [Joe_index]\r\n    ON [dbo].[myQueue] ([Main],[Sub])\r\n    INCLUDE (Thread, EndDT)\r\n    WHERE\tEndDT IS NULL\r\n\r\nOn My Machine with the new index, it took 6 minutes for five concurrent threads to process 100k total rows. With the old index, it took 27 minutes. I was watching k-pop videos throughout my tests so that may have added error to the results. I'm not saying that the filtered index is what you should do or even a good idea. What I am saying is that you should make some change to your table, indexing, or querying to get constant time results.\r\n\r\nI suggest trying to figure out how many transactions per second you need to hit with this code. For example, if you know that you need to be able to dequeue 1000 groups per second then you know that your dequeue code needs to run in under 1 ms. If it can't run in 1 ms then you may need to stop using `sp_getapplock` and switch to a more complex approach. If it can run in under 1 ms then you might be able to get by with `sp_getapplock`.\r\n\r\nGood luck!	2020-02-07 01:31:34.574322+00	14	4	1	\N	0	0	0	\N	\N	For full disclosure, I'm lazy and leave writing safe locking code under concurrency to the professionals. So perhaps this is a Non-Answer but I'm writing it up anyway because you're a good guy and I want to help you.	f	f
180	237	131	2019-12-01 22:45:27.063402+00	I'm not sure whether or not this qualifies as an answer, but in the name of this is Meta and it addresses the issue raised in the question, I would suggest **renaming** the "account recovery token" to something like "login key", and then correspondingly change the text on the "link" button to "log in" or something similar.\r\n\r\nThat would require minimal changes, but still present a more conventional-looking login experience and make it clearer what that GUID is actually used for in practice.\r\n\r\nI do imagine that people would still complain that a GUID is harder to remember than `dolphins` or `password123`, though.\r\n\r\n\r\n---\r\nupdate by Jack:\r\n\r\nwe've implemented all the very sensible wording changes suggested in this post while the wider questions are discussed.	2019-12-01 23:54:23.937628+00	4	4	1	\N	0	0	0	\N	\N	I'm not sure whether or not this qualifies as an answer, but in the name of this is Meta and it addresses the issue raised in the question, I would suggest **renaming** the "account recovery token" to something like "login key", and then correspondingly change the text on the "link" button to "log in" or something similar.	f	f
178	237	2	2019-12-01 21:46:58.035547+00	> It's probably not the best thing to use a recovery token every time I want to log in, either.\r\n\r\nFor your special use case, I think using the recovery token as a password is really the most sensible solution.\r\n\r\nIf someone like yourself has security reasons for using incognito sessions or clearing cookies, they probably also don't want to use ['dolphins'](https://github.com/danielmiessler/SecLists/pull/155) as a password. Long unmemorable strings of random chars are probably more what you are used to. You presumably already have a way of securing your ever growing list of unique-per-site passwords and you can just add this to the list :)\r\n\r\nPractically it just means hitting 'link' every time you want to log in and pasting in your recovery token [from the Windows clipboard ring](/transcript?room=2&id=5660#c5660) or whatever ;)\r\n\r\nIt's also possible to [use a userscript](https://topanswers.xyz/meta?q=464#a502) to reinject your device cookie automatically — but I don't know if that would work for you in incognito?	2019-12-24 20:31:02.254779+00	3	1	1	\N	0	0	0	\N	\N	> It's probably not the best thing to use a recovery token every time I want to log in, either.	f	f
210	261	12	2019-07-04 15:33:05+00	The type of the result is determined by the rules set out in [Precision, scale, and Length (Transact-SQL)][1]:\r\n\r\n>The following table defines how the precision and scale of the result are calculated when the result of an operation is of type **decimal**. The result is **decimal** when either:\r\n>\r\n>* Both expressions are **decimal**.\r\n>* One expression is **decimal** and the other is a data type with a lower precedence than **decimal**.\r\n>\r\n>The operand expressions are denoted as expression e1, with precision p1 and scale s1, and expression e2, with precision p2 and scale s2. The precision and scale for any expression that is not **decimal** is the precision and scale defined for the data type of the expression. The function max(a,b) means the following: take the greater value of "a" or "b". Similarly, min(a,b) indicates to take the smaller value of "a" or "b".\r\n>\r\n>[![table][2]][2]\r\n>\r\n>\\* The result precision and scale have an absolute maximum of 38. When a result precision is greater than 38, it's reduced to 38, and the corresponding scale is reduced to try to prevent truncating the integral part of a result. In some cases such as multiplication or division, scale factor won't be reduced, to maintain decimal precision, although the overflow error can be raised.\r\n>\r\n>In addition and subtraction operations, we need `max(p1 - s1, p2 - s2)` places to store integral part of the decimal number. If there isn't enough space to store them that is, `max(p1 - s1, p2 - s2) < min(38, precision) - scale`, the scale is reduced to provide enough space for integral part. Resulting scale is `MIN(precision, 38) - max(p1 - s1, p2 - s2)`, so the fractional part might be rounded to fit into the resulting scale.\r\n\r\nA convenient quick way to see the resulting type is to use [`SQL_VARIANT_PROPERTY`][3]:\r\n\r\n```\r\nDECLARE @V1 decimal (9, 6) = 123.456789;\r\nDECLARE @V2 decimal (7, 2) = 12345.67;\r\n\r\nSELECT \r\n    [BaseType] = SQL_VARIANT_PROPERTY(@V1 + @V2, 'BaseType'),\r\n    [Precision] = SQL_VARIANT_PROPERTY(@V1 + @V2, 'Precision'),\r\n    [Scale] = SQL_VARIANT_PROPERTY(@V1 + @V2, 'Scale');\r\n```\r\n\r\nOutput:\r\n\r\n```none\r\n╔══════════╦═══════════╦═══════╗\r\n║ BaseType ║ Precision ║ Scale ║\r\n╠══════════╬═══════════╬═══════╣\r\n║ decimal  ║        12 ║     6 ║\r\n╚══════════╩═══════════╩═══════╝\r\n```\r\n\r\nDemo:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=94c5a290b7b882253e713f8752771cde\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/data-types/precision-scale-and-length-transact-sql\r\n  [2]: https://i.stack.imgur.com/YKdYG.png\r\n  [3]: https://docs.microsoft.com/en-us/sql/t-sql/functions/sql-variant-property-transact-sql\r\n  [4]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=94c5a290b7b882253e713f8752771cde	2019-12-04 03:33:56.950195+00	2	4	1	242103	0	0	0	2019-12-04 03:31:46.709077+00	\N	The type of the result is determined by the rules set out in [Precision, scale, and Length (Transact-SQL)][1]:	f	f
243	281	16	2017-08-03 14:54:47+00	Full text indexes generally aren't a magic bullet, and require additional maintenance, disk space, and fairly intrusive changes to query patterns. \r\n\r\nUnless you're truly in need of indexing large documents (think email bodies, PDFs, Word docs, etc.), they're overkill (and if we're being honest, I'd take that process out of SQL Server entirely and use Elasticsearch or something similar).\r\n\r\nFor smaller use-cases, computed columns are generally a better approach.\r\n\r\nHere's a quick demo setup:\r\n\r\n    use tempdb\r\n    \r\n    CREATE TABLE #fulltextindexesarestupid (Id INT PRIMARY KEY CLUSTERED, StopAbusingFeatures VARCHAR(100))\r\n    \r\n    INSERT #fulltextindexesarestupid (Id)\r\n    SELECT TOP 1000000 ROW_NUMBER() OVER (ORDER BY (@@ROWCOUNT))\r\n    FROM sys.messages AS m\r\n    CROSS JOIN sys.messages AS m2\r\n    \r\n    UPDATE #fulltextindexesarestupid\r\n    SET StopAbusingFeatures = CASE WHEN Id % 15 = 0 THEN 'Bad'\r\n    \t\t\t\t\t\t\t   WHEN Id % 3 = 0 THEN 'Idea'\r\n    \t\t\t\t\t\t\t   WHEN Id % 5 = 0 THEN 'Jeans'\r\n    \t\t\t\t\t\t\t   END\r\n    \r\n    \r\n    ALTER TABLE #fulltextindexesarestupid \r\n    ADD LessBad AS CONVERT(BIT, CASE WHEN StopAbusingFeatures LIKE '%Bad%' THEN 1\r\n    \t\t\t\t\tWHEN StopAbusingFeatures LIKE '%Idea%' THEN 1\r\n    \t\t\t\t\tELSE 0 END)\r\n        \r\n    CREATE UNIQUE NONCLUSTERED INDEX ix_whatever ON #fulltextindexesarestupid (LessBad, Id)\r\n\r\nQuerying based on even a non-persisted column gives us a plan that 'uses indexes' and everything :)\r\n    \r\n    SELECT COUNT(*)\r\n    FROM #fulltextindexesarestupid AS f\r\n    WHERE LessBad = 1\r\n\r\n[![NUTS][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/BJwhp.jpg	2019-12-04 14:25:07.18323+00	2	4	1	182609	0	0	0	2019-12-04 14:25:07.18323+00	\N	Full text indexes generally aren't a magic bullet, and require additional maintenance, disk space, and fairly intrusive changes to query patterns.	f	f
198	252	12	2019-03-22 06:42:40+00	The [documentation](https://docs.microsoft.com/en-us/sql/t-sql/statements/set-nocount-transact-sql) for `SET NOCOUNT` says:\r\n\r\n>`SET NOCOUNT ON` prevents the sending of `DONE_IN_PROC` messages to the client for each statement in a stored procedure. For stored procedures that contain several statements that do not return much actual data, or for procedures that contain Transact-SQL loops, setting `SET NOCOUNT` to `ON` can provide a significant performance boost, because network traffic is greatly reduced.\r\n\r\nYou are not running the statements in a stored procedure, so SQL Server sends [`DONE` tokens][2] (code `0xFD`) to indicate the completion status of each SQL statement. These messages are deferred, and sent asynchronously when the network packet is full. When the client does not consume network packets quickly enough, eventually the buffers fill up, and the operation becomes blocking for SQL Server, generating the `ASYNC_NETWORK_IO` waits.\r\n\r\nNote the `DONE` tokens are different from [`DONEINPROC`][3] (code `0xFF`) as the documentation notes:\r\n\r\n>* A `DONE` token is returned for each SQL statement in the SQL batch except variable declarations.\r\n\r\n>* For execution of SQL statements within stored procedures, `DONEPROC` and `DONEINPROC` tokens are used in place of `DONE` tokens.\r\n\r\nYou will see a dramatic reduction in `ASYNC_NETWORK_IO` waits using:\r\n\r\n    CREATE PROCEDURE #P AS\r\n    SET NOCOUNT ON;\r\n    \r\n    DECLARE\r\n        @outer_loop integer = 0,\r\n        @big_string_for_u varchar(8000);\r\n    \r\n    \r\n    WHILE @outer_loop < 5000000\r\n    BEGIN\r\n        SET @big_string_for_u = 'ZZZZZZZZZZ';\r\n        SET @outer_loop = @outer_loop + 1;\r\n    END;\r\n    GO\r\n    EXECUTE dbo.#P;\r\n\r\nYou could also use [`sys.sp_executesql`][4] to achieve the same result.\r\n\r\nExample stack trace captured just as an `ASYNC_NETWORK_IO` wait begins:\r\n\r\n[![sending a packet][5]][5]\r\n\r\nAn example TDS packet as seen in the inline function `sqllang!srv_completioncode_ex<1>` had the following 13 bytes:\r\n\r\n```none\r\nfd 01 00 c1 00 01 00 00 00 00 00 00 00          \r\n```\r\n\r\nWhich decodes to:\r\n\r\n* TokenType = 0xfd `DONE_TOKEN`\r\n* Status = 0x0001 `DONE_MORE`\r\n* CurCmd = 0x00c1 (193)\r\n* DoneRowCount = 0x00000001 (1)\r\n\r\nUltimately, the number of `ASYNC_NETWORK_IO` waits depends on the client and driver, and what it does, if anything, with all the `DONE` messages. Testing with a loop 1/10th of the size given in the question (5,000,000 loop iterations) I found SSMS ran for about 4 seconds with 200-300 ms of waits. `sqlcmd` ran for 2-3 seconds with single digit ms waits; `osql` around the same run time with around 10 ms of waits.\r\n\r\nThe worst client by far for this test was Azure Data Studio. It ran for almost 6 hours:\r\n\r\n[![ADS][6]][6]\r\n\r\n  [2]: https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-tds/3c06f110-98bd-4d5b-b836-b1ba66452cb7\r\n  [3]: https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-tds/43e891c5-f7a1-432f-8f9f-233c4cd96afb\r\n  [4]: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql\r\n  [5]: https://i.stack.imgur.com/2HerO.png\r\n  [6]: https://i.stack.imgur.com/pf9sX.png	2019-12-03 13:05:50.906403+00	3	4	1	232822	0	0	0	2019-12-03 12:38:28.450365+00	\N	The [documentation](https://docs.microsoft.com/en-us/sql/t-sql/statements/set-nocount-transact-sql) for `SET NOCOUNT` says:	f	f
159	222	12	2018-07-03 09:00:56+00	The full cost derivation logic is complex, but for the relatively simple case in the question:\r\n\r\n### Inputs\r\n\r\n1. The number of times the operator is executed  \r\n This is the *Estimated Number of Executions*: **504**\r\n\r\n0. The cardinality (total number of rows) in the index  \r\n The *TableCardinality* property of the *Index Seek* operator gives this: **113,443**\r\n0. The number of data pages in the index: **201**  \r\n This number can be obtained multiple ways, for example from `sys.allocation_units`:\r\n\r\n        SELECT \r\n            AU.data_pages\r\n        FROM sys.allocation_units AS AU\r\n        JOIN sys.partitions AS P\r\n            ON P.hobt_id = AU.container_id\r\n        WHERE\r\n            AU.[type_desc] = N'IN_ROW_DATA'\r\n            AND P.[object_id] = OBJECT_ID(N'Production.TransactionHistory', N'U')\r\n            AND P.index_id = \r\n                INDEXPROPERTY(P.[object_id], N'IX_TransactionHistory_ProductID', 'IndexID');\r\n\r\n0. The density (1 / *distinct values*) of the index: **0.002267574**  \r\n This is available in the density vector of the index statistics: \r\n\r\n        DBCC SHOW_STATISTICS \r\n        (\r\n            N'Production.TransactionHistory', \r\n            N'IX_TransactionHistory_ProductID'\r\n        ) \r\n        WITH DENSITY_VECTOR;\r\n\r\n [![density][1]][1]\r\n\r\n### Computation\r\n\r\n    -- Input numbers\r\n    DECLARE\r\n        @Executions float = 504,\r\n        @Density float = 0.002267574,\r\n        @IndexDataPages float = 201,\r\n        @Cardinality float = 113443;\r\n    \r\n    -- SQL Server cost model constants\r\n    DECLARE\r\n        @SeqIO float = 0.000740740740741,\r\n        @RandomIO float = 0.003125,\r\n        @CPUbase float = 0.000157,\r\n        @CPUrow float = 0.0000011;\r\n    \r\n    -- Computation\r\n    DECLARE\r\n        @IndexPages float = CEILING(@IndexDataPages * @Density),\r\n        @Rows float = @Cardinality * @Density,\r\n        @Rebinds float = @Executions - 1e0;\r\n    \r\n    DECLARE\r\n        @CPU float = @CPUbase + (@Rows * @CPUrow),\r\n        @IO float = @RandomIO + (@SeqIO * (@IndexPages - 1e0)),\r\n        -- sample with replacement\r\n        @PSWR float = @IndexDataPages * (1e0 - POWER(1e0 - (1e0 / @IndexDataPages), @Rebinds));\r\n    \r\n    -- Cost components (no rewinds)\r\n    DECLARE\r\n        @InitialCost float = @RandomIO + @CPUbase + @CPUrow,\r\n        @RebindCPU float = @Rebinds * (1e0 * @CPUbase + @CPUrow),\r\n        @RebindIO float = (1e0 / @Rows) * ((@PSWR - 1e0) * @IO);\r\n    \r\n    -- Result\r\n    SELECT \r\n        OpCost = @InitialCost + @RebindCPU + @RebindIO;\r\n\r\n[*db<>fiddle*][3]\r\n\r\n[![Result][2]][2]\r\n\r\n  [1]: https://i.stack.imgur.com/X9wed.png\r\n  [2]: https://i.stack.imgur.com/Zu21K.png\r\n  [3]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=6fe4fadf9f37f451e5c49b8c0a3e6e25	2019-12-01 15:00:56.59155+00	1	4	1	211214	0	0	0	2019-12-01 14:58:58.862942+00	\N	The full cost derivation logic is complex, but for the relatively simple case in the question:	f	f
129	188	12	2017-05-09 20:04:10+00	When deleting from the parent table, SQL Server must check for the existence of any FK child rows that refer to that row. When there is no suitable child index, this check performs a full scan of the child table:\r\n\r\n[![Full child scan][1]][1]\r\n\r\nIf the scan encounters a row that has been modified since the delete command's snapshot transaction started, it will fail with an update conflict (by definition). A full scan will obviously touch every row in the table.\r\n\r\nWith a suitable index, SQL Server can locate and test just the rows in the child table that could match the to-be-deleted parent. When these particular rows have not been modified, no update conflict occurs:\r\n\r\n[![Child seek][2]][2]\r\n\r\nNote that foreign key checks under row versioning isolation levels take shared locks (for correctness) as well as detecting update conflicts. For example, the internal hints on the child table accesses above are:\r\n\r\n``` none\r\nPhyOp_Range TBL: [dbo].[Child]\r\n    Hints( READ-COMMITTEDLOCK FORCEDINDEX DETECT-SNAPSHOT-CONFLICT )\r\n```\r\n\r\nSadly this is not currently exposed in execution plans.\r\n\r\nRelated articles of mine:\r\n\r\n* [The SNAPSHOT Isolation Level][3]\r\n* [Data Modifications under Read Committed Snapshot Isolation][4]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/XcO1s.png\r\n  [2]: https://i.stack.imgur.com/mK8vZ.png\r\n  [3]: https://sqlperformance.com/2014/06/sql-performance/the-snapshot-isolation-level\r\n  [4]: https://sqlperformance.com/2014/05/t-sql-queries/data-modifications-under-rcsi	2019-11-28 16:33:17.455419+00	2	4	1	173132	0	0	0	2019-11-28 16:32:39.43043+00	\N	When deleting from the parent table, SQL Server must check for the existence of any FK child rows that refer to that row. When there is no suitable child index, this check performs a full scan of the child table:	f	f
742	668	16	2017-10-04 00:09:06+00	**Batch Mode Adaptive Joins**\r\n\r\nFor Batch Mode Adaptive Joins, the goal is to not pin join choice to a specific type at compile time. \r\n\r\nWhen available, Adaptive Joins allow the optimizer to choose between Nested Loops Joins and Hash Joins based on row thresholds at run time. \r\n\r\nAt this time, Merge Joins are not considered. Pure speculation is that needing data to be sorted, or needing to inject a sort into the plan would add too much overhead when changing the course of a query.\r\n\r\n**When Do Batch Mode Adaptive Joins Occur?**\r\n\r\nAt this time, Batch Mode query processing requires the presence of a ColumnStore index. They also require, well, a join, and an index that allows for the choice of a Nested Loops or Hash Join.\r\n\r\n**How do I know if my Join is Adaptive?**\r\n\r\nQuery plans for adaptive joins are quite distinctive. \r\n\r\n[![NUTS][1]][1]\r\n\r\nThe Adaptive Join operator is new to SQL Server 2017, and currently has the following properties in actual execution plans.\r\n\r\n[![NUTS][2]][2]\r\n\r\n - Physical Operation: Adaptive Join\r\n\r\n - Actual Join Type: Will be Hash match or Nested Loops\r\n\r\n - Adaptive Threshold Rows: Signifies the tipping point when the join type will switch to Hash Match\r\n\r\n - Is Adaptive: True for Adaptive Joins\r\n\r\n - Estimated Join Type: Rather self-explanatory!\r\n\r\nIn an estimated or cached plan, there is rather less information:\r\n\r\n[![NUTS][3]][3]\r\n\r\nMost notably, the Actual Join Type is missing. \r\n\r\n**What breaks Batch Mode Adaptive Joins?**\r\n\r\nTo monitor this, there's an Extended Event session called `adaptive_join_skipped`, and it has the following reasons for skipping a Batch Mode Adaptive Join:\r\n\r\n - eajsrExchangeTypeNotSupported\r\n - eajsrHJorNLJNotFound\r\n - eajsrInvalidAdaptiveThreshold\r\n - eajsrMultiConsumerSpool \r\n - eajsrOuterCardMaxOne\r\n - eajsrOuterSideParallelMarked\r\n - eajsrUnMatchedOuter\r\n\r\nAside from those, Batch Mode Adaptive Joins may be skipped for other reasons. Take these two queries for example:\r\n\r\n    /*Selecting just integer data*/\r\n    SELECT uc.Id, uc.Reputation, p.Score\r\n    FROM   dbo.Users_cx AS uc\r\n    JOIN   dbo.Posts AS p\r\n    ON p.OwnerUserId = uc.Id\r\n    WHERE  uc.LastAccessDate >= '20160101';\r\n    \r\n    /*Selecting one string column from Users*/\r\n    SELECT uc.Id, uc.DisplayName, uc.Reputation, p.Score\r\n    FROM   dbo.Users_cx AS uc\r\n    JOIN   dbo.Posts AS p\r\n    ON p.OwnerUserId = uc.Id\r\n    WHERE  uc.LastAccessDate >= '20160101';\r\n\r\nThey're identical except that the second query selects the `DisplayName` column, which has a type of `NVARCHAR(40)`.  \r\n\r\n[![NUTS][4]][4]\r\n\r\nThe Batch Mode Adaptive Join is skipped for the second query, but no reason is logged to the XE session. It appears that string data remains the steadfast enemy of ColumnStore indexes.\r\n\r\nThere are other query patterns that fail to get Adaptive Joins, that also do not trigger events.\r\n\r\nSome examples:\r\n\r\n- `CROSS APPLY` with a `TOP`\r\n \r\n- `OUTER APPLY`\r\n\r\n**eajsrExchangeTypeNotSupported**\r\n\r\nOne thing that will trigger this event appears to be the presence of a Repartition Streams operator. In this query, the partitioning type is Hash Match. Special thanks to the Intergalactic Celestial Being masquerading as a humble blogging man known as [Paul White][5] for the bizarre query.\r\n\r\n\tSELECT uc.Id, uc.Reputation, CONVERT(XML, CONVERT(NVARCHAR(10), p.Score)).value('(xml/text())[1]', 'INT') AS [Surprise!]\r\n\tFROM   dbo.Users_cx AS uc\r\n\tJOIN   dbo.Posts AS p\r\n\tON p.OwnerUserId = uc.Id\r\n\tWHERE  uc.LastAccessDate <= '2009-08-01'\r\n\tOPTION ( USE HINT ( 'ENABLE_PARALLEL_PLAN_PREFERENCE' ));\r\n\tGO \r\n\r\n**eajsrHJorNLJNotFound**\r\n\r\nNo queries have triggered this XE yet. \r\nWhat doesn't work:\r\n \r\n- Merge Join hint\r\n\r\n- Query patterns that rule out join type, for example Hash and Merge joins require at least one equality predicate. Writing a join on `>=` and `<=` does not trigger the event.\r\n\r\n**eajsrInvalidAdaptiveThreshold**\r\n\r\nThis event can be triggered by various `TOP`, `FAST N`, and `OFFSET/FETCH` queries. Here are some examples:\r\n\r\n\tSELECT uc.Id, uc.DisplayName, uc.Reputation, p.Score\r\n\tFROM   dbo.Users_cx AS uc\r\n\tINNER JOIN   dbo.Posts AS p\r\n\tON p.OwnerUserId = uc.Id\r\n\tWHERE  uc.LastAccessDate <= '2008-01-01'\r\n\tOPTION ( FAST 1);\r\n\tGO \r\n\r\n\tSELECT TOP 1 uc.Id, uc.DisplayName, uc.Reputation, p.Score\r\n\tFROM   dbo.Users_cx AS uc\r\n\tINNER JOIN   dbo.Posts AS p\r\n\tON p.OwnerUserId = uc.Id\r\n\tWHERE  uc.LastAccessDate <= '2008-01-01';\r\n\tGO \r\n\r\nIn some circumstances, they can also be triggered by paging-style queries:\r\n\r\n\tWITH pages\r\n\tAS ( SELECT TOP 100 uc.Id, ROW_NUMBER() OVER ( ORDER BY uc.Id ) AS n\r\n\t     FROM   dbo.Users_cx AS uc ),\r\n\t     rows\r\n\tAS ( SELECT TOP 50 p.Id\r\n\t     FROM   pages AS p\r\n\t     WHERE  p.n > 50 )\r\n\tSELECT u.Id, u.Reputation\r\n\tFROM   pages AS p\r\n\tJOIN   dbo.Users AS u\r\n\tON p.Id = u.Id;\r\n\r\n**eajsrMultiConsumerSpool** \r\n\r\nNo known query patterns have triggered this event yet. \r\n\r\nWhat hasn't triggered it so far:\r\n \r\n- Recursive CTEs\r\n \r\n- Grouping sets/Cube/Rollup\r\n \r\n- PIVOT and UNPIVOT\r\n \r\n- Windowing functions\r\n\r\n**eajsrOuterCardMaxOne**\r\n\r\nA couple different types of queries have triggered this event. A derived join with a `TOP 1`, and a join combined with a WHERE clause with an equality predicate on a unique column:\r\n\r\n\r\n\tSELECT uc.Id, uc.Reputation, p.Score\r\n\tFROM   dbo.Users_cx AS uc\r\n\tJOIN   (SELECT TOP 1 p2.OwnerUserId, p2.Score FROM dbo.Posts AS p2 ORDER BY Id) AS p\r\n\tON p.OwnerUserId = uc.Id\r\n\tWHERE  uc.LastAccessDate <= '2009-08-01';\r\n\tGO \r\n\r\n\tSELECT p.Id, p.ParentId, p.OwnerUserId\r\n\tFROM dbo.Posts AS p\r\n\tJOIN dbo.Users_cx AS uc\r\n\tON uc.Id = p.OwnerUserId\r\n\tWHERE p.Id = 17333;\r\n\tGO \r\n\r\n**eajsrOuterSideParallelMarked**\r\n\r\nOne type of query that can trigger this event is a Recursive CTE.\r\n\r\n\tWITH postparent AS \r\n\t\t(\r\n\t\tSELECT p.Id, p.ParentId, p.OwnerUserId \r\n\t\tFROM dbo.Posts_cx AS p\r\n\t\tWHERE p.Id = 17333\r\n\r\n\t\t\tUNION ALL\r\n\r\n\t\tSELECT p2.Id, p2.ParentId, p2.OwnerUserId\r\n\t\tFROM postparent pp\r\n\t\tJOIN dbo.Posts_cx AS p2\r\n\t\tON pp.Id = p2.ParentId\r\n\t\t)\r\n\t\tSELECT pp.Id, pp.ParentId, pp.OwnerUserId, u.DisplayName\r\n\t\tFROM postparent pp\r\n\t\tJOIN dbo.Users AS u\r\n\t\tON u.Id = pp.OwnerUserId\r\n\t\tORDER BY pp.Id\r\n\t\tOPTION (USE HINT('ENABLE_PARALLEL_PLAN_PREFERENCE'));\r\n\r\nThe reason here appears to be that the recursive portion of the CTE, which causes a serial zone in the plan, disallows Batch Mode Adaptive Join choice.\r\n\r\n**eajsrUnMatchedOuter**\r\n\r\nThis is the most common, and seems to occur when an index is used for the join that cannot support a seek. For instance, this query causes a Key Lookup:\r\n\r\n\tSELECT uc.Id, uc.Reputation, p.Score, p.LastActivityDate \r\n\tFROM   dbo.Users_cx AS uc\r\n\tJOIN   dbo.Posts AS p\r\n\tON p.OwnerUserId = uc.Id\r\n\tWHERE  uc.LastAccessDate >= '20080101'\r\n\t       AND uc.DisplayName = 'harrymc'\r\n\t       AND p.Score > 1;\r\n\r\n[![NUTS][6]][6]\r\n\r\n\r\nThe resulting query chooses a Row Mode Nested Loops join to execute both the Key Lookup and the table joins, which triggers the event.\r\n\r\nAnother example is a query that skips the narrow nonclustered index in favor of the PK/CX. In this case, the PK/CX does not lead with `OwnerUserId`, so the only join choice is a Hash Join.\r\n\r\nIn both cases, the "unmatched outer" seems to indicate that the index chosen does not sufficiently cover our query.\r\n\r\n\tSELECT uc.Id, uc.Reputation, p.*\r\n\tFROM   dbo.Users_cx AS uc\r\n\tJOIN   dbo.Posts AS p\r\n\tON p.OwnerUserId = uc.Id\r\n\tWHERE  uc.LastAccessDate >= '20160101';\r\n\r\n**Do Batch Mode Adaptive Joins work with multiple joins?**\r\n\r\nYes, but as of this writing, there appears to be a limitation:\r\n\r\nJoining from one ColumnStore index to multiple Row Store indexes will yield multiple Adaptive Joins, whereas a join between multiple ColumnStore indexes will not be Adaptive.\r\n\r\nFor example, these two queries\r\n\r\n\tSELECT uc.Id, uc.Reputation, p.Score\r\n\tFROM   dbo.Users_cx AS uc\r\n\tJOIN   dbo.Posts AS p\r\n\tON p.OwnerUserId = uc.Id\r\n\tJOIN dbo.Comments AS c\r\n\tON c.PostId = p.Id\r\n\tAND  c.UserId = uc.Id\r\n\tWHERE  uc.LastAccessDate >= '20160101';\r\n\t\r\n\tSELECT uc.Id, uc.Reputation, p.Score\r\n\tFROM   dbo.Users_cx AS uc\r\n\tJOIN   dbo.Posts AS p\r\n\tON p.OwnerUserId = uc.Id\r\n\tJOIN dbo.Comments_cx AS c\r\n\tON c.PostId = p.Id\r\n\tAND  c.UserId = uc.Id\r\n\tWHERE  uc.LastAccessDate >= '20160101';\r\n\r\nThe first query joins one ColumnStore index (on Users) to two Row Store indexes on Posts and Comments. This yields two Adaptive Join operators.\r\n\r\nThe second query joins two ColumnStore tables (Users and Comments) to one Row Store table (Posts), and yields one Adaptive Join.\r\n\r\n[![NUTS][7]][7]\r\n\r\n**Is there any overhead to Batch Mode Adaptive Joins?**\r\n\r\nYes, all Batch Mode Adaptive Join plans receive a memory grant. This isn't always the case with Nested Loops join, unless they receive the Nested Loops prefetch optimization. The memory grant is to support a Hash Join should the plan require one based on row thresholds. \r\n\r\n\r\n  [1]: https://i.stack.imgur.com/nxaLE.jpg\r\n  [2]: https://i.stack.imgur.com/8f5iv.jpg\r\n  [3]: https://i.stack.imgur.com/c6sjP.jpg\r\n  [4]: https://i.stack.imgur.com/FbZ7e.jpg\r\n  [5]: https://dba.stackexchange.com/users/1192/paul-white\r\n  [6]: https://i.stack.imgur.com/bSMNB.jpg\r\n  [7]: https://i.stack.imgur.com/3XCqv.jpg	2020-02-03 03:08:05.409981+00	3	4	1	187577	0	0	0	2020-02-03 03:08:05.409981+00	\N	**Batch Mode Adaptive Joins**	f	f
577	528	8	2017-07-12 07:50:43+00	If you start SQL Server in single user mode only one administrator can connect at the same time.\r\n\r\nWhat's probably happening here is that some service is using a sysadmin login to connect such as Reporting services or SQL Server agent.\r\n\r\nWhen starting SQL Server in single user mode you have the option to specify a client application so only that application can connect.\r\n\r\nHave a look at the [command line options](https://technet.microsoft.com/en-us/library/ms190737(v=sql.105).aspx) where it lists:\r\n\r\n> -m "Client Application Name" \t\r\n> \r\n> When you use the -m option with SQLCMD or SQL Server Management\r\n> Studio, you can limit the connections to a specified client\r\n> application. For example, -m"SQLCMD" limits connections to a single\r\n> connection and that connection must identify itself as the SQLCMD\r\n> client program. Use this option when you are starting SQL Server in\r\n> single-user mode and an unknown client application is taking the only\r\n> available connection. To connect through the Query Editor in\r\n> Management Studio, use -m"Microsoft SQL Server Management Studio -\r\n> Query".\r\n> \r\n> Client Application Name is case sensitive.\r\n\r\nIf you are able to connect this way, change the maximum server memory to something reasonable. I suppose you will be able to connect because otherwise you would probably receive something as "there is no process on the other end of the pipe" so I would assume your server is running.\r\n\r\nIf however you can't log in to SQL Server because your memory configuration doesn't allow you, you can start SQL Server in minimal configuration using the `-f` option.\r\n\r\n> -f \t\r\n> \r\n> Starts an instance of SQL Server with minimal configuration. This is\r\n> useful if the setting of a configuration value (for example,\r\n> over-committing memory) has prevented the server from starting.\r\n> Starting SQL Server in minimal configuration mode places SQL Server in\r\n> single-user mode. For more information, see the description for -m\r\n> that follows.\r\n\r\nFor reference: [SQL Server can’t start after accidently set the "max server memory" to 0](https://blogs.msdn.microsoft.com/alexfeng/2013/07/07/sql-server-cant-start-after-accidently-set-the-max-server-memory-to-0/)\r\n	2020-01-06 07:39:58.680873+00	4	4	1	179626	0	0	0	2020-01-06 07:39:58.680873+00	\N	If you start SQL Server in single user mode only one administrator can connect at the same time.	f	f
174	238	12	2018-10-03 11:08:28+00	> Why is there a large difference in CPU time for these two queries?\r\n\r\nThe scan plan evaluates the following pushed non-sargable (residual) predicate for each row:\r\n\r\n    [two_col_key_test].[ID1]<>N'1' \r\n    AND [two_col_key_test].[ID1]<>N'10' \r\n    AND [two_col_key_test].[ID1]<>N'11' \r\n    AND [two_col_key_test].[ID1]<>N'12' \r\n    AND [two_col_key_test].[ID1]<>N'13' \r\n    AND [two_col_key_test].[ID1]<>N'14' \r\n    AND [two_col_key_test].[ID1]<>N'15' \r\n    AND [two_col_key_test].[ID1]<>N'16' \r\n    AND [two_col_key_test].[ID1]<>N'17' \r\n    AND [two_col_key_test].[ID1]<>N'18' \r\n    AND [two_col_key_test].[ID1]<>N'19' \r\n    AND [two_col_key_test].[ID1]<>N'2' \r\n    AND [two_col_key_test].[ID1]<>N'20' \r\n    AND [two_col_key_test].[ID1]<>N'3' \r\n    AND [two_col_key_test].[ID1]<>N'4' \r\n    AND [two_col_key_test].[ID1]<>N'5' \r\n    AND [two_col_key_test].[ID1]<>N'6' \r\n    AND [two_col_key_test].[ID1]<>N'7' \r\n    AND [two_col_key_test].[ID1]<>N'8' \r\n    AND [two_col_key_test].[ID1]<>N'9' \r\n    AND \r\n    (\r\n        [two_col_key_test].[ID1]=N'FILLER TEXT' \r\n        AND [two_col_key_test].[ID2]>=N'' \r\n        OR [two_col_key_test].[ID1]>N'FILLER TEXT'\r\n    )\r\n\r\n[![scan residual][1]][1]\r\n\r\nThe seek plan does two seeking operations:\r\n\r\n\r\n\r\n    Seek Keys[1]: \r\n        Prefix: \r\n        [two_col_key_test].ID1 = Scalar Operator(N'FILLER TEXT'), \r\n            Start: [two_col_key_test].ID2 >= Scalar Operator(N'')\r\n    Seek Keys[1]: \r\n        Start: [two_col_key_test].ID1 > Scalar Operator(N'FILLER TEXT')\r\n\r\n...to match this part of the predicate:\r\n\r\n    (ID1 = N'FILLER TEXT' AND ID2 >= N'' OR (ID1 > N'FILLER TEXT'))\r\n\r\nA residual predicate is applied to rows that pass the seek conditions above (all rows in your example). \r\n\r\nHowever, each inequality is replaced by two separate tests for *less than* `OR` *greater than*:\r\n\r\n    ([two_col_key_test].[ID1]<N'1' OR [two_col_key_test].[ID1]>N'1') \r\n    AND ([two_col_key_test].[ID1]<N'10' OR [two_col_key_test].[ID1]>N'10') \r\n    AND ([two_col_key_test].[ID1]<N'11' OR [two_col_key_test].[ID1]>N'11') \r\n    AND ([two_col_key_test].[ID1]<N'12' OR [two_col_key_test].[ID1]>N'12') \r\n    AND ([two_col_key_test].[ID1]<N'13' OR [two_col_key_test].[ID1]>N'13') \r\n    AND ([two_col_key_test].[ID1]<N'14' OR [two_col_key_test].[ID1]>N'14') \r\n    AND ([two_col_key_test].[ID1]<N'15' OR [two_col_key_test].[ID1]>N'15') \r\n    AND ([two_col_key_test].[ID1]<N'16' OR [two_col_key_test].[ID1]>N'16') \r\n    AND ([two_col_key_test].[ID1]<N'17' OR [two_col_key_test].[ID1]>N'17') \r\n    AND ([two_col_key_test].[ID1]<N'18' OR [two_col_key_test].[ID1]>N'18') \r\n    AND ([two_col_key_test].[ID1]<N'19' OR [two_col_key_test].[ID1]>N'19') \r\n    AND ([two_col_key_test].[ID1]<N'2' OR [two_col_key_test].[ID1]>N'2') \r\n    AND ([two_col_key_test].[ID1]<N'20' OR [two_col_key_test].[ID1]>N'20') \r\n    AND ([two_col_key_test].[ID1]<N'3' OR [two_col_key_test].[ID1]>N'3') \r\n    AND ([two_col_key_test].[ID1]<N'4' OR [two_col_key_test].[ID1]>N'4') \r\n    AND ([two_col_key_test].[ID1]<N'5' OR [two_col_key_test].[ID1]>N'5') \r\n    AND ([two_col_key_test].[ID1]<N'6' OR [two_col_key_test].[ID1]>N'6') \r\n    AND ([two_col_key_test].[ID1]<N'7' OR [two_col_key_test].[ID1]>N'7') \r\n    AND ([two_col_key_test].[ID1]<N'8' OR [two_col_key_test].[ID1]>N'8') \r\n    AND ([two_col_key_test].[ID1]<N'9' OR [two_col_key_test].[ID1]>N'9')\r\n\r\n[![seek residual][2]][2]\r\n\r\nRewriting each inequality e.g.:\r\n\r\n\r\n\r\n    [ID1] <> N'1'  ->  [ID1]<N'1' OR [ID1]>N'1'\r\n\r\n...is counterproductive here. Collation-aware string comparisons are expensive. Doubling the number of comparisons explains most of the difference in CPU time you see.\r\n\r\nYou can see this more clearly by disabling the pushing of non-sargable predicates with undocumented trace flag 9130. That will show the residual as a separate Filter, with performance information you can inspect separately:\r\n\r\n[![scan][3]][3]\r\n\r\n[![seek][4]][4]\r\n\r\nThis will also highlight the slight cardinality misestimate on the seek, which explains why the optimizer chose the seek over the scan in the first place (it expected the seeking portion to eliminate some rows).\r\n\r\nWhile the inequality rewrite may make (possibly filtered) index matching possible (to make the best use of the seeking ability of b-tree indexes), it would be better to subsequently revert this expansion if both halves end up in the residual. You could suggest this as an improvement on the [SQL Server feedback site][5].\r\n\r\nNote also that the original ("legacy") cardinality estimation model happens to select a scan by default for this query. \r\n\r\n  [1]: https://i.stack.imgur.com/Fnsv4.png\r\n  [2]: https://i.stack.imgur.com/3EJns.png\r\n  [3]: https://i.stack.imgur.com/8qK1l.png\r\n  [4]: https://i.stack.imgur.com/H6nPo.png\r\n  [5]: https://feedback.azure.com/forums/908035-sql-server	2019-12-01 20:29:56.322242+00	3	4	1	219162	0	0	0	2019-12-01 20:29:56.322242+00	\N	> Why is there a large difference in CPU time for these two queries?	f	f
134	193	12	2017-07-29 12:52:59+00	`INSTEAD OF` triggers completely replace the triggering action.\r\n\r\nThe *inserted* and *deleted* pseudo-tables represent changes that would have been made, had the triggering statement actually executed. Row-versioning cannot be used for these triggers because no modifications have yet occurred, by definition.\r\n\r\nSQL Server modifies the execution plan for the triggering DML statement when an `INSTEAD OF` trigger exists. Rather than modifying the affected tables directly, the execution plan writes information about the changes to a hidden worktable.\r\n\r\nThis worktable contains all the data needed to perform the original changes, the type of modification to perform on each row (delete or insert), as well as any information needed in the trigger for an `OUTPUT` clause.\r\n\r\nThe Insert in your execution plan represents writing to this hidden worktable. When you capture a post-execution plan for the statement, you will see this hidden worktable being used as the *deleted* and *inserted* pseudo-tables.\r\n\r\nSee my SQLPerformance.com article, [Interesting Things About INSTEAD OF Triggers][1].\r\n\r\n\r\n  [1]: https://sqlperformance.com/2015/09/sql-plan/instead-of-triggers	2019-11-29 08:55:05.25081+00	2	4	1	182169	0	0	0	2019-11-29 08:55:05.25081+00	\N	`INSTEAD OF` triggers completely replace the triggering action.	f	f
225	271	45	2018-05-15 23:38:24+00	I'm going to assume that you have skewed data, that you don't want to use query hints to force the optimizer what to do, and that you need to get good performance for all possible input values of `@Id`. You can get a query plan guaranteed to require just a few handfuls of logical reads for any possible input value if you're willing to create the following pair of indexes (or their equivalent):\r\n\r\n    CREATE INDEX GetMinSomeTimestamp ON dbo.MyTable (Id, SomeTimestamp) WHERE SomeBit = 1;\r\n    CREATE INDEX GetMaxSomeInt ON dbo.MyTable (Id, SomeInt) WHERE SomeBit = 1;\r\n\r\nBelow is my test data. I put 13 M rows into the table and made half of them have a value of `'3A35EA17-CE7E-4637-8319-4C517B6E48CA'` for the `Id` column.\r\n\r\n    DROP TABLE IF EXISTS dbo.MyTable;\r\n    \r\n    CREATE TABLE dbo.MyTable (\r\n    \tId uniqueidentifier,\r\n    \tSomeTimestamp DATETIME2,\r\n    \tSomeInt INT,\r\n    \tSomeBit BIT,\r\n    \tFILLER VARCHAR(100)\r\n    );\r\n    \r\n    INSERT INTO dbo.MyTable WITH (TABLOCK)\r\n    SELECT NEWID(), CURRENT_TIMESTAMP, 0, 1, REPLICATE('Z', 100)\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2;\r\n    \r\n    INSERT INTO dbo.MyTable WITH (TABLOCK)\r\n    SELECT '3A35EA17-CE7E-4637-8319-4C517B6E48CA', CURRENT_TIMESTAMP, 0, 1, REPLICATE('Z', 100)\r\n    FROM master..spt_values t1\r\n    CROSS JOIN master..spt_values t2;\r\n\r\nThis query might look a little strange at first:\r\n\r\n    DECLARE @Id UNIQUEIDENTIFIER = '3A35EA17-CE7E-4637-8319-4C517B6E48CA'\r\n\r\n    SELECT\r\n      @Id,\r\n      st.SomeTimestamp,\r\n      si.SomeInt\r\n    FROM (\r\n    \tSELECT TOP (1) SomeInt, Id\r\n    \tFROM dbo.MyTable\r\n    \tWHERE Id = @Id\r\n    \tAND SomeBit = 1\r\n    \tORDER BY SomeInt DESC\r\n    ) si\r\n    CROSS JOIN (\r\n    \tSELECT TOP (1) SomeTimestamp, Id\r\n    \tFROM dbo.MyTable\r\n    \tWHERE Id = @Id\r\n    \tAND SomeBit = 1\r\n    \tORDER BY SomeTimestamp ASC\r\n    ) st;\r\n\r\nIt's designed to take advantage of the ordering of the indexes to find the min or max value with a few logical reads. The `CROSS JOIN` is there to get correct results when there aren't any matching rows for the `@Id` value. Even if I filter on the most popular value in the table (matching 6.5 million rows) I only get 8 logical reads:\r\n\r\n> Table 'MyTable'. Scan count 2, logical reads 8\r\n\r\nHere's the query plan:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nBoth index seeks find 0 or 1 rows. It's extremely efficient, but creating two indexes might be overkill for your scenario. You could consider the following index instead:\r\n\r\n    CREATE INDEX CoveringIndex ON dbo.MyTable (Id) INCLUDE (SomeTimestamp, SomeInt) WHERE SomeBit = 1;\r\n\r\nNow the query plan for the original query (with an optional `MAXDOP 1` hint) looks a bit different:\r\n\r\n[![enter image description here][2]][2]\r\n\r\nThe key lookups are no longer necessary. With a better access path that should work well for all inputs you shouldn't have to worry about the optimizer picking the wrong query plan due to the density vector. However, this query and index won't be as efficient as the other one if you seek on a popular `@Id` value.\r\n\r\n> Table 'MyTable'. Scan count 1, logical reads 33757\r\n\r\n  [1]: https://i.stack.imgur.com/cRe9Q.png\r\n  [2]: https://i.stack.imgur.com/fRHSP.png	2019-12-04 14:16:15.415851+00	2	4	1	206824	0	0	0	2019-12-04 14:16:15.415851+00	\N	I'm going to assume that you have skewed data, that you don't want to use query hints to force the optimizer what to do, and that you need to get good performance for all possible input values of `@Id`. You can get a query plan guaranteed to require just a few handfuls of logical reads for any possible input value if you're willing to create the following pair of indexes (or their equivalent):	f	f
226	271	16	2018-05-15 22:15:27+00	In SQL Server, there are three common forms of non-join predicate:\r\n\r\nWith a **literal** value:\r\n-\r\n    SELECT COUNT(*) AS records\r\n    FROM   dbo.Users AS u\r\n    WHERE  u.Reputation = 1;\r\n\r\nWith a **parameter**:\r\n-\r\n    CREATE PROCEDURE dbo.SomeProc(@Reputation INT)\r\n    AS\r\n    BEGIN\r\n    \tSELECT COUNT(*) AS records\r\n    \tFROM   dbo.Users AS u\r\n    \tWHERE  u.Reputation = @Reputation;\r\n    END;\r\n\r\nWith a **local variable**:\r\n-\r\n    DECLARE @Reputation INT = 1\r\n    \r\n    SELECT COUNT(*) AS records\r\n    FROM   dbo.Users AS u\r\n    WHERE  u.Reputation = @Reputation;\r\n\r\nOutcomes\r\n-\r\nWhen you use a **literal** value, and your plan isn't a) [Trivial][1] and b) Simple Parameterized or c) you don't have [Forced Parameterization][2] turned on, the optimizer creates a very special plan just for that value.\r\n\r\nWhen you use a **parameter**, the optimizer will create a plan for that parameter (this is called [parameter sniffing][3]), and then reuse that plan, absent recompile hints, plan cache eviction, etc.\r\n\r\nWhen you use a **local variable**, the optimizer makes a plan for... [Something][4]. \r\n\r\nIf you were to run this query:\r\n\r\n    DECLARE @Reputation INT = 1\r\n    \r\n    SELECT COUNT(*) AS records\r\n    FROM   dbo.Users AS u\r\n    WHERE  u.Reputation = @Reputation;\r\n\r\nThe plan would look like this:\r\n\r\n[![NUTS][5]][5]\r\n\r\nAnd the estimated number of rows for that local variable would look like this:\r\n\r\n[![NUTS][6]][6]\r\n\r\nEven though the query returns a count of 4,744,427.\r\n\r\nLocal variables, being unknown, don't use the 'good' part of the histogram for cardinality estimation. They use a guess based on the density vector.\r\n\r\n[![NUTS][7]][7]\r\n\r\n`SELECT  5.280389E-05 * 7250739 AS [poo]`\r\n\r\nThat'll give you `382.86722457471`, which is the guess the optimizer makes.\r\n\r\nThese unknown guesses are usually very bad guesses, and can often lead to bad plans and bad index choices.\r\n\r\nFixing It?\r\n--\r\n**Your options generally are:**\r\n\r\n - Brittle index hints\r\n - Potentially expensive recompile hints\r\n - Parameterized dynamic SQL\r\n - A stored procedure\r\n - Improve the current index\r\n\r\n**Your options specifically are:**\r\n\r\nImproving the current index means extending it to cover all the columns needed by the query:\r\n\r\n    CREATE NONCLUSTERED INDEX IX_MyTable_Id_SomeBit_Includes\r\n    ON dbo.MyTable (Id, SomeBit)\r\n    INCLUDE (TotallyUnrelatedTimestamp, SomeTimestamp, SomeInt)\r\n    WITH (DROP_EXISTING = ON);\r\n\r\nAssuming that `Id` values are reasonably selective, this will give you a good plan, and help the optimizer by giving it an 'obvious' data access method.\r\n\r\nMore Reading\r\n--\r\nYou can read more about parameter embedding here:\r\n\r\n - [Parameter Sniffing, Embedding, and the RECOMPILE Options][8], by Paul White\r\n - [Why You’re Tuning Stored Procedures Wrong (the Problem with Local Variables)][9], Kendra Little\r\n\r\n\r\n  [1]: https://dba.stackexchange.com/questions/206304/why-does-a-plan-with-full-optimization-show-simple-parameterization\r\n  [2]: https://www.brentozar.com/blitzcache/forced-parameterization/\r\n  [3]: http://www.sommarskog.se/query-plan-mysteries.html\r\n  [4]: https://www.brentozar.com/archive/2013/06/optimize-for-unknown-sql-server-parameter-sniffing/\r\n  [5]: https://i.stack.imgur.com/uJLI6.jpg\r\n  [6]: https://i.stack.imgur.com/iiSdl.jpg\r\n  [7]: https://i.stack.imgur.com/A5CZh.jpg\r\n  [8]: https://sqlperformance.com/2013/08/t-sql-queries/parameter-sniffing-embedding-and-the-recompile-options\r\n  [9]: https://www.brentozar.com/archive/2014/06/tuning-stored-procedures-local-variables-problems/\r\n	2019-12-04 14:16:15.699022+00	3	4	1	206817	0	0	0	2019-12-04 14:16:15.699022+00	\N	In SQL Server, there are three common forms of non-join predicate:	f	f
342	358	167	2017-01-06 18:28:39+00	> No new ducks will be added to the question, instead this project moved on to become a latex package. Please visit the project repository at https://github.com/samcarter/tikzducks or have a look at https://www.ctan.org/pkg/tikzducks if you want to see new ducks :)\r\n\r\n[![enter image description here][1]][1]\r\n\r\n   \r\n    \\documentclass{article}\r\n    \\usepackage[svgnames]{xcolor}\r\n    \\usepackage{tikz}\r\n    \\usetikzlibrary{decorations.pathmorphing}\r\n    \r\n    \\usepackage[paperwidth=38cm,paperheight=42cm,margin=1cm]{geometry}\r\n    \r\n    \\usepackage{bbding}\r\n    \r\n    \\newcommand{\\duck}[1]{%\r\n    \r\n    \t\\colorlet{duck}{#1}\r\n    \t\\colorlet{eye}{Cornsilk}\r\n    \t\\colorlet{pupil}{black}\r\n    \t\\colorlet{bill}{orange}\r\n    \t\t\r\n    \t%body\r\n    \t\\path[fill=duck] (51.2815,135.5394) .. controls (26.6859,139.7884) and (-12.5215,184.2616) .. (28.9411,223.8858) .. controls (70.4036,263.5099) and (286.2675,236.9673) .. (181.7701,108.1215) .. controls (93.7517,155.4266) and (123.9624,112.1537) .. (51.2815,135.5394) -- cycle;\r\n    \r\n    \t%head\r\n    \t\\path[fill=duck] (90,100) ellipse (1.4cm and 1.75cm);\r\n    \t\r\n    \t% duck's bill\r\n    \t\\path[fill=bill, xshift=-11pt, xscale=1.1] (49.3866,102.7929) .. controls (70.9472,97.0244) and (61.6632,119.6616) .. (95.1826,113) .. controls (20,165) and (36.9082,113.0997) .. (49.3866,102.7929) -- cycle;\r\n    \t\r\n    \t% right eye\r\n    \t\\path[fill=eye, rotate=20, xshift=-4pt, yshift=1pt] (112,58) ellipse (0.25cm and 0.35cm);\r\n    \t\\path[fill=pupil, rotate=20, xshift=-4pt, yshift=1pt] (115,59) ellipse (0.1cm and 0.2cm);\r\n    \t\r\n    \t% left eye\r\n    \t\\path[fill=eye, rotate=20] (78,62) ellipse (0.22cm and 0.32cm);\r\n    \t\\path[fill=pupil, rotate=20] (81,63) ellipse (0.08cm and 0.18cm);\r\n    \r\n    }\r\n    \r\n    \\newcommand{\\grumpyduck}[1]{%\r\n    \r\n    \t\\colorlet{duck}{#1}\r\n    \t\\colorlet{eye}{Cornsilk}\r\n    \t\\colorlet{pupil}{black}\r\n    \t\\colorlet{bill}{orange}\r\n    \t\t\r\n    \t%body\r\n    \t\\path[fill=duck] (51.2815,135.5394) .. controls (26.6859,139.7884) and (-12.5215,184.2616) .. (28.9411,223.8858) .. controls (70.4036,263.5099) and (286.2675,236.9673) .. (181.7701,108.1215) .. controls (93.7517,155.4266) and (123.9624,112.1537) .. (51.2815,135.5394) -- cycle;\r\n    \r\n    \t%head\r\n    \t\\path[fill=duck] (90,100) ellipse (1.4cm and 1.75cm);\r\n    \t\r\n    \t% duck's bill\r\n    \t\\path[fill=bill, xshift=-11pt, xscale=1.1] (49.3866,102.7929) .. controls (70.9472,97.0244) and (70.9472,97.0244) .. (85.1826,120) .. controls (20,165) and (36.9082,113.0997) .. (49.3866,102.7929) -- cycle;\r\n    \t\r\n    \t% right eye\r\n    \t\\path[fill=eye, rotate=20, xshift=-4pt, yshift=1pt] (112,58) ellipse (0.25cm and 0.35cm);\r\n    \t\\path[fill=pupil, rotate=20, xshift=-4pt, yshift=1pt] (115,59) ellipse (0.1cm and 0.2cm);\r\n    \t\r\n    \t% left eye\r\n    \t\\path[fill=eye, rotate=20] (78,62) ellipse (0.22cm and 0.32cm);\r\n    \t\\path[fill=pupil, rotate=20] (81,63) ellipse (0.08cm and 0.18cm);\r\n    \r\n    }\r\n    \r\n    \\newcommand{\\addalien}{%\r\n    \t\\draw[line width=3pt,color=YellowGreen] (110,62) -- (130,20);\r\n    \t\\draw[line width=3pt,color=YellowGreen] (72,58) -- (60,15);\r\n    \t\\path[fill=YellowGreen] (130,20) circle (0.2cm);\r\n    \t\\path[fill=YellowGreen] (60,15) circle (0.2cm);\r\n    }\r\n    \r\n    \\newcommand{\\addhat}[1]{%\r\n    \t\\path[fill=#1] (90,55) ellipse (1.3cm and 0.25cm);\r\n    \t\\path[fill=#1] (90,30) ellipse (0.7cm and 0.2cm);\r\n    \t\\path[fill=#1] (115,30) rectangle (65,55);\r\n    }\r\n    \r\n    \\newcommand{\\addsunglasses}{\r\n    \t\\draw[line width=3pt,color=black] (48,95) arc (190:370:20) ;\r\n    \t\\path[draw=black,line width=3pt] (95,90) -- (130,100);\r\n    \t\\path[fill=black, rotate=20] (111,58) ellipse (0.4cm and 0.4cm);\r\n    \t\\path[fill=black, rotate=20] (80,60) ellipse (0.37cm and 0.37cm);\r\n    }\r\n    \r\n    \\newcommand{\\addglasses}[1]{\r\n    \t\\draw[line width=2pt,color=#1] (62,80) arc (267:297:20) ;\r\n    \t\\path[draw=#1,line width=2pt] (93,88) -- (130,100);\r\n    \t\\path[draw=#1,line width=2pt, rotate=20] (107,58) ellipse (0.35cm and 0.35cm);\r\n    \t\\path[draw=#1,line width=2pt, rotate=20] (78,61) ellipse (0.32cm and 0.32cm);\r\n    }\r\n    \r\n    \\newcommand{\\addicecream}[3]{\r\n    \t\\path[draw=Sienna,fill=Goldenrod,line width=1pt,yshift=50pt,rotate=20,xshift=50pt] \r\n    \t(45,60)--(60,120)--(75,60);\r\n    \t\\path[draw=Sienna, fill=Goldenrod, rotate=20,line width=1pt] (144,118) ellipse (0.4cm and 0.25cm);\r\n    \t\\path[fill=#1, rotate=20] (138,116) circle (0.3cm);\r\n    \t\\path[fill=#2, rotate=20] (148,116) circle (0.3cm);\r\n    \t\\path[fill=#3, rotate=20] (142,102) circle (0.3cm);\r\n    }\r\n    \r\n    \\newcommand{\\addunicorn}{\r\n    \t\\path[draw=VioletRed,fill=Pink,line width=1pt,yshift=20pt,rotate=-25,xshift=0pt] \r\n    \t(50,60)--(60,20)--(70,60);\r\n    }\r\n    \r\n    \\newcommand{\\addhair}[1]{%\r\n    \t\\path[fill=#1, xshift=-5pt] (151.3277,174.3473) .. controls (157.7099,171.1213) and (164.7938,167.8644) .. (168.7230,161.6896) .. controls (164.8427,161.5316) and (153.5102,155.4255) .. (162.1164,152.9395) .. controls (169.4207,153.1460) and (176.4092,149.5358) .. (179.3920,142.6587) .. controls (185.5577,133.4026) and (172.4051,138.2448) .. (169.0163,134.3455) .. controls (174.7801,132.5948) and (184.6532,131.7138) .. (187.4798,127.5635) .. controls (176.4675,125.1191) and (163.1258,123.3733) .. (156.8817,112.6068) .. controls (152.4387,98.5734) and (153.2098,83.5059) .. (149.6492,69.2411) .. controls (131.4926,-1.1678) and (29.6020,22.0627) .. (47.7294,90.0940) .. controls (49.6639,62.0732) and (72.5401,38.6998) .. (96.3583,54.2220) .. controls (130.5162,76.1752) and (139.7469,117.8581) .. (115.3043,143.8986) .. controls (115.2213,148.9109) and (117.2762,158.3403) .. (124.2981,163.2993) .. controls (131.3200,168.2584) and (141.2814,171.4676) .. (151.3277,174.3473) -- cycle;\r\n    }\r\n    \r\n    \\newcommand{\\addshirt}[1]{%\r\n    \t\\path[fill=#1] (50,135.5394) .. controls (26.6859,139.7884) and (-12.5215,184.2616) .. (28.9411,223.8858) .. controls (70.4036,263.5099) and (286.2675,236.9673) .. (181.7701,108.1215) .. controls (93.7517,155.4266) and (123.9624,112.1537) .. (51.2815,180) -- cycle;\r\n    }\r\n    \r\n    \\newcommand{\\addtie}[1]{\r\n    \t\\draw[line width=10pt,color=#1] (60,150) -- (50,190);\r\n    }\r\n    \r\n    \\newcommand{\\addtshirt}[1]{\r\n    \t\\path[fill=#1] (50,135.5394) .. controls (26.6859,139.7884) and (-12.5215,184.2616) .. (28.9411,223.8858) .. controls (70.4036,263.5099) and (286.2675,236.9673) .. (181.7701,108.1215) .. controls (93.7517,155.4266) and (123.9624,122.1537) .. (59,150) -- cycle;\r\n    }\r\n    \r\n    \\newcommand{\\addshorthair}[1]{\r\n    \t\\path[fill=#1, xshift=-5pt] (145.7190,108.2466) .. controls (151.7052,104.8240) and (153.2448,84.3447) .. (149.6842,70.0799) .. controls (131.5276,-0.3291) and (29.6371,22.9015) .. (47.7644,90.9328) .. controls (49.6989,62.9120) and (80.4610,40.0060) .. (101.1924,59.4599) .. controls (128.6626,85.2375) and (139.4074,111.8552) .. (145.7190,108.2466) -- cycle;\r\n    }\r\n    \r\n    \\newcommand{\\addwizzard}{\r\n    \t\\path[fill=BlueViolet!50!Pink,line width=1pt,yshift=-40pt,rotate=5,xshift=32pt] \r\n    \t(20,100)--(60,0)--(100,100);\r\n    \t\\pgftext[at=\\pgfpoint{71}{-15}, left, base]{\\color{Gold}\\EightStarBold}\r\n    \t\\pgftext[at=\\pgfpoint{63}{0}, left, base]{\\color{Gold}\\EightStarBold~\\EightStarBold}\r\n    \t\\pgftext[at=\\pgfpoint{56}{15}, left, base]{\\color{Gold}\\EightStarBold~\\EightStarBold~\\EightStarBold}\r\n    \t\\pgftext[at=\\pgfpoint{56}{30}, left, base]{\\color{Gold}\\EightStarBold~\\EightStarBold~\\EightStarBold}\r\n    \t\\draw[line width=6pt,color=black] (90,160) -- (60,210);\r\n    \t\\draw[line width=6pt,color=white] (85,168.333) -- (80,176.666);\r\n    }\r\n    \r\n    \\newcommand{\\makeeinstein}[1]{\r\n    \t\\path[fill=#1,xshift=-4pt] (24.7738,59.7679) .. controls (31.3318,44.0108) and (53.1939,44.5256) .. (52.1285,41.2094) .. controls (51.4718,28.9400) and (30.1562,26.0780) .. (30.1562,26.0780) .. controls (48.5184,20.6686) and (79.2723,56.9617) .. (62.4298,11.1490) .. controls (65.1702,13.6709) and (88.0663,32.6096) .. (85.8201,7.5499) .. controls (87.7073,21.1456) and (96.9496,45.9959) .. (104.3286,23.2724) .. controls (113.1425,-15.9325) and (101.8773,49.6572) .. (138.4988,13.8484) .. controls (141.5713,17.0755) and (121.2714,39.5689) .. (139.2365,36.1290) .. controls (136.0271,66.8833) and (155.4874,23.7659) .. (155.4874,23.7659) .. controls (155.4874,23.7659) and (151.7518,45.0675) .. (147.8705,54.3986) .. controls (147.8705,54.3986) and (173.2987,55.3029) .. (176.7812,61.7670) .. controls (124.2612,69.4624) and (206.6010,77.2343) .. (154.2298,77.2507) .. controls (158.6628,83.1536) and (135.1295,89.9115) .. (169.9992,90.5427) .. controls (93.3592,92.0258) and (132.6373,57.7941) .. (94.5654,45.5826) .. controls (60.1628,40.4228) and (57.2813,64.7295) .. (51.9497,70.3679) .. controls (53.2460,53.9344) and (41.0100,59.0530) .. (24.7738,59.7679) -- cycle;\r\n    \t\r\n    \t\\draw[line width=5pt,color=#1,line cap=round] (96,80) -- (82,73);\r\n    \t\\draw[line width=5pt,color=#1,line cap=round] (50,69) -- (60,68);\r\n    \r\n    }\r\n    \r\n    \\newcommand{\\addbook}[2]{\r\n    \t\t\\path[fill=#1,rotate=20] (110,120) rectangle (150,180);\r\n    \t\t\\node[rotate=-20, color=white] at (73,180)  {\\makebox[2cm][c]{#2}};\r\n    }\r\n    \r\n    \\newcommand{\\addwater}[1]{\r\n    \t\\draw [decorate,decoration=snake, line width=3pt, color=#1] (0,200) -- (100,200);\r\n    \t\\draw [decorate,decoration=snake, line width=3pt, color=#1] (180,200) -- (220,200);\r\n    \t\\draw [decorate,decoration=snake, line width=3pt, color=#1] (50,210) -- (150,210);\r\n    \t\\draw [decorate,decoration=snake, line width=3pt, color=#1] (110,230) -- (250,230);\r\n    \t\\draw [decorate,decoration=snake, line width=3pt, color=#1] (20,240) -- (70,240);\r\n    \t\\draw [decorate,decoration=snake, line width=3pt, color=#1] (60,250) -- (250,250);\r\n    }\r\n    \r\n    \r\n    \\begin{document}\r\n    \r\n    % grumpy\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\grumpyduck{Gold}\r\n    \\end{tikzpicture}\r\n    %\r\n    %Christian Hupfer\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \t\\addwizzard\r\n    \\end{tikzpicture}\r\n    %\r\n    % short hairs\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Wheat}\r\n    \t\\addtshirt{LightBlue!50!white}\r\n    \t\\addshirt{LightSlateGrey}\r\n    \t\\addshorthair{brown!50!Grey}\r\n    \\end{tikzpicture}\r\n    %\r\n    %Men in black\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\grumpyduck{Wheat}\r\n    \t\\addtshirt{white}\r\n    \t\\addshirt{black}\r\n    \t\\addtie{black}\r\n    \t\\addhat{black}\r\n    \t\\addsunglasses\r\n    \\end{tikzpicture}\r\n    %\r\n    \r\n    % ducklings\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-0.6, xscale=0.6, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \t\\begin{scope}[xshift=300pt, scale=.3, yshift=400pt]\r\n    \t\t\\duck{Gold}\r\n    \t\\end{scope}\r\n    \t\\begin{scope}[xshift=180pt, scale=.3, yshift=350pt]\r\n    \t\t\\duck{Gold}\r\n    \t\\end{scope}\r\n    \t\\begin{scope}[xshift=240pt, scale=.3, yshift=450pt]\r\n    \t\t\\duck{Gold}\r\n    \t\\end{scope}\t\t\r\n    \\end{tikzpicture}\r\n    %\r\n    % samcarter\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Wheat!95!red}\r\n    \t\\addshirt{MidnightBlue}\r\n    \t\\addhair{OrangeRed!50!Brown}\r\n    \\end{tikzpicture}\r\n    %\r\n    % hair\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \t\\addhair{SeaGreen}\r\n    \\end{tikzpicture}\r\n    %\r\n    % unicorn\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Pink}\r\n    \t\\addhair{MediumVioletRed}\r\n    \t\\addunicorn\r\n    \\end{tikzpicture}\r\n    \r\n    % icecream\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \t\\addicecream{Wheat}{Plum}{Chocolate}\r\n    \\end{tikzpicture}\r\n    %\r\n    % sunglasses\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \t\\addsunglasses\r\n    \\end{tikzpicture}\r\n    %\r\n    % normal duck\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \\end{tikzpicture}\r\n    % blue duck\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{SteelBlue}\r\n    \\end{tikzpicture}\r\n    \r\n    % alien duck\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \t\\addalien\r\n    \\end{tikzpicture}\r\n    %\r\n    % hat duck\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \t\\addhat{SaddleBrown}\r\n    \\end{tikzpicture}\r\n    %\r\n    % swimming\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold}\r\n    \t\\addwater{blue!50!cyan} \r\n    \\end{tikzpicture}\r\n    %\r\n    % Brazil colour duck for Paulo\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\definecolor{brazilgreen}{RGB}{0,155,58}\r\n    \t\\definecolor{brazilyellow}{RGB}{254,223,0}\r\n    \t\\definecolor{brazilblue}{RGB}{0,39,118}\r\n    \t\\duck{brazilyellow}\r\n    \t\\addshirt{brazilblue}\r\n    \t\\addshorthair{brazilgreen}\r\n    \\end{tikzpicture}\r\n    \r\n    % prof. van duck\r\n    \\begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]\r\n    \t\\duck{Gold!40!white}\r\n    \t\\makeeinstein{gray!50!white}\r\n    \t\\addglasses{brown!70!black}\r\n    \t\\addbook{brown!70!black}{\\TeX}\r\n    \\end{tikzpicture}\r\n    \r\n    \\end{document}\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/a2J0o.png	2019-12-06 10:39:45.371359+00	4	4	1	347458	0	0	0	2019-12-06 10:39:45.371359+00	\N	> No new ducks will be added to the question, instead this project moved on to become a latex package. Please visit the project repository at https://github.com/samcarter/tikzducks or have a look at https://www.ctan.org/pkg/tikzducks if you want to see new ducks :)	f	f
344	358	233	2017-01-06 20:33:31+00	Here is what I have done till now (to be improved, of course).\r\n\r\n[![enter image description here][1]][1]\r\n\r\nThis is my `tikzducks.sty`:\r\n\r\n    \\NeedsTeXFormat{LaTeX2e}\r\n    \\ProvidesPackage{tikzducks}[2016/01/05 All we need is\\dots ducks!]\r\n    \r\n    \\RequirePackage{tikz}\r\n    \\usetikzlibrary{shapes.geometric,shapes.symbols,positioning,decorations.pathmorphing}\r\n    \\usetikzlibrary{shapes,arrows,decorations.markings}\r\n    \r\n    \\RequirePackage{halloweenmath}\r\n    \r\n    \\tikzset{%\r\n    \tpics/wing/.style={code={%\r\n    \t\t\\draw[fill=yellow] (0,0) to[bend right] (1,0) to[bend left] (.7,-.6) to[bend left] cycle;\r\n    \t}}, \r\n    \tpics/eye/.style={code={%\r\n    \t\t\\draw[fill=white] (-.1,-.05) ellipse (.3 and .2);\r\n    \t\t\\draw[fill=black] (0,0) circle[radius=.1];\r\n    \t}},\r\n    \tpics/paw/.style={code={%\r\n    \t\t\\draw[fill=orange] (0,0) -- (0,.8) -- (.1,.8) -- (.1,.05) -- (.8,.05) to[bend right] (.7,-.2) to[bend right] (.6,-.45) -- cycle;\r\n    \t}},\r\n    \tpics/body/.style={code={%\r\n    \t\t\\draw[fill=yellow] (0,0) to[bend right] (.6,-1.2) to[bend right] (2,-2.4) to[bend right=70, smooth] (3.7,-.1) to[bend left] cycle;\r\n    \t}},\r\n    \tpics/head/.style={code={%\r\n    \t\t\\draw[fill=yellow] (0,0) circle[radius=.9];\r\n    \t}},\r\n    \tpics/closedbeak/.style={code={%\r\n    \t\t\\draw[fill=orange] (0,0) -- (-.35,.1) -- (-.65,-.3) -- (-.1,-.7) to[bend right=70] (.2,-.5) -- cycle;\r\n    \t}},\r\n    \tpics/openbeak/.style={code={%\r\n    \t\t\\draw[fill=orange] (0,0) -- (.5,.35) to[bend left=70] (.55,.25) -- (.23,-.29) -- (.3,-.7) to[bend left=70] (.25,-.75) -- (-.01,-.45) -- cycle;\r\n    \t}},\r\n    \tcreate duck coordinates/.style = {insert path={(0,0) coordinate (duck)}},\r\n    \tpics/duck/.style={code={%\r\n    \t\t\\pic[rotate=30] at (.5,-1.8) {paw};\r\n    \t\t\\pic at (-2.5,1) {body};\r\n    \t\t\\pic at (-1,0) {wing};\r\n    \t\t\\pic at (1.5,1) {head};\r\n    \t\t\\pic at (2.2,.8) {closedbeak};\r\n    \t\t\\pic[rotate=300] at (1.6,.9) {eye};\r\n    \t\t\\pic[xscale=1,yscale=-1,rotate=75] at (1.91,1.08) {eye};\r\n    \t\t\\path (-1,-1.8) coordinate (paw);\r\n    \t\t\\pic[rotate=330] at (paw) {paw};\r\n    \t}},\r\n    \tcreate duckwithopenbeak coordinates/.style = {insert path={(0,0) coordinate (duckwithopenbeak)}},\r\n    \tpics/duckwithopenbeak/.style={code={%\r\n    \t\t\\pic[rotate=30] at (.5,-1.8) {paw};\r\n    \t\t\\pic at (-2.5,1) {body};\r\n    \t\t\\pic at (-1,0) {wing};\r\n    \t\t\\pic at (1.5,1) {head};\r\n    \t\t\\pic[rotate=300] at (1.94,1.06) {eye};\r\n    \t\t\\pic[rotate=-20] at (2.4,.9) {openbeak};\r\n    \t\t\\path (-1,-1.8) coordinate (paw);\r\n    \t\t\\pic[rotate=330] at (paw) {paw};\r\n    \t}},\r\n    \tcreate bow coordinates/.style = {insert path={(1.3,1.8) coordinate (bow)}},\r\n    \tpics/bow/.style={code={%\r\n    \t\t\\draw[red, fill=pink, rounded corners] (0,0) -- (.7,.55) -- (.7,-.55) -- cycle;\r\n    \t\t\\draw[red, fill=pink, rounded corners] (0,0) -- (-.7,.55) -- (-.7,-.55) -- cycle;\r\n    \t\t\\draw[red, fill=pink] (0,0) circle [radius=1/4];\r\n    \t}},\r\n    \tpics/daisy/.style={code={%\r\n    \t\t\\foreach \\i in {0,45,...,325}\r\n    \t\t\t\\draw[blue, fill=white, rotate=\\i]\r\n    \t\t\t\t(0,0) .. controls ++(-30:1/2) and ++(30:1/2) .. cycle;\r\n    \t\t\t\\draw[orange, fill=yellow] circle [radius=1/8];\r\n    \t}},\r\n    \tcreate nacklace coordinates/.style = {insert path={(.6,.7) coordinate (nacklace)}},\r\n    \tpics/nacklace/.style={code={%\r\n    \t\t\\path (0,0) coordinate (nacklacebase);\r\n    \t\t\\pic at (nacklacebase) {daisy};\r\n    \t\t\\pic[below right=1em and 1em of nacklacebase] {daisy};\r\n    \t\t\\pic[below right=2em and 2em of nacklacebase] {daisy};\r\n    \t}},\r\n    \tcreate blackboard coordinates/.style = {insert path={(3.5,1.8) coordinate (blackboard)}},\r\n    \tpics/blackboard/.style={code={%\r\n    \t\t\\node at (0,0) [rectangle, fill=black, draw=brown, very thick,font={\\color{white}\\bfseries\\small},minimum height=10ex] {$ax^2+bx+c=0$};\r\n    \t}},\r\n    \tcreate glasses coordinates/.style = {insert path={(1.5,.8) coordinate (glasses)}},\r\n    \tpics/glasses/.style={code={%\r\n    \t\t\\node [circle, draw=blue, very thick,minimum height=1.1em] (rglass) {};\r\n    \t\t\\node[right=.5em of rglass, circle, draw=blue, very thick,minimum height=1.1em] (lglass) {};\r\n    \t\t\\coordinate[above left=.1em and .9em of rglass] (rrglass);\r\n    \t\t\\coordinate[below left=.5em and .1em of rrglass] (rrrglass);\r\n    \t\t\\coordinate[above right=.1em and .9em of lglass] (llglass);\r\n    \t\t\\coordinate[below right=.5em and .1em of llglass] (lllglass);\r\n    \t\t\\draw[draw=blue, very thick] (rglass) -- (lglass);\r\n    \t\t\\draw[draw=blue, very thick] (rglass) -- (rrglass) -- (rrrglass);\r\n    \t\t\\draw[draw=blue, very thick] (lglass) -- (llglass) -- (lllglass);\r\n    \t}},\r\n    \tcreate soccerball coordinates/.style = {insert path={(2.6,-1.1) coordinate (soccerball)}},\r\n    \tpics/soccerball/.style={code={%\r\n    \t\t\\draw[fill=white] (0,0) circle [radius=.9];\r\n    \t\t\\begin{scope}\r\n    \t\t\t\\clip (0,0) circle [radius=.9];\r\n    \t\t\t\\foreach \\i in {-.4,.4}\r\n    \t\t\t\\node[regular polygon,regular polygon sides=6,fill=black,minimum height=3.5ex] at (\\i,.4) {};\r\n    \t\t\t\\foreach \\i in {-.8,0,.8} \r\n    \t\t\t\\node[regular polygon,regular polygon sides=6,fill=black,minimum height=3.5ex] at (\\i,-.4) {};\r\n    \t\t\\end{scope}\r\n    \t}},\r\n    \tcreate befana coordinates/.style = {insert path={(0,0) coordinate (befana)}},\r\n    \tpics/befana/.style={code={%\r\n    \t\t\\node {$\\displaystyle\\mathwitch$};\r\n    \t}},\r\n    \tcreate ghost coordinates/.style = {insert path={(-.8,-.12) coordinate (ghost)}},\r\n    \tpics/ghost/.style={code={%\r\n    \t\t\\node {$\\xrightswishingghost{\\mspace{30mu}}$};\r\n    \t}},\r\n    \tcreate help coordinates/.style = {insert path={(4.6,2.3) coordinate (help)}},\r\n    \tpics/help/.style={code={%\r\n    \t\t\\node[draw, fill=white, shape=starburst, text width=5.7em, text centered] (helpnode) {%\r\n    \t\t\t\tHelp! \\\\ A \\texttt{\\textbackslash phantom}!\r\n    \t\t\t};\r\n    \t\t\\coordinate[below left=-.1em and -0.3em of helpnode] (lhelpnode);\r\n    \t\t\\coordinate[below right=0.35em and -1.5em of helpnode] (rhelpnode);\r\n    \t\t\\coordinate[below left=2.3em and 1.1em of helpnode] (bhelpnode);\r\n    \t\t\\coordinate[below left=-.3em and -0.4em of helpnode] (llhelpnode);\r\n    \t\t\\coordinate[below right=0em and -.9em of helpnode] (rrhelpnode);\r\n    \t\t\\path[fill=white] (llhelpnode) -- (bhelpnode) -- (rrhelpnode);  \r\n    \t\t\\draw[fill=white] (lhelpnode) -- (bhelpnode) -- (rhelpnode);\r\n    \t}},\r\n    \tcreate righttooth coordinates/.style = {insert path={(2.11,.1) coordinate (righttooth)}},\r\n    \tcreate lefttooth coordinates/.style = {insert path={(2.29,.15) coordinate (lefttooth)}},\r\n    \tpics/tooth/.style={code={%\r\n    \t\t\\draw[fill=white] (0,0) -- (.05,-.2) -- (.11,.1);\r\n    \t}},\r\n    \tcreate cloak coordinates/.style = {insert path={(.65,.69) coordinate (cloak)}},\r\n    \tpics/cloak/.style={code={%\r\n    \t\t\\draw[white,fill=black] (0,0) to[bend left=10] (-2.1,.1) \r\n    \t\t\tdecorate [decoration={snake, segment length=10pt, amplitude=3pt}] \r\n    \t\t\t{to  (-2,-1.4)}   to[bend left=9]  (.3,-.4)  to[bend left=30] cycle;\r\n    \t\t\\draw[white,fill=black] (0,0) to[bend left=10] (-.1,.7) to[bend right=15] (.7,-.3) -- (.3,-.4) to[bend left=30] cycle;\r\n    \t}},\r\n    \tpics/batflock/.style={code={%\r\n    \t\\path[postaction={decorate,decoration={markings,\r\n    \t\t\tmark=at position 0.01 with {\\node[scale=0.4,rotate=-50] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.04 with {\\node[scale=0.5,rotate=-40] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.07 with {\\node[scale=0.6,rotate=-35] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.11 with {\\node[scale=0.7,rotate=-30] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.15 with {\\node[scale=0.8,rotate=-20] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.20 with {\\node[scale=0.9,rotate=-10] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.25 with {\\node[scale=1.0,rotate=5] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.31 with {\\node[scale=1.1,rotate=15] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.38 with {\\node[scale=1.2,rotate=15] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.44 with {\\node[scale=1.3,rotate=0] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.50 with {\\node[scale=1.4,rotate=-15] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.57 with {\\node[scale=1.5,rotate=-25] {$\\mathbat$};},\r\n    \t\t\tmark=at position 0.65 with {\\node[scale=1.6,rotate=-30] {$\\mathbat$};},\t\r\n    \t\t\tmark=at position 0.73 with {\\node[scale=1.7,rotate=-35] {$\\mathbat$};},\t\r\n    \t\t\tmark=at position 0.81 with {\\node[scale=1.8,rotate=-40] {$\\mathbat$};},\t\t\r\n    \t\t\tmark=at position 0.89 with {\\node[scale=1.9,rotate=-42] {$\\mathbat$};},\t\t\r\n    \t\t}}] plot[smooth, tension=1] coordinates {(0,0) (2.5,1) (1,3) (7,4.5)};\r\n    \t\\path[postaction={decorate,decoration={markings,\r\n    \t\t\tmark=at position 0.01 with {\\node[scale=0.4,rotate=10] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.06 with {\\node[scale=0.5,rotate=10] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.11 with {\\node[scale=0.6,rotate=10] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.17 with {\\node[scale=0.7,rotate=10] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.23 with {\\node[scale=0.8,rotate=5] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.30 with {\\node[scale=0.9,rotate=0] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.38 with {\\node[scale=1.0,rotate=-10] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.48 with {\\node[scale=1.1,rotate=-20] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.58 with {\\node[scale=1.2,rotate=-30] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.68 with {\\node[scale=1.3,rotate=-40] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.79 with {\\node[scale=1.4,rotate=-41] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.90 with {\\node[scale=1.5,rotate=-42] {$\\xleftflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t}}] plot[smooth, tension=1] coordinates {(1.7,1.3) (0,2.7) (2,4.5) (5,5.5)};\r\n    \t\\path[postaction={decorate,decoration={markings,\r\n    \t\t\tmark=at position 0.16 with {\\node[scale=0.4,rotate=-5] {$\\xrightflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.25 with {\\node[scale=0.5,rotate=-25] {$\\xrightflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.34 with {\\node[scale=0.6,rotate=-36] {$\\xrightflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.44 with {\\node[scale=0.7,rotate=-38] {$\\xrightflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.55 with {\\node[scale=0.8,rotate=-40] {$\\xrightflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.67 with {\\node[scale=0.9,rotate=-42] {$\\xrightflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.81 with {\\node[scale=1.0,rotate=-46] {$\\xrightflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t\tmark=at position 0.95 with {\\node[scale=1.1,rotate=-50] {$\\xrightflutteringbat{\\mspace{30mu}}$};},\r\n    \t\t}}] plot[smooth, tension=1] coordinates {(2.5,2) (2,2.7) (6.5,3.3)};\r\n    \t}},\r\n    }\r\n    \r\n    \\newcommand{\\duck}{%\r\n    \t\\begin{tikzpicture}\r\n    \t\t\\pic {duck};\r\n    \t\\end{tikzpicture}\r\n    }\r\n    \r\n    \\newcommand{\\daisyduck}{%\r\n    \t\\begin{tikzpicture}\r\n    \t\t\\pic {duck};\r\n    \t\t\\path[create nacklace coordinates];\r\n    \t\t\\pic at (nacklace) {nacklace};  \r\n    \t\\end{tikzpicture}\r\n    }\r\n    \r\n    \\newcommand{\\femaleduck}{%\r\n    \t\\begin{tikzpicture}\r\n    \t\t\\pic {duck};\r\n    \t\t\\path[create bow coordinates];\r\n    \t\t\\pic[rotate=35] at (bow) {bow}; \r\n    \t\\end{tikzpicture}\r\n    }\r\n    \r\n    \\newcommand{\\mathduck}{%\r\n    \t\\begin{tikzpicture}\r\n    \t\t\\path[create blackboard coordinates];\r\n    \t\t\\pic at (blackboard) {blackboard};\r\n    \t\t\\pic {duck};\r\n    \t\t\\path[create glasses coordinates];\r\n    \t\t\\pic[rotate=30, transform shape] at (glasses) {glasses};\r\n    \t\\end{tikzpicture}\r\n    }\r\n    \r\n    \\newcommand{\\soccerduck}{%\r\n    \t\\begin{tikzpicture}\r\n    \t\t\\pic {duck};\r\n    \t\t\\path[create soccerball coordinates];\r\n    \t\t\\pic[rotate=30] at (soccerball) {soccerball};\r\n    \t\\end{tikzpicture}\r\n    }\r\n    \r\n    \\newcommand{\\befanaduck}{%\r\n    \t\\begin{tikzpicture}\r\n    \t\t\\path[use as bounding box] (-2.5,-1.5) rectangle (2.5,1.5);  \r\n    \t\t\\pic[scale=.45,transform canvas={scale=.45},transform shape,rotate=20] at (3.17,1.59) {duck};\r\n    \t\t\\path[create befana coordinates];\r\n    \t\t\\pic[transform canvas={scale=4}] at (befana) {befana};\r\n    \t\t\\pic[rotate=330,scale=.45,transform canvas={scale=.45},transform shape,rotate=20] at (paw) {paw};\r\n    \t\\end{tikzpicture}\r\n    }\r\n    \r\n    \\newcommand{\\ghostduck}{%\r\n    \t\\begin{tikzpicture}\r\n    \t\t\\path[use as bounding box] (-6,-2.5) rectangle (6,2.5);  \r\n    \t\t\\path[create ghost coordinates];\r\n    \t\t\\pic[transform canvas={scale=7}] at (ghost) {ghost};\r\n    \t\t\\pic {duckwithopenbeak};\r\n    \t\t\\path[create help coordinates];\r\n    \t\t\\pic at (help) {help};\r\n    \t\\end{tikzpicture}\r\n    }\r\n    \r\n    \\newcommand{\\vampireduck}{%\r\n    \t\\begin{tikzpicture}\r\n    \t\t\\pic at (-6,-1) {batflock};\r\n    \t\t\\path[create righttooth coordinates];\r\n    \t\t\\pic at (righttooth) {tooth};\r\n    \t\t\\path[create lefttooth coordinates];\r\n    \t\t\\pic at (lefttooth) {tooth};\r\n    \t\t\\pic {duck};\r\n    \t\t\\path[create cloak coordinates];\r\n    \t\t\\pic at (cloak) {cloak};\r\n    \t\t\\pic at (-1,0) {wing};\r\n    \t\\end{tikzpicture}\r\n    }\r\n\r\nThis is the code of the presentation:\r\n\r\n    % arara: pdflatex\r\n    % arara: convert: {density: 160, otheroptions: -dispose previous -delay 140 -loop 1, format: gif}\r\n    \\documentclass{beamer}\r\n    \\usepackage{cprotect}\r\n    \\usepackage{tikzducks}\r\n    \r\n    \\usetheme{Madrid}\r\n    \\setbeamertemplate{navigation symbols}{}\r\n    \\cprotect\\title{Quack guide to \\verb|tikzducks| package}\r\n    \\subtitle{Actually, some examples to be improved and transformed into a package}\r\n    \\author{Car\\LaTeX}\r\n    \\institute{Duck fan club}\r\n    \r\n    \\begin{document}\r\n    \\begin{frame}[plain]\r\n    \t\\titlepage\r\n    \\end{frame}\r\n    \\begin{frame}[fragile=singleslide]\r\n    \t\\frametitle{Standard version}\r\n    \tCommand: \r\n    \t\\begin{center}\r\n    \t\t\\verb|\\duck|\r\n    \t\\end{center}\r\n    \tResult:\r\n    \t\\begin{figure}\r\n    \t\t\\centering\r\n    \t\t\\duck   \r\n    \t\\end{figure}\r\n    \\end{frame}\r\n    \\begin{frame}[fragile=singleslide]\r\n    \t\\frametitle{Female version}\r\n    \tCommand: \r\n    \t\\begin{center}\r\n    \t\\verb|\\femaleduck|\r\n    \t\\end{center}\r\n    \tResult:\r\n    \t\\begin{figure}\r\n    \t\\centering\r\n    \t\\femaleduck\r\n    \t\\end{figure}\r\n    \tAccessory \\verb|pic| available: \\verb|bow| \r\n    \t\\begin{tikzpicture}\r\n    \t\\pic[scale=.7, transform canvas={scale=.7}] at (.6,.2) {bow};\r\n    \t\\end{tikzpicture}\r\n    \\end{frame}\r\n    \\begin{frame}[fragile=singleslide]\r\n    \t\\frametitle{Daisy version}\r\n    \tCommand: \r\n    \t\\begin{center}\r\n    \t\\verb|\\daisyduck|\r\n    \t\\end{center}\r\n    \tResult:\r\n    \t\\begin{figure}\r\n    \t\\centering\r\n    \t\\daisyduck\r\n    \t\\end{figure}\r\n    \tAccessory \\verb|pic|s available: \\verb|daisy| \r\n    \t\\begin{tikzpicture}\r\n    \t\\pic[scale=.75, transform canvas={scale=.75}] at (.35,.25) {daisy};\r\n    \t\\end{tikzpicture} \r\n    \t\\hspace{1em} and \\verb|nacklace| \r\n    \t\\begin{tikzpicture}\r\n    \t\\pic[scale=.5, transform canvas={scale=.5}] at (.5,1) {nacklace};\r\n    \t\\end{tikzpicture}\\\\\r\n    \\end{frame}\r\n    \\begin{frame}[fragile=singleslide]\r\n    \t\\frametitle{Mathematician version}\r\n    \tCommand: \r\n    \t\\begin{center}\r\n    \t\\verb|\\mathduck|\r\n    \t\\end{center}\r\n    \tResult:\r\n    \t\\begin{figure}\r\n    \t\\centering\r\n    \t\\mathduck\r\n    \t\\end{figure}\r\n    \tAccessory \\verb|pic|s available:\r\n    \t\\verb|glasses| \r\n    \t\\begin{tikzpicture}\r\n    \t\\pic[scale=.5, transform canvas={scale=.5}] at (.7,.2) {glasses};\r\n    \t\\end{tikzpicture}\r\n    \t\\hspace{2.4em} and \\verb|blackboard| \r\n    \t\\begin{tikzpicture}\r\n    \t\\pic[scale=.4, transform canvas={scale=.4}] at (1.4,.5) {blackboard};\r\n    \t\\end{tikzpicture}\\\\\r\n    \\end{frame}\r\n    \\begin{frame}[fragile=singleslide]\r\n    \t\\frametitle{Soccer player version}\r\n    \tCommand: \r\n    \t\\begin{center}\r\n    \t\\verb|\\soccerduck|\r\n    \t\\end{center}\r\n    \tResult:\r\n    \t\\begin{figure}\r\n    \t\\centering\r\n    \t\\soccerduck\r\n    \t\\end{figure}\r\n    \tAccessory \\verb|pic| available: \\verb|soccerball| \r\n    \t\\begin{tikzpicture}\r\n    \t\\pic[scale=.5,transform canvas={scale=.5},transform shape] at (.5,.3) {soccerball};\r\n    \t\\end{tikzpicture}\\\\ \r\n    \\end{frame}\r\n    \\begin{frame}[fragile=singleslide]\r\n    \t\\frametitle{The Befana duck\\dots}\r\n    \tUsing \\verb|\\displaystyle\\mathwitch| $\\displaystyle\\mathwitch$ from the gorgeous Gustavo Mezzetti's package \\verb|halloweenmath|, here is the Befana\\footnote{For anyone who doesn't know who the Befana is, she's a sort of female version of Santa Claus, who brings gifts or coal (to good and bad children, respectively) every year on January, 6\\textsuperscript{th}.} duck!\r\n    \t\r\n    \tCommand: \r\n    \t\\begin{center}\r\n    \t\\verb|\\befanaduck|\r\n    \t\\end{center}\r\n    \tResult:\r\n    \t\\begin{figure}\r\n    \t\\centering\r\n    \t\\befanaduck\r\n    \t\\end{figure}\r\n    \\end{frame}\r\n    \\begin{frame}[fragile=singleslide]\r\n    \t\\frametitle{\\dots the ghost duck\\dots}\r\n    \tUsing \\verb|\\xrightswishingghost| $\\xrightswishingghost{}$, again from \\verb|halloweenmath| package, here is the ghost duck!\\\\\r\n    \tCommand: \r\n    \t\\begin{center}\r\n    \t\\verb|\\ghostduck|\r\n    \t\\end{center}\r\n    \tResult:\r\n    \t\\begin{figure}\r\n    \t\\centering\r\n    \t\\resizebox{0.7\\textwidth}{!}{%\r\n    \t\\ghostduck\r\n    \t}\r\n    \t\\end{figure}\r\n    \\end{frame}\r\n    \\begin{frame}[fragile=singleslide]\r\n    \t\\frametitle{\\dots and the vampire duck!}\r\n    \tThe vampire duck, with bats from \\verb|halloweenmath| package!\r\n    \t\r\n    \tCommand: \r\n    \t\\begin{center}\r\n    \t\\verb|\\vampireduck|\r\n    \t\\end{center}\r\n    \tResult:\r\n    \t\\begin{figure}\r\n    \t\\centering\r\n    \t\\resizebox{0.5\\textwidth}{!}{%\r\n    \t\\vampireduck\r\n    \t}\r\n    \t\\end{figure}\r\n    \\end{frame}\r\n    \\end{document}\r\n\r\n  [1]: https://i.stack.imgur.com/9H2aj.gif	2019-12-06 10:39:45.984062+00	2	4	1	347474	0	0	0	2019-12-06 10:39:45.984062+00	\N	Here is what I have done till now (to be improved, of course).	f	f
343	358	252	2017-01-02 13:30:16+00	    \\documentclass{article}\r\n    \\usepackage[T1]{fontenc}\r\n    \\usepackage[utf8]{inputenc}\r\n    \\usepackage{tikzpeople}\r\n    \\begin{document}\r\n    \\begin{tikzpicture}\r\n    \\node[duck,evil,minimum size=1.5cm] (B){};\r\n    \\end{tikzpicture}\r\n    \\end{document}\r\n\r\n[![enter image description here][1]][1]\r\n\r\nWithout code ...\r\n\r\n[![enter image description here][2]][2]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/D2Aey.png\r\n  [2]: https://i.stack.imgur.com/yzWa1.png	2019-12-06 10:39:45.637274+00	2	4	1	346697	0	0	0	2019-12-06 10:39:45.637274+00	\N	\\documentclass{article}	f	f
14	27	2	2019-11-13 13:08:32.129063+00	As of 7th Jan 2020, the complete source code is [available on GitHub](https://github.com/topanswers/topanswers).\r\n\r\n---\r\n\r\n### original answer (13th Nov 2019):\r\n\r\nThe short answer is 'yes' and 'nowhere' — yet.\r\n\r\n> Is the source code of TopAnswers open?\r\n\r\nWe have made a commitment that [as much as possible of the platform will be open-source and publically available](https://topanswers.xyz/meta?q=1). Before releasing the code [I'd like to get feedback on what license(s) to release it under](/meta?q=280).\r\n\r\n> If so, where is it accessible?\r\n\r\nLocally, [we are using git](https://topanswers.xyz/transcript?room=2&id=39&year=2019&month=10#c39). When we do release the code we could self-host it or use some external platform like GitHub	2020-01-07 14:18:00.969874+00	6	1	1	\N	0	0	0	\N	\N	As of 7th Jan 2020, the complete source code is [available on GitHub](https://github.com/topanswers/topanswers).	f	f
777	342	2	2020-02-10 15:30:13.642017+00	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/13) so work can begin, and will update the post here when it is complete.	2020-02-10 15:30:13.642017+00	0	1	1	\N	0	0	0	\N	\N	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/13) so work can begin, and will update the post here when it is complete.	t	f
213	264	12	2019-07-30 01:11:17+00	The single row insert acquires an `X` (exclusive) lock on the new row.\r\n\r\nThe `SELECT` attempts to acquire a range-shared, key shared (`RangeS-S`) lock.\r\n\r\nThis request is reported by the `lock_acquired` Extended Event as mode = `RS_S`.\r\n\r\nIt is reported by the Profiler event class `Lock:Acquired` as mode 13 (`LCK_M_RS_S`).\r\n\r\nThe requested mode is combined with the existing *exclusive* lock mode in `Lock::CalculateGrantMode` in `sqlmin.dll`. There is no combined mode of range-shared, key exclusive (`RangeS-X`) so the outcome of the calculation is range-exclusive, key exclusive (`RangeX-X`), which happens to be mode 15.\r\n\r\nThe grant mode calculation above is performed just before the extended event is generated by `lck_ProduceExtendedEvent<XeSqlPkg::lock_acquired>`. Nevertheless, both Profiler and Extended Events log the *requested* `RangeS-S` mode, not the resulting lock mode `RangeX-X`. This is counter to the limited [documentation][1], which says:\r\n\r\n>Mode | **int** | *Resulting mode after the lock was acquired.*\r\n\r\nThe *mode* column of the extended event has no documentation at all, and the description in the meta data is blank. Perhaps Microsoft themselves weren't even sure of the behaviour.\r\n\r\nI have often thought it would be more useful if lock events reported both the *requested* and *resulting* modes, but that is not what we have. The current arrangement makes it pretty much impossible to track and match up lock acquisition and release.\r\n\r\nThere *might* be a good reason for reporting locks this way. If it doesn't meet your needs, you could open a support case with Microsoft, or create an Azure Feedback item.\r\n\r\n---\r\n\r\n### `LAST_MODE`\r\n\r\nThe mysterious `LAST_MODE` is something Erik Darling has [remarked on before][2]. It is the highest `map_key` value in the list of lock modes exposed by [`sys.dm_xe_map_values`][3]:\r\n\r\n```\r\nSELECT\r\n    DXMV.map_key,\r\n    DXMV.map_value\r\nFROM sys.dm_xe_map_values AS DXMV\r\nWHERE \r\n    DXMV.[name] = N'lock_mode'\r\nORDER BY\r\n    DXMV.map_key;\r\n```\r\n\r\n```none\r\n╔═════════╦═══════════╗\r\n║ map_key ║ map_value ║\r\n╠═════════╬═══════════╣\r\n║       0 ║ NL        ║\r\n║       1 ║ SCH_S     ║\r\n║       2 ║ SCH_M     ║\r\n║       3 ║ S         ║\r\n║       4 ║ U         ║\r\n║       5 ║ X         ║\r\n║       6 ║ IS        ║\r\n║       7 ║ IU        ║\r\n║       8 ║ IX        ║\r\n║       9 ║ SIU       ║\r\n║      10 ║ SIX       ║\r\n║      11 ║ UIX       ║\r\n║      12 ║ BU        ║\r\n║      13 ║ RS_S      ║\r\n║      14 ║ RS_U      ║\r\n║      15 ║ RI_NL     ║\r\n║      16 ║ RI_S      ║\r\n║      17 ║ RI_U      ║\r\n║      18 ║ RI_X      ║\r\n║      19 ║ RX_S      ║\r\n║      20 ║ RX_U      ║\r\n║      21 ║ LAST_MODE ║\r\n╚═════════╩═══════════╝\r\n```\r\n\r\nThe memory structure accessed via the DMV (using `sqlmin!CMapValuesTable`) is stored starting at the address `sqlmin!XeSqlPkg::g_lock_mode`. Each 16-byte entry in the structure contains the `map_key` and a pointer to the string returned as `map_value` by the streaming TVF.\r\n\r\nThe strings are stored exactly as shown in the table above (though not in that order). It seems to be an error that entry 21 has a `map_value` of "LAST_MODE" instead of the expected "RX_X". Erik Darling has [reported the issue on Azure Feedback][4].\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/event-classes/lock-acquired-event-class\r\n  [2]: https://www.brentozar.com/archive/2018/09/quirks-when-working-with-extended-events-to-track-locks/\r\n  [3]: https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-xe-map-values-transact-sql?view=sql-server-2017\r\n  [4]: https://feedback.azure.com/forums/908035-sql-server/suggestions/35166271-last-mode-should-map-to-lck-m-rx-x-in-the-lock-mod	2019-12-04 12:21:13.650855+00	1	4	1	244084	0	0	0	2019-12-04 12:21:13.650855+00	\N	The single row insert acquires an `X` (exclusive) lock on the new row.	f	f
145	201	12	2017-10-01 06:21:30+00	**Yes** if you:\r\n\r\n* are running SQL Server 2014 or later; and\r\n* are able to run the query with **trace flag 176** active; and\r\n* the computed column is `PERSISTED`\r\n\r\nSpecifically, at least the following versions [are required][1]:\r\n\r\n* Cumulative Update 2 for SQL Server 2016 SP1\r\n* Cumulative Update 4 for SQL Server 2016 RTM\r\n* Cumulative Update 6 for SQL Server 2014 SP2\r\n\r\n**BUT** to avoid a bug (ref for [2014][2], and for [2016 and 2017][3]) introduced in those fixes, instead apply:\r\n\r\n* [Cumulative Update 1 for SQL Server 2017][4]\r\n* [Cumulative Update 5 for SQL Server 2016 SP1][5]\r\n* [Cumulative Update 8 for SQL Server 2016 RTM][6]\r\n* [Cumulative Update 8 for SQL Server 2014 SP2][7]\r\n\r\nThe trace flag is effective as a start-up `–T` option, at both global and session scope using `DBCC TRACEON`, and per query with `OPTION (QUERYTRACEON)` or a plan guide.\r\n\r\nTrace flag 176 prevents persisted computed column expansion.\r\n\r\nThe initial metadata load performed when compiling a query brings in all columns, not just those directly referenced. This makes all computed column definitions available for matching, which is generally a good thing.\r\n\r\nAs an unfortunate side-effect, if one of the loaded (computed) columns uses a scalar user-defined function, its presence **disables parallelism** for the whole query, **even when** the computed column is **not actually used**.\r\n\r\nTrace flag 176 helps with this, if the column is persisted, by not loading the definition (since expansion is skipped). This way, a scalar user-defined function is never present in the compiling query tree, so parallelism is not disabled.\r\n\r\nThe main drawback of trace flag 176 (aside from being only lightly documented) is that it also prevents query expression matching to persisted computed columns: If the query contains an expression matching a persisted computed column, trace flag 176 will prevent the expression being replaced by a reference to the computed column.\r\n\r\nFor more details, see my SQLPerformance.com article, [Properly Persisted Computed Columns][8].\r\n\r\nSince the question mentions XML, as an alternative to promoting values using a computed column and scalar function, you could also look at using a Selective XML Index, as you wrote about in [Selective XML Indexes: Not Bad At All][9].\r\n\r\n\r\n  [1]: https://support.microsoft.com/en-nz/help/3213683\r\n  [2]: https://support.microsoft.com/en-us/help/4043951/fix-warning-and-incorrect-computed-column-results-after-applying-hotfi\r\n  [3]: https://support.microsoft.com/en-us/help/4040533/fix-returns-incorrect-results-when-computed-column-is-queried-after-in\r\n  [4]: https://support.microsoft.com/en-us/help/4038634\r\n  [5]: https://support.microsoft.com/en-us/help/4040714\r\n  [6]: https://support.microsoft.com/en-us/help/4040713\r\n  [7]: https://support.microsoft.com/en-us/help/4037356\r\n  [8]: https://sqlperformance.com/2017/05/sql-plan/properly-persisted-computed-columns\r\n  [9]: https://www.brentozar.com/archive/2016/12/selective-xml-indexes-not-bad/	2019-11-30 07:47:56.355273+00	1	4	1	187370	0	0	0	2019-11-30 07:47:56.355273+00	\N	**Yes** if you:	f	f
688	625	167	2020-01-27 15:23:19.951054+00	For simple cases like text etc. you could use the `shapepar` package to produce the desired cutout.\r\n\r\nA small example to give you something to start with:\r\n\r\n```\r\n\\documentclass{beamer}\r\n\r\n\\usepackage{lipsum}\r\n\\usepackage{shapepar}\r\n\\setlength{\\cutoutsep}{5cm} \r\n\r\n\\setbeamertemplate{logo}{\\parbox{\\paperwidth}{\\includegraphics[width=4cm,height=3cm]{example-image-duck}}}\r\n\r\n\\setbeamertemplate{navigation symbols}{}\r\n\r\n\\newcommand{\\webcam}{\\cutout{l}(-1cm,.9\\paperheight)\\shapepar{\\squareshape}\\textcolor{bg}{.}\\par}\r\n\r\n\\begin{document}\r\n\r\n\\begin{frame}\r\n\\webcam\r\n\\lipsum*[1-2]\r\n\\end{frame}\r\n\r\n\\begin{frame}\r\n\\frametitle{title}\r\n\\webcam\r\n\\lipsum*[1-2]\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n\r\nFor more complicate frames with columns etc. some manual fine tuning will be necessary. \r\n\r\n*(I would prefer if this answer would not be copied to tex.se)*	2020-01-27 15:46:41.899147+00	4	4	3	\N	0	0	0	\N	\N	For simple cases like text etc. you could use the `shapepar` package to produce the desired cutout.	f	f
139	196	12	2017-09-10 20:12:10+00	SQL Server always uses the Split, Sort, and Collapse combination of operators when [maintaining a unique index][1] as part of an update that affects (or might affect) more than one row.\r\n\r\nWorking through the example in the question, we could write the update as a separate single-row update for each of the four rows present:\r\n\r\n    -- Per row updates\r\n    UPDATE dbo.Banana SET pk = 2 WHERE pk = 1;\r\n    UPDATE dbo.Banana SET pk = 3 WHERE pk = 2;\r\n    UPDATE dbo.Banana SET pk = 4 WHERE pk = 3;\r\n    UPDATE dbo.Banana SET pk = 5 WHERE pk = 4;\r\n\r\nThe problem is that the first statement would fail, since it changes `pk` from 1 to 2, and there is already a row where `pk` = 2. The SQL Server storage engine requires that unique indexes remain unique at every stage of processing, even within a single statement. This is the problem solved by Split, Sort, and Collapse.\r\n\r\n### Split [![Split][2]][2]\r\n\r\nThe first step is to Split each update statement into a delete followed by an insert:\r\n\r\n    DELETE dbo.Banana WHERE pk = 1;\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (2, 'A', 'W');\r\n    \r\n    DELETE dbo.Banana WHERE pk = 2;\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (3, 'B', 'X');\r\n    \r\n    DELETE dbo.Banana WHERE pk = 3;\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (4, 'C', 'Y');\r\n    \r\n    DELETE dbo.Banana WHERE pk = 4;\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (5, 'D', 'Z');\r\n\r\nThe Split operator adds an action code column to the stream (here labelled Act1007):\r\n\r\n[![Split properties][3]][3]\r\n\r\nThe action code is 1 for an update, 3 for a delete, and 4 for an insert. \r\n\r\n### Sort [![Sort][4]][4]\r\n\r\nThe split statements above would still produce a false transient unique key violation, so the next step is to sort the statements by the keys of the unique index being updated (`pk` in this case), then by the action code. For this example, this simply means that deletes (3) on the same key are ordered before inserts (4). The resulting order is:\r\n\r\n    -- Sort (pk, action)\r\n    DELETE dbo.Banana WHERE pk = 1;\r\n    DELETE dbo.Banana WHERE pk = 2;\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (2, 'A', 'W');\r\n    DELETE dbo.Banana WHERE pk = 3;\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (3, 'B', 'X');\r\n    DELETE dbo.Banana WHERE pk = 4;\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (4, 'C', 'Y');\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (5, 'D', 'Z');\r\n\r\n[![Sort properties][5]][5]\r\n\r\n### Collapse [![Collapse][6]][6]\r\n\r\nThe preceding stage is enough to guarantee the avoidance of false uniqueness violations in all cases. As an optimization, Collapse combines *adjacent* deletes and inserts *on the same key value* into an update:\r\n\r\n    -- Collapse (pk)\r\n    DELETE dbo.Banana WHERE pk = 1;\r\n    UPDATE dbo.Banana SET c1 = 'A', c2 = 'W' WHERE pk = 2;\r\n    UPDATE dbo.Banana SET c1 = 'B', c2 = 'X' WHERE pk = 3;\r\n    UPDATE dbo.Banana SET c1 = 'C', c2 = 'Y' WHERE pk = 4;\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (5, 'D', 'Z');\r\n\r\nThe delete/insert pairs for `pk` values 2, 3, and 4 have been combined into an update, leaving a single delete on `pk` = 1, and an insert for `pk` = 5.\r\n\r\nThe Collapse operator groups rows by the key columns, and updates the action code to reflect the collapse outcome:\r\n\r\n[![Collapse properties][7]][7]\r\n\r\n### Clustered Index Update [![Clustered index update][8]][8]\r\n\r\nThis operator is labelled as an Update, but it is capable of inserts, updates, and deletes. Which action is taken by the Clustered Index Update per row is determined by the value of the action code in that row. The operator has an Action property to reflect this mode of operation:\r\n\r\n[![Clustered Index Update action property][9]][9]\r\n\r\n---\r\n\r\n## Row modification counters\r\n\r\nNote that the three updates above **do not modify the key(s)** of the unique index being maintained. In effect, we have transformed updates to the *key* columns in the index into updates of the *non-key* columns (`c1` and `c2`), plus a delete and an insert. Neither a delete nor an insert can cause a false unique-key violation.\r\n\r\nAn insert or a delete affects every single column in the row, so statistics associated with every column will have their modification counters incremented. For update(s), only statistics with any of the updated columns as the *leading column* have their modification counters incremented (even if the value is unchanged).\r\n\r\nThe statistics row modification counters therefore show 2 changes to `pk`, and 5 for `c1` and `c2`:\r\n\r\n    -- Collapse (pk)\r\n    DELETE dbo.Banana WHERE pk = 1;                         -- All columns modified\r\n    UPDATE dbo.Banana SET c1 = 'A', c2 = 'W' WHERE pk = 2;  -- c1 and c2 modified\r\n    UPDATE dbo.Banana SET c1 = 'B', c2 = 'X' WHERE pk = 3;  -- c1 and c2 modified\r\n    UPDATE dbo.Banana SET c1 = 'C', c2 = 'Y' WHERE pk = 4;  -- c1 and c2 modified\r\n    INSERT dbo.Banana (pk, c1, c2) VALUES (5, 'D', 'Z');    -- All columns modified\r\n\r\n**Note:** Only changes applied to the **base object** (heap or clustered index) affect statistics row modification counters. Non-clustered indexes are secondary structures, reflecting changes already made to the base object. They do not affect statistics row modification counters at all.\r\n\r\nIf an object has multiple unique indexes, a separate Split, Sort, Collapse combination is used to organize the updates to each. SQL Server optimizes this case for nonclustered indexes by saving the result of the Split to an Eager Table Spool, then replaying that set for each unique index (which will have its own Sort by index keys + action code, and Collapse).\r\n\r\n### Effect on statistics updates\r\n\r\nAutomatic statistics updates (if enabled) occur when the query optimizer needs statistical information and notices that existing statistics are **out of date** (or invalid due to a schema change). Statistics are considered out of date when the number of modifications recorded exceed a threshold.\r\n\r\nThe Split/Sort/Collapse arrangement results in **different** row modifications being recorded than would be expected. This, in turn, means that a statistics update may be triggered sooner or later than would be the case otherwise.\r\n\r\nIn the example above, row modifications for the key column increase by 2 (the net change) rather than 4 (one for each table row affected), or 5 (one for each delete/update/insert produced by the Collapse).\r\n\r\nIn addition, non-key columns that were logically **not changed** by the original query accumulate row modifications, which may number as many as **double** the table rows updated (one for each delete, and one for each insert).\r\n\r\n---\r\n\r\nThe number of changes recorded depend on the degree of overlap between old and new key column values (and so the degree to which the separate deletes and inserts can be collapsed). Resetting the table between each execution, the following queries demonstrate the effect on row modification counters with different overlaps:\r\n\r\n    UPDATE dbo.Banana SET pk = pk + 0; -- Full overlap\r\n\r\n[![pk = pk + 0][10]][10]\r\n\r\n    UPDATE dbo.Banana SET pk = pk + 1;\r\n\r\n[![pk = pk + 1][11]][11]\r\n\r\n    UPDATE dbo.Banana SET pk = pk + 2;\r\n\r\n[![pk = pk + 2][12]][12]\r\n\r\n    UPDATE dbo.Banana SET pk = pk + 3;\r\n\r\n[![pk = pk + 3][13]][13]\r\n\r\n    UPDATE dbo.Banana SET pk = pk + 4; -- No overlap\r\n\r\n[![pk = pk + 4][14]][14]\r\n\r\n\r\n  [1]: https://blogs.msdn.microsoft.com/craigfr/2007/09/06/maintaining-unique-indexes/\r\n  [2]: https://i.stack.imgur.com/mG7va.png\r\n  [3]: https://i.stack.imgur.com/H56Nr.png\r\n  [4]: https://i.stack.imgur.com/8bv3p.png\r\n  [5]: https://i.stack.imgur.com/gIh83.png\r\n  [6]: https://i.stack.imgur.com/HkPZd.png\r\n  [7]: https://i.stack.imgur.com/vLXyG.png\r\n  [8]: https://i.stack.imgur.com/MSNaB.png\r\n  [9]: https://i.stack.imgur.com/WXyKa.png\r\n  [10]: https://i.stack.imgur.com/dMpQV.png "pk = pk + 0"\r\n  [11]: https://i.stack.imgur.com/kLzJr.png "pk = pk + 1"\r\n  [12]: https://i.stack.imgur.com/Q1WXk.png "pk = pk + 2"\r\n  [13]: https://i.stack.imgur.com/Y4iTk.png "pk = pk + 3"\r\n  [14]: https://i.stack.imgur.com/iNeZB.png "pk = pk + 4"	2019-11-29 15:31:57.116543+00	3	4	1	185552	0	0	0	2019-11-29 15:31:57.116543+00	\N	SQL Server always uses the Split, Sort, and Collapse combination of operators when [maintaining a unique index][1] as part of an update that affects (or might affect) more than one row.	f	f
704	632	168	2020-01-29 14:41:58.73224+00	I'd go with a similar reasoning as Adám.\r\n\r\nAn answer is a self-contained section in a document, which's heading is the question, so it should be structured as such. Just because the CSS is big for h1 doesn't mean the markup should be changed. So I'd advocate for `#` as the top-level for an answer.	2020-01-29 14:41:58.73224+00	3	6	3	\N	0	0	0	\N	\N	I'd go with a similar reasoning as Adám.	f	f
699	632	811	2020-01-28 21:52:20.770817+00	Convention on codegolf SE is `#`, I guess seeing each post as a separate "document". However, here on TA, <h1>s are quite big…	2020-01-28 21:52:20.770817+00	3	1	1	\N	0	0	0	\N	\N	Convention on codegolf SE is `#`, I guess seeing each post as a separate "document". However, here on TA, <h1>s are quite big…	f	f
705	632	2	2020-01-29 15:48:20.535884+00	It makes sense to use `h1`/`#` for the top level heading rather than `h2`/`##`. However the browser defaults for large heading sizes are a bit *too* large.\r\n\r\nWe've overridden the defaults, roughly following [this style guide](https://www.smashingmagazine.com/2011/03/technical-web-typography-guidelines-and-techniques/#setting-a-scale).\r\n\r\nExample headings:\r\n\r\n* # h1 Heading: 28px\r\n  Normal paragraph text\r\n\r\n* ## h2 Heading: 25px\r\n  Normal paragraph text\r\n\r\n* ### h3 Heading: 22px\r\n  Normal paragraph text\r\n\r\n* #### h4 Heading: 19px\r\n  Normal paragraph text\r\n\r\n* ##### h5 Heading: 16px\r\n  Normal paragraph text\r\n\r\n* ###### h6 Heading: 16px\r\n  Normal paragraph text\r\n	2020-01-30 20:44:54.928637+00	4	1	1	\N	0	0	0	\N	\N	It makes sense to use `h1`/`#` for the top level heading rather than `h2`/`##`. However the browser defaults for large heading sizes are a bit *too* large.	f	f
744	672	208	2017-10-04 00:57:42+00	**The Devil Is In The Table Variable**\r\n\r\nInterleaved execution is aimed at correct cardinality misestimated in Multi-Statement Table Valued Functions. \r\n\r\nIn prior versions of SQL Server, these functions would always produced rather shoddy estimates:\r\n\r\n - 2014, 2016: 100 rows\r\n - 2005 - 2012: 1 row\r\n\r\nNeedless to say, this could cause a lot of problems when joining to other tables. \r\n\r\nWhile selecting data from a table variable does not inhibit parallelism on its own, the low row estimates would often contribute to low query costs, where parallelism wouldn't be considered. \r\n\r\nWith Interleaved Execution, cardinality estimation is paused, the subtree for the MSTVF is executed, and optimzation is resumed with a more accurate cardinality estimate.\r\n\r\n**How do I know if my MSTVF receives interleaved execution.**\r\n\r\nLike with Adaptive Joins, Interleaved Execution is noted in the query plan. Unlike Adaptive Joins, it is not noted in estimated plans, at least as of this writing.\r\n\r\nThe plan shape for a MSTVF with Interleaved Execution is a bit different from a typical plan that has a MSTVF in it.\r\n\r\nYou'll see the Table Valued Function operator at the top of the plan, and the scan of the Table Variable where the TVF operator would normally be in the graphical plan.\r\n\r\n[![NUTS][1]][1]\r\n\r\nWhen hovering over the TVF operator, you'll see the attribute `IsInterleavedExecuted` set to True, as well as a an estimated number of rows that may very nearly reflect reality. Hurrah.\r\n\r\n**Are there any Extended Events to troubleshoot when Interleaved Execution doesn't occur?**\r\n\r\nYes, a whole bunch:\r\n\r\n[![NUTS][2]][2]\r\n\r\nNote that some of these are in the Debug channel, which isn't selected by default when searching for Events to Extend.\r\n\r\n**Does Interleaved Exection Require ColumnStore Indexes?**\r\n\r\nNo, they'll work either way. Here's an example:\r\n\r\n\tSELECT u.Id, mj.*\r\n\tFROM   dbo.Users_cx AS u --ColumnStore\r\n\tJOIN   dbo.MultiStatementTVF_Join(0) AS mj\r\n\tON mj.UserId = u.Id\r\n\tWHERE  u.LastAccessDate >= '2016-12-01';\r\n\t\r\n\t\r\n\tSELECT u.Id, mj.*\r\n\tFROM   dbo.Users AS u --RowStore\r\n\tJOIN   dbo.MultiStatementTVF_Join(0) AS mj\r\n\tON mj.UserId = u.Id\r\n\tWHERE  u.LastAccessDate >= '2016-12-01';\r\n\r\nThese queries each join to different tables. One ColumnStore, one not. They both get Interleaved Execution plans.\r\n\r\n[![NUTS][3]][3]\r\n\r\n**When does Interleaved Execution work?**\r\n\r\nRight now, it only works with MSTVFs where the correlation is done *outside* of the function.\r\n\r\nHere are a couple examples:\r\n\r\nThis function has no inner correlation, meaning there's no `WHERE` clause predicated on a table column and a passed in variable.\r\n\r\n\tCREATE OR ALTER FUNCTION dbo.MultiStatementTVF_Join\r\n\t(\r\n\t    @h BIGINT\r\n\t)\r\n\tRETURNS @Out TABLE\r\n\t(\r\n\t    UserId INT,\r\n\t    BadgeCount BIGINT\r\n\t)\r\n\tAS\r\n\t    BEGIN\r\n\t        INSERT INTO @Out ( UserId, BadgeCount )\r\n\t        SELECT   b.UserId, COUNT_BIG(*) AS BadgeCount\r\n\t        FROM     dbo.Badges AS b\r\n\t        GROUP BY b.UserId\r\n\t        HAVING   COUNT_BIG(*) > @h;\r\n\t        RETURN;\r\n\t    END;\r\n\tGO\r\n\t\r\n\t\r\nThis function is the opposite, with a predicate on the `UserId` column with a passed in variable.\r\n\r\n\r\n\tCREATE OR ALTER FUNCTION dbo.MultiStatementTVF_CrossApply\r\n\t(\r\n\t    @h BIGINT,\r\n\t    @id INT\r\n\t)\r\n\tRETURNS @Out TABLE\r\n\t(\r\n\t    UserId INT,\r\n\t    BadgeCount BIGINT\r\n\t)\r\n\tAS\r\n\t    BEGIN\r\n\t        INSERT INTO @Out ( UserId, BadgeCount )\r\n\t        SELECT   b.UserId, COUNT_BIG(*) AS BadgeCount\r\n\t        FROM     dbo.Badges AS b\r\n\t        WHERE    b.UserId = @id\r\n\t        GROUP BY b.UserId\r\n\t        HAVING   COUNT_BIG(*) > @h;\r\n\t        RETURN;\r\n\t    END;\r\n\tGO\r\n\r\nIt's a common misconception that `CROSS APPLY` won't work. The real limitation is noted previously. The inner-function correlation is the deal breaker.\r\n\r\n\tSELECT u.Id, mj.*\r\n\tFROM   dbo.Users AS u --RowStore\r\n\tCROSS APPLY dbo.MultiStatementTVF_Join(0) AS mj\r\n\tWHERE  mj.UserId = u.Id\r\n\t       AND u.LastAccessDate >= '2016-12-01';\r\n\t\r\n\t\r\n\tSELECT   TOP 1 u.Id, mj.*\r\n\tFROM     dbo.Users AS u --RowStore\r\n\tCROSS APPLY dbo.MultiStatementTVF_CrossApply(2147483647, u.Id) AS mj\r\n\tWHERE    u.LastAccessDate >= '2016-12-01'\r\n\tORDER BY u.Id;\r\n\r\n[![NUTS][4]][4]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/O9lgW.jpg\r\n  [2]: https://i.stack.imgur.com/q4PUA.jpg\r\n  [3]: https://i.stack.imgur.com/RIze4.jpg\r\n  [4]: https://i.stack.imgur.com/0R8Bq.jpg	2020-02-03 03:10:04.912354+00	5	4	1	187581	0	0	0	2020-02-03 03:10:04.912354+00	\N	**The Devil Is In The Table Variable**	f	f
720	641	2	2020-01-30 23:36:23.034159+00	I think you are undoubtedly right that there are lots of good reasons why we should provide a log-out button — perhaps on the profile page if not the main page.\r\n\r\nI'll update this post when we add the feature.	2020-01-30 23:36:23.034159+00	8	1	1	\N	0	0	0	\N	\N	I think you are undoubtedly right that there are lots of good reasons why we should provide a log-out button — perhaps on the profile page if not the main page.	f	f
331	340	247	2017-10-31 09:58:40+00	You can try to use the command `\\ifthispageodd{<true part>}{<false part>}`, which is part of KOMA-Script. It uses a label in the .aux file and therefore needs at least two LaTeX passes to function. In your case it could look like:\r\n\r\n    \\ifthispageodd{\\nopagebreak[3]}{\\nopagebreak[4]}\r\n\r\nWithout a MWE, however, it is difficult to tell whether this really works or not.	2019-12-06 05:43:43.892612+00	1	4	1	398908	0	0	0	2019-12-06 05:43:43.892612+00	\N	You can try to use the command `\\ifthispageodd{<true part>}{<false part>}`, which is part of KOMA-Script. It uses a label in the .aux file and therefore needs at least two LaTeX passes to function. In your case it could look like:	f	f
645	591	752	2020-01-21 19:16:30.666755+00	I'm a huge believer that the road to expertise is paved with stupid questions.\r\n\r\nIt's not on anybody to answer.\r\n\r\nI agree with Max that this website could be a lot more fast and loose than the-site-that-shall-not-be-named. I feel like a lot of question-askers over there feel like this: https://www.youtube.com/watch?v=QpAUqu1Cz-8\r\n\r\nAnswering good questions may be entertaining, but people aren't asking so that we can all be entertained.\r\n\r\nI would also suggest differentiating the questions, too: Allow the mods to flag questions as humdingers vs homework.	2020-01-21 19:16:30.666755+00	2	4	1	\N	0	0	0	\N	\N	I'm a huge believer that the road to expertise is paved with stupid questions.	f	f
644	591	37	2020-01-21 18:10:52.803035+00	For the purposes of bringing traffic to the site, which is arguably the raison d'être for a website to even exist, we should probably allow any reasonably well-asked question that has relevance to the database-world.	2020-01-21 18:10:52.803035+00	2	4	1	\N	0	0	0	\N	\N	For the purposes of bringing traffic to the site, which is arguably the raison d'être for a website to even exist, we should probably allow any reasonably well-asked question that has relevance to the database-world.	f	f
768	687	234	2020-02-07 15:54:46.207714+00	Another way is to use `nodes near coords`.\r\n```\r\n\\documentclass[tikz,convert={density=600,outext=.tiff}]{standalone}\r\n\\usepackage{pgfplots}\r\n\\pgfplotsset{compat=1.16}\r\n\\pgfplotsset{% https://tex.stackexchange.com/a/75811/121799\r\n    name nodes near coords/.style={nodes near coords={},\r\n        every node near coord/.append style={anchor=center,coordinate,\r\n            name=#1-\\coordindex,/utils/exec=\\typeout{#1-\\coordindex},\r\n            alias=#1-last,\r\n        },\r\n    },\r\n    name nodes near coords/.default=coordnode\r\n}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\begin{axis}[width=2in, height=2in,xmin = 0, xmax = 1050, ymin = 0, ymax = 10,\r\nylabel = {[$\\sim$]}, xlabel = {[\\#]},legend style={at={(1,0)}, anchor=south\r\neast, font=\\tiny, draw=none, fill=none},declare function={f(\\x)=5*(1-exp(-0.003*\\x));}]\t;\r\n\\addplot [mesh, domain = 0:1000,samples=300] {f(x)};\r\n\\addplot [black, domain=0:1000, samples = 18, only marks, mark size = 1pt, mark=\r\n+,name nodes near coords=T] {f(x)};\r\n\\addplot [black, domain=0:1100, samples = 18, only marks, mark size = 1pt, mark\r\n= +,name nodes near coords=B] {10 -f(x)};\r\n\\end{axis}\r\n\\foreach \\X in {0,...,16}\r\n{\\draw (B-\\X) -- (T-\\X);}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2020-02-07 at 7.53.29 AM.png](/image?hash=482e506e2a903f21ce321a71ccb60db9bd7b6e581ac49476aad616c33b356798)\r\n\r\nA potential problem is that the last mark on the upper plot gets clipped away, and thus not connected.	2020-02-07 15:54:46.207714+00	7	4	3	\N	0	0	0	\N	\N	Another way is to use `nodes near coords`.	f	f
766	687	167	2020-02-07 12:09:23.291823+00	Not very elegant, but worst case you could add the lines manually:\r\n\r\n```\r\n\\documentclass{standalone}\r\n\\usepackage{tikz, pgfplots}\r\n\r\n\\tikzset{\r\n  declare function={\r\n    f(\\x)=(5*(1-exp(-0.003*\\x))));\r\n  }\r\n}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\begin{axis}[width=2in, height=2in,xmin = 0, xmax = 1050, ymin = 0, ymax = 10, ylabel = {[$\\sim$]}, xlabel = {[\\#]},legend style={at={(1,0)}, anchor=south east, font=\\tiny, draw=none, fill=none}]\t;\r\n\\addplot [mesh, domain = 0:1000,samples=300] {f(x)};\r\n\\addplot [black, domain=0:1000, samples = 18, only marks, mark size = 1pt, mark = +] {f(x)};\r\n\\addplot [black, domain=0:1100, samples = 18, only marks, mark size = 1pt, mark = +] {10 - f(x)};\r\n\\foreach \\i in {0,...,18}{\r\n  \\addplot[mark=none] coordinates {(\\i*1100/17,{10-f(x)}) (\\i*1000/17,{f(x)})};\r\n}\r\n\\end{axis}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\n![document.png](/image?hash=0c6ebc3619dcb621e8d5a64014e3f9650072622bc77cae289f1dffbef7dd9bd3)	2020-02-07 12:10:21.435511+00	4	4	3	\N	0	0	0	\N	\N	Not very elegant, but worst case you could add the lines manually:	f	t
605	558	751	2019-11-18 17:33:42+00	For SQL Server 2016 you need to have the query profiling infrastructure enabled in advance with trace flag 7412 or an extended events session capturing `query_thread_profile` (and be on [at least  SQL Server 2016 SP1][1]) but then can use\r\n\r\n    WITH XMLNAMESPACES\r\n       (DEFAULT 'http://schemas.microsoft.com/sqlserver/2004/07/showplan')\r\n    select query_plan.query('//ParameterList')\r\n    from sys.dm_exec_query_statistics_xml(@session_id)\r\n\r\nand see the `ParameterRuntimeValue` and `ParameterCompiledValue` from the execution plan of the statement being executed. \r\n\r\nFor SQL Server 2019+ the infrastructure is enabled by default so the above will just work. \r\n\r\nThis might not be all the parameters of the stored proc however as it will only include params used by the execution plan.\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-query-statistics-xml-transact-sql?view=sql-server-ver15	2020-01-12 14:11:26.456823+00	3	4	1	253558	0	0	0	2020-01-12 14:11:26.456823+00	\N	For SQL Server 2016 you need to have the query profiling infrastructure enabled in advance with trace flag 7412 or an extended events session capturing `query_thread_profile` (and be on [at least  SQL Server 2016 SP1][1]) but then can use	f	f
695	628	825	2020-01-28 15:32:55.88245+00	This can all be done using a simple hash table lookup\r\n```\r\n\\documentclass{article}\r\n\\makeatletter\r\n\\newcommand\\mycommand[1]{%\r\n  \\@ifundefined{JV@cmd@\\detokenize{#1}}\r\n    {}\r\n    {\\@nameuse{JV@cmd@\\detokenize{#1}}}%\r\n}\r\n\\newcommand\\mapmycommand[2]{%\r\n  \\@namedef{JV@cmd@\\detokenize{#1}}{#2}%\r\n}\r\n\\makeatother\r\n\\begin{document}\r\n\r\n\\mycommand{a}%\r\n\\mapmycommand{a}{b}%\r\n\\mycommand{a}%\r\n\\mapmycommand{c}{d}%\r\n\\mycommand{c}%\r\n\\mycommand{a}%\r\n\r\n\\end{document}\r\n```	2020-01-28 15:32:55.88245+00	12	4	2	\N	0	0	0	\N	\N	This can all be done using a simple hash table lookup	f	f
746	673	168	2020-02-03 08:37:27.006328+00	Your issue has nothing to do with LaTeX's math typesetting or the material you put in math mode. A minimal non-working example to this looks like the following:\r\n\r\n```tex\r\n\\documentclass{article}\r\n\r\n\\begin{document}\r\n[abc]\\\\\r\n[def]\\\\\r\n[ghi]\r\n\\end{document}\r\n```\r\n\r\nThe issue is that `\\\\` takes an optional argument, and while parsing for that argument spaces are ignored, so the above is equivalent (for LaTeX) to:\r\n\r\n```tex\r\n\\documentclass{article}\r\n\r\n\\begin{document}\r\n[abc]\\\\[def]\\\\[ghi]\r\n\\end{document}\r\n```\r\n\r\nWhat happens now is that `\\\\` takes the optional argument (`def` in the first and `ghi` in the second case) and tries to use that optional argument, which should specify an additional vertical space inserted after the linebreak, as a correct dimension. So TeX begins to parse `def` as a dimension, finds that this is not a valid skip or dimen register and parses for a valid float followed by a valid unit, hence throwing the missing number error.\r\n\r\nTo solve this you have to "hide" the `[` as the opening delimiter of the optional argument. You can do so by putting a `\\relax` after `\\\\`:\r\n\r\n```tex\r\n\\documentclass{article}\r\n\r\n\\begin{document}\r\n[abc]\\\\\\relax\r\n[def]\\\\\\relax\r\n[ghi]\r\n\\end{document}\r\n```	2020-02-03 08:37:27.006328+00	9	6	3	\N	0	0	0	\N	\N	Your issue has nothing to do with LaTeX's math typesetting or the material you put in math mode. A minimal non-working example to this looks like the following:	f	f
706	637	811	2020-01-29 16:34:48.780667+00	# Keep it transparent\r\nAvatars appear in post headers which have a silver background, in post summaries which have a white background, in chat side bars which have a silver background, and in the top bar which is site-coloure but has the avatar in a silver frame.\r\n\r\n\r\nJust let the silver/white background behind the avatar shine through.	2020-01-29 17:49:52.42168+00	2	1	1	\N	0	0	0	\N	\N	Keep it transparent	f	f
401	393	96	2019-12-10 14:37:00.466643+00	This is going to be a tricky issue, and will possibly evolve over time. For starters, playing it safe and not stepping on toes is probably the best plan.\r\n\r\nFor now...\r\n\r\nWhere ever possible, answer questions that were asked on SE on SE. For cases where that isn't feasible (e.g. poster doesn't have an SE account), somebody else is free to post a summary of what was posted here (under CC-BY-SA usage) that does answer the question, but since citing the source is required, it is quite fair to link back to the full answer here.\r\n\r\nAlso  for now...\r\n\r\nI think *commenting* on SE with "this question answered [here]" is a bad idea. Either answer the question there or don't. The primary source material can be posted here first, but either answer posts or don't, just linking without answering will only fuel animosity.	2019-12-10 14:37:00.466643+00	8	4	2	\N	0	0	0	\N	\N	This is going to be a tricky issue, and will possibly evolve over time. For starters, playing it safe and not stepping on toes is probably the best plan.	f	f
457	393	167	2019-12-14 16:01:47.084887+00	I agree with @Caleb that in the current climate posting links on tex.se might not be well received.\r\n\r\nWhile it won't be enforcible for all possible licenses, I suggest to first ask the original author before reposting answers from here on tex.se. For example I would prefer if my answers would not be reposted there.	2019-12-19 12:04:17.508478+00	8	4	3	\N	0	0	0	\N	\N	I agree with @Caleb that in the current climate posting links on tex.se might not be well received.	f	f
783	346	2	2020-02-10 15:55:45.72458+00	We are going to implement this as a profile setting.\r\n\r\nIn addition, the link back to SE will be added to the post history for all imported posts.\r\n\r\nI've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/20) so work can begin, and will update this answer when it is complete.	2020-02-10 15:55:45.72458+00	0	1	1	\N	0	0	0	\N	\N	We are going to implement this as a profile setting.	t	f
163	225	14	2018-08-16 15:08:46+00	> Can I eliminate the sort without changing the query (which is vendor code, so I'd really rather not...). I can change the table and indexes.\r\n\r\nIf you can change the indexes, then changing the order of the index on `#right` to match the order of the filters in the join removes the sort (for me):\r\n\r\n    CREATE CLUSTERED INDEX IX ON #right (c, a, b, d, e, f, g, h)\r\n\r\nSurprisingly (to me, at least), this results in neither query ending up with a sort.\r\n\r\n> Is this intentional?\r\n\r\nLooking at the output from [some weird trace flags][2], there's an interesting difference in the final Memo structure:\r\n\r\n[![screenshot of final memo structure for each query][1]][1]\r\n\r\nAs you can see in the "Root Group" at the top, both queries have the option to use a Merge Join as the main physical operation to execute this query.\r\n\r\n## Good query\r\n\r\nThe join *without* the sort is driven by group 29 option 1 and group 31 option 1 (each of which are range scans on the indexes involved).  It's filtered by group 27 (not shown), which is the series of logical comparison operations that filter the join.\r\n\r\n## Bad query\r\n\r\nThe one *with* the sort is driven by the (new) options 3 that each of those two groups (29 and 31) has.  Option 3 performs a physical sort on the results of the range scans mentioned previously (option 1 of each of those groups).\r\n\r\n## Why?\r\n\r\nFor some reason, the option to use 29.1 and 31.1 directly as sources for the merge join is not even available to the optimizer in the second query.  Otherwise, I think it would be listed under the root group among the other options.  If it were available at all, then it would definitely pick those over the massively more expensive sort operations.\r\n\r\nI can only conclude that either:\r\n\r\n- this is a bug (or more likely a limitation) in the optimizer's search algorithm\r\n  - changing the indexes and joins to only have 5 keys removes the sort for the second query (6, 7, and 8 keys all have the sort).  \r\n  - This implies that the search space with 8 keys is so large that the optimizer just doesn't have time to identify the non-sort solution as a viable option before it terminates early with the "good enough plan found" reason\r\n  - it does seem a little buggy to me that the order of the join conditions influences the optimizer's search process this much, but really that's a bit over my head\r\n- the sort is required in order to ensure correctness in the results\r\n  - this one seems unlikely, since the query *can* run without the sort when there are fewer keys, or the keys are specified in a different order\r\n\r\nHopefully someone can come along and explain *why* the sort is required, but I thought the difference in the Memo building was interesting enough to post as an answer.\r\n\r\n  [1]: https://i.stack.imgur.com/QQB5x.png\r\n  [2]: http://sqlblog.com/blogs/paul_white/archive/2012/04/28/query-optimizer-deep-dive-part-1.aspx	2019-12-01 15:20:57.811364+00	1	4	1	215099	0	0	0	2019-12-01 15:20:57.811364+00	\N	> Can I eliminate the sort without changing the query (which is vendor code, so I'd really rather not...). I can change the table and indexes.	f	f
164	225	12	2018-08-16 21:58:15+00	>Is this intentional?\r\n\r\nIt is by design, yes. The best public source for this assertion was unfortunately lost when Microsoft retired the Connect feedback site, obliterating many useful comments from developers on the SQL Server team.\r\n\r\nAnyway, the current optimizer design does not *actively seek* to avoid unnecessary sorts *per se*. This is most often encountered with windowing functions and the like, but can also be seen with other operators that are sensitive to ordering, and in particular to preserved ordering between operators.\r\n\r\nNevertheless, the optimizer is quite good (in many cases) at avoiding unnecessary sorting, but this outcome normally occurs for reasons other than aggressively trying different ordering combinations. In that sense, it is not so much a question of 'search space' as it is of the complex interactions between orthogonal optimizer features that have been shown to increase general plan quality at acceptable cost.\r\n\r\nFor example, sorting can often be avoided simply by matching an ordering requirement (e.g. top-level `ORDER BY`) to an existing index. Trivially in your case that could mean adding `ORDER BY l.a, l.b, l.c, l.d, l.e, l.f, l.g, l.h;` but this is an over-simplification (and unacceptable because you do not want to change the query).\r\n\r\nMore generally, each memo group may be associated with required or desired properties, which may include input ordering. When there is no obvious reason to *enforce* a particular order (e.g. to satisfy an `ORDER BY`, or to ensure correct results from an order-sensitive physical operator), there is an element of 'luck' involved. I wrote more about the specifics of that as it pertains to merge join (in union or join mode) in [Avoiding Sorts with Merge Join Concatenation][1]. Much of that goes beyond the supported surface area of the product, so treat it as informational, and subject to change.\r\n\r\nIn your particular case, yes, you may adjust the indexing [as jadarnel27 suggests][2] to avoid the sorts; though there is little reason to actually prefer a merge join here. You could also hint a choice between hash or loop physical join with `OPTION(HASH JOIN, LOOP JOIN)` using a Plan Guide without changing the query, depending on your knowledge of the data, and the trade-off between best, worst, and average-case performance.\r\n\r\nFinally, as a curiosity, note that the sorts can be avoided with a simple `ORDER BY l.b`, at the cost of a potentially less efficient many-to-many merge join on `b` alone, with a complex residual. I mention this mostly as an illustration of the interaction between optimizer features I mentioned previously, and the way top-level requirements can propagate.\r\n\r\n  [1]: https://sqlperformance.com/2014/09/t-sql-queries/avoiding-sorts-merge-join-concatenation\r\n  [2]: https://dba.stackexchange.com/a/215099	2019-12-01 15:20:58.079672+00	1	4	1	215128	0	0	0	2019-12-01 15:20:58.079672+00	\N	>Is this intentional?	f	f
594	548	14	2018-09-14 20:10:45+00	Your demo is being hit by a [limitation of REPLICATE][1]:\r\n\r\n> If string_expression is not of type varchar(max) or nvarchar(max), REPLICATE truncates the return value at 8,000 bytes. To return values greater than 8,000 bytes, string_expression must be explicitly cast to the appropriate large-value data type.\r\n\r\nIf I do this:\r\n\r\n    INSERT INTO dbo.t (off_row_data) VALUES (REPLICATE(CAST('A' as varchar(max)), 20000));\r\n\r\nAnd then run your DMV query from above against dm_db_database_page_allocations, I get pages with a PageTypeDesc of `TEXT_MIX_PAGE`.\r\n\r\nI can then run DBCC PAGE with trace flag 3604 enabled in order to see the details of that off-row page:\r\n\r\n    DBCC TRACEON (3604);\r\n    GO\r\n    DBCC PAGE (TestDB, 1, 20696 , 3) -- your page will be different :)\r\n\r\nThe output is large, but near the beginning you'll see:\r\n\r\n    Blob row at: Page (1:20696) Slot 0 Length: 3934 Type: 3 (DATA)\r\n\r\nAnd then, you know, a bunch of A's.\r\n\r\n[1]: https://docs.microsoft.com/en-us/sql/t-sql/functions/replicate-transact-sql?view=sql-server-2017	2020-01-10 21:25:40.193733+00	2	4	1	217666	0	0	0	2020-01-10 21:25:40.193733+00	\N	Your demo is being hit by a [limitation of REPLICATE](https://docs.microsoft.com/en-us/sql/t-sql/functions/replicate-transact-sql?view=sql-server-2017):	f	f
821	713	895	2020-02-14 20:29:04.969219+00	The problem here is the [`std::underlying_type`](https://en.cppreference.com/w/cpp/types/underlying_type) struct template parameter must be an enumeration type:\r\n\r\n> Otherwise, if `T` is not an enumeration type, there is no member `type`\r\n\r\nThis means calling `std::underlying_type_t<int>` is a compilation error. Short circuiting logical-and operators does not prevent template evaluation. To avoid this we'll need 2 things:\r\n\r\n 1. [`std::conditional`](https://en.cppreference.com/w/cpp/types/conditional) to optionally use `underlying_type`\r\n 1. And an alternative structure which has a `type` member which could be returned instead, I've chosen `template<typename... Ts> struct make_void { typedef void type; }` from https://en.cppreference.com/w/cpp/types/void_t\r\n\r\n\r\nUsing these 2 `Foo` can be written so that the `type` member of `std::underlying_type` is only accessed if `std::is_enum` is valid:\r\n\r\n    template <typename T>\r\n    std::enable_if_t<std::is_same_v<typename std::conditional_t<std::is_enum_v<T>, std::underlying_type<T>, decltype(make_void<T>())>::type, uint8_t> || std::is_same_v<T, unsigned int>> Foo(const T param) {\r\n        std::cout << static_cast<int>(param) << std::endl;\r\n    }\r\n\r\n[**Live Example**](http://coliru.stacked-crooked.com/a/88f31ccbf8914208)	2020-02-14 20:29:04.969219+00	1	4	1	\N	0	0	0	\N	\N	The problem here is the [`std::underlying_type`](https://en.cppreference.com/w/cpp/types/underlying_type) struct template parameter must be an enumeration type:	f	f
721	639	2	2020-01-30 23:40:22.474631+00	The search bar is 'live', i.e. it displays results 'as you type' (with a slight delay of course).\r\n\r\nHow to you see that working when you are on a question edit page?	2020-01-30 23:40:22.474631+00	3	1	1	\N	0	0	0	\N	\N	The search bar is 'live', i.e. it displays results 'as you type' (with a slight delay of course).	f	f
752	677	2	2020-02-04 05:40:58.334133+00	Now switched to 'Newest First'\r\n\r\n---\r\n\r\n> Is there any strong motivation for having notifications in the 'oldest first' order?\r\n\r\nI'm not sure there ever was a good reason — I think it was to mirror the 'newest messages at the bottom' idiom of the comments pane, but that pane scrolls to the bottom by default unlike the notifications pane.\r\n\r\nOn balance I think you are right and we should have 'newest at top' rather than auto-scrolling down anyway.	2020-02-04 05:40:58.334133+00	6	1	1	\N	0	0	0	\N	\N	Now switched to 'Newest First'	t	f
810	640	2	2020-02-12 14:56:36.914214+00	Update Feb 16 2020: this is now available with the (experimental) new header navigation.\r\n\r\nCurrently, you can only see it in action if you have access to the Code Golf beta, but other communities can add 'about' pages on request.\r\n\r\n---\r\n\r\nYes this would be very useful, though it may not end up being a simple link in the header because we are currently rethinking site navigation via the controls there. The site dropdown may become a more flexible custom control similar to the community dropdown on SE — and there would definitely be room for a dedicated link in there.\r\n\r\nI've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/26) so work can begin and will update the post here when it is complete.	2020-02-16 01:14:01.269148+00	3	1	1	\N	0	0	0	\N	\N	Update Feb 16 2020: this is now available with the (experimental) new header navigation.	t	f
707	638	167	2020-01-29 20:37:01.380133+00	As there is no such thing as community wiki questions or answers, it was suggested to create such posts with a throw away user account, see https://topanswers.xyz/meta?q=497#a554 for more details.\r\n\r\nAs I missed the opportunity to pick some funny name for this account, I used the boring name "Community" for the posts about the MWE and chat pings to encourage everybody to edit and expand them. I left a comment explaining this at https://topanswers.xyz/transcript?room=621&id=15211#c15211 but forgot to add one for the chat ping question.   \r\n\r\nIf you think this name is confusing, I can think about something else for the next time.	2020-01-29 21:43:28.780098+00	2	4	1	\N	0	0	0	\N	\N	As there is no such thing as community wiki questions or answers, it was suggested to create such posts with a throw away user account, see https://topanswers.xyz/meta?q=497#a554 for more details.	f	f
838	720	234	2020-02-18 03:20:39.073855+00	I would not want to change the voting system for this purpose. The arguments in favor of "one vote per user" apply pretty much everywhere. Competitions are not really about the outcome only, but also about the elegance of the code and so on. If we decide that more seasoned users have more say in voting on ordinary answers, we do so probably because we think their opinion should have more weight because they have more experience, can better gauge what a good answer is, and can tell which codes are elegant and efficient. However, if this is the case, we can make the same argument for the competitions. \r\n\r\nPersonally I do not care too much about one way or another, nor about the votes in general. However, I do care about overall consistency. And to me it seems odd to apply different rules in different situations. It is also complicated and may lead to confusion. 	2020-02-18 03:20:39.073855+00	11	4	3	\N	0	0	0	\N	\N	I would not want to change the voting system for this purpose. The arguments in favor of "one vote per user" apply pretty much everywhere. Competitions are not really about the outcome only, but also about the elegance of the code and so on. If we decide that more seasoned users have more say in voting on ordinary answers, we do so probably because we think their opinion should have more weight because they have more experience, can better gauge what a good answer is, and can tell which codes are elegant and efficient. However, if this is the case, we can make the same argument for the competitions.	f	f
837	720	168	2020-02-17 16:02:07.835617+00	I'd say that for contest answers each user should be able to award one star to each answer, not the usual reputation-dependent number of stars.\r\n\r\nContests for us are generally of creative nature and the technical expertise of users shouldn't affect their voting power on creative content.	2020-02-17 16:02:07.835617+00	4	6	3	\N	0	0	0	\N	\N	I'd say that for contest answers each user should be able to award one star to each answer, not the usual reputation-dependent number of stars.	f	t
634	30	769	2020-01-17 17:44:20.83376+00	`bug` clicking 'Back' should switch back not only the query, but also the database selection.\r\n\r\n---\r\n\r\nUsing Windows 10.0.17763 and Google Chrome 79.0.3495.\r\n\r\nSteps to reproduce:\r\n-------------------\r\nBrowse to dbfiddle.uk, change query to 'SELECT 1' and click 'run'.\r\nDefault engine is SQL Server 2019 and the URL reflects this correctly: https://dbfiddle.uk/***?rdbms=sqlserver_2019***&fiddle=<something>\r\n\r\nChange database selection to PostgreSQL 12 (or any other), change query to 'SELECT 2' and click 'run'.\r\nURL now reflects correct engine https://dbfiddle.uk/***?rdbms=postgres_12***&fiddle=<something>\r\n\r\nClick browser 'Back' button.\r\nquery is correctly reverted to 'SELECT 1' and previous result is shown. URL shows https://dbfiddle.uk/***?rdbms=sqlserver_2019***&fiddle=<something>\r\n\r\nHowever, menu selection remains in 'Postgres 12', and clicking 'Run' again changes the URL back to 'Postgres 12'.\r\n\r\nExpected result:\r\n----------------\r\npage should respect URL specification and clicking 'Back' should switch back not only the query, but also the database selection.	2020-02-04 08:46:08.713644+00	4	4	1	\N	0	0	0	\N	\N	`bug` clicking 'Back' should switch back not only the query, but also the database selection.	f	f
626	30	38	2020-01-16 12:02:03.152276+00	`feature-request` please add CockroachDB\r\n\r\n---\r\n\r\n## CockroachDB integration\r\n\r\nThe [19.2 release][2] makes it look like integrating CockroachDB to db<>fiddle might be relatively easy. Would you be interested in adding CockroachDB as a supported platform?\r\n\r\nThe [`cockroach demo`][1] command ostensibly provides an ephemeral in-memory enterprise instance; and having seen it in action it appears to do what it says on the tin :)\r\n\r\n[1]: https://www.cockroachlabs.com/docs/dev/cockroach-demo.html\r\n[2]: https://www.cockroachlabs.com/blog/cockroachdb-19dot2-release/\r\n---\r\n\r\n> **Update Nov 21 '19** - CockroachDB licensing reps have confirmed privately that db<>fiddle is within the acceptable usage of `demo`. Docs have been sent to @Jack for records keeping.\r\n\r\n---\r\n\r\n~Consolidated~ ~from~ [~here~][3] ~for~ ~tracking.~\r\n \r\n[3]: https://dba.meta.stackexchange.com/a/3285/68127	2020-02-04 08:45:53.321814+00	2	4	1	\N	0	0	0	\N	\N	`feature-request` please add CockroachDB	f	f
53	30	38	2019-11-22 09:32:06.638744+00	`feature-request` `declined` add an 'Update this post' button on embedded db<>fiddles on TopAnswers\r\n\r\n---\r\n\r\nCurrently it looks like editing and/or running the fiddle from the embed doesn't change the post. This is obviously good design, but I suggest adding an `Update this post` button or something similar alongside the embedding. This would make it clear to the user that the underlying post may need to be modified if they want it to persist the changes they have just typed \r\n\r\nSome related thoughts:\r\n\r\n* Button could hotlink to the `/edit` analogue with the commit message pre-populated à la "_updated fiddle from `abc` to `def`_"\r\n* Perhaps the button only appears on-edit or on-run when a new fiddle hash is generated?\r\n* Does it make sense to track a fiddle's "lineage" & possibly expose it to users to "_go back to that one edit I forgot what I did on but I know what the results looked like_"\r\n* Is there a not-gross way to expose the diff of either the post batch or the last-run batch to the active editor in the embed?	2020-02-04 08:44:14.422515+00	2	4	1	\N	0	0	0	\N	\N	`feature-request` `declined` add an 'Update this post' button on embedded db<>fiddles on TopAnswers	f	f
713	30	38	2020-01-30 14:28:47.993691+00	`bug` `planned` Brittle fiddle one-boxing in chat\r\n\r\nOne-boxing in small fiddles in chat is great. Sadly, it seems that leaving a page and returning causes the one-boxing render to break. \r\n\r\nSee [this chat thread][1] for context & repro steps.  \r\n\r\nNote that the transcript one-boxing appears to be stable and does not appear to be affected by this behaviour. \r\n\r\nUsing...\r\n\r\n> Google Chrome Version 79.0.3945.130 (Official Build) (64-bit)\r\n\r\n[1]: https://topanswers.xyz/transcript?room=644&id=16493#c16493\r\n[2]: https://topanswers.xyz/transcript?room=37&id=16598#c16598	2020-02-04 08:46:15.522247+00	4	4	1	\N	0	0	0	\N	\N	`bug` `planned` Brittle fiddle one-boxing in chat	f	f
432	30	38	2019-12-12 07:38:17.833153+00	`bug` Embedded fiddles should respect `&hide=` args\r\n\r\n---\r\n\r\nI'm drafting through https://topanswers.xyz/databases?q=424 and part of my reproduction includes loading ~60,000 rows of data. In the link as written... \r\n\r\n* https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=4726813c2d4d6c23fd5d7a50756de4b6&hide=2\r\n\r\n...the trailer arg `&hide=2` collapses the batch where I do the load. In the current embed though, the hidden batch is revealed. \r\n\r\nThe current fiddle has 5,000 rows (loading the 1k batches from copy-paste is rather taxing on my browser). I'll wait update the fiddle with the remaining rows for the time being 🙂	2020-02-04 08:46:46.303891+00	2	4	1	\N	0	0	0	\N	\N	`bug` Embedded fiddles should respect `&hide=` args	f	f
748	30	871	2020-02-03 10:12:00.900294+00	`feature-request` [`declined`](https://topanswers.xyz/transcript?room=37&id=17302#c17302) Allow multiple statements per batch for Firebird again\r\n\r\n---\r\n\r\nFirebird, DDL, multiple statements\r\n\r\nAs far as I remember, this used to work, but no more.\r\n\r\nPerhaps parser fails to split script into distinct SQL statements.\r\n\r\nhttps://dbfiddle.uk/?rdbms=firebird_3.0&fiddle=300756710cf77de8d754f04bbcfa07de\r\n\r\n```\r\ncreate table KPS1 ( ID integer primary key, DATE_FROM date not null, DATE_TO date not null ); \r\ncreate table KPS2 ( ID integer primary key, DATE_FROM date not null, DATE_TO date not null )\r\n```\r\n\r\nDynamic SQL Error SQL error code = -104 Token unknown - line 6, column 1 create	2020-02-04 08:53:12.812566+00	0	4	1	\N	0	0	0	\N	\N	`feature-request` [`declined`](https://topanswers.xyz/transcript?room=37&id=17302#c17302) Allow multiple statements per batch for Firebird again	f	f
654	30	32	2020-01-22 15:04:47.573236+00	`feature-request` `retracted` Add support for SQL Server `GO [count]` syntax\r\n\r\n---\r\n\r\n# Support for SQL Server `GO [count]` syntax\r\n\r\nIn SQL Server you can add the batch operator [`GO`](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/sql-server-utilities-statements-go?view=sql-server-ver15) to the end of either a single or a bunch of commands like this:\r\n\r\n    SELECT 1\r\n    GO\r\n    \r\nOr even like this:\r\n\r\n    INSERT INTO [TableName] (SomeText, SomeInt)\r\n    VALUES ('false', 1)\r\n    GO\r\n    \r\nIf I want to really insert a bunch of items into a table I can tell `GO` to do this multiple times like this:\r\n\r\n    INSERT INTO [TableName] (SomeText, SomeInt)\r\n    VALUES ('false', 1)\r\n    GO 633\r\n    \r\n...and the query "batch" will be executed 633 times. \r\n\r\nThis is helpful when inserting a fixed amount of pseudo data into a table to reproduce issues in a development environment (and would be helpful in db<>fiddle).\r\n\r\n_Sadly I can't add a db<>fiddle to show you how it should work ;-)_\r\n\r\nI think the `GO` utility statement has been discussed before, but I don't think the additional `count` was part of the discussion.\r\n\r\n\r\n### Reference Material\r\n\r\n[SQL Server Utilities Statements - GO](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/sql-server-utilities-statements-go?view=sql-server-ver15) (Microsoft | SQL Docs)\r\n	2020-02-04 08:45:19.88704+00	0	4	1	\N	0	0	0	\N	\N	`feature-request` `retracted` Add support for SQL Server `GO [count]` syntax	f	f
415	30	38	2019-12-11 14:32:39.496632+00	`feature-request` please add an (opt-in) cookie to remember the last platform a user was on?\r\n\r\n# User cookies\r\n\r\nWhen I was a SQL Server Man™️ I appreciated that db<>fiddle launches by default to a recent SQL Server `@@version`.\r\n\r\nNow that I'm mainly a PostgreSQL Comrade, I kind of wish it would launch to PostgreSQL. \r\n\r\nDoes it make sense to add an (opt-in) tracking cookie to remember the last platform a user was on? Perhaps even integrating with an topanswers profile if available?\r\n\r\nThis seems like it could be early-stages groundwork for tracking your own specific fiddle history as well, though; so I suppose it could a mixed bag as to how you feel about that as a concept (whether it's a feature or an imposition).	2020-02-04 08:46:00.953245+00	3	4	1	\N	0	0	0	\N	\N	`feature-request` please add an (opt-in) cookie to remember the last platform a user was on?	f	f
749	30	871	2020-02-03 12:06:38.331771+00	`bug` `fixed`: Firebird, datatypes, aligning\r\n\r\nhttps://dbfiddle.uk/?rdbms=firebird_3.0&fiddle=c80635f28efd64914a70160a0cc46ecf\r\n\r\nCheck last query, and it is especially visible in MarkDown export.\r\n\r\nColumns 3 to 5 are left-aligned, despite being numeric.\r\n\r\nI suspect the type BigInt or what they are is mis-detected for textual one.	2020-02-04 07:08:57.315647+00	2	4	1	\N	0	0	0	\N	\N	`bug` `fixed`: Firebird, datatypes, aligning	f	f
576	30	38	2020-01-05 20:21:38.274982+00	`bug` `fixed` YYYY-MM-DD for SQL Server\r\n\r\n![](https://imgs.xkcd.com/comics/iso_8601.png)\r\n\r\nIt looks like SQL Server is the only non-ISO localisation at the moment, but I've embedded a few of my favourites below for ongoing reference.\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=b5aa1e2bb74237fde0de2c5ccb632c98\r\n\r\n<>https://dbfiddle.uk/?rdbms=postgres_12&fiddle=a9e479467e8185aaf4e8f6eeff753dbc\r\n\r\n<>https://dbfiddle.uk/?rdbms=mysql_8.0&fiddle=47edd2b88c652aa72de5e000ae288216\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlite_3.27&fiddle=2f53a982cb229c7959b73bf014848091	2020-02-04 08:46:25.58201+00	5	4	1	\N	0	0	0	\N	\N	`bug` `fixed` YYYY-MM-DD for SQL Server	f	f
175	239	12	2019-01-05 10:11:49+00	There are a few issues in play here.\r\n\r\n### Pushing predicates past `TOP`\r\n\r\nThe optimizer cannot currently push a predicate past a `TOP`, even in the limited cases where it would be safe to do so*. This limitation accounts for the behaviour of all the queries in the question where the predicate is in a higher scope than the `TOP`.\r\n\r\nThe work around is to perform the rewrite manually. The fundamental issue is similar to the case of [pushing predicates past a window function][1], except there is no corresponding specialized rule like `SelOnSeqPrj`.\r\n\r\nMy personal opinion is that an exploration rule like `SelOnTop` remains unimplemented because people have deliberately written queries with `TOP` in an effort to provide a kind of 'optimization fence'.\r\n\r\n<sub>\\* Generally this means the predicate should appear in the `ORDER BY` clause associated with the `TOP`, and the direction of any inequality should agree with the direction of the sorting. The transformation would also need to account for the sorting behaviour of NULLs in SQL Server. Overall, the limitations probably mean this transformation would not be generally useful enough in practice to justify the additional exploration efforts.</sub>\r\n\r\n### Costing issues\r\n\r\nThe remaining execution plans in the question can be explained as cost-based choices due to the distribution of values in the `Score` column (many more rows <= 500 than >= 500), and the effect of the [row goal][2] introduced by the `TOP`.\r\n\r\nFor example, the query:\r\n\r\n    --Written normally\r\n    SELECT TOP (101)\r\n        c.UserId, \r\n        c.[Text],\r\n        c.Score\r\n    FROM dbo.Comments AS c\r\n    WHERE\r\n        c.Score <= 500\r\n    ORDER BY\r\n        c.Score ASC;\r\n\r\n...produces a plan with an apparently unpushed predicate in a Filter:\r\n\r\n[![late filter due to row goal][3]][3]\r\n\r\nNote that the Sort is estimated to produce 101 rows. This is the effect of the row goal added by the Top. This affects the estimated cost of the Sort and the Filter enough to make it seem like this is the cheaper option. The estimated cost of this plan is **2401.39** units.\r\n\r\nIf we disable row goals with a query hint:\r\n\r\n    --Written normally\r\n    SELECT TOP (101)\r\n        c.UserId, \r\n        c.[Text],\r\n        c.Score\r\n    FROM dbo.Comments AS c\r\n    WHERE\r\n        c.Score <= 500\r\n    ORDER BY\r\n        c.Score ASC\r\n    OPTION (USE HINT ('DISABLE_OPTIMIZER_ROWGOAL'));\r\n\r\n...the execution plan produced is:\r\n\r\n[![plan without row goal][4]][4]\r\n\r\nThe predicate has been pushed into the scan as a residual non-sargable predicate, and the cost of the whole plan is **2402.32** units.\r\n\r\nNotice that the `<= 500` predicate is not expected to filter out any rows. If you had chosen a smaller number, like `<= 50`, the optimizer would have preferred the pushed-predicate plan regardless of the row goal effect.\r\n\r\nFor the query with `Score DESC` and a `Score >= 500` predicate:\r\n\r\n    --Written normally\r\n    SELECT TOP (101)\r\n        c.UserId, \r\n        c.[Text],\r\n        c.Score\r\n    FROM dbo.Comments AS c\r\n    WHERE\r\n        c.Score >= 500\r\n    ORDER BY\r\n        c.Score DESC;\r\n\r\nNow the predicate is expected to be very selective, so the optimizer chooses to push the predicate *and* use the nonclustered index with lookups:\r\n\r\n[![selective predicate][5]][5]\r\n\r\nAgain, the optimizer considered multiple alternatives and chose this as the apparently cheapest option, as usual.\r\n\r\n  [1]: https://stackoverflow.com/a/15304023/440595\r\n  [2]: https://sqlkiwi.blogspot.com/2010/08/inside-the-optimiser-row-goals-in-depth.html\r\n  [3]: https://i.stack.imgur.com/joycE.png\r\n  [4]: https://i.stack.imgur.com/6nFbq.png\r\n  [5]: https://i.stack.imgur.com/DW3Cn.png	2019-12-01 20:33:58.723267+00	2	4	1	226403	0	0	0	2019-12-01 20:33:58.723267+00	\N	There are a few issues in play here.	f	f
173	211	115	2019-12-01 20:18:25.966015+00	**Update:** The community ended up trying [something that's more similar to SE](https://writing.codidact.com/questions/39219) at least for now.  (Basically, we needed a home Right Away because of SE, and a temporary site that looks more like SE made for an easier transition.  The long-term decision remains open.)\r\n\r\n----\r\n\r\nRepresentatives of [Writing.SE](https://writing.stackexchange.com/) request a community.\r\n\r\nI'm writing (ha!) on behalf of a few of the "senior" users of our community.  We haven't had a meta discussion (which I suspect SE would shut down anyway) yet; we thought it better to set up something here and then invite people to check it out.  This means our community here would be provisional, but I have high hopes for it -- we're in crisis over on SE and our users would like a place to continue as a community.\r\n\r\nOur site has about 8,000 questions that are not closed.  I have the impression you're not necessarily aiming for bulk imports; we could start with everything created by the users who come over (including questions that our users answered).  There might be some specific  tags we'd want to bring over wholesale.  (At least *I* do.)  How should we discuss import?\r\n\r\nOur users will almost certainly want to see a [way to track their own content](https://topanswers.xyz/meta?q=233), and if we don't do something about [account recovery](https://topanswers.xyz/meta?q=234) or [passwords](https://topanswers.xyz/meta?q=237) we're likely to run into problems.  (Many of our users are not hardcore tech-geeks.)  These issues don't keep us from starting to use the new site but would probably be barriers to full-on adoption.  (Nobody wants to invest and risk losing access in a cookie accident.)  We know that other areas are under active development and are likely to be improved; we're ok with a site that's still under construction.  Do expect questions where we don't yet have help/FAQ posts, of course. :-)\r\n\r\nWe have no special tool or font needs.  We were using the generic beta theme on SE.\r\n\r\nI don't know how you're handling moderation or even what moderators can do.  Our site had moderators on SE, so from a *community* perspective it's probably cleanest to appoint some of them as mods here.  If the community takes root here as I hope it will, and as TopAnswers works out how it wants to handle moderation, I would expect to then follow whatever process is developed for a community to choose moderators.  (In other words, I think of the initial mods as truly *pro-tem*, not serving for years and years without community buy-in.)\r\n\r\nWhat other matters do we need to work out with TopAnswers so we can then invite our community in?\r\n	2019-12-19 17:43:50.530838+00	11	4	1	\N	0	0	0	\N	\N	**Update:** The community ended up trying [something that's more similar to SE](https://writing.codidact.com/questions/39219) at least for now.  (Basically, we needed a home Right Away because of SE, and a temporary site that looks more like SE made for an easier transition.  The long-term decision remains open.)	f	f
284	211	96	2019-12-05 15:23:16.014796+00	We have 6 (5 + myself) LaTeX aficionados who would like to try getting a TeX based community going. All are current or formerly active members from the [TeX.SE](https://tex.stackexchange.com/). A private beta period would be preffered over public just to get things started on the right foot, experiment with how it will go, and make sure everything is copasetic before making a more public invite.\r\n\r\n1. The scope would be TeX and all major direct dirivatives[^1] and the tooling dedicated to the TeX ecosystem.\r\n\r\n1. :check:\r\n\r\nAs for special needs, we won't need anything major front.\r\n\r\n* Adding the [LPPL license](https://www.latex-project.org/lppl/) commonly used in LaTeX derivative projects as an option for source code in answers would be lovely, possibly eeven making a CC0/LPPL combo the default.[^2] See [this SE post](https://tex.meta.stackexchange.com/q/1255/5100) for previous discussion of this issue.\r\n\r\n* It is important **not** to enable MathML or MathJax support as this will inevitably conflict with users trying to covey what they want to do in LaTeX terms.\r\n\r\n* I'm sure there will be font preferences, but I don't know what they are right now.\r\n\r\n* Currently there are no "fiddle" style services that would be easy to integrate[^3], but eventually if there was such a thing that covered enough LaTeX engines it definitely be a nice thing to have.\r\n\r\nLong term vector graphics suppport would also be nice. Vector or otherwise, expect this site to have a _lot_ of images (as screen shots will be used in almost every post). Making sure the image system scales well and handles the load will be an important  point when considering long term viability.\r\n\r\n  [^1]: Personally [I'd actually advocate for a bit wider scope](https://tex.meta.stackexchange.com/q/1255/5100) covering several similar digital typesetting systems, but I'm clearly in the minority right now as far as the potential community goes.\r\n  \r\n  [^2]: The default would need some further discussion, at least having LPPL as an option would be very much appreciated as a minimum.\r\n  \r\n  [^3]: Although [SwiftLaTeX](https://github.com/SwiftLaTeX/SwiftLaTeX/issues/12) released earlier today is a promising step towards being able to make one!	2019-12-05 15:48:30.467033+00	12	4	2	\N	0	0	0	\N	\N	We have 6 (5 + myself) LaTeX aficionados who would like to try getting a TeX based community going. All are current or formerly active members from the [TeX.SE](https://tex.stackexchange.com/). A private beta period would be preffered over public just to get things started on the right foot, experiment with how it will go, and make sure everything is copasetic before making a more public invite.	f	f
799	211	895	2020-02-10 19:18:59.197227+00	I'd like to start a C++ programming questions site. I ask and answer a lot of C++ questions over at StackOverflow, or at least I did. I'm looking for a new home: https://stackoverflow.com/users/2642059/jonathan-mee?tab=summary\r\n\r\nMost C++ examples on Stack Overflow were externally hosted by ideone.com or similar, so I don't think any integration would be required. However, code markdown inside backticks or indentation to markdown a codeblock would be helpful.	2020-02-10 19:18:59.197227+00	9	4	1	\N	0	0	0	\N	\N	I'd like to start a C++ programming questions site. I ask and answer a lot of C++ questions over at StackOverflow, or at least I did. I'm looking for a new home: https://stackoverflow.com/users/2642059/jonathan-mee?tab=summary	f	f
710	211	854	2020-01-30 10:45:23.4356+00	I'd like to see a community for programming questions. I'm quite shocked to see it hasn't been created yet.\r\n\r\nFor a more focused community/topic, I'd suggest starting with Java programming questions.\r\n\r\nI'm ready to say goodbye to a certain other popular site for programming questions, and to be active here. I don't have a whole group of people with me, but I hope others will join.	2020-01-30 11:54:12.805141+00	7	4	1	\N	0	0	0	\N	\N	I'd like to see a community for programming questions. I'm quite shocked to see it hasn't been created yet.	f	f
739	653	2	2020-02-02 21:17:45.806634+00	We've changed this so it doesn't scroll down automatically if you are replying…\r\n\r\n…but it will still do so if you merely start typing a message that isn't a reply — in that case the assumption is you would want to see the latest messages before sending the message.	2020-02-02 21:17:45.806634+00	3	1	1	\N	0	0	0	\N	\N	We've changed this so it doesn't scroll down automatically if you are replying…	t	f
823	714	262	2020-02-15 14:09:34.029917+00	There are 22 TeX *basic* delimiters and some more 'special' delimiters. They are documented in *The TeXBook*:\r\n\r\n* [0] The 'blank' delimiter `.`.\r\n* [1-2] Parentheses `(` &ndash; `)`\r\n* [3-4] Brackets `[` or `\\lbrack` &ndash; `]` or `\\rbrack`\r\n* [5-6] Braces `\\{` or `\\lbrace` &ndash; `\\}` or `\\lbrace`\r\n* [7-8] Floors `\\lfloor` &ndash; `\\rfloor`\r\n* [9-10] Ceils `\\lceil` &ndash; `\\rceil`\r\n* [11-12] Angles `<` or `\\langle` &ndash; `>` or `\\rangle`\r\n* [13-14] Slashes `/` &ndash; `\\backslash`\r\n* [15] Vertical bars `|` or `\\vert`\r\n* [16] Double vertical bars `\\|` or `\\Vert`\r\n* [17-22] Arrows `\\uparrow`, `\\Uparrow`, `\\downarrow`, `\\Downarrow`, `\\updownarrow`, `\\Updownarrow`\r\n* [23-29] Some special things I have never seen anywhere else but in *The TeXBook*: `\\arrowvert`, `\\Arrowvert`, `\\bracevert`, `\\lgroup`, `\\rgroup`, `\\lmoustache`, `\\rmoustache`	2020-02-15 14:09:34.029917+00	8	4	3	\N	0	0	0	\N	\N	There are 22 TeX *basic* delimiters and some more 'special' delimiters. They are documented in *The TeXBook*:	f	f
724	654	1	2020-01-31 10:18:40.653812+00	@@@ answer 578	2020-01-31 10:18:40.653812+00	7	1	1	\N	0	0	0	\N	\N	@@@ answer 578	f	f
725	654	1	2020-01-31 10:22:42.722152+00	@@@ answer 187	2020-01-31 10:22:42.722152+00	7	1	1	\N	0	0	0	\N	\N	@@@ answer 187	f	f
753	680	233	2020-02-04 07:27:03.692939+00	You could fill the node:\r\n\r\n\r\n```\r\n\\documentclass{standalone}\r\n\\usepackage{tikz}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\draw (0,0) -- node[circle, fill=white, draw, inner sep=1.2pt]{a} (3,0);\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\n![2020-02-04 (2).png](/image?hash=ab1e916a083a500f3634216f3144dff8e48988e0a72f7ae6c8a27a4317c3e9f3)\r\n	2020-02-04 07:29:21.782231+00	7	4	1	\N	0	0	0	\N	\N	You could fill the node:	f	f
755	680	168	2020-02-04 12:02:46.473487+00	The following defines a `circ path` for the `to` syntax. There is most likely a better way to define it, but my Ti*k*Z skills are limited.\r\n\r\n```tex\r\n\\documentclass[tikz]{standalone}\r\n\r\n\\usetikzlibrary{calc}\r\n\r\n\\tikzset\r\n  {%\r\n    circ path/.style=%\r\n      {%\r\n        to path=%\r\n          {%\r\n            ($\r\n              (\\tikztostart)\r\n              - \\pgfkeysvalueof{/tikz/circ path/pos}*(\\tikztostart)\r\n              + \\pgfkeysvalueof{/tikz/circ path/pos}*(\\tikztotarget)\r\n            $)\r\n              node\r\n                [\r\n                  draw, circle, inner sep=0pt,\r\n                  minimum size={\\pgfkeysvalueof{/tikz/circ path/size}}\r\n                ]\r\n                (\\pgfkeysvalueof{/tikz/circ path/name}) {#1}\r\n            (\\tikztostart) -- (\\pgfkeysvalueof{/tikz/circ path/name})\r\n                           -- (\\tikztotarget)\r\n          }%\r\n      }\r\n    ,circ path/size/.initial = 10pt\r\n    ,circ path/name/.initial = circ path centre\r\n    ,circ path/pos/.initial = 0.5\r\n  }\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n  \\draw (0,0) -- (1.5cm-5pt,0) (1.5,0) circle(5pt) node {a} (1.5cm+5pt,0) -- (3,0);\r\n  \\draw (0,1) to[circ path=a] (3,1);\r\n  \\draw (0,2) to[circ path=b, circ path/name=mypoint] (3,3);\r\n  \\draw (mypoint) to[circ path=c, circ path/pos=0.8] (3,4);\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\n![tikzcirc-1.png](/image?hash=3e6461fc1e0787716c9a34f44c529613b1f4612515ad63c0ebfc8467639cd103)	2020-02-04 18:53:47.213223+00	7	6	3	\N	0	0	0	\N	\N	The following defines a `circ path` for the `to` syntax. There is most likely a better way to define it, but my Ti*k*Z skills are limited.	f	t
767	680	830	2020-02-07 13:12:05.327096+00	```\r\n\\documentclass{standalone}\r\n\\usepackage{tikz}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\draw (0,0) -- (3,0) -- (6,0) -- (9,0) ;\r\n\\draw[fill=white] (0,0) circle[radius=5pt] node{a};\r\n\\draw[fill=white] (3,0) circle[radius=5pt] node{a};\r\n\\draw[fill=white] (6,0) circle[radius=5pt] node{a};\r\n\\draw[fill=white] (9,0) circle[radius=5pt] node{a};\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```	2020-02-08 05:46:55.327592+00	5	4	5	\N	0	0	0	\N	\N	```	f	f
282	307	16	2019-03-06 15:25:32+00	SQL Server only automatically updates the statistics it uses. It may not be actively using the `_WA_Sys` statistics for cardinality estimation, though it may load them as part of the process.\r\n\r\nTo see which statistics are being used for your query, add these undocumented trace flags to the end of your query, and look in the SSMS *Messages* tab:\r\n\r\n`OPTION(QUERYTRACEON 3604, QUERYTRACEON 2363);`\r\n\r\nIf your database is in a compat level prior to 2014, you'll need these:\r\n\r\n`OPTION(QUERYTRACEON 3604, QUERYTRACEON 9292, QUERYTRACEON 9204);`\r\n\r\nThese won't tell you _why_ it's not using certain statistics, though generally if there are statistics created with a higher sampling percentage on the same column, statistics with lower sampling percentages will be disfavored.\r\n\r\nKeep in mind that statistics updates don't occur on modification, only when queries run that use them.	2019-12-05 21:29:26.667829+00	3	4	1	231467	0	0	0	2019-12-05 13:49:46.604059+00	\N	SQL Server only automatically updates the statistics it uses. It may not be actively using the `_WA_Sys` statistics for cardinality estimation, though it may load them as part of the process.	f	f
115	180	12	2016-12-28 13:20:49+00	The results of a multi-statement table-valued function (msTVF) are **never** cached or reused across statements (or connections), but there are a couple of ways that an msTVF result may be reused *within* the same statement. To that extent, an msTVF is not necessarily repopulated each time it is called.\r\n\r\n### Example msTVF\r\n\r\nThis (deliberately inefficient) msTVF returns a specified range of integers, with a timestamp on each row:\r\n\r\n    IF OBJECT_ID(N'dbo.IntegerRange', 'TF') IS NOT NULL\r\n    \tDROP FUNCTION dbo.IntegerRange;\r\n    GO\r\n    CREATE FUNCTION dbo.IntegerRange (@From integer, @To integer)\r\n    RETURNS @T table \r\n    (\r\n    \tn integer PRIMARY KEY, \r\n    \tts datetime DEFAULT CURRENT_TIMESTAMP\r\n    )\r\n    WITH SCHEMABINDING\r\n    AS\r\n    BEGIN\r\n    \tWHILE @From <= @To\r\n    \tBEGIN\r\n    \t\tINSERT @T (n)\r\n    \t\tVALUES (@From);\r\n    \r\n    \t\tSET @From = @From + 1;\r\n    \tEND;\r\n    \tRETURN;\r\n    END;\r\n\r\n## Static table variable\r\n\r\nIf all the parameters to the function call are constants (or runtime constants), the execution plan will populate the table variable result once. The remainder of the plan may access the table variable many times. The static nature of the table variable can be recognised from the execution plan. For example:\r\n\r\n    SELECT\r\n    \tIR.n,\r\n        IR.ts \r\n    FROM dbo.IntegerRange(1, 5) AS IR\r\n    ORDER BY\r\n    \tIR.n;\r\n\r\nReturns a result similar to:\r\n\r\n[![Simple result][1]][1]\r\n\r\nThe execution plan is:\r\n\r\n[![Simple execution plan][2]][2]\r\n\r\nThe Sequence operator first calls the Table Valued Function operator, which populates the table variable (note this operator returns no rows). Next, the Sequence calls its second input, which returns the contents of the table variable (using a Clustered Index Scan in this case).\r\n\r\nThe giveaway that the plan is using a 'static' table variable result is the Table Valued Function operator below a Sequence - the table variable needs to be fully populated once before the remainder of the plan can get going.\r\n\r\n### Multiple accesses\r\n\r\nTo show the table variable result being accessed more than once, we will use a second table with rows numbered from 1 to 5:\r\n\r\n    IF OBJECT_ID(N'dbo.T', 'U') IS NOT NULL\r\n    \tDROP TABLE dbo.T;\r\n    \r\n    CREATE TABLE dbo.T (i integer NOT NULL);\r\n    \r\n    INSERT dbo.T (i) \r\n    VALUES (1), (2), (3), (4), (5);\r\n\r\nAnd a new query that joins this table to our function (this could equally be written as an `APPLY`):\r\n\r\n    SELECT T.i,\r\n           IR.n,\r\n           IR.ts\r\n    FROM dbo.T AS T\r\n    JOIN dbo.IntegerRange(1, 5) AS IR\r\n    \tON IR.n = T.i;\r\n\r\nThe result is:\r\n\r\n[![Join result][3]][3]\r\n\r\nThe execution plan:\r\n\r\n[![Join plan][4]][4]\r\n\r\nAs before, the Sequence populates the table variable msTVF result first. Next, nested loops is used to join each row from table `T` to a row from the msTVF result. Since the function definition included a helpful index on the table variable, an index seek can be used.\r\n\r\nThe key point is that when the parameters to the msTVF are constants (including variables & parameters) or treated as runtime constants for the statement by the execution engine, the plan will feature two separate operators for the msTVF table variable result: one to populate the table; another to access the results, possibly accessing the table multiple times, and possibly making use of indexes declared in the function definition.\r\n\r\n## Correlated parameters and non-constant parameters\r\n\r\nTo highlight the differences when correlated parameters (outer references) or non-constant function parameters are used, we will change the contents of table `T` so the function has much more work to do:\r\n\r\n    TRUNCATE TABLE dbo.T;\r\n    \r\n    INSERT dbo.T (i) \r\n    VALUES (50001), (50002), (50003), (50004), (50005);\r\n\r\nThe following modified query now uses an outer reference to table `T` in one of the function parameters:\r\n\r\n    SELECT T.i,\r\n           IR.n,\r\n           IR.ts\r\n    FROM dbo.T AS T\r\n    CROSS APPLY dbo.IntegerRange(1, T.i) AS IR\r\n    WHERE IR.n = T.i;\r\n\r\nThis query takes around **8 seconds** to return results like:\r\n\r\n[![Correlated result][5]][5]\r\n\r\nNotice the time difference between rows in column `ts`. The `WHERE` clause limits the final result for a sensibly-sized output, but the inefficient function still takes a while to populate the table variable with 50,000-odd rows (depending on the correlated value of `i` from table `T`).\r\n\r\nThe execution plan is:\r\n\r\n[![Correlated execution plan][6]][6]\r\n\r\nNotice the lack of a Sequence operator. Now, there is a single Table Valued Function operator that populates the table variable and returns its rows *on each iteration* of the nested loops join.\r\n\r\nTo be clear: with just 5 rows in table T, the Table Valued Function operator runs 5 times. It generates 50,001 rows on the first iteration, 50,002 on the second...and so on. The table variable is 'thrown away' (truncated) between iterations, so each of the five calls is a full population. This is why it is so slow, and each row takes about the same time to appear in the result.\r\n\r\n**Side notes:**\r\n\r\nNaturally, the scenario above is deliberately contrived to show how poor performance can be when the msTVF populates many rows on each iteration.\r\n\r\nA *sensible* implementation of the above code would set *both* msTVF parameters to `i`, and remove the redundant `WHERE` clause. The table variable would still be truncated and repopulated on each iteration, but only with one row each time.\r\n\r\nWe could also fetch the minimum and maximum `i` values from `T` and store them in variables in a prior step. Calling the function with variables instead of correlated parameters would allow the 'static' table variable pattern to be used as noted earlier.\r\n\r\n## Caching for unchanged correlated parameters\r\n\r\nReturning to address the original question once more, where the Sequence static pattern cannot be used, SQL Server can **avoid** truncating and repopulating the msTVF table variable if none of the correlated parameters have changed since the prior iteration of a nested loop join.\r\n\r\nTo demonstrate this, we will replace the contents of `T` with five *identical* `i` values:\r\n\r\n    TRUNCATE TABLE dbo.T;\r\n    \r\n    INSERT dbo.T (i) \r\n    VALUES (50005), (50005), (50005), (50005), (50005);\r\n\r\nThe query with a correlated parameter again:\r\n\r\n    SELECT T.i,\r\n           IR.n,\r\n           IR.ts\r\n    FROM dbo.T AS T\r\n    CROSS APPLY dbo.IntegerRange(1, T.i) AS IR\r\n    WHERE IR.n = T.i;\r\n\r\nThis time the results appear in around **1.5 seconds**:\r\n\r\n[![Identical row results][7]][7]\r\n\r\nNote the identical timestamps on each row. The cached result in the table variable is reused for subsequent iterations where the correlated value `i` is unchanged. Reusing the result is much faster than inserting 50,005 rows each time.\r\n\r\nThe execution plan looks very similar to before:\r\n\r\n[![Plan for identical rows][8]][8]\r\n\r\nThe key difference is in the **Actual Rebinds** and **Actual Rewinds** properties of the Table Valued Function operator:\r\n\r\n[![Operator properties][9]][9]\r\n\r\nWhen the correlated parameters do not change, SQL Server can replay (rewind) the current results in the table variable. When the correlation changes, SQL Server must truncate and repopulate the table variable (rebind). The one rebind happens on the first iteration; the four subsequent iterations are all rewinds since the value of `T.i` is unchanged.\r\n\r\n  [1]: https://i.stack.imgur.com/WCmqK.png\r\n  [2]: https://i.stack.imgur.com/9WomN.png\r\n  [3]: https://i.stack.imgur.com/50KBJ.png\r\n  [4]: https://i.stack.imgur.com/JR3cE.png\r\n  [5]: https://i.stack.imgur.com/zdZVV.png\r\n  [6]: https://i.stack.imgur.com/3kpgx.png\r\n  [7]: https://i.stack.imgur.com/3MCmL.png\r\n  [8]: https://i.stack.imgur.com/deD58.png\r\n  [9]: https://i.stack.imgur.com/xupDQ.png	2019-11-27 23:42:05.083212+00	2	4	1	159334	0	0	0	2019-11-27 23:42:05.083212+00	\N	The results of a multi-statement table-valued function (msTVF) are **never** cached or reused across statements (or connections), but there are a couple of ways that an msTVF result may be reused *within* the same statement. To that extent, an msTVF is not necessarily repopulated each time it is called.	f	f
583	533	167	2020-01-08 09:59:49.416063+00	Mouse arrows look like this:\r\n\r\n```\r\n\\documentclass{beamer}\r\n\\setbeamertemplate{navigation symbols}{}\r\n\\usepackage{tikzlings}\r\n\\usetikzlibrary{overlay-beamer-styles}\r\n\\begin{document}\r\n\r\n\\begin{frame}\r\n  \r\n\\begin{tikzpicture}[scale=1.7]\r\n  \\mouse\r\n  \\thing[cheese,xshift=5cm,scale=2,yshift=-0.15cm]    \r\n  \r\n  \\begin{scope}[yshift=-2.3cm,xshift=-1.4cm,scale=0.7]\r\n  \r\n    % bow\r\n    \\path[fill=brown!70!black] (2.3671,3.6028) .. controls (2.8360,4.0984) and (2.6217,4.3820) .. (2.5614,4.6753) .. controls (2.6693,5.0301) and (2.8249,5.3243) .. (2.3827,5.7245) .. controls (2.6589,5.1687) and (2.5253,5.0156) .. (2.4604,4.6598) .. controls (2.5432,4.3450) and (2.6150,4.1629) .. (2.3671,3.6028) -- cycle;\r\n  \r\n    % arrow\r\n    \\path<1>[draw=black,line width=2pt,-latex] (1.1936,4.6598) -- (3.0821,4.6753);\r\n    \\path<2>[draw=black,line width=2pt] (7.5,4.6598) -- (9,4.6753);\r\n\r\n    % bowstring\r\n    \\path<1>[draw=black,line width=0.5pt] (2.3671,3.6028) -- (1.4578,4.6520) -- (2.3827,5.7245);\r\n    \\path<2>[draw=black,line width=0.5pt] (2.3671,3.6028) -- (2.3827,5.7245);\r\n    \r\n  \\end{scope}\r\n  \r\n\\end{tikzpicture}  \r\n  \r\n\\end{frame}\r\n  \r\n\\end{document}\r\n```\r\n\r\n![document.gif](/image?hash=88a6fa6e4207b39a2e9f7fbbc2d547424d381ecee4557c328abd9fc904fa4e32)\r\n\r\n(this also explains why there are holes in cheese :) )	2020-01-08 10:08:09.628503+00	9	4	3	\N	0	0	0	\N	\N	Mouse arrows look like this:	f	f
581	534	96	2020-01-08 07:51:53.120938+00	The issue of creating new tags is covered [in this question](https://topanswers.xyz/tex?q=344#question).	2020-01-08 07:51:53.120938+00	4	4	2	\N	0	0	0	\N	\N	The issue of creating new tags is covered [in this question](https://topanswers.xyz/tex?q=344#question).	f	f
598	541	702	2020-01-11 11:24:42.817539+00	# [IBM Plex Sans](https://fonts.google.com/specimen/IBM+Plex+Sans)\r\n\r\n- sans-serif[^f1]\r\n- OFL licensed\r\n- 14 members in the family = different weights of regular, italics, bold, and bold italics\r\n- extensive set of Latin characters (909 glyphs) to cover "[100 languages][1]"; also seven non-Latin scripts\r\n\r\n**Rationale**: aside from its very broad language coverage, [IBM Plex Sans][2] is especially clear and enhances legibility in a Sans font by using a "bar" upper-case `I` *and* a swung foot on lower-case `l`.[^f2] Here it is compared with the current sans options on TA:\r\n\r\n![Selection_494.png](/image?hash=1d10a279dfadc16a29351ddbc5f1d2491f10a9ad1ac150844e44f3846f1cb9f4)\r\n\r\n**Sample**\r\n\r\n![Selection_495.png](/image?hash=a751059a01c2aa8780a2223e2201fbbeeed1caaa321c0efe7ac79168cfc23a38)\r\n\r\n[1]: https://www.ibm.com/plex/languages/\r\n[2]: https://en.wikipedia.org/wiki/IBM_Plex\r\n[3]: https://fonts.google.com/specimen/IBM+Plex+Mono\r\n\r\n[^f1]: There is also a mono and serif in this family, linked on the Google Fonts page. I find the serif a bit busy, [but the `mono`][3] could be a good match if needed for code blocks.\r\n[^f2]: This is, so far as I'm aware, quite a rare combination. If you know of other sans fonts with this feature, please let me know in a comment. Thanks!	2020-01-31 09:06:33.505708+00	3	1	1	\N	0	0	0	\N	\N	[IBM Plex Sans](https://fonts.google.com/specimen/IBM+Plex+Sans)	f	f
593	541	702	2020-01-10 21:11:38.855666+00	# [Charis SIL](https://fontlibrary.org/en/font/charis) \\[added to site: 2020-01-12\\]\r\n\r\n- serif (almost "slab")\r\n- OFL licensed\r\n- four styles (regular, italic, bold, bold italic)\r\n- "In total, over 3,600 glyphs are included, supporting stylistic alternates for a number of characters as well as a large number of ligated sequences."[^f1]\r\n\r\n\r\n**Rationale**: A large glyph with extended Latin ranges, it is one of the few fonts that also correctly handles stacking diacritics. I work with "scientific" transliteration of classical Hebrew, and Charis SIL handles my "stress test" string perfectly (see screen shot, below):\r\n\r\n    bərēšīṯ bå̄rå̄ʾ ʾɛ̆lōhīm ʾēṯ haššå̄mayim wəʾēṯ hå̄ʾå̄rɛṣ\r\n\r\nIt [also includes][1] the Cyrillic range.\r\n\r\nCharis is based on [Bitstream Charter][2], and works very well both on screen and in print.\r\n\r\n**Sample**\r\n\r\n![charis-sil.png](/image?hash=db93af39b648842c388dcb141c3e5e7422f318e8bb082b6008e57cff27be7d7a)\r\n\r\n[1]: http://software.sil.org/charis/support/character-set-support/\r\n[2]: https://en.wikipedia.org/wiki/Bitstream_Charter\r\n[3]: http://software.sil.org/lcgfonts/font-subsets/\r\n\r\n[^f1]: It is also available in "[sub-sets][3]" if the full repertoire of glyphs is not required.	2020-01-31 11:09:38.521536+00	7	1	1	\N	0	0	0	\N	\N	[Charis SIL](https://fontlibrary.org/en/font/charis) \\[added to site: 2020-01-12\\]	f	f
723	541	702	2020-01-31 09:29:04.35938+00	# [Merriweather][m1]\r\n\r\n- serif\r\n- OFL license\r\n- eight members in family: normal and italic in four weights (light through "black")\r\n- it currently supports the following unicode ranges: Latin-1, Latin-2, Cyrillic, Turkish, Windows Baltic, Macintosh Character Set\r\n\r\n**Rationale**: From the [Google Fonts][g1] description:\r\n\r\n> Merriweather was designed to be a text face that is pleasant to read on screens. It features a very large x height, slightly condensed letterforms, a mild diagonal stress, sturdy serifs and open forms. \r\n\r\nAnd [from the designer][m2]:\r\n\r\n> Merriweather offers a Renaissance warmth while using proportions which are space-saving. It is suitable for editorial design, news and other kinds of space sensitive typography. Merriweather is an evolving project and will be updated.\r\n\r\nIt is the relatively large x-height, with correspondingly low ascenders/descenders, and open counters (and, to my eye, a certain elegance!) that make this an attractive choice for long-form reading. (I don't use anything else on Wikipedia, for example.) It has come a long way in terms of range of glyphs supported since its early release. For general purpose reading on websites it has become something of a staple, it seems to me.[^fn1]\r\n\r\nI debated with myself whether to "poll" for Merriweather or [Gelasio][ge] (also by [Eben Sorkin][es]), but don't want to flood this Q&A with "near neighbours". The differential for Charis (which has already been added as an option on TA) is that Charis implements diacritics in a more robust way than Merriweather (and most other fonts, tbf) so will serve linguists, e.g., in particular. Merriweather is a more general purpose option. I hope TA can support it!\r\n\r\n**Sample**\r\n\r\n![_-ta-merriweather.png](/image?hash=e1634f65f2b8ba68829300914991ecfaaa67c8484882aeee08fd666f16f3c590)\r\n\r\n[m1]: https://fontlibrary.org/en/font/merriweather\r\n[g1]: https://fonts.google.com/specimen/Merriweather\r\n[ge]: https://fonts.google.com/specimen/Gelasio\r\n[es]: http://sorkintype.com/\r\n[m2]: https://github.com/SorkinType/Merriweather#basic-font-information\r\n\r\n[^fn1]: This is not just anecdotal: according to Google Fonts, "Merriweather is featured in more than 4,200,000 websites".	2020-01-31 11:13:00.805312+00	3	1	1	\N	0	0	0	\N	\N	[Merriweather](https://fontlibrary.org/en/font/merriweather)	f	f
275	302	12	2019-08-02 13:46:56+00	> Why optimizer introduces spool here, what is the reason to do it? There is nothing complex beyond the spool.\r\n\r\nThe thing beyond the spool is not a simple table reference, which could simply be duplicated when the left join / anti semi join [alternative][1] is generated.\r\n\r\nIt may *look* a little like a table (Constant Scan) but to the optimizer[^1] it is a `UNION ALL` of the separate rows in the `VALUES` clause.\r\n\r\nThe additional complexity is enough for the optimizer to choose to spool and replay the source rows, and not replace the spool with a simple "table get" later on. For example, the initial transformation from full join looks like this:\r\n\r\n[![early plan][2]][2]\r\n\r\nNotice the extra spools introduced by the general transform. Spools above a simple table get are cleaned up later by the rule `SpoolGetToGet`.\r\n\r\nIf the optimizer had a corresponding `SpoolConstGetToConstGet` rule, it could work as you wish, in principle.\r\n\r\n> How to get rid of it in this case, what are the possible ways?\r\n\r\nUse a real table (temporary or variable), or write the transformation from full join manually, for example:\r\n\r\n```\r\nWITH \r\n    p([Id], [Code]) AS\r\n    (\r\n        SELECT @id1, 'A'\r\n        UNION ALL\r\n        SELECT @id2, 'B'\r\n    ),\r\n    FullJoin AS\r\n    (\r\n        SELECT\r\n            p.Code,\r\n            d.[Status]\r\n        FROM p\r\n        LEFT JOIN #data d \r\n            ON d.[Id] = p.[Id]\r\n        UNION ALL\r\n        SELECT\r\n            NULL,\r\n            D.[Status]\r\n        FROM #data AS D\r\n        WHERE NOT EXISTS\r\n        (\r\n            SELECT *\r\n            FROM p\r\n            WHERE p.Id = D.Id\r\n        )\r\n    )\r\nSELECT\r\n    COALESCE(FullJoin.Code, 'X') AS Code,\r\n    COALESCE(FullJoin.Status, 0) AS [Status]\r\nFROM FullJoin;\r\n```\r\n\r\nPlan for manual rewrite:\r\n\r\n[![Manual rewrite plan][3]][3]\r\n\r\nThis has an estimated cost of 0.0067201 units, compared with 0.0203412 units for the original.\r\n\r\n  [1]: https://blogs.msdn.microsoft.com/craigfr/2006/07/26/nested-loops-join/\r\n  [2]: https://i.stack.imgur.com/iy5Nm.png\r\n  [3]: https://i.stack.imgur.com/lCdoq.png "estimated cost 0.0067201"\r\n  \r\n  [^1]: It can be observed as a `LogOp_UnionAll` in the *Converted Tree* (TF 8605). In the *Input Tree* (TF 8606) it is a `LogOp_ConstTableGet`. The *Converted Tree* shows the tree of optimizer expression elements after parsing, normalization, algebrization, binding, and some other preparatory work. The *Input Tree* shows the elements after conversion to Negation Normal Form (NNF convert), runtime constant collapsing, and a few other bits and bobs. NNF convert includes logic to collapse logical unions and common table gets, among other things.	2019-12-05 02:35:01.561738+00	2	4	1	244424	0	0	0	2019-12-05 02:33:11.117062+00	\N	> Why optimizer introduces spool here, what is the reason to do it? There is nothing complex beyond the spool.	f	f
716	649	2	2020-01-30 23:23:16.258301+00	Very sensible suggestions, thank you.\r\n\r\nI'll update this post when we release a fix.	2020-01-30 23:23:16.258301+00	3	1	1	\N	0	0	0	\N	\N	Very sensible suggestions, thank you.	f	f
760	683	702	2020-02-06 14:13:58.380312+00	I really like this idea. I see that it originated in the discussion over whether tables should have [proportional or monospaced fonts][1], but the potential use-cases go beyond anticipating custom CSS.\r\n\r\nThere is, I *think*, a [Markdown-it plugin][2] which provides this functionality: `markdown-it-container-pandoc`.\r\n\r\n**Backstory**: the site on which I was most active on the SE network often involved long essays as answers (sometimes as questions!) and often enough I wished I could add "id" to my headings to function as inner-links. TA already generates an ID based on any `<h*>...</h*>` text, but that could be quite cumbersome (almost inevitably spaces are involved which require `%20` in the link). But something like:\r\n\r\n```markdown\r\n::: {#myhead3}\r\n## My subheading in a very long essay\r\n:::\r\n```\r\n\r\nWould provide:\r\n\r\n```html\r\n<div id="myhead3">\r\n<h2 id="all_the_TA_stuff_here">My subheading in a very long essay</h2>\r\n</div>\r\n```\r\n\r\nwhich I think would be great. There may well be drawbacks I'm missing (e.g. is this the "arbitrary CSS" that Adám's warned about at the end of his question?), but meanwhile ... yes, please!\r\n\r\n[1]: https://topanswers.xyz/meta?q=646\r\n[2]: https://www.npmjs.com/package/markdown-it-container-pandoc?activeTab=readme	2020-02-06 14:16:03.011744+00	0	1	1	\N	0	0	0	\N	\N	I really like this idea. I see that it originated in the discussion over whether tables should have [proportional or monospaced fonts](https://topanswers.xyz/meta?q=646), but the potential use-cases go beyond anticipating custom CSS.	f	f
770	683	2	2020-02-08 13:01:41.336225+00	I'm marking this as `wont-fix` because it is (too) generic — we'll consider requests for markdown plugins that serve a specific need (such as monospace perhaps) but I don't think there needs to be a generic implementation like this.\r\n\r\nSo we might add something like:\r\n\r\n```\r\n:::monospace\r\ncontent\r\n:::\r\n```\r\n\r\nbut not \r\n```\r\n:::font-family:monospace\r\ncontent\r\n:::\r\n```\r\n\r\n> Then we can always add more CSS-rule classes later.\r\n\r\nadding plugins is relatively trivial from a code perspective, so that side of things isn't an issue. We just want to be very careful we are sure they meet a specific need first, because removing them isn't so straightforward once they are in use, and each one will add it's own maintenance burden.	2020-02-08 13:01:41.336225+00	4	1	1	\N	0	0	0	\N	\N	I'm marking this as `wont-fix` because it is (too) generic — we'll consider requests for markdown plugins that serve a specific need (such as monospace perhaps) but I don't think there needs to be a generic implementation like this.	t	f
784	693	167	2020-02-10 16:36:48.183091+00	I suggest to use the `enumitem` package to place multiple subitems in a single line. You don't need to be worried about suspending and restarting the linguex counter because you could just place the the additional `enumerate` environment between your `\\ex.` ones.\r\n\r\n```\r\n\\documentclass{article}\r\n\\usepackage{multicol}\r\n\\usepackage{linguex}\r\n\\usepackage{tikz}\r\n\\usepackage[inline]{enumitem}\r\n\r\n\\begin{document}\r\n\r\n\t\\ex. Two something\r\n\t\t\\a. foo\r\n\t\t\\b. bar\r\n\r\n\\begin{enumerate}[label={(\\arabic*)},labelsep=\\Exlabelsep\\relax,labelwidth=\\Exlabelwidth]\r\n\t\\setcounter{enumi}{\\value{ExNo}}\r\n\t\\item \\hspace{0.6em}Two ways of forming a triangle.\r\n\t\r\n\t\\begin{enumerate*}[label={\\hspace{-0.9em}\\alph*.}]\r\n\t\t\\item\r\n\t\t\\begin{tikzpicture}\r\n\t\t\t\\draw (0,0) -- (-1.5,-3) -- (1.5,-3) -- cycle;\r\n\t\t\\end{tikzpicture}\r\n\t\t\\hspace*{\\fill}\r\n\t\t\\item\r\n\t\t\\begin{tikzpicture}\r\n\t\t\t\\draw (0,0) -- (3,0) -- (3,3) -- cycle;\r\n\t\t\\end{tikzpicture}\r\n\t\\end{enumerate*}\r\n\\end{enumerate}\r\n\\stepcounter{ExNo}\r\n\t\r\n\t\\ex. Two something\r\n\t\t\\a. foo\r\n\t\t\\b. bar\r\n\r\n\\end{document}\r\n```\r\n![Screen Shot 2020-02-10 at 17.30.16.png](/image?hash=bcd9bf5c67f1bccfac9ba7dd91e9999003f09806fb2bfc3ee390373bba147bdd)	2020-02-10 18:27:51.529662+00	7	4	3	\N	0	0	0	\N	\N	I suggest to use the `enumitem` package to place multiple subitems in a single line. You don't need to be worried about suspending and restarting the linguex counter because you could just place the the additional `enumerate` environment between your `\\ex.` ones.	f	t
832	692	234	2020-02-17 03:55:18.89842+00	Secretly recorded dialogue.\r\n\r\n```\r\n\\documentclass{beamer}\r\n\\setbeamertemplate{navigation symbols}{}\r\n\\usepackage{tikz,tikzducks}\r\n\\usepackage{tikzlings}\r\n\\usepackage{textcomp}\r\n\\newcommand{\\musicnotes}{\\raisebox{-2pt}{\\raisebox{4pt}{\\textmusicalnote}%\r\n    \\textmusicalnote\\hspace{2pt}% https://tex.stackexchange.com/a/458440\r\n    \\raisebox{4pt}{\\textmusicalnote}}}\r\n\\usetikzlibrary{shapes.geometric,shapes.callouts,calc}\r\n\\newcommand\\MarmotSays[2][]{\\only<.(1)>{\r\n\\node[ellipse callout, draw,fill=white,align=center,text width=2.5cm,\r\nanchor=south east,\r\n callout absolute pointer={(0.41,0.35)},#1] at (0.35,0.5)  {#2};\r\n}}\r\n\\newcommand\\CatSays[2][]{\\only<.(1)>{\r\n\\node[ellipse callout, draw,fill=white,align=center,text width=2.5cm,\r\nanchor=south west,\r\n callout absolute pointer={(0.61,0.35)},#1] at (0.65,0.5)  {#2};\r\n}}\r\n\\begin{document}\r\n\\begin{frame}\r\n\\begin{tikzpicture}[overlay,remember picture]\r\n\\pgfmathsetseed{123}\r\n\\foreach \\X in {1,...,42} \r\n{\\pgfmathsetmacro{\\myx}{rnd}%\r\n\\pgfmathsetmacro{\\myy}{rnd}%\r\n\\ifnum\\X=1\r\n\\xdef\\LstStars{(\\myx,\\myy)}%\r\n\\else\r\n\\xdef\\LstStars{\\LstStars,(\\myx,\\myy)}%\r\n\\fi}\r\n\\fill (current page.south west) rectangle (current page.north east);\r\n\\begin{scope}[shift={(current page.south west)},%\r\nx={($(current page.south east)-(current page.south west)$)},\r\ny={($(current page.north east)-(current page.south east)$)}]\r\n\\only<7->{\\foreach \\X in {1,...,84}\r\n{\\pgfmathsetmacro{\\myscale}{0.4*(1+2*rnd)/3}\r\n\\duck[shift={(rnd,rnd)},xscale=0.069*\\myscale,yscale=0.1*\\myscale,body=gray]}}\r\n\\foreach \\X in \\LstStars\r\n{\\path \\X node[star,fill=yellow,inner sep={2*(1+rnd)*1pt}] {};}\r\n\\fill[gray] ([yshift=2cm]current page.south west) to[bend left=8]\r\n([yshift=2cm]current page.south east) |- (current page.south west);\r\n\\cat[shift={(0.6,0.2)},xscale=0.069,yscale=0.1,eyes=green]\r\n\\marmot[shift={(0.42,0.2)},xscale=0.069,yscale=0.1,teeth,whiskers]\r\n\\MarmotSays{\\musicnotes}\r\n\\pause \r\n\\CatSays[node font=\\large\\bfseries]{MEOW!}\r\n\\pause \r\n\\MarmotSays{I think we can talk normal. Nobody is watching us.}\r\n\\pause \r\n\\CatSays{Think so, too.}\r\n\\pause \r\n\\MarmotSays{BTW, did you know that 80\\%\r\n of the matter of our universe is duck matter?}\r\n\\pause \r\n\\CatSays{Really? I always\r\n thought it was \\emph{dark} matter.}\r\n\\pause \r\n\\CatSays{OK OK, I see your  point.}\r\n\\end{scope}\r\n\\end{tikzpicture}\r\n\\end{frame}\r\n\\end{document}\r\n```\r\n\r\n\r\n![ani.gif](/image?hash=ff110013b4c49d8c5739fdbc08df35b56eeceaf5dbd85ae1813e9f0a1fe4a6dd)\r\n	2020-02-17 05:50:51.861534+00	10	4	3	\N	0	0	0	\N	\N	Secretly recorded dialogue.	f	f
809	692	167	2020-02-11 22:07:23.984787+00	# Live from the red carpet\r\n\r\n```\r\n% !TeX TS-program = latexmk -latexoption="-shell-escape -interaction=nonstopmode" % | txs:///view-pdf | convert -delay 300 -loop 0 -density 100 -alpha remove %.pdf %.gif\r\n\r\n\\documentclass{beamer}\r\n\\setbeamertemplate{navigation symbols}{}\r\n\\setbeamercolor{background canvas}{bg=black}\r\n\\usepackage{tikzlings}\r\n\\usepackage{bearwear}\r\n\\usetikzlibrary{patterns.meta,shapes.callouts}\r\n\\usetikzlibrary{fadings,calc,decorations.pathmorphing}\r\n\\usetikzlibrary{overlay-beamer-styles}\r\n\r\n\\newcommand{\\pole}[1][100]{%\r\n\t\\fill[gray!#1!black] (0,0) ellipse[x radius=1,y radius=0.3];\r\n\t\\fill[gray!#1!black] (-1,0.05) -- (1,0.05) -- (0,0.6) -- cycle;\r\n\t\\fill[gray!#1!black] (-0.1,0) rectangle (0.1,3);\r\n}\r\n\r\n\\newcommand{\\rope}{%\r\n\t\\shade[top color=red!30!black, bottom color=red!70!black]  (1.1405,3.1581) .. controls (1.0576,3.1548) and (1.0042,3.1710) .. (1.0042,3.1710){[rotate=0.09] arc(-39.804:3.675:0.143)} .. controls (1.0369,3.2718) and (1.1932,3.2168) .. (1.3962,3.3507) .. controls (1.5993,3.4846) and (1.8498,3.8188) .. (1.9890,4.6312){[rotate=0.09] arc(131.018:94.676:0.143)} .. controls (2.2524,4.5418) and (2.3985,4.5361) .. (2.5303,4.5897) .. controls (2.6620,4.6433) and (2.7801,4.7642) .. (2.8764,4.9049) .. controls (3.0688,5.1863) and (3.1711,5.5375) .. (3.1711,5.5375){[rotate=0.09] arc(232.107:275.586:0.143)} .. controls (3.2729,5.5082) and (3.1694,5.1457) .. (2.9638,4.8451) .. controls (2.8610,4.6948) and (2.7320,4.5575) .. (2.5702,4.4916) .. controls (2.4266,4.4332) and (2.2572,4.4428) .. (2.0789,4.5419) .. controls (1.9344,3.7664) and (1.6842,3.4138) .. (1.4546,3.2623) .. controls (1.3361,3.1842) and (1.2235,3.1614) .. (1.1405,3.1581) -- cycle;\r\n}\r\n\r\n\\tikzset{\r\n\tmystyle/.style={\r\n\t\tfill=lightgray,\r\n\t\tellipse callout, \r\n\t\ttext width=3.5cm,\r\n\t\talign=center,\r\n\t}\r\n}\r\n\r\n% Externalizing the golden marmot statue because the transparency did not work for the conversion to gif\r\n\\usetikzlibrary{external}\r\n%\\tikzset{external/force remake}\r\n\r\n\\tikzset{\r\n\texternal/system call/.add={}{\r\n\t\t;\r\n\t\t/usr/bin/sips -s format png "\\image.pdf" --out "\\image.png";\r\n\t},\r\n\t/pgf/images/external info,\r\n\t/pgf/images/include external/.code={\\includegraphics[width=\\pgfexternalwidth]{#1.png}},\r\n}%\r\n\r\n\r\n\\begin{tikzfadingfrompicture}[name=temp]%\r\n\t\\begin{scope}[transparent!0] \r\n\t\t\\marmot[scale=1.5]\r\n\t\\end{scope}\r\n\\end{tikzfadingfrompicture}%\r\n\r\n\\tikzexternalize\r\n\r\n\\begin{document}\r\n\r\n\\begin{frame}\r\n\\centering\r\n\r\n% code for the golden marmot statue and GoldLeaf.jpg from https://tex.stackexchange.com/a/510465\r\n\\begin{tikzpicture}\r\n\t\\begin{scope}[local bounding box=X,opacity=0]%\r\n\t\t\\marmot[scale=1.5]\r\n\t\\end{scope}\r\n\t\\path[overlay] (X.center)  node[opacity=0,inner sep=0pt] (img) {\\includegraphics{GoldLeaf.jpg}}[\r\n\t\tpath fading=temp,\r\n\t\tfit fading=false,\r\n\t\tfading transform={shift={(X.center)}}\r\n\t] \r\n  \tlet \\p1=($(X.north east)-(X.south west)+(0.3,0.3)$),\r\n  \t\t\t\\p2=($(img.north east)-(img.south west)+(0.3,0.3)$) in (X.center)  \r\n  \t\t\tnode[inner sep=0pt,anchor=center]{\r\n  \t\t\t\t\\pgfmathsetmacro{\\myscale}{max(\\x1/\\x2,\\y1/\\y2)}%\r\n  \t\t\t\t\\includegraphics[scale=\\myscale]{GoldLeaf.jpg}\r\n  \t\t\t};\r\n\\end{tikzpicture}\r\n\r\n\\Huge\\color{yellow!20!white}\r\nLive from the\\linebreak TikZling Academy Awards\r\n\\end{frame}\r\n\r\n\\tikzexternaldisable\r\n\r\n\\begin{frame}\r\n\\begin{tikzpicture}[remember picture, overlay]\r\n\r\n% red carpet\r\n\\shade[top color=black, bottom color=red!50!black] (current page.south east) -- (current page.south west) -- (current page.north) -- cycle;\r\n\r\n% poles\r\n\\begin{scope}[yshift=-4.5cm,scale=0.7]\r\n\\pole\r\n\\end{scope}\r\n\\begin{scope}[yshift=-4.5cm,scale=0.7,xshift=15.5cm]\r\n\\pole\r\n\\end{scope}\r\n\\begin{scope}[yshift=-2.5cm,scale=0.5,xshift=2cm]\r\n\\pole[80]\r\n\\end{scope}\r\n\\begin{scope}[yshift=-2.5cm,scale=0.5,xshift=19.7cm]\r\n\\pole[80]\r\n\\end{scope}\r\n\\begin{scope}[yshift=-1cm,scale=0.3,xshift=7cm]\r\n\\pole[60]\r\n\\end{scope}\r\n\\begin{scope}[yshift=-1cm,scale=0.3,xshift=29cm]\r\n\\pole[60]\r\n\\end{scope}\r\n\r\n% rope\r\n\\begin{scope}[yshift=-5.55cm,xshift=-1.05cm]\r\n\t\\rope\r\n\\end{scope}\r\n\\begin{scope}[yshift=-5.55cm,xshift=12cm,xscale=-1]\r\n\t\\rope\r\n\\end{scope}\r\n\r\n% spotlights\r\n\\fill[yellow!20!white] (0,3.5) circle[radius=0.25];\r\n\\fill[yellow!20!white] (1.5,3.3) circle[radius=0.2];\r\n\\fill[yellow!20!white] (3,3.1) circle[radius=0.15];\r\n\\begin{scope}[opacity=.2]\r\n\t\\fill[yellow!20!white] (0.25,3.5) -- (-0.25,3.5) -- (1.5,-4.5) -- (5.5,-4.5) -- cycle (3.5,-4.5) ellipse[x radius=2, y radius=0.8];\r\n\t\\fill[yellow!20!white] (1.7,3.3) -- (1.3,3.3) -- (2,-3) -- (5.5,-3) -- cycle (3.75,-3) ellipse[x radius=1.75, y radius=0.8];\r\n\t\\fill[yellow!20!white] (3.15,3.1) -- (2.85,3.1) -- (2.5,-1.5) -- (5.5,-1.5) -- cycle (4,-1.5) ellipse[x radius=1.5, y radius=0.8];\r\n\\end{scope}\r\n\\begin{scope}[xscale=-1,xshift=-11cm]\r\n\t\\fill[yellow!20!white] (0,3.5) circle[radius=0.25];\r\n\t\\fill[yellow!20!white] (1.5,3.3) circle[radius=0.2];\r\n\t\\fill[yellow!20!white] (3,3.1) circle[radius=0.15];\r\n\t\\begin{scope}[opacity=.2]\r\n\t\t\\fill[yellow!20!white] (0.25,3.5) -- (-0.25,3.5) -- (1,-4) -- (5,-4) -- cycle (3,-4) ellipse[x radius=2, y radius=0.8];\r\n\t\t\\fill[yellow!20!white] (1.7,3.3) -- (1.3,3.3) -- (2,-2.5) -- (5.5,-2.5) -- cycle (3.75,-2.5) ellipse[x radius=1.75, y radius=0.8];\r\n\t\t\\fill[yellow!20!white] (3.15,3.1) -- (2.85,3.1) -- (2.5,-1) -- (5.5,-1) -- cycle (4,-1) ellipse[x radius=1.5, y radius=0.8];\r\n\t\\end{scope}\r\n\\end{scope}\r\n\r\n% Ms. Marmot\r\n\\marmot[scale=1.4,xshift=2.75cm,yshift=-3cm]\r\n\r\n% open mouth\r\n\\begin{scope}[scale=1.4,xshift=2.75cm,yshift=-3cm]\r\n\t\\draw<1,2,4,5,6,9,10,13,14,16>[brown!50!black, fill=red!30!gray] (0.145,1.51) arc [start angle=-20, end angle=-160, radius=0.16] -- (-0.14,1.51) -- (-0.14,1.51) arc [start angle=-160, end angle=-20, x radius=0.144, y radius=0.05] -- cycle ;\r\n\\end{scope}\r\n\r\n% hair\r\n\\begin{scope}[scale=0.5,yscale=-1,xshift=5.5cm,yshift=2cm]\r\n\t\\fill[brown!50!white] (2.0201,0.0901) .. controls (1.6323,0.0966) and (1.2459,0.3061) .. (0.9988,0.8087) .. controls (0.6334,1.5906) and (0.5612,1.4448) .. (0.1769,1.4988) .. controls (0.2345,1.8704) and (0.4938,1.8402) .. (0.5104,1.8485) .. controls (0.4226,1.9130) and (0.0072,2.1269) .. (0.1047,2.3381) .. controls (0.2681,2.1960) and (0.9031,2.2368) .. (0.3121,2.5500) .. controls (0.2977,2.5556) and (0.4744,2.6744) .. (0.7330,2.5749) .. controls (1.4192,2.2718) and (0.6557,0.9667) .. (2.0513,0.6112) .. controls (2.1041,0.5913) and (2.1570,0.5779) .. (2.2097,0.5701) .. controls (2.2623,0.5779) and (2.3152,0.5913) .. (2.3681,0.6112) .. controls (3.7636,0.9667) and (2.9121,2.4353) .. (3.5983,2.7384) .. controls (3.8568,2.8379) and (4.0713,2.8072) .. (4.1827,2.5878) .. controls (4.0987,2.6004) and (3.8318,2.5241) .. (4.0075,2.4390) .. controls (4.1653,2.4134) and (4.3019,2.3124) .. (4.3398,2.1620) .. controls (4.4372,1.9508) and (4.1728,2.1017) .. (4.0850,2.0372) .. controls (4.2023,1.9786) and (4.4112,1.9206) .. (4.4562,1.8258) .. controls (4.2101,1.8217) and (3.9166,1.8409) .. (3.7412,1.6503) .. controls (3.5922,1.3864) and (3.5515,1.0808) .. (3.4205,0.8087) .. controls (3.1331,0.2241) and (2.6571,0.0361) .. (2.2097,0.1029) .. controls (2.1468,0.0935) and (2.0835,0.0891) .. (2.0201,0.0901) -- cycle;\r\n\\end{scope}\r\n\r\n% dress\r\n\\begin{scope}[scale=2,yscale=-1,yshift=-4.35cm,xshift=-1.6cm]\r\n\t\\fill[cyan!40!gray] (3.9392,6.2701) -- (3.1076,6.2934) .. controls (3.2239,6.2379) and (3.0852,6.0280) .. (3.1085,5.6297) .. controls (3.2745,5.3921) and (3.2946,5.7384) .. (3.5301,5.7384) .. controls (3.7655,5.7384) and (3.7390,5.3987) .. (3.9383,5.6264) .. controls (3.9837,5.8279) and (3.8306,6.1943) .. (3.9392,6.2701) -- cycle;\r\n\\end{scope}\r\n\r\n% re-drawing arms on top of dress\r\n\\begin{scope}[scale=1.4,xshift=2.75cm,yshift=-3cm]\r\n\t\\fill[brown!50!black,rotate around={70:(0.385,0.93)}] (0.385,0.93) ellipse[x radius=0.24, y radius=0.13];\r\n\t\\fill[brown!50!black,rotate around={-70:(-0.385,0.93)}] (-0.385,0.93) ellipse[x radius=0.24, y radius=0.13];\r\n\\end{scope}\r\n\r\n% microphone\r\n\\fill[lightgray] (4.35,-3.3) -- (4.45,-3.3) -- (4.5,-2.7) -- (4.3,-2.7);\r\n\\fill[darkgray] (4.4,-2.6) circle[radius=0.18];\r\n\r\n% Mr. Bär\r\n\\bear[scale=1.4,xshift=5cm,yshift=-3cm]\r\n\\begin{scope}[scale=1.4,xshift=5cm,yshift=-3cm]\r\n\t\\bearwear[\r\n\t\tshirt={\r\n\t\t\tshade,\r\n    \ttop color=blue,\r\n    \tbottom color=green\r\n    }\r\n   ];\r\n\\end{scope}\r\n\r\n% open mouth\r\n\\begin{scope}[scale=1.4,xshift=5cm,yshift=-3.15cm]\r\n\t\\draw<3,7,8,11,12,15>[brown!50!black, fill=red!30!gray] (0.145,1.51) arc [start angle=-20, end angle=-160, radius=0.16] -- (-0.14,1.51) -- (-0.14,1.51) arc [start angle=-160, end angle=-20, x radius=0.144, y radius=0.05] -- cycle ;\r\n\\end{scope}\r\n\r\n% dialogue\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {This is Christine live from the red carpet};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {With me today: the famous B\\"ar!};\r\n\\node<+>[mystyle, callout absolute pointer={(7.4,-2.25)}] at (8.5,1) {Hi! Nice to see you!};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {You are nominated for the TikZling Academy Awards in two categories \\ldots};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {\\ldots as "best actor" and "best assistant b\\"ar".};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {How did you manage that?};\r\n\\node<+>[mystyle, callout absolute pointer={(7.4,-2.25)}] at (8.5,1) {Both nominations are for the "The Great Tikzlings Christmas Extravaganza" \\ldots};\r\n\\node<+>[mystyle, callout absolute pointer={(7.4,-2.25)}] at (8.5,1) {\\ldots an incredible movie series I really enjoy to contribute both behind and in front of the camera};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {I heard rumours that the whole production of the last episode was done in only a couple of weeks \\ldots};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {\\ldots wasn't that very stressful?};\r\n\\node<+>[mystyle, callout absolute pointer={(7.4,-2.25)}] at (8.5,1) {You're right. Many of the team members had to fit TGTCE around other commitments \\ldots};\r\n\\node<+>[mystyle, callout absolute pointer={(7.4,-2.25)}] at (8.5,1) {\\ldots but we managed to finish in time for Christmas.};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {And now the traditional red carpet question:};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {Which designer created this fabulous shirt?};\r\n\\node<+>[mystyle, callout absolute pointer={(7.4,-2.25)}] at (8.5,1) {Actually, it is from my new BearWear collection!};\r\n\\node<+>[mystyle, callout absolute pointer={(3.3,-2)}] at (2,1) {Oh! Is it also available in marmot size?};\r\n\\end{tikzpicture}\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n![RedCarpet.gif](/image?hash=e449edee6e75a54d426cff248e0c1e94369298962871d6bb96e26b0f1e62fe73)\r\n\r\nIn case someone has not yet seen the movie the bear talks about: https://vimeo.com/380684973	2020-02-17 16:22:50.006827+00	9	4	3	\N	0	0	0	\N	\N	Live from the red carpet	f	t
747	675	167	2020-02-03 10:06:57.201212+00	Normally the subsection name is shown at the top right of this theme. In case you don't need subsections to structure your document, you could use them to add the desired information or you could modify the theme to insert a new macro which you can alter for each frame.\r\n\r\n```\r\n\\documentclass{beamer}\r\n\r\n\\mode<presentation>\r\n{ \r\n\\usetheme{AnnArbor}\r\n\\usecolortheme[named=green]{structure}\r\n\\useinnertheme{circles}\r\n\\usefonttheme[onlymath]{serif}\r\n\\setbeamercovered{transparent}\r\n\\setbeamertemplate{blocks}[rounded][shadow=true]\r\n}\r\n\r\n\\newcommand{\\yannis}{}\r\n\r\n\\setbeamertemplate{headline}{%\r\n  \\leavevmode%\r\n  \\hbox{%\r\n  \\begin{beamercolorbox}[wd=.5\\paperwidth,ht=2.65ex,dp=1.5ex,right]{section in head/foot}%\r\n    \\usebeamerfont{section in head/foot}\\insertsectionhead\\hspace*{2ex}\r\n  \\end{beamercolorbox}%\r\n  \\begin{beamercolorbox}[wd=.5\\paperwidth,ht=2.65ex,dp=1.5ex,right]{subsection in head/foot}%\r\n    \\usebeamerfont{subsection in head/foot}\\hspace*{2ex}\\yannis\\hspace*{2ex}\r\n  \\end{beamercolorbox}}%\r\n  \\vskip0pt%\r\n}\r\n\r\n\\begin{document}\r\n\r\n\r\n{\r\n\\renewcommand{\\yannis}{Mr. Duck, 2019}\r\n\\begin{frame}\r\n\\frametitle{title}\r\ncontent...\r\n\\end{frame}\r\n}\r\n\r\n\\begin{frame}\r\n\\frametitle{title}\r\ncontent...\r\n\\end{frame}\r\n\r\n{\r\n\\renewcommand{\\yannis}{Clever Marmot, 2012}\r\n\\begin{frame}\r\n\\frametitle{title}\r\ncontent...\r\n\\end{frame}\r\n}\r\n\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2020-02-03 at 11.09.23.png](/image?hash=2906c3ba99bb7354ae1bf8ee7e45335629a543c222201891264ac793d99d584b)\r\n\r\n(I would prefer if this answer would not be copied to tex.se)	2020-02-04 12:45:41.759335+00	10	4	3	\N	0	0	0	\N	\N	Normally the subsection name is shown at the top right of this theme. In case you don't need subsections to structure your document, you could use them to add the desired information or you could modify the theme to insert a new macro which you can alter for each frame.	f	t
774	691	168	2020-02-09 23:54:29.697879+00	I did not test thoroughly whether all the features work, but it seems like loading `babel` after `unicode-math` fixes the issue at hand.\r\n\r\n```tex\r\n\\documentclass{scrartcl}\r\n\r\n\\usepackage{unicode-math}\r\n\\usepackage[spanish, activeacute]{babel}\r\n\\usepackage{hyperref}\r\n\r\n\\begin{document}\r\n\\section{A section}\r\n'a\r\n\\begin{equation}\r\n  j =\\sqrt{-1}\r\n\\end{equation}\r\n\\end{document}\r\n```	2020-02-09 23:54:54.893941+00	5	6	3	\N	0	0	0	\N	\N	I did not test thoroughly whether all the features work, but it seems like loading `babel` after `unicode-math` fixes the issue at hand.	f	t
199	253	12	2019-05-30 08:53:35+00	This is a bug in *project normalization*, exposed by using a subquery inside a case expression with a non-deterministic function.\r\n\r\nTo explain, we need to note two things up front:\r\n\r\n1. SQL Server cannot execute subqueries directly, so they are always unrolled or converted to an *apply*.\r\n2. The semantics of `CASE` are such that a `THEN` expression should only be evaluated if the `WHEN` clause returns true.\r\n\r\nThe (trivial) subquery introduced in the problematic case therefore results in an apply operator (nested loops join). To meet the second requirement, SQL Server initially places the expression `dbo.test6(1) + dbo.test6(2)` on the inner side of the apply: \r\n\r\n[![highlighted compute scalar][1]][1]\r\n\r\n```\r\n[Expr1000] = Scalar Operator([dbo].[test6]((1))+[dbo].[test6]((2)))\r\n```\r\n\r\n...with the `CASE` semantics honoured by a *pass-through* predicate on the join:\r\n\r\n```\r\n[@i]=(1) OR [@i]=(2) OR IsFalseOrNull [@i]=(3)\r\n```\r\n\r\nThe inner side of the loop is only evaluated if the *pass-through* condition evaluates to *false* (meaning `@i = 3`). This is all correct so far. The *Compute Scalar* following the nested loops join also honours the `CASE` semantics correctly:\r\n\r\n```\r\n[Expr1001] = Scalar Operator(CASE WHEN [@i]=(1) THEN (1) ELSE CASE WHEN [@i]=(2) THEN (2) ELSE CASE WHEN [@i]=(3) THEN [Expr1000] ELSE NULL END END END)\r\n```\r\n\r\nThe problem is that the *project normalization* stage of query compilation sees that `Expr1000` is uncorrelated and determines that it would be safe (*narrator: it isn't*) to move it outside the loop:\r\n\r\n[![moved project][2]][2]\r\n\r\n```\r\n[Expr1000] = Scalar Operator([dbo].[test6]((1))+[dbo].[test6]((2)))\r\n```\r\n\r\nThis breaks* the semantics implemented by the *pass-through* predicate, so the function is evaluated when it should not be, and an infinite loop results.\r\n\r\nYou should report this bug. A workaround is to prevent the expression being moved outside the apply by making it correlated (i.e. including `@i` in the expression) but this is a hack of course. There is a way to disable project normalization, but I have been asked before not to share it publicly, so I won't.\r\n\r\nThis problem does not arise in SQL Server 2019 when the [scalar function is inlined][3], because the inlining logic operates directly on the parsed tree (well before project normalization). The simple logic in the question can be simplified by the inlining logic to the non-recursive:\r\n\r\n```\r\n[Expr1019] = (Scalar Operator((1)))\r\n[Expr1045] = Scalar Operator(CONVERT_IMPLICIT(int,CONVERT_IMPLICIT(int,[Expr1019],0)+(2),0))\r\n```\r\n\r\n...which returns 3.\r\n\r\n\r\nAnother way to illustrate the core issue is:\r\n\r\n```\r\n-- Not schema bound to make it non-det\r\nCREATE OR ALTER FUNCTION dbo.Error() \r\nRETURNS integer \r\n-- WITH INLINE = OFF -- SQL Server 2019 only\r\nAS\r\nBEGIN\r\n    RETURN 1/0;\r\nEND;\r\nGO\r\nDECLARE @i integer = 1;\r\n\r\nSELECT\r\n    CASE \r\n        WHEN @i = 1 THEN 1\r\n        WHEN @i = 2 THEN 2\r\n        WHEN @i = 3 THEN (SELECT dbo.Error()) -- 'subquery'\r\n        ELSE NULL\r\n    END;\r\n```\r\n\r\nReproduces on the latest builds of all versions from 2008 R2 to 2019 CTP 3.0.\r\n\r\nA further example (without a scalar function) provided by [Martin Smith][4]:\r\n\r\n```\r\nSELECT IIF(@@TRANCOUNT >= 0, 1, (SELECT CRYPT_GEN_RANDOM(4)/ 0))\r\n```\r\n\r\nThis has all the key elements needed:\r\n\r\n* `CASE` (implemented internally as `ScaOp_IIF`)\r\n* A non-deterministic function (`CRYPT_GEN_RANDOM`)\r\n* A subquery on the branch that should not be executed (`(SELECT ...)`)\r\n\r\n---\r\n\r\n\\*Strictly, the above transformation could still be correct if evaluation of `Expr1000` was deferred correctly, since it is referenced only by the safe construction:\r\n\r\n```\r\n[Expr1002] = Scalar Operator(CASE WHEN [@i]=(1) THEN (1) ELSE CASE WHEN [@i]=(2) THEN (2) ELSE CASE WHEN [@i]=(3) THEN [Expr1000] ELSE NULL END END END)\r\n```\r\n\r\n...but this requires an internal *ForceOrder* flag (not query hint), which is not set either. In any case, the implementation of the logic applied by *project normalization* is incorrect or incomplete.\r\n\r\n**[Bug report][5]** on the Azure Feedback site for SQL Server.\r\n\r\n  [1]: https://i.stack.imgur.com/i0QaY.png\r\n  [2]: https://i.stack.imgur.com/l8olq.png\r\n  [3]: https://docs.microsoft.com/en-us/sql/relational-databases/user-defined-functions/scalar-udf-inlining\r\n  [4]: https://dba.stackexchange.com/users/3690/martin-smith\r\n  [5]: https://feedback.azure.com/forums/908035-sql-server/suggestions/37774186-self-referencing-scalar-function-nesting-level-exc	2019-12-03 14:04:29.249075+00	3	4	1	239432	0	0	0	2019-12-03 14:04:29.249075+00	\N	This is a bug in *project normalization*, exposed by using a subquery inside a case expression with a non-deterministic function.	f	f
807	695	167	2020-02-11 16:08:13.090831+00	How about an explicit "cancel" button (for writing questions, answers and edits)? It could be located besides the `post answer/question` or `update post` button. \r\n\r\n	2020-02-11 16:08:13.090831+00	0	4	1	\N	0	0	0	\N	\N	How about an explicit "cancel" button (for writing questions, answers and edits)? It could be located besides the `post answer/question` or `update post` button.	f	f
761	685	167	2020-02-06 20:59:51.916602+00	A TikZification of a Cubist sculpture outside the Strong Museum in Rochester:\r\n\r\n```\r\n\\documentclass{standalone}\r\n\r\n\\usepackage{tikz}\r\n\r\n\\begin{document}\r\n\r\n\\begin{tikzpicture}\r\n\r\n\\fill[orange,rotate around={10:(-0.77, -0.65)} ] (-0.77,-0.65) rectangle ++(0.34,0.34);\r\n\r\n\\fill[red!20!black] (1.5,0.3) -- (0.8,0.2) -- (1.37,0.45) -- cycle;\r\n\\fill[red] (1.37,0.45) -- (0.8,0.2) -- (0.2,0.35) -- (1.22,0.99) -- cycle;\r\n\\fill[red!30!lightgray] (1.37,0.45) -- (1.5,0.3) -- (1.5,0.55) -- (1.22,0.99) -- cycle;\r\n\r\n\\fill[blue,rotate around={19:(-0.57,-0.7)}] (-0.57,-0.7) rectangle ++(1.2,1.2);\r\n\r\n\\fill[yellow] (-0.15,-0.74) -- (-0.15,0.42) -- (0.8,0.3) -- (0.8,-0.7) -- cycle;\r\n\\fill[yellow!70!black] (-0.15,-0.74) -- (-0.15,0.42) -- (-0.28,0.25) -- (-0.28,-0.7) -- cycle;\r\n\r\n\\fill[green!50!black] (0.37,0.26) -- (1.5,0.3) -- (1.5,-0.85) -- (0.45,-0.7) -- cycle;\r\n\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n![document.png](/image?hash=1126b13dd8435d11a164d354bf49b8dc3ed51723e97436ab5201b3c56e8e043c)\r\n\r\n\r\n(Photo from https://en.wikipedia.org/wiki/File:Strong_national_museum_of_play_sculpture_(25751934112).jpg)	2020-02-06 22:46:39.209498+00	7	4	3	\N	0	0	0	\N	\N	A TikZification of a Cubist sculpture outside the Strong Museum in Rochester:	f	t
771	685	234	2020-02-08 20:27:26.965098+00	A version of the [flag of Rochester](https://en.wikipedia.org/wiki/Rochester,_New_York) that is adjusted to the event.\r\n\r\n```\r\n\\documentclass[tikz,border=3mm]{standalone}\r\n\\usepackage{tikzducks}\r\n\\definecolor{RochesterYellow}{RGB}{241,203,48}\r\n\\definecolor{RochesterBlue}{RGB}{0,0,250}\t\r\n\\begin{document}\r\n\\begin{tikzpicture}[pics/pft/.style={code={\r\n \\pgfmathsetmacro{\\myangle}{acos(4*cos(60)/3)}\r\n \\fill (60:0.2) arc[start angle=60,end angle=-240,radius=0.2]\r\n  arc[start angle=180-\\myangle,end angle=360+\\myangle,radius=0.15];}}]\r\n  \\begin{scope}[xshift=0.5mm]\r\n   \\draw[line width=2pt,line join=round] (-1.2,4.9) -- (-1.3,5) --\r\n\t(-1.2,5.1) -- (-0.6,5.1) \r\n\tarc[x radius=0.6cm,y radius=0.75cm,start angle=180,end angle=0]\r\n\t-- (1.2,5.1) -- (1.3,5) --  (1.2,4.9) -- (1.2,2.7) -- (1,2.5) -- (0.2,2.5)\r\n\t-- (0,2.3) -- (-0.2,2.5) -- (-1,2.5) -- (-1.2,2.7) -- cycle;\r\n   \\draw[line width=1pt,rounded corners=1mm,red,fill=RochesterYellow] (0,4.5) -- (0.8,4.5) -| (0.7,4) |- (0.3,3)\r\n   -| (0,2.9) |- (-0.3,3) -| (-0.7,4) |- (-0.8,4.5) -- cycle;\r\n   \\fill (-0.7cm+0.5pt,3.95) rectangle (0.7cm-0.5pt,3.55);\r\n   \\path (-0.35,4.2) pic{pft} (0.35,4.2) pic{pft} (0,3.3) pic{pft};\r\n  \\end{scope}\r\n  \\fill[RochesterBlue] (-6,0) rectangle (-2,7.2);\r\n  \\fill[RochesterYellow] (6,0) rectangle (2,7.2);\r\n  \\node[font=\\bfseries\\sffamily,scale=1.67] at (0,1.6) {ROCHESTER};\r\n  \\duck[yshift=4.55cm,xshift=-5mm,body=RochesterYellow,scale=0.5]\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2020-02-08 at 12.27.10 PM.png](/image?hash=e2c0f3b46f4b45fbb8b3368553e766083de6e7888a1e490bc27195d26c429837)\r\n\r\n![ani.gif](/image?hash=0242505bb84e54beae1c419b639c92096aa9c937bfd085e5de44bd02e84b1c3e)	2020-02-09 22:10:41.037527+00	7	4	3	\N	0	0	0	\N	\N	A version of the [flag of Rochester](https://en.wikipedia.org/wiki/Rochester,_New_York) that is adjusted to the event.	f	f
827	717	167	2020-02-16 19:54:21.424617+00	We could use the same icon as for your chat room. The tophat in the code below looks a bit nicer than the one from the chatroom, but I'm sure we can ask to unify both\r\n\r\n```\r\n\\documentclass[margin=0.3mm]{standalone}\r\n\\usepackage{tikz}\r\n\\definecolor{dred}{RGB}{159,57,61}\r\n\\pagecolor{white}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\begin{scope}[transform shape,rotate=-17.5,font=\\sffamily]\r\n\r\n% brim\r\n\\fill[black] (0.62, 0.66) .. controls (0.62, 0.52) and (0.34, 0.37) .. (0, 0.37) .. controls (-0.34, 0.37) and (-0.62, 0.52) .. (-0.62, 0.66) .. controls (-0.62, 0.8) and (-0.34, 0.7) .. (0, 0.7) .. controls (0.34, 0.7) and (0.62, 0.8) .. (0.62, 0.66) -- cycle;\r\n\r\n% top ellipse\r\n\\fill[black] (0,1.13) ellipse (0.42 and 0.1);\r\n\\fill[black] (-0.37,0.61) -- (0.37,0.61) -- (0.42,1.13) -- (-0.42,1.13) -- cycle;\r\n\r\n% bottom ellipse\r\n\\fill[dred] (0,0.61) ellipse (0.37 and 0.1);\r\n\\fill[dred] (-0.37,0.61) -- (0.37,0.61) -- (0.38,0.73) -- (-0.38,0.73) -- cycle;\r\n\r\n% cutout\r\n\\fill[black] (0,0.73) ellipse (0.38 and 0.1);\r\n\r\n% text\r\n\\node[white] at (-0.2,0.92) {T};\r\n\\node[white] at (-0.04,0.84) {E};\r\n\\node[white] at (0.16,0.92) {X};\r\n\r\n\\end{scope}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\nThe result would then look like https://topanswers.xyz/tex?q=606\r\n\r\n![tophat.png](/image?hash=5fedcf67f47450895d51925ebc31ba5e537bc89c94728f6cec45d5ca830d8c71)	2020-02-16 19:54:21.424617+00	8	4	3	\N	0	0	0	\N	\N	We could use the same icon as for your chat room. The tophat in the code below looks a bit nicer than the one from the chatroom, but I'm sure we can ask to unify both	f	t
836	717	702	2020-02-17 13:38:33.248846+00	Does it have to be drawn in something \\*TeX-ish? If not, here's a thought...\r\n\r\n![tex-ta-logo0.png](/image?hash=fa3d43a1999b8321ac24769a81baf0baf0391bdc76f285f5b68b58e0afb44dda)\r\n\r\nThat's just the TA "logo" with three question marks placed to suggest the TeX identity. I thought I chose the `?` strategically (is it from "erewhon"?) but it doesn't scale down so well. Here's 24x24px:\r\n\r\n![tex-ta-logo1.png](/image?hash=de80bb394f54f0ec5c9dbbb496b6eff8080368892471dddd8e057e5bd0bbfab6)\r\n\r\nOh well! Just for the banter in any case! :grin: It was "drawn" in Inkscape, so there is an SVG ... not that anyone would want it.	2020-02-17 13:39:24.079481+00	4	1	1	\N	0	0	0	\N	\N	Does it have to be drawn in something \\*TeX-ish? If not, here's a thought...	f	f
826	717	167	2020-02-16 19:51:58.33355+00	No special symbol for TopTeX community wiki, instead use the global symbol (which is currently shown as Earth)	2020-02-16 19:51:58.33355+00	3	4	2	\N	0	0	0	\N	\N	No special symbol for TopTeX community wiki, instead use the global symbol (which is currently shown as Earth)	f	t
829	694	2	2020-02-17 00:26:26.650805+00	Now, partly implemented, with one other aspect 'planned' and the other leaning towards 'wont-fix':\r\n\r\n---\r\n\r\n> …this syntax should filter search results to only include posts tagged thusly\r\n\r\nThis small part of the feature request is now implemented.\r\n\r\n---\r\n\r\n> Tags are necessarily brief, so non-insiders need access to more info about them. Having a title-attribute with a short description like on SE would help.\r\n\r\nThis is now planned, as is now detailed on this earlier feature-request:\r\n\r\n@@@ answer 828\r\n\r\n---\r\n\r\n> Clicking on a tag should change the search field\r\n\r\nthis makes sense on SE but I'm not (yet) convinced that it does here	2020-02-17 00:26:26.650805+00	3	1	1	\N	0	0	0	\N	\N	Now, partly implemented, with one other aspect 'planned' and the other leaning towards 'wont-fix':	t	f
83	111	12	2019-11-26 09:14:19.975224+00	You need to give the database a valid owner. Use the [`ALTER AUTHORIZATION`][1] command to do this:\r\n\r\n    ALTER AUTHORIZATION \r\n    ON DATABASE::[DatabaseName]\r\n    TO [A Suitable Login];\r\n\r\n[Related Knowledge Base article][2]\r\n\r\nNote the [sp_changedbowner][3] system stored procedure has been deprecated in favour of `ALTER AUTHORIZATION`.\r\n\r\n\r\n  [1]: https://msdn.microsoft.com/en-us/library/ms187359(v=sql.105).aspx\r\n  [2]: https://support.microsoft.com/en-us/kb/913423\r\n  [3]: https://technet.microsoft.com/en-us/library/ms178630(v=sql.105).aspx	2019-11-26 09:14:19.975224+00	1	4	2	\N	0	0	0	\N	\N	You need to give the database a valid owner. Use the [`ALTER AUTHORIZATION`][1] command to do this:	f	f
830	696	2	2020-02-17 00:28:30.475263+00	Now, partly implemented, still thinking about the rest:\r\n\r\n---\r\n\r\n> …and that syntax should be enabled to search for just that post type\r\n\r\nThis small part of the feature request is now implemented.	2020-02-17 00:28:30.475263+00	0	1	1	\N	0	0	0	\N	\N	Now, partly implemented, still thinking about the rest:	t	f
192	249	158	2019-02-03 20:41:07+00	I'm not sure if parallelism will be any / significantly better with SQLCLR. However, it is really easy to test since there is a hash function in the Free version of the [SQL#][1] SQLCLR library (which I wrote) called **Util\\_HashBinary**. Supported algorithms are: MD5, SHA1, SHA256, SHA384, and SHA512.\r\n\r\nIt takes a `VARBINARY(MAX)` value as input, so you can either concatenate the string version of each field (as you are currently doing) and then convert to `VARBINARY(MAX)`, or you can go directly to `VARBINARY` for each column and concatenate the converted values (this might be faster since you aren't dealing with strings or the extra conversion from string to `VARBINARY`). Below is an example showing both of these options. It also shows the `HASHBYTES` function so you can see that the values are the same between it and **SQL#.Util\\_HashBinary**. \r\n\r\nPlease note that the hash results when concatenating the `VARBINARY` values won't match the hash results when concatenating the `NVARCHAR` values. This is because the binary form of the `INT` value "1" is 0x00000001, while the UTF-16LE (i.e. `NVARCHAR`) form of the `INT` value of "1" (in binary form since that is what a hashing function will operate on) is 0x3100.\r\n\r\n\r\n    SELECT so.[object_id],\r\n           SQL#.Util_HashBinary(N'SHA256',\r\n                                CONVERT(VARBINARY(MAX),\r\n                                        CONCAT(so.[name], so.[schema_id], so.[create_date])\r\n                                       )\r\n                               ) AS [SQLCLR-ConcatStrings],\r\n           HASHBYTES(N'SHA2_256',\r\n                     CONVERT(VARBINARY(MAX),\r\n                             CONCAT(so.[name], so.[schema_id], so.[create_date])\r\n                            )\r\n                    ) AS [BuiltIn-ConcatStrings]\r\n    FROM sys.objects so;\r\n    \r\n    \r\n    SELECT so.[object_id],\r\n           SQL#.Util_HashBinary(N'SHA256',\r\n                                CONVERT(VARBINARY(500), so.[name]) + \r\n                                CONVERT(VARBINARY(500), so.[schema_id]) +\r\n    \t\t\t\t\t\t\tCONVERT(VARBINARY(500), so.[create_date])\r\n                               ) AS [SQLCLR-ConcatVarBinaries],\r\n           HASHBYTES(N'SHA2_256',\r\n                     CONVERT(VARBINARY(500), so.[name]) + \r\n                     CONVERT(VARBINARY(500), so.[schema_id]) +\r\n                     CONVERT(VARBINARY(500), so.[create_date])\r\n                    ) AS [BuiltIn-ConcatVarBinaries]\r\n    FROM sys.objects so;\r\n\r\nYou can test something more comparable to the non-LOB Spooky using:\r\n\r\n    CREATE FUNCTION [SQL#].[Util_HashBinary8k]\r\n    (@Algorithm [nvarchar](50), @BaseData [varbinary](8000))\r\n    RETURNS [varbinary](8000) \r\n    WITH EXECUTE AS CALLER, RETURNS NULL ON NULL INPUT\r\n    AS EXTERNAL NAME [SQL#].[UTILITY].[HashBinary];\r\n\r\nNote: **Util\\_HashBinary** uses the managed SHA256 algorithm that is built into .NET, and should not be using the "bcrypt" library.\r\n\r\nBeyond that aspect of the question, there are some additional thoughts that might help this process:\r\n\r\nAdditional Thought \\#1 (pre-calculate hashes, at least some)\r\n--\r\n\r\nYou mentioned a few things:\r\n\r\n1. >  we compare rows from staging against the reporting database to figure out if any of the columns have actually changed since the data was last loaded.\r\n\r\n  and: \r\n\r\n1. > I cannot save off the value of the hash for the reporting table. It's a CCI which doesn't support triggers or computed columns\r\n\r\n  and:\r\n\r\n1. > the tables can be updated outside of the ETL process\r\n\r\nIt sounds like the data in this reporting table is stable for a period of time, and is only modified by this ETL process.\r\n\r\nIf nothing else modifies this table, then we really don't need a trigger or indexed view after all (I originally thought that you might).\r\n\r\nSince you can't modify the schema of the reporting table, would it at least be possible to create a related table to contain the pre-calculated hash (and UTC time of when it was calculated)? This would allow you to have a pre-calculated value to compare against next time, leaving only the incoming value that requires calculating the hash of. This would reduce the number of calls to either `HASHBYTES` or `SQL#.Util_HashBinary` by half. You would simply join to this table of hashes during the import process.\r\n\r\nYou would also create a separate stored procedure that simply refreshes the hashes of this table. It just updates the hashes of any related row that has changed to be current, and updates the timestamp for those modified rows. This proc can/should be executed at the end of any other process that updates this table. It can also be scheduled to run 30 - 60 minutes prior to this ETL starting (depending on how long it takes to execute, and when any of these other processes might run). It can even be executed manually if you ever suspect there might be rows that are out of sync.\r\n\r\nIt was then noted that:\r\n\r\n> there are over 500 tables\r\n\r\nThat many tables does make it more difficult to have an extra table per each to contain the current hashes, but this is not impossible as it could be scripted since it would be a standard schema. The scripting would just need to account for source table name and discovery of source table PK column(s).\r\n\r\nStill, regardless of which hash algorithm ultimately proves to be the most scalable, I still _highly_ recommend finding at least a few tables (perhaps there are some that are MUCH larger than the rest of the 500 tables) and setting up a related table to capture current hashes so the "current" values can be known prior to the ETL process. Even the fastest function can't out-perform never having to call it in the first place ;-).\r\n\r\n\r\nAdditional Thought \\#2 (`VARBINARY` instead of `NVARCHAR`)\r\n--\r\n\r\nRegardless of SQLCLR vs built-in `HASHBYTES`, I would still recommend converting directly to `VARBINARY` as that _should_ be faster. Concatenating strings is just not terribly efficient. _And_, that's in addition to converting non-string values into strings in the first place, which requires extra effort (I assume the amount of effort varies based on the base type: `DATETIME` requiring more than `BIGINT`), whereas converting to `VARBINARY` simply gives you the underlying value (in most cases).\r\n\r\nAnd, in fact, testing the same dataset that the other tests used, and using `HASHBYTES(N'SHA2_256',...)`, showed a 23.415% increase in total hashes calculated in one minute. And that increase was for doing nothing more than using `VARBINARY` instead of `NVARCHAR`! &#x1F638; (please see [community wiki answer][2] for details)\r\n\r\nAdditional Thought \\#3 (be mindful of input parameters)\r\n--\r\n\r\nFurther testing showed that one area that impacts performance (over this volume of executions) is input parameters: how many and what type(s).\r\n\r\nThe **Util\\_HashBinary** SQLCLR function that is currently in my SQL# library has two input parameters: one `VARBINARY` (the value to hash), and one `NVARCHAR` (the algorithm to use). This is due to my mirroring the signature of the `HASHBYTES` function. However, I found that if I removed the `NVARCHAR` parameter and created a function that only did SHA256, then performance improved quite nicely. I assume that even switching the `NVARCHAR` parameter to `INT` would have helped, but I also assume that not even having the extra `INT` parameter is at least _slightly_ faster.\r\n\r\nAlso, `SqlBytes.Value` might perform better than `SqlBinary.Value`.\r\n\r\nI created two new functions: **Util\\_HashSHA256Binary** and **Util\\_HashSHA256Binary8k** for this testing. These will be included in the next release of SQL# (no date set for that yet).\r\n\r\nI also found that the testing methodology could be slightly improved, so I updated the test harness in the community wiki answer below to include:\r\n\r\n1. pre-loading of the SQLCLR assemblies to ensure that the load time overhead doesn't skew the results.\r\n1. a verification procedure to check for collisions. If any are found, it displays the number of unique/distinct rows and the total number of rows. This allows one to determine if the number of collisions (if there are any) is beyond the limit for the given use case. Some use cases might allows for a small number of collisions, others might require none. A super-fast function is useless if it can't detect changes to the desired level of accuracy. For example, using the test harness provided by the O.P., I increased the row count to 100k rows (it was originally 10k) and found that `CHECKSUM` registered over 9k collisions, which is 9% (yikes).\r\n\r\n\r\nAdditional Thought \\#4 (`HASHBYTES` + SQLCLR together?)\r\n--\r\n\r\nDepending on where the bottleneck is, it might even help to use a combination of built-in `HASHBYTES` and a SQLCLR UDF to do the same hash. If built-in functions are constrained differently / separately from SQLCLR operations, then this approach might be able to accomplish more concurrently than either `HASHBYTES` or SQLCLR individually. It's definitely worth testing.\r\n\r\nAdditional Thought \\#5 (hashing object caching?)\r\n--\r\n\r\nThe caching of the hashing algorithm object as suggested in [David Browne's answer][3] certainly seems interesting, so I tried it and found the following two points of interest:\r\n\r\n1. For whatever reason, it does not seem to provide much, if any, performance improvement. I could have done something incorrectly, but here is what I tried:\r\n\r\n    \r\n\r\n    \tstatic readonly ConcurrentDictionary<int, SHA256Managed> hashers =\r\n    \t    new ConcurrentDictionary<int, SHA256Managed>();\r\n    \r\n    \t[return: SqlFacet(MaxSize = 100)]\r\n    \t[SqlFunction(IsDeterministic = true)]\r\n    \tpublic static SqlBinary FastHash([SqlFacet(MaxSize = 1000)] SqlBytes Input)\r\n    \t{\r\n    \t\tSHA256Managed sh = hashers.GetOrAdd(Thread.CurrentThread.ManagedThreadId,\r\n    \t\t                                    i => new SHA256Managed());\r\n    \r\n    \t\treturn sh.ComputeHash(Input.Value);\r\n    \t}\r\n\r\n1. The `ManagedThreadId` value appears to be the same for all SQLCLR references in a particular query. I tested multiple references to the same function, as well as a reference to a different function, all 3 being given different input values, and returning different (but expected) return values. For both test functions, the output was a string that included the `ManagedThreadId` as well as a string representation of the hash result. The `ManagedThreadId` value was the same for all UDF references in the query, and across all rows. But, the hash result was the same for the same input string and different for different input strings.\r\n\r\n  While I didn't see any erroneous results in my testing, wouldn't this increase the chances of a race condition? If the key of the dictionary is the same for all SQLCLR objects called in a particular query, then they would be sharing the same value or object stored for that key, right? The point being, even thought it seemed to work here (to a degree, again there did not seem to be much performance gain, but functionally nothing broke), that doesn't give me confidence that this approach will work in other scenarios.\r\n\r\n\r\n  [1]: https://sqlsharp.com/?ref=db_228792\r\n  [2]: https://dba.stackexchange.com/a/228966/30859\r\n  [3]: https://dba.stackexchange.com/a/229282/30859	2019-12-03 11:59:44.773048+00	1	4	1	228792	0	0	0	2019-12-03 11:59:44.773048+00	\N	I'm not sure if parallelism will be any / significantly better with SQLCLR. However, it is really easy to test since there is a hash function in the Free version of the [SQL#][1] SQLCLR library (which I wrote) called **Util\\_HashBinary**. Supported algorithms are: MD5, SHA1, SHA256, SHA384, and SHA512.	f	f
193	249	159	2019-02-05 19:25:19+00	This isn't a traditional answer, but I thought it would be helpful to post benchmarks of some of the techniques mentioned so far. I'm testing on a 96 core server with SQL Server 2017 CU9.\r\n\r\nMany scalability problems are caused by concurrent threads contending over some global state. For example, consider classic PFS page contention. This can happen if too many worker threads need to modify the same page in memory. As code becomes more efficient it may request the latch faster. That increases contention. To put it simply, efficient code is more likely to lead to scalability issues because the global state is contended over more severely. Slow code is less likely to cause scalability issues because the global state isn't accessed as frequently.\r\n\r\n`HASHBYTES` scalability is partially based on the length of the input string. My theory was to why this occurs is that access to some global state is needed when the `HASHBYTES` function is called. The easy global state to observe is a memory page needs to be allocated per call on some versions of SQL Server. The harder one to observe is that there's some kind of OS contention. As a result, if `HASHBYTES` is called by the code less frequently then contention goes down. One way to reduce the rate of `HASHBYTES` calls is to increase the amount of hashing work needed per call. Hashing work is partially based on the length of the input string. To reproduce the scalability problem I saw in the application I needed to change the demo data. A reasonable worst case scenario is a table with 21 `BIGINT` columns. The definition of the table is included in the code at the bottom. To reduce Local Factors™, I'm using concurrent `MAXDOP 1` queries that operate on relatively small tables. My quick benchmark code is at the bottom.\r\n\r\nNote the functions return different hash lengths. `MD5` and `SpookyHash` are both 128 bit hashes, `SHA256` is a 256 bit hash.\r\n\r\nRESULTS (`NVARCHAR` vs `VARBINARY` conversion and concatenation)\r\n--\r\n\r\nIn order to see if converting to, and concatenating, `VARBINARY` is truly more efficient / performant than `NVARCHAR`, an `NVARCHAR` version of the `RUN_HASHBYTES_SHA2_256` stored procedure was created from the same template (see "Step 5" in **BENCHMARKING CODE** section below). The only differences are:\r\n\r\n1. Stored Procedure name ends in `_NVC`\r\n1. `BINARY(8)` for the `CAST` function was changed to be `NVARCHAR(15)`\r\n1. `0x7C` was changed to be `N'|'`\r\n\r\nResulting in:\r\n\r\n    CAST(FK1 AS NVARCHAR(15)) + N'|' +\r\n\r\ninstead of:\r\n\r\n    CAST(FK1 AS BINARY(8)) + 0x7C +\r\n\r\n\r\nThe table below contains the number of hashes performed in 1 minute. The tests were performed on a different server than was used for the other tests noted below.\r\n\r\n    ╔════════════════╦══════════╦══════════════╗\r\n    ║    Datatype    ║  Test #  ║ Total Hashes ║\r\n    ╠════════════════╬══════════╬══════════════╣\r\n    ║ NVARCHAR       ║        1 ║     10200000 ║\r\n    ║ NVARCHAR       ║        2 ║     10300000 ║\r\n    ║ NVARCHAR       ║  AVERAGE ║ * 10250000 * ║\r\n    ║ -------------- ║ -------- ║ ------------ ║\r\n    ║ VARBINARY      ║        1 ║     12500000 ║\r\n    ║ VARBINARY      ║        2 ║     12800000 ║\r\n    ║ VARBINARY      ║  AVERAGE ║ * 12650000 * ║\r\n    ╚════════════════╩══════════╩══════════════╝\r\n\r\nLooking at just the averages, we can calculate the benefit of switching to `VARBINARY`:\r\n\r\n    SELECT (12650000 - 10250000) AS [IncreaseAmount],\r\n           ROUND(((126500000 - 10250000) / 10250000) * 100.0, 3) AS [IncreasePercentage]\r\n\r\nThat returns:\r\n\r\n    IncreaseAmount:    2400000.0\r\n    IncreasePercentage:   23.415\r\n\r\n\r\nRESULTS (hash algorithms and implementations)\r\n--\r\n\r\nThe table below contains the number of hashes performed in 1 minute. For example, using `CHECKSUM` with 84 concurrent queries resulted in over 2 billion hashes being performed before time ran out.\r\n\r\n    ╔════════════════════╦════════════╦════════════╦════════════╗\r\n    ║      Function      ║ 12 threads ║ 48 threads ║ 84 threads ║\r\n    ╠════════════════════╬════════════╬════════════╬════════════╣\r\n    ║ CHECKSUM           ║  281250000 ║ 1122440000 ║ 2040100000 ║\r\n    ║ HASHBYTES MD5      ║   75940000 ║  106190000 ║  112750000 ║\r\n    ║ HASHBYTES SHA2_256 ║   80210000 ║  117080000 ║  124790000 ║\r\n    ║ CLR Spooky         ║  131250000 ║  505700000 ║  786150000 ║\r\n    ║ CLR SpookyLOB      ║   17420000 ║   27160000 ║   31380000 ║\r\n    ║ SQL# MD5           ║   17080000 ║   26450000 ║   29080000 ║\r\n    ║ SQL# SHA2_256      ║   18370000 ║   28860000 ║   32590000 ║\r\n    ║ SQL# MD5 8k        ║   24440000 ║   30560000 ║   32550000 ║\r\n    ║ SQL# SHA2_256 8k   ║   87240000 ║  159310000 ║  155760000 ║\r\n    ╚════════════════════╩════════════╩════════════╩════════════╝\r\n\r\nIf you prefer to see the same numbers measured in terms of work per thread-second:\r\n\r\n    ╔════════════════════╦════════════════════════════╦════════════════════════════╦════════════════════════════╗\r\n    ║      Function      ║ 12 threads per core-second ║ 48 threads per core-second ║ 84 threads per core-second ║\r\n    ╠════════════════════╬════════════════════════════╬════════════════════════════╬════════════════════════════╣\r\n    ║ CHECKSUM           ║                     390625 ║                     389736 ║                     404782 ║\r\n    ║ HASHBYTES MD5      ║                     105472 ║                      36872 ║                      22371 ║\r\n    ║ HASHBYTES SHA2_256 ║                     111403 ║                      40653 ║                      24760 ║\r\n    ║ CLR Spooky         ║                     182292 ║                     175590 ║                     155982 ║\r\n    ║ CLR SpookyLOB      ║                      24194 ║                       9431 ║                       6226 ║\r\n    ║ SQL# MD5           ║                      23722 ║                       9184 ║                       5770 ║\r\n    ║ SQL# SHA2_256      ║                      25514 ║                      10021 ║                       6466 ║\r\n    ║ SQL# MD5 8k        ║                      33944 ║                      10611 ║                       6458 ║\r\n    ║ SQL# SHA2_256 8k   ║                     121167 ║                      55316 ║                      30905 ║\r\n    ╚════════════════════╩════════════════════════════╩════════════════════════════╩════════════════════════════╝\r\n\r\n\r\n\r\nSome quick thoughts on all of the methods:\r\n\r\n - `CHECKSUM`: very good scalability as expected\r\n - `HASHBYTES`: scalability issues include one memory allocation per call and a large amount of CPU spent in the OS\r\n - `Spooky`: surprisingly good scalability\r\n - `Spooky LOB`: the spinlock `SOS_SELIST_SIZED_SLOCK` spins out of control. I suspect this is a general issue with passing LOBs through CLR functions, but I'm not sure\r\n - `Util_HashBinary`: looks like it gets hit by the same spinlock. I haven't looked into this so far because there's probably not a lot that I can do about it:\r\n\r\n[![spin your lock][1]][1]\r\n\r\n\r\n - `Util_HashBinary 8k`: very surprising results, not sure what's going on here\r\n\r\n\r\n**Final results tested on a smaller server:**\r\n\r\n    ╔═════════════════════════╦════════════════════════╦════════════════════════╗\r\n    ║     Hash Algorithm      ║ Hashes over 11 threads ║ Hashes over 44 threads ║\r\n    ╠═════════════════════════╬════════════════════════╬════════════════════════╣\r\n    ║ HASHBYTES SHA2_256      ║               85220000 ║              167050000 ║\r\n    ║ SpookyHash              ║              101200000 ║              239530000 ║\r\n    ║ Util_HashSHA256Binary8k ║               90590000 ║              217170000 ║\r\n    ║ SpookyHashLOB           ║               23490000 ║               38370000 ║\r\n    ║ Util_HashSHA256Binary   ║               23430000 ║               36590000 ║\r\n    ╚═════════════════════════╩════════════════════════╩════════════════════════╝\r\n\r\n\r\nBENCHMARKING CODE\r\n--\r\n\r\n**SETUP 1: Tables and Data**\r\n\r\n    DROP TABLE IF EXISTS dbo.HASH_SMALL;\r\n    \r\n    CREATE TABLE dbo.HASH_SMALL (\r\n        ID BIGINT NOT NULL,\r\n        FK1 BIGINT NOT NULL,\r\n        FK2 BIGINT NOT NULL,\r\n        FK3 BIGINT NOT NULL,\r\n        FK4 BIGINT NOT NULL,\r\n        FK5 BIGINT NOT NULL,\r\n        FK6 BIGINT NOT NULL,\r\n        FK7 BIGINT NOT NULL,\r\n        FK8 BIGINT NOT NULL,\r\n        FK9 BIGINT NOT NULL,\r\n        FK10 BIGINT NOT NULL,\r\n        FK11 BIGINT NOT NULL,\r\n        FK12 BIGINT NOT NULL,\r\n        FK13 BIGINT NOT NULL,\r\n        FK14 BIGINT NOT NULL,\r\n        FK15 BIGINT NOT NULL,\r\n    \tFK16 BIGINT NOT NULL,\r\n        FK17 BIGINT NOT NULL,\r\n        FK18 BIGINT NOT NULL,\r\n        FK19 BIGINT NOT NULL,\r\n        FK20 BIGINT NOT NULL\r\n    );\r\n    \r\n    INSERT INTO dbo.HASH_SMALL WITH (TABLOCK)\r\n    SELECT RN,\r\n    4000000 - RN, 4000000 - RN\r\n    ,200000000 - RN, 200000000 - RN\r\n    , RN % 500000 , RN % 500000 , RN % 500000\r\n    , RN % 500000 , RN % 500000 , RN % 500000 \r\n    , 100000 - RN % 100000, RN % 100000\r\n    , 100000 - RN % 100000, RN % 100000\r\n    , 100000 - RN % 100000, RN % 100000\r\n    , 100000 - RN % 100000, RN % 100000\r\n    , 100000 - RN % 100000, RN % 100000\r\n    FROM (\r\n        SELECT TOP (10000) ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) RN\r\n        FROM master..spt_values t1\r\n        CROSS JOIN master..spt_values t2\r\n    ) q\r\n    OPTION (MAXDOP 1);\r\n    \r\n    \r\n    DROP TABLE IF EXISTS dbo.LOG_HASHES;\r\n    CREATE TABLE dbo.LOG_HASHES (\r\n    LOG_TIME DATETIME,\r\n    HASH_ALGORITHM INT,\r\n    SESSION_ID INT,\r\n    NUM_HASHES BIGINT\r\n    );\r\n\r\n**SETUP 2: Master Execution Proc**\r\n    \r\n    GO\r\n    CREATE OR ALTER PROCEDURE dbo.RUN_HASHES_FOR_ONE_MINUTE (@HashAlgorithm INT)\r\n    AS\r\n    BEGIN\r\n    DECLARE @target_end_time DATETIME = DATEADD(MINUTE, 1, GETDATE()),\r\n            @query_execution_count INT = 0;\r\n    \r\n    SET NOCOUNT ON;\r\n    \r\n    DECLARE @ProcName NVARCHAR(261); -- schema_name + proc_name + '[].[]'\r\n    \r\n    DECLARE @RowCount INT;\r\n    SELECT @RowCount = SUM(prtn.[row_count])\r\n    FROM   sys.dm_db_partition_stats prtn\r\n    WHERE  prtn.[object_id] = OBJECT_ID(N'dbo.HASH_SMALL')\r\n    AND    prtn.[index_id] < 2;\r\n    \r\n    \r\n    -- Load assembly if not loaded to prevent load time from skewing results\r\n    DECLARE @OptionalInitSQL NVARCHAR(MAX);\r\n    SET @OptionalInitSQL = CASE @HashAlgorithm\r\n           WHEN 1 THEN N'SELECT @Dummy = dbo.SpookyHash(0x1234);'\r\n           WHEN 2 THEN N'' -- HASHBYTES\r\n           WHEN 3 THEN N'' -- HASHBYTES\r\n           WHEN 4 THEN N'' -- CHECKSUM\r\n           WHEN 5 THEN N'SELECT @Dummy = dbo.SpookyHashLOB(0x1234);'\r\n           WHEN 6 THEN N'SELECT @Dummy = SQL#.Util_HashBinary(N''MD5'', 0x1234);'\r\n           WHEN 7 THEN N'SELECT @Dummy = SQL#.Util_HashBinary(N''SHA256'', 0x1234);'\r\n           WHEN 8 THEN N'SELECT @Dummy = SQL#.Util_HashBinary8k(N''MD5'', 0x1234);'\r\n           WHEN 9 THEN N'SELECT @Dummy = SQL#.Util_HashBinary8k(N''SHA256'', 0x1234);'\r\n    /* -- BETA / non-public code\r\n           WHEN 10 THEN N'SELECT @Dummy = SQL#.Util_HashSHA256Binary8k(0x1234);'\r\n           WHEN 11 THEN N'SELECT @Dummy = SQL#.Util_HashSHA256Binary(0x1234);'\r\n    */\r\n       END;\r\n\r\n    \r\n    IF (RTRIM(@OptionalInitSQL) <> N'')\r\n    BEGIN\r\n        SET @OptionalInitSQL = N'\r\n    SET NOCOUNT ON;\r\n    DECLARE @Dummy VARBINARY(100);\r\n    ' + @OptionalInitSQL;\r\n    \r\n        RAISERROR(N'** Executing optional initialization code:', 10, 1) WITH NOWAIT;\r\n        RAISERROR(@OptionalInitSQL, 10, 1) WITH NOWAIT;\r\n        EXEC (@OptionalInitSQL);\r\n        RAISERROR(N'-------------------------------------------', 10, 1) WITH NOWAIT;\r\n    END;\r\n    \r\n    \r\n    SET @ProcName = CASE @HashAlgorithm\r\n                        WHEN 1 THEN N'dbo.RUN_SpookyHash'\r\n                        WHEN 2 THEN N'dbo.RUN_HASHBYTES_MD5'\r\n                        WHEN 3 THEN N'dbo.RUN_HASHBYTES_SHA2_256'\r\n                        WHEN 4 THEN N'dbo.RUN_CHECKSUM'\r\n                        WHEN 5 THEN N'dbo.RUN_SpookyHashLOB'\r\n                        WHEN 6 THEN N'dbo.RUN_SR_MD5'\r\n                        WHEN 7 THEN N'dbo.RUN_SR_SHA256'\r\n                        WHEN 8 THEN N'dbo.RUN_SR_MD5_8k'\r\n                        WHEN 9 THEN N'dbo.RUN_SR_SHA256_8k'\r\n    /* -- BETA / non-public code\r\n                        WHEN 10 THEN N'dbo.RUN_SR_SHA256_new'\r\n                        WHEN 11 THEN N'dbo.RUN_SR_SHA256LOB_new'\r\n    */\r\n                        WHEN 13 THEN N'dbo.RUN_HASHBYTES_SHA2_256_NVC'\r\n                    END;\r\n    \r\n    RAISERROR(N'** Executing proc: %s', 10, 1, @ProcName) WITH NOWAIT;\r\n    \r\n    WHILE GETDATE() < @target_end_time\r\n    BEGIN\r\n        EXEC @ProcName;\r\n    \r\n        SET @query_execution_count = @query_execution_count + 1;\r\n    END;\r\n    \r\n    INSERT INTO dbo.LOG_HASHES\r\n    VALUES (GETDATE(), @HashAlgorithm, @@SPID, @RowCount * @query_execution_count);\r\n    \r\n    END;\r\n    GO\r\n\r\n**SETUP 3: Collision Detection Proc**\r\n    \r\n    GO\r\n    CREATE OR ALTER PROCEDURE dbo.VERIFY_NO_COLLISIONS (@HashAlgorithm INT)\r\n    AS\r\n    SET NOCOUNT ON;\r\n    \r\n    DECLARE @RowCount INT;\r\n    SELECT @RowCount = SUM(prtn.[row_count])\r\n    FROM   sys.dm_db_partition_stats prtn\r\n    WHERE  prtn.[object_id] = OBJECT_ID(N'dbo.HASH_SMALL')\r\n    AND    prtn.[index_id] < 2;\r\n    \r\n    \r\n    DECLARE @CollisionTestRows INT;\r\n    DECLARE @CollisionTestSQL NVARCHAR(MAX);\r\n    SET @CollisionTestSQL = N'\r\n    SELECT @RowsOut = COUNT(DISTINCT '\r\n    + CASE @HashAlgorithm\r\n           WHEN 1 THEN N'dbo.SpookyHash('\r\n           WHEN 2 THEN N'HASHBYTES(''MD5'','\r\n           WHEN 3 THEN N'HASHBYTES(''SHA2_256'','\r\n           WHEN 4 THEN N'CHECKSUM('\r\n           WHEN 5 THEN N'dbo.SpookyHashLOB('\r\n           WHEN 6 THEN N'SQL#.Util_HashBinary(N''MD5'','\r\n           WHEN 7 THEN N'SQL#.Util_HashBinary(N''SHA256'','\r\n           WHEN 8 THEN N'SQL#.[Util_HashBinary8k](N''MD5'','\r\n           WHEN 9 THEN N'SQL#.[Util_HashBinary8k](N''SHA256'','\r\n    --/* -- BETA / non-public code\r\n           WHEN 10 THEN N'SQL#.[Util_HashSHA256Binary8k]('\r\n           WHEN 11 THEN N'SQL#.[Util_HashSHA256Binary]('\r\n    --*/\r\n       END\r\n    + N'\r\n        CAST(FK1 AS BINARY(8)) + 0x7C +\r\n        CAST(FK2 AS BINARY(8)) + 0x7C +\r\n        CAST(FK3 AS BINARY(8)) + 0x7C +\r\n        CAST(FK4 AS BINARY(8)) + 0x7C +\r\n        CAST(FK5 AS BINARY(8)) + 0x7C +\r\n        CAST(FK6 AS BINARY(8)) + 0x7C +\r\n        CAST(FK7 AS BINARY(8)) + 0x7C +\r\n        CAST(FK8 AS BINARY(8)) + 0x7C +\r\n        CAST(FK9 AS BINARY(8)) + 0x7C +\r\n        CAST(FK10 AS BINARY(8)) + 0x7C +\r\n        CAST(FK11 AS BINARY(8)) + 0x7C +\r\n        CAST(FK12 AS BINARY(8)) + 0x7C +\r\n        CAST(FK13 AS BINARY(8)) + 0x7C +\r\n        CAST(FK14 AS BINARY(8)) + 0x7C +\r\n        CAST(FK15 AS BINARY(8)) + 0x7C +\r\n        CAST(FK16 AS BINARY(8)) + 0x7C +\r\n        CAST(FK17 AS BINARY(8)) + 0x7C +\r\n        CAST(FK18 AS BINARY(8)) + 0x7C +\r\n        CAST(FK19 AS BINARY(8)) + 0x7C +\r\n        CAST(FK20 AS BINARY(8))  ))\r\n    FROM dbo.HASH_SMALL;';\r\n    \r\n    PRINT @CollisionTestSQL;\r\n    \r\n    EXEC sp_executesql\r\n      @CollisionTestSQL,\r\n      N'@RowsOut INT OUTPUT',\r\n      @RowsOut = @CollisionTestRows OUTPUT;\r\n    \r\n    \r\n    IF (@CollisionTestRows <> @RowCount)\r\n    BEGIN\r\n        RAISERROR('Collisions for algorithm: %d!!!  %d unique rows out of %d.',\r\n        16, 1, @HashAlgorithm, @CollisionTestRows, @RowCount);\r\n    END;\r\n    GO\r\n\r\n**SETUP 4: Cleanup (DROP All Test Procs)**\r\n\r\n    DECLARE @SQL NVARCHAR(MAX) = N'';\r\n    SELECT @SQL += N'DROP PROCEDURE [dbo].' + QUOTENAME(sp.[name])\r\n                + N';' + NCHAR(13) + NCHAR(10)\r\n    FROM  sys.objects sp\r\n    WHERE sp.[name] LIKE N'RUN[_]%'\r\n    AND   sp.[type_desc] = N'SQL_STORED_PROCEDURE'\r\n    AND   sp.[name] <> N'RUN_HASHES_FOR_ONE_MINUTE'\r\n    \r\n    PRINT @SQL;\r\n    \r\n    EXEC (@SQL);\r\n    \r\n**SETUP 5: Generate Test Procs**\r\n\r\n    SET NOCOUNT ON;\r\n    \r\n    DECLARE @TestProcsToCreate TABLE\r\n    (\r\n      ProcName sysname NOT NULL,\r\n      CodeToExec NVARCHAR(261) NOT NULL\r\n    );\r\n    DECLARE @ProcName sysname,\r\n            @CodeToExec NVARCHAR(261);\r\n    \r\n    INSERT INTO @TestProcsToCreate VALUES\r\n      (N'SpookyHash', N'dbo.SpookyHash('),\r\n      (N'HASHBYTES_MD5', N'HASHBYTES(''MD5'','),\r\n      (N'HASHBYTES_SHA2_256', N'HASHBYTES(''SHA2_256'','),\r\n      (N'CHECKSUM', N'CHECKSUM('),\r\n      (N'SpookyHashLOB', N'dbo.SpookyHashLOB('),\r\n      (N'SR_MD5', N'SQL#.Util_HashBinary(N''MD5'','),\r\n      (N'SR_SHA256', N'SQL#.Util_HashBinary(N''SHA256'','),\r\n      (N'SR_MD5_8k', N'SQL#.[Util_HashBinary8k](N''MD5'','),\r\n      (N'SR_SHA256_8k', N'SQL#.[Util_HashBinary8k](N''SHA256'',')\r\n    --/* -- BETA / non-public code\r\n      , (N'SR_SHA256_new', N'SQL#.[Util_HashSHA256Binary8k]('),\r\n      (N'SR_SHA256LOB_new', N'SQL#.[Util_HashSHA256Binary](');\r\n    --*/\r\n    DECLARE @ProcTemplate NVARCHAR(MAX),\r\n            @ProcToCreate NVARCHAR(MAX);\r\n    \r\n    SET @ProcTemplate = N'\r\n    CREATE OR ALTER PROCEDURE dbo.RUN_{{ProcName}}\r\n    AS\r\n    BEGIN\r\n    DECLARE @dummy INT;\r\n    SET NOCOUNT ON;\r\n    \r\n    SELECT @dummy = COUNT({{CodeToExec}}\r\n        CAST(FK1 AS BINARY(8)) + 0x7C +\r\n        CAST(FK2 AS BINARY(8)) + 0x7C +\r\n        CAST(FK3 AS BINARY(8)) + 0x7C +\r\n        CAST(FK4 AS BINARY(8)) + 0x7C +\r\n        CAST(FK5 AS BINARY(8)) + 0x7C +\r\n        CAST(FK6 AS BINARY(8)) + 0x7C +\r\n        CAST(FK7 AS BINARY(8)) + 0x7C +\r\n        CAST(FK8 AS BINARY(8)) + 0x7C +\r\n        CAST(FK9 AS BINARY(8)) + 0x7C +\r\n        CAST(FK10 AS BINARY(8)) + 0x7C +\r\n        CAST(FK11 AS BINARY(8)) + 0x7C +\r\n        CAST(FK12 AS BINARY(8)) + 0x7C +\r\n        CAST(FK13 AS BINARY(8)) + 0x7C +\r\n        CAST(FK14 AS BINARY(8)) + 0x7C +\r\n        CAST(FK15 AS BINARY(8)) + 0x7C +\r\n        CAST(FK16 AS BINARY(8)) + 0x7C +\r\n        CAST(FK17 AS BINARY(8)) + 0x7C +\r\n        CAST(FK18 AS BINARY(8)) + 0x7C +\r\n        CAST(FK19 AS BINARY(8)) + 0x7C +\r\n        CAST(FK20 AS BINARY(8)) \r\n        )\r\n        )\r\n        FROM dbo.HASH_SMALL\r\n        OPTION (MAXDOP 1);\r\n    \r\n    END;\r\n    ';\r\n    \r\n    DECLARE CreateProcsCurs CURSOR READ_ONLY FORWARD_ONLY LOCAL FAST_FORWARD\r\n    FOR SELECT [ProcName], [CodeToExec]\r\n        FROM @TestProcsToCreate;\r\n    \r\n    OPEN [CreateProcsCurs];\r\n    \r\n    FETCH NEXT\r\n    FROM  [CreateProcsCurs]\r\n    INTO  @ProcName, @CodeToExec;\r\n    \r\n    WHILE (@@FETCH_STATUS = 0)\r\n    BEGIN\r\n        -- First: create VARBINARY version\r\n        SET @ProcToCreate = REPLACE(REPLACE(@ProcTemplate,\r\n                                            N'{{ProcName}}',\r\n                                            @ProcName),\r\n                                    N'{{CodeToExec}}',\r\n                                    @CodeToExec);\r\n    \r\n        EXEC (@ProcToCreate);\r\n    \r\n        -- Second: create NVARCHAR version (optional: built-ins only)\r\n        IF (CHARINDEX(N'.', @CodeToExec) = 0)\r\n        BEGIN\r\n            SET @ProcToCreate = REPLACE(REPLACE(REPLACE(@ProcToCreate,\r\n                                                        N'dbo.RUN_' + @ProcName,\r\n                                                        N'dbo.RUN_' + @ProcName + N'_NVC'),\r\n                                                N'BINARY(8)',\r\n                                                N'NVARCHAR(15)'),\r\n                                        N'0x7C',\r\n                                        N'N''|''');\r\n    \r\n            EXEC (@ProcToCreate);\r\n        END;\r\n\r\n        FETCH NEXT\r\n        FROM  [CreateProcsCurs]\r\n        INTO  @ProcName, @CodeToExec;\r\n    END;\r\n    \r\n    CLOSE [CreateProcsCurs];\r\n    DEALLOCATE [CreateProcsCurs];\r\n\r\n**TEST 1: Check For Collisions**\r\n\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 1;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 2;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 3;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 4;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 5;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 6;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 7;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 8;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 9;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 10;\r\n    EXEC dbo.VERIFY_NO_COLLISIONS 11;\r\n\r\n**TEST 2: Run Performance Tests**\r\n\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 1;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 2;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 3; -- HASHBYTES('SHA2_256'\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 4;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 5;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 6;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 7;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 8;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 9;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 10;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 11;\r\n    EXEC dbo.RUN_HASHES_FOR_ONE_MINUTE 13; -- NVC version of #3\r\n\r\n\r\n    SELECT *\r\n    FROM   dbo.LOG_HASHES\r\n    ORDER BY [LOG_TIME] DESC;\r\n\r\nVALIDATION ISSUES TO RESOLVE\r\n--\r\n\r\nWhile focusing on the performance testing of a singular SQLCLR UDF, two issues that were discussed early on were not incorporated into the tests, but ideally should be investigated in order to determine which approach meets _all_ of the requirements.\r\n\r\n1. The function will be executed twice per each query (once for the import row, and once for the current row). The tests so far have only referenced the UDF one time in the test queries. This factor might not change the ranking of the options, but it shouldn't be ignored, just in case.\r\n1. In a comment that has since been deleted, Paul White had mentioned:\r\n\r\n  > One downside of replacing `HASHBYTES` with a CLR scalar function - it appears that CLR functions cannot use batch mode whereas `HASHBYTES` can. That might be important, performance-wise.\r\n\r\n  So that is something to consider, and clearly requires testing. If the SQLCLR options do not provide any benefit over the built-in `HASHBYTES`, then that adds weight to [Solomon's suggestion][2] of capturing existing hashes (for at least the largest tables) into related tables.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/VAjtQ.png\r\n  [2]: https://dba.stackexchange.com/a/228792/30859\r\n\r\n\r\n	2019-12-03 11:59:45.121133+00	1	4	1	228966	0	0	0	2019-12-03 11:59:45.121133+00	\N	This isn't a traditional answer, but I thought it would be helpful to post benchmarks of some of the techniques mentioned so far. I'm testing on a 96 core server with SQL Server 2017 CU9.	f	f
194	249	160	2019-02-08 22:41:51+00	You can probably improve the performance, and perhaps the scalability of all the .NET approaches by pooling and caching any objects created in the function call.  EG for Paul White's code above:\r\n\r\n    static readonly ConcurrentDictionary<int,ISpookyHashV2> hashers = new ConcurrentDictonary<ISpookyHashV2>()\r\n    public static byte[] SpookyHash([SqlFacet (MaxSize = 8000)] SqlBinary Input)\r\n    {\r\n        ISpookyHashV2 sh = hashers.GetOrAdd(Thread.CurrentThread.ManagedThreadId, i => SpookyHashV2Factory.Instance.Create());\r\n\r\n        return sh.ComputeHash(Input.Value).Hash;\r\n    }\r\n\r\nSQL CLR discourages and tries to prevent using static/shared variables, but it will let you use shared variables if you mark them as readonly.  Which, of course, is meaningless as you can just assign a single instance of some mutable type, like `ConcurrentDictionary`.	2019-12-03 11:59:45.404581+00	0	4	1	229282	0	0	0	2019-12-03 11:59:45.404581+00	\N	You can probably improve the performance, and perhaps the scalability of all the .NET approaches by pooling and caching any objects created in the function call.  EG for Paul White's code above:	f	f
195	249	12	2019-02-04 15:25:03+00	Since you're just looking for changes, you don't need a cryptographic hash function.\r\n\r\nYou could choose from one of the faster non-cryptographic hashes in the open-source [Data.HashFunction library][1] by Brandon Dahler, licensed under the permissive and OSI approved [MIT license][2]. `SpookyHash` is a popular choice.\r\n\r\n### Example implementation\r\n\r\n### Source Code\r\n\r\n```cs\r\nusing Microsoft.SqlServer.Server;\r\nusing System.Data.HashFunction.SpookyHash;\r\nusing System.Data.SqlTypes;\r\n\r\npublic partial class UserDefinedFunctions\r\n{\r\n    [SqlFunction\r\n        (\r\n            DataAccess = DataAccessKind.None,\r\n            SystemDataAccess = SystemDataAccessKind.None,\r\n            IsDeterministic = true,\r\n            IsPrecise = true\r\n        )\r\n    ]\r\n    public static byte[] SpookyHash\r\n        (\r\n            [SqlFacet (MaxSize = 8000)]\r\n            SqlBinary Input\r\n        )\r\n    {\r\n        ISpookyHashV2 sh = SpookyHashV2Factory.Instance.Create();\r\n        return sh.ComputeHash(Input.Value).Hash;\r\n    }\r\n\r\n    [SqlFunction\r\n        (\r\n            DataAccess = DataAccessKind.None,\r\n            IsDeterministic = true,\r\n            IsPrecise = true,\r\n            SystemDataAccess = SystemDataAccessKind.None\r\n        )\r\n    ]\r\n    public static byte[] SpookyHashLOB\r\n        (\r\n            [SqlFacet (MaxSize = -1)]\r\n            SqlBinary Input\r\n        )\r\n    {\r\n        ISpookyHashV2 sh = SpookyHashV2Factory.Instance.Create();\r\n        return sh.ComputeHash(Input.Value).Hash;\r\n    }\r\n}\r\n```\r\n\r\nThe source provides two functions, one for inputs of 8000 bytes or less, and a LOB version. The non-LOB version should be significantly quicker.\r\n\r\nYou might be able to wrap a LOB binary in [`COMPRESS`][3] to get it under the 8000 byte limit, if that turns out to be worthwhile for performance. Alternatively, you could break the LOB up into sub-8000 byte segments, or simply reserve the use of `HASHBYTES` for the LOB case (since longer inputs scale better).\r\n\r\n### Pre-built code\r\n\r\nYou can obviously grab the package for yourself and compile everything, but I built the assemblies below to make quick testing easier:\r\n\r\nhttps://gist.github.com/SQLKiwi/365b265b476bf86754457fc9514b2300\r\n\r\n### T-SQL functions\r\n\r\n```\r\nCREATE FUNCTION dbo.SpookyHash\r\n(\r\n    @Input varbinary(8000)\r\n)\r\nRETURNS binary(16)\r\nWITH \r\n    RETURNS NULL ON NULL INPUT, \r\n    EXECUTE AS OWNER\r\nAS EXTERNAL NAME Spooky.UserDefinedFunctions.SpookyHash;\r\nGO\r\nCREATE FUNCTION dbo.SpookyHashLOB\r\n(\r\n    @Input varbinary(max)\r\n)\r\nRETURNS binary(16)\r\nWITH \r\n    RETURNS NULL ON NULL INPUT, \r\n    EXECUTE AS OWNER\r\nAS EXTERNAL NAME Spooky.UserDefinedFunctions.SpookyHashLOB;\r\nGO\r\n```\r\n\r\n### Usage\r\n\r\nAn example use given the sample data in the question:\r\n\r\n```\r\nSELECT\r\n    HT1.ID\r\nFROM dbo.HB_TBL AS HT1\r\nJOIN dbo.HB_TBL_2 AS HT2\r\n    ON HT2.ID = HT1.ID\r\n    AND dbo.SpookyHash\r\n    (\r\n        CONVERT(binary(8), HT2.FK1) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK2) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK3) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK4) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK5) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK6) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK7) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK8) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK9) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK10) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK11) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK12) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK13) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK14) + 0x7C +\r\n        CONVERT(binary(8), HT2.FK15) + 0x7C +\r\n        CONVERT(varbinary(1000), HT2.STR1) + 0x7C +\r\n        CONVERT(varbinary(1000), HT2.STR2) + 0x7C +\r\n        CONVERT(varbinary(1000), HT2.STR3) + 0x7C +\r\n        CONVERT(varbinary(1000), HT2.STR4) + 0x7C +\r\n        CONVERT(varbinary(1000), HT2.STR5) + 0x7C +\r\n        CONVERT(binary(1), HT2.COMP1) + 0x7C +\r\n        CONVERT(binary(1), HT2.COMP2) + 0x7C +\r\n        CONVERT(binary(1), HT2.COMP3) + 0x7C +\r\n        CONVERT(binary(1), HT2.COMP4) + 0x7C +\r\n        CONVERT(binary(1), HT2.COMP5)\r\n    )\r\n    <> dbo.SpookyHash\r\n    (\r\n        CONVERT(binary(8), HT1.FK1) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK2) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK3) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK4) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK5) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK6) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK7) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK8) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK9) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK10) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK11) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK12) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK13) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK14) + 0x7C +\r\n        CONVERT(binary(8), HT1.FK15) + 0x7C +\r\n        CONVERT(varbinary(1000), HT1.STR1) + 0x7C +\r\n        CONVERT(varbinary(1000), HT1.STR2) + 0x7C +\r\n        CONVERT(varbinary(1000), HT1.STR3) + 0x7C +\r\n        CONVERT(varbinary(1000), HT1.STR4) + 0x7C +\r\n        CONVERT(varbinary(1000), HT1.STR5) + 0x7C +\r\n        CONVERT(binary(1), HT1.COMP1) + 0x7C +\r\n        CONVERT(binary(1), HT1.COMP2) + 0x7C +\r\n        CONVERT(binary(1), HT1.COMP3) + 0x7C +\r\n        CONVERT(binary(1), HT1.COMP4) + 0x7C +\r\n        CONVERT(binary(1), HT1.COMP5)\r\n    );\r\n```\r\n\r\nWhen using the LOB version, the first parameter should be cast or converted to `varbinary(max)`.\r\n\r\n### Execution plan\r\n\r\n[![plan][4]][4]\r\n\r\n\r\n---\r\n\r\n## Safe Spooky\r\n\r\nThe *Data.HashFunction* library uses a number of CLR language features that are considered `UNSAFE` by SQL Server. It is possible to write a basic Spooky Hash compatible with `SAFE` status. An example I wrote based on [Jon Hanna's *SpookilySharp*][5] is below:\r\n\r\nhttps://gist.github.com/SQLKiwi/7a5bb26b0bee56f6d28a1d26669ce8f2\r\n\r\n  [1]: https://github.com/brandondahler/Data.HashFunction#datahashfunction-\r\n  [2]: http://opensource.org/licenses/MIT\r\n  [3]: https://docs.microsoft.com/en-us/sql/t-sql/functions/compress-transact-sql\r\n  [4]: https://i.stack.imgur.com/yC5Nq.png\r\n  [5]: https://bitbucket.org/JonHanna/spookilysharp/src/master/	2019-12-03 11:59:45.6754+00	3	4	1	228854	0	0	0	2019-12-03 11:59:45.6754+00	\N	Since you're just looking for changes, you don't need a cryptographic hash function.	f	f
796	566	2	2020-02-10 17:39:47.064932+00	I've come to think this would be confusing for new users, and not necessarily that helpful for old users, so  we are marking this `wont-fix`	2020-02-10 17:39:47.064932+00	3	1	1	\N	0	0	0	\N	\N	I've come to think this would be confusing for new users, and not necessarily that helpful for old users, so  we are marking this `wont-fix`	t	f
397	394	96	2019-12-10 06:56:11.397862+00	Technically this is already possible. Answers can be liked using the link to the question followed by a link anchor `#a` + the answer post ID. For example, the link to this answer is: https://topanswers.xyz/meta?q=394#a397\r\n\r\nThe issue is there is no obvious way to get this link since it does not appear on the post itself. The question overview page actually directly links answers, but it isn't discoverable.\r\n\r\nLinking the post date to the post itself would probably be the most readily discoverable way to expose this link. A small link icon on the post itself would also work.	2019-12-11 07:56:51.488997+00	10	4	2	\N	0	0	0	\N	\N	Technically this is already possible. Answers can be liked using the link to the question followed by a link anchor `#a` + the answer post ID. For example, the link to this answer is: https://topanswers.xyz/meta?q=394#a397	f	f
790	394	1	2020-02-10 17:12:56.02112+00	We have added a permalink button at the bottom of every post, next to the flag and subscribe buttons:\r\n\r\n![Screenshot.png](/image?hash=c56ed660e6a66e3a75c4176ffc32abd361b8a3ffd5057da9f26249fe409e237b)	2020-02-10 17:23:28.394779+00	3	1	1	\N	0	0	0	\N	\N	We have added a permalink button at the bottom of every post, next to the flag and subscribe buttons:	f	f
648	595	2	2020-01-22 11:05:14.300202+00	Appropriate participation on meta.se?\r\n\r\nSometimes people ask whether there are any alternatives to SE around, and it would be good to have an answer on [Are there any clones/alternatives for running a Stack Exchange style Q&A site?](https://meta.stackexchange.com/questions/2267/are-there-any-clones-alternatives-for-running-a-stack-exchange-style-qa-site) for example.\r\n\r\nHere's a great example of this kind of promotion in action: https://meta.stackexchange.com/a/342244/166851, please upvote it if you have a meta.se account.\r\n\r\nThere may be a other questions and/or comment threads where an answer or comment might be appropriate.	2020-01-22 23:19:50.412159+00	4	1	1	\N	0	0	0	\N	\N	Appropriate participation on meta.se?	f	f
651	595	2	2020-01-22 12:11:53.48266+00	Twitter ([https://twitter.com/TopAnswers_xyz](https://twitter.com/TopAnswers_xyz))?\r\n\r\nThings that might help get the word out on Twitter:\r\n\r\n* Suggest things we should tweet\r\n* Follow/retweet/like TopAnswers tweets (such as [this one](https://twitter.com/TopAnswers_xyz/status/1219936680339214336?s=20))\r\n* Tweet links to good questions/answers and tag us so we can retweet	2020-01-22 12:13:07.032411+00	3	1	1	\N	0	0	0	\N	\N	Twitter ([https://twitter.com/TopAnswers_xyz](https://twitter.com/TopAnswers_xyz))?	f	f
805	595	861	2020-02-11 11:30:04.523528+00	There has to be an incentive to be here, and that are currently only the TeX, DB and CodeGolf Communities, plus Meta if you want to actively build up TopAnswers.\r\n\r\nSince StackOverflow is the largest community on SE, we'd have to have an equivalent of that. I don't think it's possible to create a critical base of users without at least one very popular community. I'm a bit surprised that it doesn't exist, but also unsure if there is maybe a reason for that. I searched Meta a bit, but only could find another proposal for a C++ community - I don't know how fragmented TopAnswers is planned.	2020-02-11 11:30:04.523528+00	1	4	1	\N	0	0	0	\N	\N	There has to be an incentive to be here, and that are currently only the TeX, DB and CodeGolf Communities, plus Meta if you want to actively build up TopAnswers.	f	f
803	595	2	2020-02-10 22:53:22.729971+00	Advertise TopAnswers in your username on Stack Exchange?\r\n\r\nAt least four people have adopted this and most appear on a [Google search for "TopAnswers"](https://www.google.com/search?q=topanswers).\r\n\r\nPossible tags include:\r\n\r\n* … is at TopAnswers.xyz\r\n* … says try TopAnswers.xyz\r\n* … likes TopAnswers.xyz\r\n\r\nDo bear in mind that:\r\n\r\n* display names appear to be limited to 30 chars\r\n* "[when changing display name, you have 15 minutes grace period to change it back, otherwise you'll have to wait 30 days before being able to change it back](https://meta.stackexchange.com/a/85820/166851)".\r\n	2020-02-10 22:53:22.729971+00	3	1	1	\N	0	0	0	\N	\N	Advertise TopAnswers in your username on Stack Exchange?	t	f
801	697	2	2020-02-10 21:01:35.903639+00	This could be a compelling argument to enable down-votes at least on meta — but there is still an alternative:\r\n\r\nSimply propose the opposite and let that compete on up-votes. Something along those lines happened here:\r\n\r\n@@@ answer 257	2020-02-10 21:01:35.903639+00	0	1	1	\N	0	0	0	\N	\N	This could be a compelling argument to enable down-votes at least on meta — but there is still an alternative:	t	f
800	697	702	2020-02-10 20:39:18.775885+00	I'm not sure that this is quite a "duplicate", but there is already a healthy discussion about the desirability of having some disapproval mechanism (a.k.a. "DV") on the site:\r\n\r\n@@@ answer 185\r\n\r\nOP's scenario here is another one to consider, apart from regular "Q&A" which is the focus of the linked discussion.\r\n\r\n	2020-02-10 20:39:18.775885+00	2	1	1	\N	0	0	0	\N	\N	I'm not sure that this is quite a "duplicate", but there is already a healthy discussion about the desirability of having some disapproval mechanism (a.k.a. "DV") on the site:	f	f
788	425	2	2020-02-10 17:10:27.112411+00	I'm marking this as `wont-fix` for now.\r\n\r\nIn practice, the chat transcript isn't very connected with the Q&A, and that is by design. Q&A post history should be completely independent of the comments, which are just transient (albeit permanently available in archive form in the transcript) conversations around improving posts.	2020-02-10 17:10:27.112411+00	0	1	1	\N	0	0	0	\N	\N	I'm marking this as `wont-fix` for now.	t	f
825	715	168	2020-02-16 18:52:07.449591+00	Since the original answer has the problem that `comment` requires the environment's `\\end{foo}` to be completely on its own in a line without anything preceding it and anything following it (including spaces), the following is a new answer, that should work (almost) arbitrarily.\r\n\r\nIt utilizes the `parser` module of `pgf` to gobble everything regardless of braces. It even tries to locally change the category code of `%` to ignore comments, but this will only work if it's not in an argument of another macro (so that the category change does apply). As long as you don't use `%\\end{foo}` to end the environment this limitation doesn't really matter, though.\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\usepackage{enumitem}\r\n\\newlist{foo}{enumerate}{2}\r\n\\setlist[foo]{label=\\arabic*.}\r\n\r\n\\usepackage{pgf}\r\n\\usepgfmodule{parser}\r\n\r\n% save the definitions of \\foo as a list\r\n\\let\\fooList\\foo\r\n\\let\\endfooList\\endfoo\r\n\r\n% clear \\endfoo and set up \\foo to start the comment behaviour\r\n\\def\\commentfoo\r\n  {%\r\n    \\let\\endfoo\\empty\r\n    \\def\\foo\r\n      {%\r\n        % set up a parser rule for \\end, which grabs an argument and compares\r\n        % that argument to the string 'foo', if they match, end the parser\r\n        \\pgfparserdef{commentfoo}{initial}\\end[m]\r\n          {%\r\n            \\def\\tmpA{####1}\\def\\tmpB{foo}%\r\n            \\ifx\\tmpA\\tmpB\r\n              \\pgfparserswitch{final}%\r\n            \\fi\r\n          }%\r\n        % need to formally \\end the environment, so do this after the parser is\r\n        % done\r\n        \\pgfparserdeffinal{commentfoo}{\\end{foo}}%\r\n        % ignore input for which no parser rule is defined\r\n        \\pgfparserset{commentfoo/silent=true}%\r\n        \\catcode`\\%=12\r\n        \\pgfparserparse{commentfoo}%\r\n      }%\r\n  }\r\n% restore both \\foo and \\endfoo\r\n\\def\\uncommentfoo{\\let\\foo\\fooList\\let\\endfoo\\endfooList}\r\n\r\n\\begin{document}\r\n\\begin{foo}\r\n  \\item This should be shown 1\r\n\\end{foo}\r\n% works until this point and even works beyond.\r\n\\commentfoo\r\n\\begin{foo}\\item This should be hidden\\end{foo}This should be shown because it's\r\nnot inside of \\texttt{foo}.\r\n\\uncommentfoo\r\n\\begin{foo}\r\n  \\item This should be shown 2\r\n\\end{foo}\r\n\\commentfoo\r\n\\begin{foo}THis}is an{ unbalanced token list that's completely #gobbled\\end{foo}\r\n\\end{document}\r\n```	2020-02-16 18:52:07.449591+00	6	6	3	\N	0	0	0	\N	\N	Since the original answer has the problem that `comment` requires the environment's `\\end{foo}` to be completely on its own in a line without anything preceding it and anything following it (including spaces), the following is a new answer, that should work (almost) arbitrarily.	f	t
824	715	168	2020-02-16 12:02:09.023978+00	The problem is that the `comment` package doesn't seem to alter the `\\endfoo` macro. The following does `\\let` the `\\endfoo` to `\\empty` so that it doesn't call the `enumitem` internals which lead to the error. Also we have to restore `\\foo` and `\\endfoo` in `\\uncommentfoo`, for this we again just use `\\let` and don't let the `comment` package do this job (it will not restore the list behaviour, and then we'd get some `Lonely \\item` errors).\r\n\r\n```tex\r\n\\documentclass{article}\r\n\r\n\\usepackage{enumitem}\r\n\\usepackage{comment}\r\n\\newlist{foo}{enumerate}{2}\r\n\\setlist[foo]{label=\\arabic*.}\r\n\r\n% save the definitions of \\foo as a list\r\n\\let\\fooList\\foo\r\n\\let\\endfooList\\endfoo\r\n\r\n% clear \\endfoo and set up \\foo to start the comment behaviour\r\n\\def\\commentfoo{\\let\\endfoo\\empty\\excludecomment{foo}}\r\n% restore both \\foo and \\endfoo\r\n\\def\\uncommentfoo{\\let\\foo\\fooList\\let\\endfoo\\endfooList}\r\n\r\n\\begin{document}\r\n\\begin{foo}\r\n  \\item This should be shown 1\r\n\\end{foo}\r\n% works until this point and even works beyond\r\n\\commentfoo\r\n\\begin{foo}\r\n  \\item This should be hidden\r\n\\end{foo}\r\n\\uncommentfoo\r\n\\begin{foo}\r\n  \\item This should be shown 2\r\n\\end{foo}\r\n\\end{document}\r\n```	2020-02-16 12:38:12.609931+00	9	6	3	\N	0	0	0	\N	\N	The problem is that the `comment` package doesn't seem to alter the `\\endfoo` macro. The following does `\\let` the `\\endfoo` to `\\empty` so that it doesn't call the `enumitem` internals which lead to the error. Also we have to restore `\\foo` and `\\endfoo` in `\\uncommentfoo`, for this we again just use `\\let` and don't let the `comment` package do this job (it will not restore the list behaviour, and then we'd get some `Lonely \\item` errors).	f	t
558	503	234	2020-01-02 03:21:57.971546+00	@samcarter told you what the culprit is. It is, however, also true that the (preferred) syntax for circles has been changed. For some time it is \r\n\r\n```\r\n\\draw circle[radius=<length>];\r\n```\r\n\r\ninstead of \r\n\r\n```\r\n\\draw circle(<length>);\r\n```\r\n\r\nSo the following also works:\r\n\r\n\r\n```\r\n\\documentclass[11pt]{standalone}\r\n\\usepackage{pgfplots}\r\n\\pgfplotsset{compat=newest}\r\n\\usepgfplotslibrary{fillbetween}\r\n\\usetikzlibrary{arrows.meta}\r\n\t\\pgfplotsset{\r\n\t\tassi/.style={\r\n\t\t\taxis lines=middle,\r\n\t\t\taxis line style={-Stealth},\r\n\t\t\tticks=none,\r\n\t\t\tsamples=300,\r\n\t\t\tclip=false,\r\n\t\t\twidth=10cm\r\n\t\t\t},\r\n\t}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}[declare function={\r\n\t\tiperbole(\\x)=\r\n\t\t\t(sqrt((.4)+(.04*(\\x-2)^2)))\r\n\t\t\t;}]\r\n  \\begin{axis}[\r\n    assi,\r\n\t\tymin=-3,\r\n\t\txmin=-0.05,\r\n\t\tenlargelimits={value=.05,upper},\r\n    xlabel = {$\\sigma$},\r\n    ylabel = {$\\mu$},\r\n    every axis x label/.style={at=(current axis.right of origin),\t\tanchor=north east},\r\n    every axis y label/.style={at=(current axis.above origin),\t\tanchor=north east},\r\n\t] \r\n\t\\addplot[\r\n\t\tthick,\r\n\t\tdomain={-3:11},\r\n        restrict y to domain={2:11},\r\n        smooth, name path=frontier\r\n\t\t]\r\n\t\t({iperbole(\\x)},{x});\r\n    \\addplot[\r\n\t\tthick, dashed,\r\n\t\tdomain={-3:7},\r\n        restrict y to domain={-2:2},\r\n        smooth, name path=boundary\r\n\t\t]\r\n\t\t({iperbole(\\x)},{x});\r\n    \\pgfmathsetmacro{\\MVPx}{iperbole(2)}\r\n    \\pgfmathsetmacro{\\Sx}{iperbole(2.5)}\r\n    \\pgfmathsetmacro{\\Lx}{iperbole(5)}\r\n    \\pgfmathsetmacro{\\MAXx}{iperbole(11)}\r\n    \\pgfmathsetmacro{\\INTER}{7.5*\\MVPx-2}\r\n    \\addplot[thick,dotted,domain={0:\\MAXx}, smooth, name path=varbasso] {7.5*x-\\INTER};\r\n    \\addplot[thick,domain={0:\\MAXx}, smooth, name path=varalto] {7.5*x-2};\r\n    \\draw[fill=black,name intersections={of=frontier and varalto, by={i1,i2}}] \r\n        (i1) circle[radius=1pt] node [above left] {$I$}\r\n        (i2) circle[radius=1pt] node [above left] {$J$};\r\n  \\end{axis}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\nNotice that I also removed libraries that were not used and loaded the `pgfplots` library `fillbetween` to do the intersections.	2020-01-02 03:22:40.417243+00	5	4	3	\N	0	0	0	\N	\N	@samcarter told you what the culprit is. It is, however, also true that the (preferred) syntax for circles has been changed. For some time it is	f	f
557	503	167	2020-01-01 23:23:51.304029+00	I think the problem is that tikz/pgf has changed the definition of the circle in [commit 1302de8 ](https://github.com/pgf-tikz/pgf/commit/1302de8a99834bb0e30ffc214a7350952c446ab6) to have only one argument instead of two. At the same time pgfplots still assumes two arguments.\r\n\r\nI'd say this is a bug, but not sure if it should be reported to tikz or pgfplots.\r\n\r\nA workaround could be to adjust the changes pgfplots makes to only have one argument:\r\n\r\n\r\n```\r\n\\documentclass[11pt]{standalone}\r\n\\usepackage{tikz}\r\n\\usepackage{pgfplots}\r\n\\pgfplotsset{compat=newest}\r\n\\usetikzlibrary{calc, intersections, fit}\r\n\\usetikzlibrary{arrows.meta}\r\n\\usetikzlibrary{positioning}\r\n\\usetikzlibrary{backgrounds}\r\n\\tikzset{\r\n\tdeclare function={\r\n\t\tiperbole(\\x)=\r\n\t\t\t(sqrt((.4)+(.04*(\\x-2)^2)))\r\n\t\t\t;\r\n\t\t}\r\n\t}\r\n\t\\pgfplotsset{\r\n\t\tassi/.style={\r\n\t\t\taxis lines=middle,\r\n\t\t\taxis line style={-Stealth},\r\n\t\t\tticks=none,\r\n\t\t\tsamples=300,\r\n\t\t\tclip=false,\r\n\t\t\twidth=10cm\r\n\t\t\t},\r\n\t}\r\n\t\r\n\\makeatletter\r\n\\def\\pgfplots@path@@tikz@do@circle#1{%\r\n  \\begingroup\r\n  \\ifpgfplots@usefpu\r\n\t\\pgfkeys{/pgf/fpu,/pgf/fpu/output format=fixed}%\r\n  \\fi\r\n  \\pgfmathparse{#1}%\r\n  \\let\\tikz@ellipse@x=\\pgfmathresult%\r\n  \\ifpgfmathunitsdeclared%\r\n    \\pgfmathparse{#1}%\r\n    \\let\\tikz@ellipse@y=\\pgfmathresult%\r\n    \\ifpgfmathunitsdeclared%\r\n      \\pgfpathellipse{\\pgfpointorigin}{%\r\n        \\pgfqpoint{\\tikz@ellipse@x pt}{0pt}}{\\pgfpoint{0pt}{\\tikz@ellipse@y pt}}%\r\n    \\else%\r\n      \\PackageError{tikz}{You cannot mix dimensions and dimensionless values in an ellipse}{}%\r\n    \\fi%\r\n  \\else%\r\n    \\pgfmathparse{#1}%\r\n    \\let\\tikz@ellipse@y=\\pgfmathresult%\r\n    \\ifpgfmathunitsdeclared%\r\n      \\PackageError{tikz}{You cannot mix dimensions and dimensionless values in an ellipse}{}%\r\n    \\else%\r\n      \\pgfpathellipse{\\pgfpointorigin}\r\n\t  \t{\\pgfplotspointaxisdirectionxy{\\tikz@ellipse@x}{0}}\r\n\t\t{\\pgfplotspointaxisdirectionxy{0}{\\tikz@ellipse@y}}%\r\n    \\fi%\r\n  \\fi%\r\n  \\endgroup\r\n}\r\n\\makeatother\r\n\t\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n  \\begin{axis}[\r\n    assi,\r\n\t\tymin=-3,\r\n\t\txmin=-0.05,\r\n\t\tenlargelimits={value=.05,upper},\r\n    xlabel = {$\\sigma$},\r\n    ylabel = {$\\mu$},\r\n    every axis x label/.style={at=(current axis.right of origin),\t\tanchor=north east},\r\n    every axis y label/.style={at=(current axis.above origin),\t\tanchor=north east},\r\n\t] \r\n\t\\addplot[\r\n\t\tthick,\r\n\t\tdomain={-3:11},\r\n        restrict y to domain={2:11},\r\n        smooth, name path=frontier\r\n\t\t]\r\n\t\t({iperbole(\\x)},{x});\r\n    \\addplot[\r\n\t\tthick, dashed,\r\n\t\tdomain={-3:7},\r\n        restrict y to domain={-2:2},\r\n        smooth, name path=boundary\r\n\t\t]\r\n\t\t({iperbole(\\x)},{x});\r\n    \\pgfmathsetmacro{\\MVPx}{iperbole(2)}\r\n    \\pgfmathsetmacro{\\Sx}{iperbole(2.5)}\r\n    \\pgfmathsetmacro{\\Lx}{iperbole(5)}\r\n    \\pgfmathsetmacro{\\MAXx}{iperbole(11)}\r\n    \\pgfmathsetmacro{\\INTER}{7.5*\\MVPx-2}\r\n    \\addplot[thick,dotted,domain={0:\\MAXx}, smooth, name path=varbasso] {7.5*x-\\INTER};\r\n    \\addplot[thick,domain={0:\\MAXx}, smooth, name path=varalto] {7.5*x-2};\r\n    \\draw[fill=black,name intersections={of=frontier and varalto, by={i1,i2}}] \r\n        (i1) circle (1pt) node [above left] {$I$}\r\n        (i2) circle (1pt) node [above left] {$J$};\r\n  \\end{axis}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n	2020-01-01 23:44:56.628997+00	3	4	3	\N	0	0	0	\N	\N	I think the problem is that tikz/pgf has changed the definition of the circle in [commit 1302de8 ](https://github.com/pgf-tikz/pgf/commit/1302de8a99834bb0e30ffc214a7350952c446ab6) to have only one argument instead of two. At the same time pgfplots still assumes two arguments.	f	f
205	259	12	2019-06-22 11:20:20+00	When there is a mismatch between the data types of the column and variable, SQL Server cannot directly use the seeking ability of a b-tree index to locate the correct range of values.\r\n\r\nWhen the rules of [data type precedence][1] mean that the column data would have to be converted to the data type of the variable, this would mean scanning the whole table or index, converting each value and testing it against the variable as a residual predicate.\r\n\r\nThis is obviously not ideal, but so common (unfortunately) that SQL Server has a built-in way to achieve an index seek in these cases. It takes the supplied value and computes the range of values it maps to, accounting for the type conversion and collation.\r\n\r\nThis feature is known as a [dynamic seek][2] and the internal method that computes the mapped range is called `GetRangeThroughConvert`.\r\n\r\nFor example, when the `nvarchar` variable contains 'a', the mapped range of values for data type `varchar` might be 'a' to 'B' (the exact range depends on the collation). This means SQL Server can seek the `varchar` index between 'a' and 'B', testing only the matches for equality with 'a' (as `nvarchar`) as a residual predicate.\r\n\r\nWhen the supplied value is the empty string, the computed range is infinite, so the whole index is effectively scanned.\r\n\r\nFor example:\r\n\r\n```\r\nDROP TABLE IF EXISTS dbo.MyTable;\r\nGO\r\nCREATE TABLE dbo.MyTable\r\n(\r\n    ColumnA varchar(22) COLLATE Latin1_General_CI_AS NOT NULL\r\n);\r\nGO\r\nINSERT dbo.MyTable \r\n    WITH (TABLOCKX)\r\n    (ColumnA)\r\nSELECT TOP (1000)\r\n    REPLICATE(CHAR(65 + ROW_NUMBER() OVER (ORDER BY @@SPID) % 26), 22)\r\nFROM master.sys.all_columns AS AC1\r\nCROSS JOIN master.sys.all_columns AS AC2;\r\nGO\r\nCREATE INDEX IX_MyIndex_A ON dbo.MyTable(ColumnA);\r\n```\r\n\r\nThe following query uses a dynamic seek with a range of 'a' to 'B' and a residual predicate of `CONVERT_IMPLICIT(nvarchar(22),[dbo].[MyTable].[ColumnA],0)=[@varA]`:\r\n\r\n```\r\nDECLARE @varA nvarchar(22) = N'a';\r\n\r\nSELECT MT.ColumnA \r\nFROM dbo.MyTable AS MT \r\nWHERE MT.ColumnA = @varA;\r\n```\r\n\r\nThe [execution plan][3] shows the dynamic seek shape with 38 rows qualified by the seek, but all ultimately rejected by the residual:\r\n\r\n[![dynamic seek plan][4]][4]\r\n\r\nThe 38 rows are those counted by the query:\r\n\r\n```\r\nSELECT COUNT_BIG(*)\r\nFROM dbo.MyTable AS MT \r\nWHERE MT.ColumnA > 'a' \r\nAND MT.ColumnA < 'B';\r\n```\r\n\r\nWhen the variable contains an empty string, the calculated range is unbounded so the seek effectively scans the whole index:\r\n\r\n```\r\nDECLARE @varA nvarchar(22) = N''; -- empty string\r\n\r\nSELECT MT.ColumnA \r\nFROM dbo.MyTable AS MT \r\nWHERE MT.ColumnA = @varA;\r\n```\r\n\r\nThe [execution plan][5] shows all 1000 rows being read from the index (but again, discarded by the residual):\r\n\r\n[![Seeking the whole table][6]][6]\r\n\r\nThe empty string is a special case where `GetRangeThroughConvert` cannot produce a useful range. A single space character does produce a narrow seek range ([plan][7]).\r\n\r\nAnyway, the message is to pay careful attention to data types.\r\n\r\nDemo:\r\n\r\n<>https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=03f3a7b6a081ec7a417f4bb89e59275f\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-type-precedence-transact-sql\r\n  [2]: https://www.sql.kiwi/2012/01/dynamic-seeks-and-hidden-implicit-conversions.html\r\n  [3]: https://www.brentozar.com/pastetheplan/?id=rJGQg9jJS\r\n  [4]: https://i.stack.imgur.com/b8iSZ.png\r\n  [5]: https://www.brentozar.com/pastetheplan/?id=B17dl9jJB\r\n  [6]: https://i.stack.imgur.com/oxplz.png\r\n  [7]: https://www.brentozar.com/pastetheplan/?id=HJeix5jkB\r\n  [8]: https://dbfiddle.uk/?rdbms=sqlserver_2017&fiddle=03f3a7b6a081ec7a417f4bb89e59275f	2019-12-04 01:28:01.774247+00	3	4	1	241184	0	0	0	2019-12-04 01:26:57.935481+00	\N	When there is a mismatch between the data types of the column and variable, SQL Server cannot directly use the seeking ability of a b-tree index to locate the correct range of values.	f	f
789	426	2	2020-02-10 17:11:35.375451+00	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/21) so work can begin, and will update the post here when it is complete.	2020-02-10 17:11:35.375451+00	0	1	1	\N	0	0	0	\N	\N	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/21) so work can begin, and will update the post here when it is complete.	t	f
791	427	2	2020-02-10 17:13:45.661271+00	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/23) so work can begin, and will update the post here when it is complete.	2020-02-10 17:13:45.661271+00	3	1	1	\N	0	0	0	\N	\N	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/23) so work can begin, and will update the post here when it is complete.	t	f
395	390	168	2019-12-09 19:44:52.164647+00	Idea on how to prevent those conflicting edits: Branch out the message at the moment one hits edit and if there were edits in the meantime try to merge. If that's not possible show the merge conflicts. This would be similar to git.	2019-12-09 19:44:52.164647+00	7	4	1	\N	0	0	0	\N	\N	Idea on how to prevent those conflicting edits: Branch out the message at the moment one hits edit and if there were edits in the meantime try to merge. If that's not possible show the merge conflicts. This would be similar to git.	f	f
778	390	2	2020-02-10 15:39:56.499872+00	This is going to get more important as more people join so we need to start thinking about implementation soon.\r\n\r\nI've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/16) so work can begin, and will update the post here when it is complete.	2020-02-10 15:39:56.499872+00	0	1	1	\N	0	0	0	\N	\N	This is going to get more important as more people join so we need to start thinking about implementation soon.	t	f
560	504	2	2020-01-02 22:52:34.040356+00	Each star is given a weight of `1/(g+1)`, where `g` is the number of seconds ago the chat was starred. Then the messages are sorted in order of the sum of the star 'weights' for each star given.\r\n\r\nIn plain English, messages are shown according to how many stars they have, and how recently those stars were given. Messages *at the bottom* are those that have the most 'weight'.\r\n\r\nI think this roughly mirrors how SE sorts starred messages though I confess I didn't look too hard at that.	2020-01-15 18:12:48.422906+00	4	1	1	\N	0	0	0	\N	\N	Each star is given a weight of `1/(g+1)`, where `g` is the number of seconds ago the chat was starred. Then the messages are sorted in order of the sum of the star 'weights' for each star given.	f	f
833	719	702	2020-02-17 06:29:17.177981+00	- `[fun]` will be used for humorous, light-hearted, seasonal or celebratory activities/posts/etc.\r\n- `[mix-up]` tags items that might have been the subject of confusion or misunderstanding; it seems this designation is not yet set in stone.\r\n\r\n(*Caveat lector*! My guesses, so I hope they're not too far off the mark. ;))	2020-02-17 06:29:17.177981+00	8	1	1	\N	0	0	0	\N	\N	- `[fun]` will be used for humorous, light-hearted, seasonal or celebratory activities/posts/etc.	f	f
831	719	2	2020-02-17 00:38:33.779934+00	it's now possible to search by tag, and find the questions with these two tags.\r\n\r\n> What are these two tags for?\r\n\r\nI'm guessing we might want to keep `fun` and maybe rename `mix-up` to `by-design` or something like that?\r\n	2020-02-17 00:38:33.779934+00	3	1	1	\N	0	0	0	\N	\N	it's now possible to search by tag, and find the questions with these two tags.	t	f
828	538	2	2020-02-17 00:23:35.244329+00	I've moved this on to to [a GitHub issue](https://github.com/topanswers/topanswers/issues/28) so work can begin and will update the post here when it is complete.	2020-02-17 00:23:35.244329+00	3	1	1	\N	0	0	0	\N	\N	I've moved this on to to [a GitHub issue](https://github.com/topanswers/topanswers/issues/28) so work can begin and will update the post here when it is complete.	t	f
135	194	12	2017-08-02 08:57:53+00	The SQL Server optimizer **cannot** produce the execution plan you are after with the guarantee you need, because the *Hash Match Flow Distinct* operator is not order-preserving.\r\n\r\n> Though, I'm not sure (and suspect not) if this truly guarantees ordering.\r\n\r\nYou may *observe* order preservation in many cases, but this is an implementation detail; there is no guarantee, so you cannot rely on it. As always, presentation order can only be guaranteed by a top-level `ORDER BY` clause.\r\n\r\n### Example\r\n\r\nThe script below shows that Hash Match Flow Distinct does not preserve order. It sets up the table in question with matching numbers 1-50,000 in both columns:\r\n\r\n    IF OBJECT_ID(N'dbo.Updates', N'U') IS NOT NULL\r\n        DROP TABLE dbo.Updates;\r\n    GO\r\n    CREATE TABLE Updates\r\n    (\r\n        UpdateId INT NOT NULL IDENTITY(1,1),\r\n        ObjectId INT NOT NULL,\r\n    \r\n        CONSTRAINT PK_Updates_UpdateId PRIMARY KEY (UpdateId)\r\n    );\r\n    GO\r\n    INSERT dbo.Updates (ObjectId)\r\n    SELECT TOP (50000)\r\n        ObjectId =\r\n            ROW_NUMBER() OVER (\r\n                ORDER BY C1.[object_id]) \r\n    FROM sys.columns AS C1\r\n    CROSS JOIN sys.columns AS C2\r\n    ORDER BY\r\n        ObjectId;\r\n\r\nThe test query is:\r\n\r\n    DECLARE @Rows bigint = 50000;\r\n    \r\n    -- Optimized for 1 row, but will be 50,000 when executed\r\n    SELECT DISTINCT TOP (@Rows)\r\n        U.ObjectId \r\n    FROM dbo.Updates AS U\r\n    WHERE \r\n        U.UpdateId > 0\r\n    OPTION (OPTIMIZE FOR (@Rows = 1));\r\n\r\nThe estimated plan shows an index seek and flow distinct:\r\n\r\n[![Estimated plan][2]][2]\r\n\r\nThe output certainly seems ordered to start with:\r\n\r\n[![Start of results][3]][3]\r\n\r\n...but further down values start to go 'missing':\r\n\r\n[![Pattern breaking down][4]][4]\r\n\r\n...and eventually:\r\n\r\n[![Chaos breaks out][5]][5]\r\n\r\nThe explanation in this particular case, is that the hash operator spills:\r\n\r\n[![Post-execution plan][6]][6]\r\n\r\nOnce a partition spills, all rows that hash to the same partition also spill. Spilled partitions are processed later, breaking the expectation that distinct values encountered will be emitted immediately in the sequence they are received.\r\n\r\n---\r\n\r\nThere are many ways to write an efficient query to produce the ordered result you want, such as recursion or using a cursor. However, it cannot be done using *Hash Match Flow Distinct*.\r\n\r\n  [2]: https://i.stack.imgur.com/6M0va.png\r\n  [3]: https://i.stack.imgur.com/uWTxU.png\r\n  [4]: https://i.stack.imgur.com/Rx9u0.png\r\n  [5]: https://i.stack.imgur.com/H42Q4.png\r\n  [6]: https://i.stack.imgur.com/oV34e.png	2019-11-29 12:34:43.700455+00	1	4	1	182446	0	0	0	2019-11-29 12:34:43.700455+00	\N	The SQL Server optimizer **cannot** produce the execution plan you are after with the guarantee you need, because the *Hash Match Flow Distinct* operator is not order-preserving.	f	f
75	98	12	2019-11-25 10:07:01.343341+00	The thing about `CHECK` constraints is they only disallow rows for which the predicate returns `FALSE`. If the check returns `UNKNOWN`, that is not `FALSE`, so the row passes the check:\r\n\r\n    CREATE TABLE dbo.T1 (id int NULL CHECK (id = 1));\r\n\r\n    INSERT dbo.T1 VALUES (1); -- Ok\r\n    INSERT dbo.T1 VALUES (2); -- Error\r\n    INSERT dbo.T1 VALUES (NULL); -- Ok!\r\n\r\nYour check constraint does not disallow `NULL` values, which is the out-of-range 'value' the `SWITCH` statement is objecting to. Your switch-in table might contain nulls, which do not belong in partition 2.\r\n\r\nAdd `AND source_id IS NOT NULL` to your `CHECK` constraint, when the destination partition is not partition 1 (where the nulls go).	2019-11-25 10:07:01.343341+00	1	4	2	\N	0	0	0	\N	\N	The thing about `CHECK` constraints is they only disallow rows for which the predicate returns `FALSE`. If the check returns `UNKNOWN`, that is not `FALSE`, so the row passes the check:	f	f
179	232	12	2019-12-01 21:50:05.582926+00	You earn stars when someone stars your question or answer.\r\n\r\n* Initially, you can award one star to each post.\r\n* When you reach a total star score of 10, you will be able to award one or two stars to each post.\r\n* When you reach 100 stars, you can award up to three stars.\r\n* ...and so on.\r\n\r\nQuestions can only be awarded stars if they are *meta* posts (all questions on meta, questions marked as local meta questions on main).\r\n\r\nYour total stars are maintained separately on each site.	2020-02-10 23:09:44.604124+00	15	4	1	\N	0	0	0	\N	\N	You earn stars when someone stars your question or answer.	f	f
255	291	16	2017-10-18 01:48:56+00	There's no real difference there, but when you start using `DATETIME2` values, or functions that return `DATETIME2` values, you'll get errors.\r\n\r\n    SELECT SYSDATETIME() - 1 AS [Incompatible]\r\n\r\n> Msg 206, Level 16, State 2, Line 17 Operand type clash: datetime2 is\r\n> incompatible with int\r\n\r\nFor these, you have to use date math functions. \r\n\r\n    SELECT DATEADD(DAY, -1, SYSDATETIME()) AS [Compatible]\r\n\r\nAaron Bertrand speaks about this issue briefly in his [Bad Habits to Kick][1] series. \r\n\r\n\r\n  [1]: https://sqlblog.org/2011/09/20/bad-habits-to-kick-using-shorthand-with-date-time-operations	2019-12-04 14:34:26.116045+00	1	4	1	188716	0	0	0	2019-12-04 14:32:20.539635+00	\N	There's no real difference there, but when you start using `DATETIME2` values, or functions that return `DATETIME2` values, you'll get errors.	f	f
362	370	12	2019-12-06 14:13:23.732478+00	*Basic* [markdown](https://markdown-it.github.io/) in **answer snippets** is useful.\r\n\r\nIt would be nice if markdown that isn't supported in snippets were suppressed.	2019-12-06 14:13:23.732478+00	4	4	1	\N	0	0	0	\N	\N	*Basic* [markdown](https://markdown-it.github.io/) in **answer snippets** is useful.	f	f
361	370	96	2019-12-06 14:08:04.370745+00	## For exhibit **A**\r\n\r\nMy proposal is that the post snippet for this post would show up as:\r\n\r\n> Answer: For exhibit A\r\n\r\nCurrently a mix and match edition with some markup stripped, others not is presented:\r\n\r\n![2019-12-06_17-08.png](/image?hash=194801b9a63b2c82620e6c71e7c72d767c5c70b0d299dd421e9a0aa52165f01b)\r\n\r\nOne or two cases is not a big deal, but it quickly becomes noise:\r\n\r\n![2019-12-06_17-09.png](/image?hash=27d6ceddcaf4098a9ec39637cbc51bbe49fde5303e49a76b6860b21f1829b7d2)	2019-12-06 15:05:12.696552+00	2	4	2	\N	0	0	0	\N	\N	For exhibit **A**	f	f
701	370	821	2020-01-29 02:08:19.298596+00	Might this be done in a way that doesn't remove the `#` character from code? It's a common character for some languages in Code Golf (i.e., input for Mathematica), and such answers may be quoted fully in the snippet where the silent deletion would be confusing.\r\n\r\n(Edit: Actually, it looks like the code doesn't appear in the summary when  a header comes first, so this is moot. Though maybe ideally the code would appear if it's short.)	2020-01-29 02:15:33.830737+00	0	4	1	\N	0	0	0	\N	\N	Might this be done in a way that doesn't remove the `#` character from code? It's a common character for some languages in Code Golf (i.e., input for Mathematica), and such answers may be quoted fully in the snippet where the silent deletion would be confusing.	f	f
363	370	2	2019-12-06 17:11:17.607809+00	We are now (as of 28 Jan 2020) removing leading (and trailing) '#' characters from answer summaries.\r\n\r\nAs Paul [says](/meta?q=370#a362), "Basic markdown in answer snippets is useful"\r\n\r\nThe biggest eyesore is the '#' heading markers, which are very common in the first line of an answer, and don't add any value to the answer summary. As of now we are stripping those out (but leaving the rest in).\r\n\r\nWe thought about removing '>' characters, but that can make the answer summary look misleading. We think answers that start with quoting the question, and answers with reference-style links, both need to be tweaked with an edit to improve the answer summaries.\r\n\r\nYou may also be interested to know we are now also cheat-rendering reference links:\r\n\r\n@@@ answer 715	2020-01-30 23:49:36.478927+00	7	1	1	\N	0	0	0	\N	\N	We are now (as of 28 Jan 2020) removing leading (and trailing) '#' characters from answer summaries.	f	f
782	399	2	2020-02-10 15:51:10.070875+00	This is a popular request and I've moved it on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/19) so work can begin. I will update the post here when it is complete.	2020-02-10 15:51:10.070875+00	0	1	1	\N	0	0	0	\N	\N	This is a popular request and I've moved it on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/19) so work can begin. I will update the post here when it is complete.	t	f
394	387	234	2019-12-09 18:36:27.303208+00	Of course. You should just ask the authors of the Christmas Exravaganza. ;-)\r\n\r\n```\r\n\\documentclass{beamer}\r\n\r\n\\usepackage{tikz}\r\n\\usetikzlibrary{calc,decorations.markings}\r\n\r\n\\setbeamertemplate{background}{%\r\n  \\begin{tikzpicture}[\r\n    remember picture,\r\n    overlay,\r\n    decoration={\r\n      markings, \r\n      mark=at position \\thepage/10 with {\r\n        \\path (-0.1cm,0) coordinate (aux0) (0.1cm,0) coordinate(aux1)\r\n\t\t(0,0) coordinate(aux2);\r\n\t\t\\pgftransformreset\r\n\t\t\\path let \\p1=($(aux1)-(aux0)$), \\n1={atan2(\\y1,\\x1)} in \r\n\t\t(aux2) node[rotate=\\n1]{\\includegraphics[width=2cm]{example-image-duck}};\r\n      }\r\n    }\r\n  ]\r\n    \\path[postaction=decorate,draw] (-0.0024, -0.846) .. controls (2.9731, -2.564) and (4.9397, -0.8115) .. (4.9397, -4.2473) .. controls (4.9397, -8.2498) and (-2.9933, -2.7915) .. (0.9993, -3.0728) .. controls (10.1424, -3.7168) and (9.4455, -9.435) .. (6.1477, -7.1566) .. controls (3.4609, -5.3004) and (10.1189, 2.4134) .. (13.8065, -10.4522);\r\n  \\end{tikzpicture}%\r\n}\r\n\r\n\\begin{document}\r\n\r\n\\begin{frame}\r\n  \\pause[10]\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n\r\n![ani.gif](/image?hash=1d06a2ee1dfee9358e3dc25eefef06c9156974ae12f8dd1e4820a650dd5cce84)	2019-12-09 19:25:13.142468+00	10	4	3	\N	0	0	0	\N	\N	Of course. You should just ask the authors of the Christmas Exravaganza. ;-)	f	f
839	724	895	2020-02-18 16:03:04.962582+00	As of C++17 the official answer is [`std::from_chars`](https://en.cppreference.com/w/cpp/utility/from_chars):\r\n\r\n> Unlike other parsing functions in C++ and C libraries, `std::from_chars` is locale-independent, non-allocating, and non-throwing. Only a small subset of parsing policies used by other libraries (such as `std::sscanf`) is provided. This is intended to allow the fastest possible implementation.\r\n\r\nAdditionally, `std::from_chars`:\r\n\r\n * Supports bases 2 to 36\r\n * Works with both `std::string`s and `char*`s\r\n * Does not throw on error\r\n\r\nIt can be used by doing something like:\r\n\r\n    int num;\r\n    const auto finish = &*std::cend(str);\r\n    const auto length = std::from_chars(&*std::cbegin(str), finish, num);\r\n\r\nThe entire string has been consumed if `length.ptr == finish`.\r\n\r\n[**Live Example**](https://ideone.com/uVzE4J)	2020-02-18 16:03:04.962582+00	1	4	1	\N	0	0	0	\N	\N	As of C++17 the official answer is [`std::from_chars`](https://en.cppreference.com/w/cpp/utility/from_chars):	f	f
814	702	167	2020-02-12 23:49:48.378338+00	Assuming you want all the underlines on the same height, here two possibilities\r\n\r\n### ulem package\r\n\r\n```\r\n% !TeX TS-program = xelatex\r\n\r\n\\documentclass{article}\r\n\\usepackage{fontspec}\r\n\\setmainfont[Script=Devanagari,Mapping=devanagarinumerals]{Shobhika}\r\n\r\n\\usepackage[normalem]{ulem}\r\n\\newlength{\\mydepth}\r\n\r\n% insert character with heighest depth here\r\n\\settodepth{\\mydepth}{क्लू}\r\n\r\n\\renewcommand{\\ULthickness}{0.15ex}\r\n\\setlength{\\ULdepth}{\\dimexpr\\mydepth+0.3ex\\relax}\r\n\r\n\\begin{document}\r\n\t\r\n\t\\uline{अबकड} \\uline{कू} \\uline{क्लू} \\uline{ट्टू}\r\n\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2020-02-13 at 00.44.43.png](/image?hash=4fa6e596e2c0145a0cd8f73d507f251cf5abfcd9bb3ce7f403d4154db8b95f3c)\r\n\r\n### classic underline\r\n\r\n```\r\n% !TeX TS-program = xelatex\r\n\r\n\\documentclass{article}\r\n\\usepackage{fontspec}\r\n\\setmainfont[Script=Devanagari,Mapping=devanagarinumerals]{Shobhika}\r\n\r\n\\newlength{\\mydepth}\r\n\\newlength{\\myheight}\r\n\r\n% insert character with heighest depth here\r\n\\settodepth{\\mydepth}{क्लू}\r\n\\settoheight{\\myheight}{क्लू}\r\n\r\n\\newcommand{\\fix}{\\rule[-\\mydepth]{0pt}{\\dimexpr\\mydepth+\\myheight\\relax}}\r\n\\let\\oldunderline\\underline\r\n\\renewcommand{\\underline}[1]{\\oldunderline{\\fix #1}}\r\n\r\n\\begin{document}\r\n\r\n\t\\underline{अबकड} \\underline{कू} \\underline{क्लू} \\underline{ट्टू}\r\n\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2020-02-13 at 00.47.07.png](/image?hash=694cf275653794b981c0769210669d7fcf1edad89488673272cae74c08eda9ce)	2020-02-13 09:12:19.471592+00	5	4	3	\N	0	0	0	\N	\N	Assuming you want all the underlines on the same height, here two possibilities	f	t
638	585	167	2020-01-20 14:50:15.822662+00	You can temporarily switch on the behaviour of handout mode before the frame you want to show fully uncovered and then go back to normal beamer mode after the frame:\r\n\r\n\r\n```\r\n\\documentclass{beamer}\r\n\r\n\\makeatletter\r\n\\newcommand{\\switchhandout}{\\gdef\\beamer@currentmode{handout}}\r\n\\newcommand{\\switchbeamer}{\\gdef\\beamer@currentmode{beamer}}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\r\n\\begin{frame}\r\nnormal \r\n\\pause\r\nmode\r\n\\end{frame}\r\n\r\n\\switchhandout\r\n\\begin{frame}\r\nhandout\r\n\\pause\r\nmode\r\n\\end{frame}\r\n\\switchbeamer\r\n\r\n\\begin{frame}\r\nnormal\r\n\\pause\r\nmode\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n\r\n![document.gif](/image?hash=4592108c7dacc35e0e82f261662c0e9be03da09e09a2e579a2cc639758cdbbdd)\r\n\r\n(I would prefer if this answer would not be reposted on tex.se)	2020-01-20 14:54:11.971013+00	2	4	3	\N	0	0	0	\N	\N	You can temporarily switch on the behaviour of handout mode before the frame you want to show fully uncovered and then go back to normal beamer mode after the frame:	f	f
367	350	234	2019-12-07 00:54:01.140971+00	# A Snow(wo)man\r\n\r\n...because TeX is cool. The top hat is shamelessly stolen from [samcarter's answer](https://topanswers.xyz/tex?q=350#a360), and the snow man from their package.\r\n\r\n```\r\n\\documentclass[tikz, border=3mm]{standalone}\r\n\\usepackage{tikzlings}\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\snowman[tophat=black!90!white,scale=1.1111]\r\n\\node[text=white,rotate=-15] at (0.18,2.4) {\\TeX};\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2019-12-06 at 4.53.40 PM.png](/image?hash=5e6bba9800fce81902685d15d4d4d4448df7d74e3718aacb527b64f5408af660)\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_snowman_24px.png](/image?hash=56d4628b66bfffbc0fa1526ea65ac282f41960da54183a25648bbcf522a730eb)	2019-12-11 07:08:53.984958+00	3	4	3	\N	0	0	0	\N	\N	A Snow(wo)man	f	f
359	350	167	2019-12-06 13:06:25.137238+00	# TeX Speech Ballon\r\n\r\n![logo.png](/image?hash=32f3d0b6e7848abb7d8abd0e6a951fcd7bad7a4a3917289e5f4e998ea5f95599)\r\n\r\n![logo.svg](/image?hash=c62d45c38b9e559d4ddc0a6978e9231620d91935384662681f9aa0b8c0fa5474)\r\n\r\nThis would be a simple icon for most situations and could be combined with e.g. ducksay, tikzducks or tikzlings if we need some funny image somewhere.\r\n\r\nPNG with 24px:\r\n\r\n![TeX_speech_bubble_24px.png](/image?hash=02422d2652079b3bf942625d406b838d6990f92086bafa5f63d20e2235ad8701)\r\n\r\n	2019-12-10 13:48:55.567346+00	4	4	3	\N	0	0	0	\N	\N	TeX Speech Ballon	f	f
340	350	167	2019-12-06 09:38:43.049033+00	# The TeX Logo\r\n\r\nThe plain and simple TeX logo in a colour of our choice (the red  shown here is just an example to match the site colour):\r\n\r\n![document.png](/image?hash=1e1ed6ee65f222fb8541f2436c886c261c2879d8fe62ea9f08dc9259bfdf2c0b)\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_tex_24px.png](/image?hash=f4d41d567713a897e9896990fc0ed0d3335f611362726523798eb5c96c409150)\r\n\r\n```\r\n\\documentclass[margin=3mm]{standalone}\r\n\\usepackage{xcolor}\r\n\\definecolor{dred}{RGB}{159,57,61}\r\n\\begin{document}\r\n\\color{dred}\\Huge \\TeX\r\n\\end{document}\r\n```	2019-12-07 10:17:40.292134+00	3	4	3	\N	0	0	0	\N	\N	The TeX Logo	f	f
337	350	96	2019-12-06 07:38:19.574634+00	# A Duck\r\n\r\nFor example DEK duck.\r\n\r\n...because [ducks](https://github.com/samcarter/tikzducks).\r\n\r\nTransparent DEK at 300dpi:\r\n\r\n![topanswerstex300dpi.png](/image?hash=792e76f4a16efef5a6eeaef5b3efdb6b418371d674c4a75a182d90aab4d545e0)\r\n\r\nTransparent DEK with 24px:\r\n\r\n![topanswerstex24px.png](/image?hash=50cad760a7f29881d74878120091c6ff2262da103dbb6e9d2ee29b1590769787)	2019-12-07 14:47:56.716508+00	4	4	3	\N	0	0	0	\N	\N	A Duck	f	f
370	350	168	2019-12-07 10:54:47.619468+00	# A Small Duck Saying "TeX"\r\n\r\nSince the CM font used by `\\tiny` is created to be readable at tiny sizes, I figured it might be a good idea to use it and scale it up.\r\n\r\n```tex\r\n\\documentclass[]{standalone}\r\n\\usepackage{ducksay}\r\n\\usepackage{xcolor}\r\n\\usepackage[]{graphicx}\r\n\\definecolor{dred}{RGB}{159,57,61}\r\n\\AddAnimal{small-duck2}\r\n{ \\\r\n   >()_\r\n    (__)__}\r\n\\def\\arraystretch{0.8}\r\n\\begin{document}\r\n\\ducksay\r\n  [%\r\n    small-duck2,msg=\\rmfamily\\color{dred}\\bfseries\\tiny,hpad=0,\r\n    bubble-bot-kern=3.1pt\r\n  ]\r\n  {\\kern-2.5pt\\raisebox{4pt}{\\scalebox{3.4}{\\TeX}}\\kern-2.5pt}%\r\n\\end{document}\r\n```\r\n\r\nPNG at 300dpi:\r\n\r\n![TeXlogo_smallduck_300dpi.png](/image?hash=7abaef46801cd66a363c5f6a74f17e88972f5ced66e4cfda7a553590321dfd66)\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_smallduck_24px.png](/image?hash=3b9b2255dee8fce150b0fa1ec930d096e9e87b41eed3bcb9c117aa075da97f35)\r\n\r\n---\r\n\r\n```tex\r\n\\documentclass[]{standalone}\r\n\\usepackage{ducksay}\r\n\\usepackage{xcolor}\r\n\\definecolor{dred}{RGB}{159,57,61}\r\n\\AddAnimal{small-duck2}\r\n{ \\\r\n   >()_\r\n    (__)__}\r\n\\def\\arraystretch{0.8}\r\n\\begin{document}\r\n\\ducksay\r\n  [%\r\n    small-duck2,msg=\\rmfamily\\color{dred}\\bfseries\\LARGE,hpad=0,\r\n    bubble-bot-kern=3.1pt\r\n  ]\r\n  {\\raisebox{.5pt}{\\TeX}}%\r\n\\end{document}\r\n```\r\n\r\nPNG at 300dpi:\r\n\r\n![TeXlogo_smallduck_300dpi.png](/image?hash=6c3086701109b151b66c6d44ef46fbd7fa771497343559d9f6ccbfaf6492b547)\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_smallduck_24px.png](/image?hash=57a075031245112b55b94b4c5a0026c859a091e617fbe07840342f4404774f30)	2019-12-13 18:43:10.017134+00	4	4	1	\N	0	0	0	\N	\N	A Small Duck Saying "TeX"	f	f
369	350	168	2019-12-07 09:35:18.101032+00	# TeX logo with question mark\r\n\r\nI created this with legebility in mind, it is simple and shows what the site is about. Only the E with the question mark in the background might be a bit hard to decipher.\r\n\r\n```tex\r\n\\documentclass[]{standalone}\r\n\\usepackage{tikz}\r\n\\definecolor{dred}{RGB}{159,57,61}\r\n\\pagecolor{gray!20!white}\r\n\\begin{document}\r\n\\begin{tikzpicture}[inner sep=0pt]\r\n  \\node{\\textcolor{dred}{\\sffamily\\bfseries\\huge?}};\r\n  \\node{\\TeX};\r\n  % to get a square canvas\r\n  \\node[rotate=90,transform shape]{\\phantom{\\TeX}};\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\nPNG at 300dpi:\r\n\r\n![TeXlogo_300dpi.png](/image?hash=5bd66e956cc073e42f7f4f330bb23cd7780f707c3760e6e897bab0cc4135a7b7)\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_24px.png](/image?hash=697026770e8ec2b321c57a87fd9b96f243adc01a49ad64d3522c87ba39a29c26)\r\n\r\n## Old Version with non-bold question mark\r\n\r\nPNG at 300dpi:\r\n\r\n![TeXlogo_300dpi.png](/image?hash=f3e75213cf726db7d0793a9ad6b9ecd0eb4754ef10406a53d70cd6d81de4082a)\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_24px.png](/image?hash=29ffcc19ad048b7b0300aa0b805b0c910bae14f927412899a3ac74db33e12ec8)	2019-12-07 09:40:27.369898+00	1	4	1	\N	0	0	0	\N	\N	TeX logo with question mark	f	f
360	350	167	2019-12-06 13:32:55.824089+00	# TeX Tophat\r\n\r\n...for topanswers:\r\n\r\n![tophat.png](/image?hash=4f21c2725250fcebe340cfb3b6786114fe6867f19d3512e448a2836d313a1012)\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_tophat_nocolor_24px.png](/image?hash=776b80b422ba631225b943f70e6bd8bd397e8512a2fd0d03ee228d2de6adb5d6)\r\n\r\n```\r\n\\documentclass[margin=0.3mm]{standalone}\r\n\\usepackage{tikz}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\begin{scope}[transform shape,rotate=-17.5,font=\\sffamily]\r\n\r\n% brim\r\n\\fill[black] (0.62, 0.66) .. controls (0.62, 0.52) and (0.34, 0.37) .. (0, 0.37) .. controls (-0.34, 0.37) and (-0.62, 0.52) .. (-0.62, 0.66) .. controls (-0.62, 0.8) and (-0.34, 0.7) .. (0, 0.7) .. controls (0.34, 0.7) and (0.62, 0.8) .. (0.62, 0.66) -- cycle;\r\n\r\n% top ellipse\r\n\\fill[black] (0,1.13) ellipse (0.42 and 0.1);\r\n\\fill[black] (-0.37,0.61) -- (0.37,0.61) -- (0.42,1.13) -- (-0.42,1.13) -- cycle;\r\n\r\n% bottom ellipse\r\n\\fill[black] (0,0.61) ellipse (0.37 and 0.1);\r\n\r\n% text\r\n\\begin{scope}[yshift=-0.05cm]\r\n\\node[white] at (-0.2,0.92) {T};\r\n\\node[white] at (-0.04,0.84) {E};\r\n\\node[white] at (0.16,0.92) {X};\r\n\\end{scope}\r\n\r\n\\end{scope}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\nThe tophat can also be combined with colour:\r\n\r\n![tophat.png](/image?hash=028f37099069675c4afcf78887be066244c4996aec0a8c7a19be3ddf669e6a1f)\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_tophat_24px.png](/image?hash=080707628a7a4433a980790b315d225836fdb522a4b1c990c886dabae40d391c)\r\n\r\n```\r\n\\documentclass[margin=0.3mm]{standalone}\r\n\\usepackage{tikz}\r\n\\definecolor{dred}{RGB}{159,57,61}\r\n\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\begin{scope}[transform shape,rotate=-17.5,font=\\sffamily]\r\n\r\n% brim\r\n\\fill[black] (0.62, 0.66) .. controls (0.62, 0.52) and (0.34, 0.37) .. (0, 0.37) .. controls (-0.34, 0.37) and (-0.62, 0.52) .. (-0.62, 0.66) .. controls (-0.62, 0.8) and (-0.34, 0.7) .. (0, 0.7) .. controls (0.34, 0.7) and (0.62, 0.8) .. (0.62, 0.66) -- cycle;\r\n\r\n% top ellipse\r\n\\fill[black] (0,1.13) ellipse (0.42 and 0.1);\r\n\\fill[black] (-0.37,0.61) -- (0.37,0.61) -- (0.42,1.13) -- (-0.42,1.13) -- cycle;\r\n\r\n% bottom ellipse\r\n\\fill[dred] (0,0.61) ellipse (0.37 and 0.1);\r\n\\fill[dred] (-0.37,0.61) -- (0.37,0.61) -- (0.38,0.73) -- (-0.38,0.73) -- cycle;\r\n\r\n% cutout\r\n\\fill[black] (0,0.73) ellipse (0.38 and 0.1);\r\n\r\n% text\r\n\\node[white] at (-0.2,0.92) {T};\r\n\\node[white] at (-0.04,0.84) {E};\r\n\\node[white] at (0.16,0.92) {X};\r\n\r\n\\end{scope}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```	2019-12-07 10:17:14.216893+00	8	4	3	\N	0	0	0	\N	\N	TeX Tophat	f	f
336	350	96	2019-12-06 07:37:11.976049+00	# Braces\r\n\r\nJust an empty pair of braces, perhaps colored to match the site.\r\n\r\n# **{ }**\r\n    \r\n...because TeX without braces is like a duck without water.	2019-12-06 13:58:03.293361+00	1	4	3	\N	0	0	0	\N	\N	Braces	f	f
347	350	168	2019-12-06 11:19:41.240394+00	# A Duck Saying "TeX"\r\n\r\n![TeXlogo-1.png](/image?hash=c1c8e8fcbb9f1139e9a6f7dd90f29e08584cdf7fe07553e1f7e9f660a812c32d).\r\n\r\nPNG with 24px:\r\n\r\n![TeXlogo_ducksay_24px.png](/image?hash=75c6607854d87b19a2050043c37c83017a4cb47edc9621000dd4c38ddc50cfd4)\r\n\r\n```tex\r\n\\documentclass[margin=3.14]{standalone}\r\n\\usepackage{ducksay}\r\n\\usepackage{xcolor}\r\n\\definecolor{dred}{RGB}{159,57,61}\r\n\\begin{document}\r\n\\ducksay[vpad=1,msg=\\rmfamily]{\\color{dred}\\Huge\\TeX}\r\n\\end{document}\r\n```	2019-12-07 10:49:43.817763+00	3	4	1	\N	0	0	0	\N	\N	A Duck Saying "TeX"	f	f
449	350	202	2019-12-13 10:51:54.447905+00	[Latex lion](https://ctan.org/lion?lang=en)	2019-12-13 10:51:54.447905+00	7	4	3	\N	0	0	0	\N	\N	[Latex lion](https://ctan.org/lion?lang=en)	f	f
840	726	895	2020-02-19 14:25:48.167067+00	No, but you can make a function which uses [`getline`](https://en.cppreference.com/w/cpp/string/basic_string/getline) on the stream and applies the result pretty easily:\r\n\r\n    void Foo(const std::regex& r, std::istream& stream) {\r\n      std::string i;\r\n      std::smatch m;\r\n\r\n      while(std::getline(stream, i)) {\r\n        if(std::regex_match(i, m, r)) {\r\n            // Operate on matching line here\r\n        } else {\r\n            // Operate on non-matching line here\r\n        }\r\n      }\r\n    }\r\n\r\nI give an example using an `istringstream` [here](https://ideone.com/A65yAQ) but this could be substituted with any stream.	2020-02-19 14:25:48.167067+00	1	4	1	\N	0	0	0	\N	\N	No, but you can make a function which uses [`getline`](https://en.cppreference.com/w/cpp/string/basic_string/getline) on the stream and applies the result pretty easily:	f	f
842	645	1	2020-02-19 15:34:05.04618+00	We have recently revisited the site switcher, and it is now on all pages and looks like this:\r\n\r\n![Screenshot 2020-02-19 at 15.31.47.png](/image?hash=9344f80291b5828285d87d5bf07ed24c2e61e53e86fe90f09c0a2819dc9dd3ac)\r\n\r\n~~However we will also be rolling out icons for each community, like the C++ logo in the image above, which will eventually replace the colour blobs in the dropdown.~~\r\n\r\n**Update:**\r\n\r\nWe now use community images in the dropdown:\r\n\r\n![Screenshot 2020-02-19 at 16.56.25.png](/image?hash=250815fd45b4b09894ea8d1457c7cc88b4e31eaf59116032b2f6215ab5397053)	2020-02-19 16:58:04.954798+00	8	1	1	\N	0	0	0	\N	\N	We have recently revisited the site switcher, and it is now on all pages and looks like this:	f	f
719	645	2	2020-01-30 23:33:49.427644+00	Now implemented; see [@James's answer](/meta?q=645#a842)\r\n\r\n---\r\n\r\nThanks, we will be revisiting the navigation in the header shortly and will consider making it (or its replacement) universal	2020-02-19 15:36:06.373107+00	5	1	1	\N	0	0	0	\N	\N	Now implemented; see [@James's answer](/meta?q=645#a842)	f	f
822	244	702	2020-02-15 11:46:53.997658+00	The case for "Computing.TA" | As OP opined....\r\n\r\n> I'd like to [propose them as sites][1] but am unsure what level of granularity to propose:\r\n> \r\n> 1.    Computing (Superuser + ...)\r\n\r\nAnd this "answer" is simply to state my own hope that a "Computing.TA" site might spring to life sooner rather than later. **Why?** I have two main factors in mind:\r\n\r\n1. **Likelihood of meaningful activity** | TA is coming along nicely, but it's main *raison d'être*—getting good ("top", even!) answers to some question—is not yet quite being realized. In the SE ecosystem, "Superuser" (referenced by OP) occupies a place pretty near the top of that network:  \r\n&nbsp;  \r\n![__0-se-rank.jpg](/image?hash=c20a38825598383631bfa545618c486cf6e396fb833e797481ca3f4da84c6b9e)\r\n&nbsp;  \r\n  At time of typing, SU is ranked:\r\n  - **2nd** by "traffic" and "number of users"; and\r\n  - **3rd** by "Questions" (asked) and "Answers" (total).\r\n  \r\n2. **Breadth of scope** | All this activity relates to the range of "user" type questions relating to hardware, software, and network usage, while remaining software package, OS, etc. agnostic (for terms of "scope"). Although SU excludes "recommendations" (because SoftwareRecs and HardwareRecs exist as SE sites), my own hope and inclination would be to *include* recommendations on a new "Computing.TA" type site.\r\n\r\nAnd a third: I have questions I would like to ask in such a community that don't fit into the more granular programming or OS-specific sites (only \\*nix.TA available at the moment, of course) currently available in the TA framework.\r\n\r\nI post this here, rather than in the "[propose a new community][1]" Q&A, because I don't think of this as a *formal* proposal, but rather an opportunity to gauge others' sense of whether a new site at this level of breadth would make sense at this moment in TA's development. I'm thinking, too, of the question about [how TA might best be promoted][2]: my hunch is that a "Computing.TA" site would contribute to that effort. FWIW!\r\n\r\n[1]: https://topanswers.xyz/meta?q=211\r\n[2]: https://topanswers.xyz/meta?q=595	2020-02-15 12:35:17.326697+00	2	1	1	\N	0	0	0	\N	\N	The case for "Computing.TA" | As OP opined....	f	f
806	244	2	2020-02-11 15:26:30.958258+00	The precise detail of community scope will be up to each community, as it takes expertise to decide on all the edge cases, but broadly:\r\n\r\n**We don't want a general 'coding' SO-like mess on TopAnswers.**\r\n\r\n> Language X (tighter scope than anything on SE)\r\n\r\nThis sounds much more like what we will be aiming for, especially bearing in mind that [the top 4 tags on SO](https://stackoverflow.com/tags?tab=popular) each have well over a million questions. `javascript` alone is >10% the size of the whole of SO, so still *much bigger* than [any non-SO site on the SE network](https://stackexchange.com/sites#questions).\r\n\r\n> My initial question(s) might be Lua (for example). Should I consider getting some Lua folks involved and propose per-language sites (this would quickly run into scope overlap and other issues)\r\n\r\nScope overlap isn't a problem here or on SE — as long as each topic is coherent enough that experts can coalesce around it.\r\n\r\n---\r\n\r\n*An aside:*\r\n\r\nWe'd also like to help people contribute across communities here is ways that aren't so easy on SE — for example opt-out profile settings to show questions from related communities where appropriate. Not like HNQ where it is indiscriminate (though there may be some cope for that too in a more limited fashion), but where there is significant overlap of expertise between two communities. A theoretical example might be `javascript` and `typescript` (or it might not but hopefully you get what I mean — perhaps `SILE` and `TeX` are better examples).	2020-02-11 15:26:30.958258+00	7	1	1	\N	0	0	0	\N	\N	The precise detail of community scope will be up to each community, as it takes expertise to decide on all the edge cases, but broadly:	t	f
348	361	96	2019-12-06 11:42:37.611466+00	The language name definition used by the tech stack here is `tex` not `latex`.\r\n\r\nAlso for the tex site, that is the default language, so you don't have to specify it. To disable, use `none` as the language.	2019-12-06 11:42:37.611466+00	2	4	2	\N	0	0	0	\N	\N	The language name definition used by the tech stack here is `tex` not `latex`.	f	f
834	652	702	2020-02-17 09:15:58.893636+00	Logged on Github:[^f1] "[Enhance internal site navigation #29][g]"\r\n\r\n---\r\n\r\nLike OP, I "don't tend to use browser 'back' if there's a convenient in-page affordance". At present, especially with the tweaked topbar, there is no smooth "in-page affordance", so it strikes me there are three elements here:\r\n\r\n- provide clean, quick inner-community links ("in-page affordance") when in the context of a specific Q&A;\r\n- keep top-level nav between communities accessible (this seems already to be well under development with the upper-left logo/dropdown); and\r\n- ensure use of the browser's "back" function remains inuitive (e.g., takes you back to previous context, especially when the "previous" page was a Summary Page where `page_number > 1`).\r\n\r\nThere is probably an element of growing familiar with a new interface: the wee chat/Q&A icons in the upper-right side bar of summary and Q&A pages are very handy, but not somewhere I yet look to "naturally" for navigating around the site. Yet.\r\n\r\n[g]: https://github.com/topanswers/topanswers/issues/29\r\n\r\n[^f1]: I reckoned 10 stars was adequate response to suggest the presence of this matter on Github was warranted. Hope that was judged rightly!\r\n	2020-02-17 09:53:28.345063+00	10	1	1	\N	0	0	0	\N	\N	Logged on Github:[^f1](I reckoned 10 stars was adequate response to suggest the presence of this matter on Github was warranted. Hope that was judged rightly!) "[Enhance internal site navigation #29](https://github.com/topanswers/topanswers/issues/29)"	f	f
338	352	96	2019-12-06 08:24:40.715489+00	Currently there are no ways to filter the question list or sort it by anything except the default (last activity date).\r\n\r\nAnd searching.\r\n\r\nClearly other sort and filter methods will need to be a thing.\r\n\r\n1. Unanswered\r\n2. By tag\r\n\r\nSee also [this request on the main meta](https://topanswers.xyz/meta?q=369).	2019-12-06 13:48:00.156369+00	3	4	2	\N	0	0	0	\N	\N	Currently there are no ways to filter the question list or sort it by anything except the default (last activity date).	f	f
439	433	96	2019-12-12 16:28:14.452351+00	Yes I think it does make sense. In fact I [proposed the same thing](https://topanswers.xyz/meta?q=425#question) earlier today.	2019-12-12 16:28:14.452351+00	1	4	2	\N	0	0	0	\N	\N	Yes I think it does make sense. In fact I [proposed the same thing](https://topanswers.xyz/meta?q=425#question) earlier today.	f	f
792	433	2	2020-02-10 17:16:00.72911+00	@@@ answer 788	2020-02-10 17:16:00.72911+00	0	1	1	\N	0	0	0	\N	\N	@@@ answer 788	t	f
845	729	895	2020-02-20 20:08:08.423179+00	We should start by talking about [`std::match_results`](https://en.cppreference.com/w/cpp/regex/match_results) which is what `std::regex_match` or `std::regex_search` would store the function results into. Provided the regex suceeded it will contain [`std::match_results::size`](https://en.cppreference.com/w/cpp/regex/match_results/size) [`std::sub_match`](https://en.cppreference.com/w/cpp/regex/sub_match)s. Provided that the function generating the `std::match_results` succeeded there will be a 1-to-1 mapping from the captures in the regex to the `std::sub_match`s in the `std::match_results`. When indexing a `std::match_results`' `std::sub_match`s:\r\n\r\n * The `std::sub_match`s at indices less than 0 contain the portion of the matched string which precededs the first matched character of the entire regex\r\n * The `std::sub_match` at index 0 contains the portion of the string matched by the entire regex\r\n * The `std::sub_match`s greater than 0 and less than `std::match_results::size` contain the portion of the string matched by the regex's corresponding 1-based capture\r\n * The `std::sub_match`s greater than or equal to `std::match_results::size` contain the portion of the string which follows the last matched character of the entire regex\r\n\r\n We can use a [`std::regex_iterator`](https://en.cppreference.com/w/cpp/regex/regex_iterator). To obtain the `std::match_results`s from the 1st capture we could do:\r\n\r\n     const std::vector<std::cmatch> output = { std::cregex_iterator(std::cbegin(input), std::cend(input), r), std::cregex_iterator() };\r\n\r\nTo obtain the matched range from these `std::cmatch`s you can use the `position` method to find the offset and the `length` method to find the size of the match, simply provide these methods the index of the desired capture.\r\n\r\nSo for example the 1st captures offset in the 1st match could be found by doing: `ouput.front().position(1U)`\r\n\r\nThe length of this match could be found by doing: `output.front().length(1U)`\r\n\r\nThese could be added together to find the end of the range.\r\n\r\n[**Live Example**](https://ideone.com/x0Ekkt)\r\n	2020-02-20 20:08:08.423179+00	0	4	1	\N	0	0	0	\N	\N	We should start by talking about [`std::match_results`](https://en.cppreference.com/w/cpp/regex/match_results) which is what `std::regex_match` or `std::regex_search` would store the function results into. Provided the regex suceeded it will contain [`std::match_results::size`](https://en.cppreference.com/w/cpp/regex/match_results/size) [`std::sub_match`](https://en.cppreference.com/w/cpp/regex/sub_match)s. Provided that the function generating the `std::match_results` succeeded there will be a 1-to-1 mapping from the captures in the regex to the `std::sub_match`s in the `std::match_results`. When indexing a `std::match_results`' `std::sub_match`s:	f	f
233	276	179	2019-06-14 16:43:31+00	Why storing the data on a SAN? What's the point? All database performance is tied to Disk I/O and you are using 3 servers with only one device for the I/O behind them. That makes no sense... and unfortunately so common.\r\n\r\nI spend my life encountering poorly designed hardware platforms where people just try to design a large scale computer. All CPU power here, all disks there... hopefully there is not such a thing as remote RAM. And the saddest is they compensate the lack of efficiency of this design with huge servers that cost ten time more than they should. I saw $400k infra slower than a $1k laptop.\r\n\r\nA SQL server software is a very advanced piece of software, it is designed to take advantage of any bits of hardware, CPU cores, CPU cache, TLB, RAM, disk controllers, hard drive cache... They almost include all filesystem logic. They are developed on regular computer and benchmarked on high end systems. Therfore a SQL server must have its own disks. Installing them on a SAN is like "emulating" a computer, you lose all performance optimisations. SANs are for storing backups, immutable files, and files you just append data to (logs).\r\n\r\nDatacenter administrators tend to put all they can on SANs because this way they have only one pool of storage to manage, it's more easy than caring for storage on each server. It's a "I don't want to do my job" choice, and a very bad one, because then they have to deal with performance problems and all the company suffer from this. Just install software on the hardware it is designed for. Keep it simple. Care for I/O bandwidth, cache and context switch overhead, ressource jitter (happens when ressource is shared). You'll end up maintaining 1/10th of the devices for the same raw output power, save your ops team lot of headaches, gain performance that makes your end users happy and more productive, make your company a better place to work in, and save lot of energy (the planet will thank you).\r\n\r\nYou said in comments, you are considering to put SSD in your server. You will not recognize your setup with dedicated SSDs, compared to a SAN you'll get something like 500x improvement even with data and transaction log files on same drive. A state of the art SQL Server would have fast separate SSD for data and transaction log on different hardware controllers channels (most server motherboard have several). But compared to your current setup we are talking of sci-fi there. Just give SSD a try.\r\n	2019-12-04 14:22:12.193997+00	0	4	1	240597	0	0	0	2019-12-04 14:22:12.193997+00	\N	Why storing the data on a SAN? What's the point? All database performance is tied to Disk I/O and you are using 3 servers with only one device for the I/O behind them. That makes no sense... and unfortunately so common.	f	f
235	276	16	2019-06-13 12:16:08+00	This is far less often a disk issue, and far more often a networking issue. You know, the N in SAN?\r\n\r\nIf you go to your SAN team and start talking about the disks being slow, they're gonna show you a fancy graph with 0 millisecond latency on it and then point a stapler at you.\r\n\r\nInstead, ask them about the network path to the SAN. Get speeds, if it's multipathed, etc. Get numbers from them about the speeds you should be seeing. Ask if they have benchmarks from when the servers were set up.\r\n\r\nThen you can use [Crystal Disk Mark][1] or [diskpd][2] to validate those speeds. If they don't line up, again, it's most likely the networking.\r\n\r\nYou should also search your error log for messages that contain "FlushCache" and "saturation", because those can also be signs of network contention.\r\n\r\nOne thing you can do to avoid those things as a DBA is make sure that your maintenance and any other data-heavy tasks (like ETL) aren't going on at the same time. That can definitely put a lot of pressure on storage networking.\r\n\r\nYou may also want to check the answers here for more suggestions: [Slow checkpoint and 15 second I/O warnings on flash storage](https://dba.stackexchange.com/questions/233705/slow-checkpoint-and-15-second-i-o-warnings-on-flash-storage)\r\n\r\nI blogged about a similar topic here: [From The Server To The SAN](https://www.brentozar.com/archive/2019/01/from-the-server-to-the-san/)\r\n\r\n\r\n  [1]: https://crystalmark.info/en/software/crystaldiskmark/\r\n  [2]: https://gallery.technet.microsoft.com/DiskSpd-A-Robust-Storage-6ef84e62	2019-12-04 14:22:12.748019+00	2	4	1	240485	0	0	0	2019-12-04 14:22:12.748019+00	\N	This is far less often a disk issue, and far more often a networking issue. You know, the N in SAN?	f	f
232	276	178	2019-06-14 01:50:29+00	We have a similar setup and recently encountered these messages in the logs. We are using a DELL Compellent SAN. Here are some things to check when receiving these messages that helped us find a solution\r\n\r\n - Review your windows performance counters for your disks that the warning messages are pointing to, specifically:\r\n    - Disk avg. read time\r\n    - Disk avg. write time\r\n    - Disk read bytes/sec\r\n    - Disk write bytes/sec\r\n    - Disk Transfers/sec\r\n    - Avg. disk queue length\r\n - The above are averages. If you have many database files on one drive these averages can skew the result and mask a bottle neck on specific database files. Check out [this](https://www.sqlskills.com/blogs/paul/how-to-examine-io-subsystem-latencies-from-within-sql-server/) query from Paul S. Randal which returns average latency for each file from the dmv `sys.dm_io_virtual_file_stats`. In our case the average latency reported was acceptable, but underneath the covers we had many files with > 200 ms average latency.\r\n - Check the timings. Is there any pattern? Does it happen more frequently at certain a time in the night? If so check if any maintenance jobs are running at that time or any scheduled activity which may increase disk activity and expose a bottle neck in your IO subsystem.\r\n - Check the windows event viewer for errors. If your switch or SAN is being overloaded or not setup properly for your application you may find some messages in this log, and it is good to take this information to your SAN admin. In our case we were receiving iSCSI connection errors often throughout the day, hinting at the problem. \r\n - Review your SQL Server code. When you receive these messages you shouldn't immediately think it is an IO subsystem issue and pass it to your SAN admin. You need to do your part and review the database. Do you have really bad queries being run often churning through tons of data? Bad indexing? Excessive transaction log writes? You can use some open source queries to get a health check on your database, an example for checking how your query plan looks is [sp_blitzCache](https://www.brentozar.com/blitzcache/)\r\n - Don't ignore these. Today you may be receiving them a few times a day... then several months later when your workload increases and you forgot to monitor them they start to increase. Receiving lots of these messages can prevent SQL Server from accessing a certain file, and if it is **tempdb**, that is not good. In our case it got so bad that SQL Server shut itself down.\r\n\r\nOur solution was upgrading our switch to a SAN switch. Yes, these are all points to cover within SQL Server. What led us to finding out it was the switch was that we were receiving about 1500 iSCSI pdu disconnect errors in the Windows application event viewer on the SQL Server every day. That prompted the investigation by our SAN admins into the switch.\r\n\r\nImmediately after upgrading, the iSCSI errors were gone and average latency came down to around 50 ms for all the files, and that correlated to better performance in the application. With these points in mind hopefully you can find your solution.\r\n\r\n	2019-12-04 14:22:12.008133+00	0	4	1	240548	0	0	0	2019-12-04 14:22:12.008133+00	\N	We have a similar setup and recently encountered these messages in the logs. We are using a DELL Compellent SAN. Here are some things to check when receiving these messages that helped us find a solution	f	f
234	276	177	2019-11-08 15:29:40+00	Ok, for anyone interested, \r\n\r\nWe solved issue in Question couple months ago simply by installing directly attached SSD drives into each of 3 servers, \r\nand moving DB data and log files from SAN to those SSD drives\r\n\r\nHere summary on what I did to research on this issue (using recommendations from all the posts it this question), before we decided to install SSD drives:\r\n\r\n\r\n    1) started collecting PerfMon counters for following drives at all 3 servers:\r\n\r\n`Disk F:` is logical disk based on SAN, contains MDF data files  \r\n`Disk I:` is logical disk based on SAN, contains LDF log files  \r\n`Disk T:` is directly attached SSD, dedicated solely to tempDB\r\n\r\nPicture below is average values collected for 2 weeks period\r\n\r\n\r\n[![Disk Performance Counters][1]][1]\r\n\r\n\r\n`Disk I: (LDF)` has such a small IO and Latency is very low, so Disk I: can be ignored  \r\nYou can see that `Disk T: (TempDB)` has bigger IO compared to `Disk F: (MDF)`, and it has much better Latency at the same time - 0 ms\r\n\r\nObviously something is wrong with Disk F: where data files reside, it has high Latency and Avg Disk Write Queue, despite low IO\r\n\r\n\r\n    2) Checked Latency for individual databases using query from this website\r\nhttps://www.brentozar.com/blitz/slow-storage-reads-writes/\r\n\r\nFew active databases on Primary server had 150-250 ms read latency and 150-450 ms write latency  \r\nWhat is interesting, master and msdb database files had read latency up to 90 ms which is suspicious given the small size of their data and low IO - another indication something is wrong with SAN\r\n\r\n\r\n    3) There were no specific timings \r\nDuring which "SQL Server has encountered occurences..." messages showed up  \r\nThere were no maintenance or disk heavy ETL running when those messages were logged\r\n\r\n\r\n    4) Windows Event Viewer \r\nDid not show any other entries that would hint the problem, except "SQL Server has encountered occurences..."\r\n\r\n\r\n    5) Started checking top 10 queries \r\nFrom sp_BlitzCache (cpu, reads, etc.), and omptimizing where possible  \r\nNo super IO heavy queries that would churn tons of data and impacting storage heavily, though   \r\nIndexing in databases is OK, I maintain it\r\n\r\n\r\n    6) We do not have SAN team\r\n\r\nWe only have 1 sysadmin who helps on occassion  \r\nNetwork path to SAN - it is multipathed, each of 3 servers have 2 network cables leading to switches and then to SAN, and its supposed to be 1 Gigabyte / sec\r\n\r\n\r\n    7) There were no CrystalDiskMark results \r\nOr any other benchmark test results from when the servers were setup up, so I do not know what the speeds *should* be, \r\nand its not possible to benchmark at this point to see what the speeds currently are, as it would have impacted Production\r\n\r\n\r\n\r\n\r\n    8) Setup Extended Events session on checkpoint event for database in question\r\nXE session helped to discover that during "SQL Server has encountered occurences..." messages, checkpoint happened really slow (up to 90 seconds)\r\n\r\n\r\n\r\n    9) SQL Server Error Log \r\nContained "FlushCache" "Saturation" entries   \r\nThese supposed to show up when checkpoint time for given database exceeds recovery interval settings\r\n\r\nDetails showed that amount of data that checkpoint is trying to flush is small and it is taking long time to complete, and the overall speed is about 0.25 MB / sec... weird\r\n\r\n\r\n\r\n    10) Finally, this picture shows storage troubleshooting chart:\r\n\r\n\r\n[![Slow Disk IO Troubleshooting Steps][2]][2]\r\n\r\n\r\nIt appears we simply have a "Hardware Problem:- Work with system admin/hardware vendor to fix any misconfiguration of SAN, old/faulty drivers, controllers, firmware, etc."\r\n\r\nIn another question "Slow checkpoint..." https://dba.stackexchange.com/questions/233705/slow-checkpoint-and-15-second-i-o-warnings-on-flash-storage\r\nSean had very nice list of what items have to be checked at hardware and software level to troubleshoot\r\n\r\nOur sysadmin could not check all things from the list, so we simply choose to throw some hardware at this issue - it was not expensive at all\r\n\r\n\r\n    Summary: \r\n\r\nMoving to local SSD resolved not only storage performance issues but also data safety that I was concerned about \r\n(if SAN fails, all 3 servers lose their data at the same time)\r\n\r\nNow each server has local copy of DB data, and full/diff/log backups are done to the mentioned SAN   \r\nNo more "SQL Server has encountered occurences..." messages in Windows Event Viewer logs, and performance of backups, integrity checks, index rebuilds, queries etc. has increased significantly\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/2MQei.png\r\n  [2]: https://i.stack.imgur.com/SMlFE.png	2019-12-04 14:22:12.461055+00	2	4	1	252843	0	0	0	2019-12-04 14:22:12.461055+00	\N	Ok, for anyone interested,	f	f
421	397	96	2019-12-11 19:21:26.431069+00	**Adendum:** This also needs to happen for post chat room transcripts. They show the name of the post they are connected with, but it is not linked and there is no obvious way to get back to the post.	2019-12-11 19:21:26.431069+00	3	4	2	\N	0	0	0	\N	\N	**Adendum:** This also needs to happen for post chat room transcripts. They show the name of the post they are connected with, but it is not linked and there is no obvious way to get back to the post.	f	f
399	397	96	2019-12-10 07:34:28.276946+00	This question got jinxed by [this issue](https://topanswers.xyz/meta?q=398). I wrote it the other day, got locked out by rate limits, and left the tab open to post later, then had to leave for the day. It looks like the link back got implemented sometime between when I noticed and wrote this post and now.	2019-12-10 07:34:28.276946+00	2	4	2	\N	0	0	0	\N	\N	This question got jinxed by [this issue](https://topanswers.xyz/meta?q=398). I wrote it the other day, got locked out by rate limits, and left the tab open to post later, then had to leave for the day. It looks like the link back got implemented sometime between when I noticed and wrote this post and now.	f	f
604	557	751	2019-05-02 16:07:35+00	I can give you a partial answer that explains why you are seeing the performance difference - though that still leaves some open questions (such as **can** SQL Server produce the more optimal plan without introducing an intermediate table expression that projects the expression as a column?)\r\n\r\n___\r\n\r\nThe difference is that in the fast plan the work needed to parse the JSON array elements and create the Geography is done 4 times (once for each row emitted from the `openjson` function) - whereas it is done more than 100,000 *times* that in the slow plan. \r\n\r\nIn the fast plan...\r\n\r\n    geography::Point(\r\n                    convert(float,json_value(value,'$[0]'))\r\n                   ,convert(float,json_value(value,'$[1]'))\r\n                   ,4326)\r\n\r\nIs assigned to `Expr1000` in the compute scalar to the left of the `openjson` function. This corresponds to `geo` in your derived table definition. \r\n\r\n[![enter image description here][1]][1]\r\n\r\nIn the fast plan the filter and stream aggregate reference `Expr1000`. In the slow plan they reference the full underlying expression.\r\n\r\n**Stream aggregate properties**\r\n\r\n[![enter image description here][2]][2]\r\n\r\nThe filter is executed 116,995 times with each execution requiring an expression evaluation. The stream aggregate has 110,520 rows flowing into it for aggregation and creates three separate aggregates using this expression. `110,520 * 3 + 116,995 = 448,555`. Even if each individual evaluation takes 18 microseconds this adds up to 8 seconds additional time for the query as a whole. \r\n\r\nYou can see the effect of this in the actual time statistics in the plan XML (annotated in red below from the slow plan and blue for the fast plan - times are in ms)\r\n\r\n[![enter image description here][3]][3]\r\n\r\nThe stream aggregate has an elapsed time 6.209 seconds greater than its immediate child. And the bulk of the child time was taken up by the filter. This corresponds to the extra expression evaluations.\r\n\r\n___\r\n\r\nBy the way.... In general it [is not a sure thing][4] that underlying expressions with labels like `Expr1000` are only calculated once and not re-evaluated but clearly in this case from the execution timing discrepancy this happens here.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/DePyW.png\r\n  [2]: https://i.stack.imgur.com/KuaxF.png\r\n  [3]: https://i.stack.imgur.com/5CSpl.png\r\n  [4]: https://dba.stackexchange.com/a/30947/3690	2020-01-12 14:06:47.372365+00	4	4	1	237231	0	0	0	2020-01-12 14:06:47.372365+00	\N	I can give you a partial answer that explains why you are seeing the performance difference - though that still leaves some open questions (such as **can** SQL Server produce the more optimal plan without introducing an intermediate table expression that projects the expression as a column?)	f	f
208	243	115	2019-12-04 03:15:22.970422+00	This is related to [Paul's answer](https://topanswers.xyz/meta?q=243#a207) suggesting reactions.\r\n\r\nAn idea that has come up in Codidact's discussions (inconclusively so far) is having reactions to *supplement* votes rather than replace them.  Reactions, unlike votes, are public (attributed).  They serve two main purposes:\r\n\r\n- Highlighting an under-valued, good answer -- this answer doesn't have a high score (maybe it was missed, maybe it was late, whatever) but look, Jon Skeet gave it a thumbs-up!\r\n\r\n- Providing warnings about things that are dangerous -- the crowd upvoted this because it sounded right, but it has caution marks from three people and I recognize two of them as knowing their stuff.\r\n\r\nI agree with Paul that reactions shouldn't contribute to score, and also his concern that people might use them *instead* of votes/stars, but if we can provide the right guidance in the UI I think it's worth allowing this extra signal, or at least giving it a try and seeing what happens.  (As with many other things, individual communities should be able to turn it on or off.)\r\n	2019-12-04 03:21:47.230857+00	8	4	1	\N	0	0	0	\N	\N	This is related to [Paul's answer](https://topanswers.xyz/meta?q=243#a207) suggesting reactions.	f	f
185	243	2	2019-12-02 11:57:29.181147+00	> I realize this will be controversial\r\n\r\nNot controversial with me, as I'm on record as being a fan of downvotes on SE:\r\n\r\n* https://dba.meta.stackexchange.com/a/423/1396\r\n* https://hermeneutics.meta.stackexchange.com/questions/1096/we-need-to-downvote-more#comment3585_1096\r\n\r\nHowever the points you raise can be solved another way, by deleting dangerous and very bad content. Note that I agree that we *do* need to solve this one way or another.\r\n\r\nWe haven't quite worked through the nuances of how flagging/deletion will work in its first iteration, but when we do I'll update this post.\r\n\r\n---\r\n\r\nUpdate: now that we have the first tools in place to deal with dangerous or very bad answers, I'm marking this as "won't-fix" — with the proviso that we have the option of changing course later if necessary. Adding downvotes will not be an impossible task later because the database has been designed with the assumption that we might need them.	2019-12-24 20:24:03.92359+00	11	1	1	\N	0	0	0	\N	\N	> I realize this will be controversial	f	f
629	243	772	2020-01-16 21:34:18.079741+00	The SE model was to make down votes cost reputation. I think instead, they should take a little extra work. My suggestion would be more like a down vote should be accompanied by a reason, like close votes on SE. If an answer collects enough "down votes" of the same type, then a post notice should get added to the top of the answer explaining what happened.\r\n\r\nOff the top of my head, the "down vote" reasons could be something like:\r\n\r\n-- This answer does not answer the question\r\n\r\n-- This answer is missing key details\r\n\r\n-- This answer does not work (this should open another dialog that makes the user explain why it does not work)\r\n\r\n-- This answer is dangerous (this should probably open another dialog also)\r\n\r\nIf the person who left the answer comes back and fixes the problems and the fixes are verified by users, then everything should get reset and it should not depend on the people who "down voted" to coming back to change their votes.\r\n\r\n\r\n	2020-01-16 21:34:18.079741+00	3	4	3	\N	0	0	0	\N	\N	The SE model was to make down votes cost reputation. I think instead, they should take a little extra work. My suggestion would be more like a down vote should be accompanied by a reason, like close votes on SE. If an answer collects enough "down votes" of the same type, then a post notice should get added to the top of the answer explaining what happened.	f	f
207	243	12	2019-12-04 02:09:45.869201+00	I would not like to see downvotes implemented as they are on Stack Overflow Inc. sites.\r\n\r\nThat said, there is an argument to be made for more feedback options than simply stars, or lack thereof. I haven't seen that argument made convincingly yet.\r\n\r\nNevertheless, if needed, I am quite tempted by the idea of emoji feedback:\r\n\r\n|emoji options|description|\r\n|:--:|:--:|\r\n| :star: :) |good|\r\n| :\\|  | neutral |\r\n| :( :cry: :angry: ❌ |not good|\r\n\r\nFor example, a good answer might be labelled:\r\n\r\n> :star: x8\r\n\r\n...a reasonably good answer:\r\n\r\n> :star: x2 :| x4\r\n\r\n...and a poor one:\r\n\r\n> :( x5 :| x1\r\n\r\n---\r\n\r\nI think only :star: (= :)) should award +1 score. The other emoji should affect display ranking, but not user score.\r\n\r\nMy main reservation is that adding new "lazy feedback" options will in practice dissuade people from doing what we really want them to do: give written feedback, or edit to improve.\r\n\r\nOutright dangerous posts should be flagged and deleted. We don't want those. And to be clear, at least for the *Databases* site, I would prefer to see *mediocre* content removed over time as well.	2019-12-05 09:40:43.806811+00	9	4	2	\N	0	0	0	\N	\N	I would not like to see downvotes implemented as they are on Stack Overflow Inc. sites.	f	f
617	243	761	2020-01-14 22:15:34.166897+00	The problem is in the concept of a **simplicistic, single, rating**.\r\n\r\nI have a strong feeling that a large part of the problems not only of Stack Exchange but of the current web altogether derive from the squeezing of the complexity of the thought into a single vote, and to the hyper-magnification of the importance of this vote.\r\n\r\n---\r\n\r\nOn **Stack Exchange** for example, on the surface it would seem that you should upvote every post that you liked a lot, and downvote every one that you didn't.  \r\nBut is it really how it goes? How many posts with hundreds of downvotes are there?  \r\nPretty much none apart from those of the faceless company spokepersons!  \r\nThat's because if you see 3 downvotes to a poor answer you think "the poor guy has had it enough". And you're aware of the effect of the downvotes to the reputation, and of what that affects in turn.  \r\nAs for the upvotes, did it ever feel unfair that the first guy who sent a stupid question about the news of the day got 5k points straightaway, when you barely made a few hundreds in years of diligent contribution?  \r\nAnd have you ever refrained from upvoting a good post after seeing it has already 200 upvotes?  \r\nOf course you have, otherwise there would be many posts in the tens of thousands of votes!\r\n\r\nSo what the people is actually doing is, very roughly, **giving a score** to the posts.\r\n\r\nBut even though the "collective mind" unwittingly goes in the direction of scores, with the current "indirect" system that are inevitably frequent unfair extremes, with people gaining immense sudden gains in reputation and, much worse, people losing everything and having to scramble again with the insane limitations of a beginner account.\r\n\r\n---\r\n\r\nSo, rather than going roundabout with it, do the real thing and **let people give scores** to the posts!  \r\n+10 to -10, or +20 to -20 or whatever, then you display the average and the number of voters (and the distribution, or whatever you want) and if you want to base a reputation system on it you base it on the *real data*, giving the proper weight and separation to the average score and to the number of people who substantiated it.  \r\n(and to their agreement? Whatever you want, with the real data!)\r\n\r\nAnd you can have "downvotes" without all their moral and psychological concerns.\r\n\r\n---\r\n\r\nAnd, going back to the beginning, do we really need **one, catch-all** rating?\r\n\r\nWhat if a question is dull but it gave rise to great answers?  \r\nWhat if an answer is very useful but, alas, it doesn't answer well the question it refers to?  \r\nWhat if a guy gave it all to help (but didn't answer)?\r\n\r\nIn short, I'm not proposing to make available a hundred votable metrics (which might lead to a futile voting-fatigue, among other things), but to think about the ones, if any, that the system needs or that would benefit it enough, and allow those who feel like to use them.\r\n\r\nSpecifically and in their most suitable format, instead of jamming everything in a single, trending-ready, "Like" (or dislike).	2020-01-14 22:15:34.166897+00	4	6	1	\N	0	0	0	\N	\N	The problem is in the concept of a **simplicistic, single, rating**.	f	f
4	14	12	2019-11-09 09:21:21.176195+00	A five-minute time limit on chat message edits seems reasonable to me.\r\n\r\nThe two-minute limit on SE chat is a bit too short.\r\n\r\nAllowing edits on all messages visible in the current window sounds like an attractive alternative, but it might be misused.	2019-11-09 09:21:21.176195+00	7	4	1	\N	0	0	0	\N	\N	A five-minute time limit on chat message edits seems reasonable to me.	f	f
47	14	2	2019-11-21 17:54:34.415404+00	We have implemented this with a 5-minute limit per [Paul's suggestion](https://topanswers.xyz/meta?q=14#a4):\r\n\r\n> A five-minute time limit on chat message edits seems reasonable to me.\r\n\r\nClick the new button that appears to the right of your message on hover: \r\n\r\n!['edit chat' button](/image?hash=7f0efc8afbbb404eea34b3d5cd8a85c81d04586d55e85e665821608ff75bf7e9)\r\n\r\nAlternatively, the 'up' arrow is a shortcut to editing your latest chat message (this only works if the chat input box is empty).	2019-11-22 12:41:01.175018+00	3	1	1	\N	0	0	0	\N	\N	We have implemented this with a 5-minute limit per [Paul's suggestion](https://topanswers.xyz/meta?q=14#a4):	f	f
141	198	60	2012-12-24 09:40:19+00	The lack of access to temporary tables in other sessions is not a matter of permissions, it's a technical limitation of the design. A PostgreSQL backend *can't* access temporary tables of another backend because none of the usual housekeeping to allow concurrent access is done for temporary tables.\r\n\r\nIn 9.2 you will want to use an `UNLOGGED` table instead; this can be visible from other sessions, but retains most of the performance benefits of a temporary table.	2019-11-29 17:39:31.18164+00	2	4	1	30988	0	0	0	2019-11-29 17:39:31.18164+00	\N	The lack of access to temporary tables in other sessions is not a matter of permissions, it's a technical limitation of the design. A PostgreSQL backend *can't* access temporary tables of another backend because none of the usual housekeeping to allow concurrent access is done for temporary tables.	f	f
142	198	2	2011-08-31 09:34:01+00	The short answer is "No". [Temporary tables](http://www.postgresql.org/docs/current/static/sql-createtable.html#AEN62073) in \r\nother sessions [are invisible](http://postgresql.1045698.n5.nabble.com/GENERAL-Temporary-table-visibility-td1856035.html) by design. It makes no difference if two sessions have the same user. Even:\r\n\r\n> The autovacuum daemon cannot access and therefore cannot vacuum or\r\n> analyze temporary tables	2019-11-29 17:39:31.422184+00	0	4	1	5237	0	0	0	2019-11-29 17:39:31.422184+00	\N	The short answer is "No". [Temporary tables](http://www.postgresql.org/docs/current/static/sql-createtable.html#AEN62073) in	f	f
661	400	234	2020-01-23 18:22:00.665345+00	We feel that information is a basic need and should not be controlled by a company with commercial interests. So this site is an alternative to similar sites which are for profit.	2020-01-23 18:26:50.185665+00	2	4	3	\N	0	0	0	\N	\N	We feel that information is a basic need and should not be controlled by a company with commercial interests. So this site is an alternative to similar sites which are for profit.	f	f
400	400	167	2019-12-10 13:15:15.007877+00	Some suggestions:\r\n\r\n>We are building a friendly community for TeX questions and answers running on the non-profit site topanswers.xyz/tex \r\n\r\nor a bit longer\r\n\r\n> For all of you who like to ask or answer TeX related questions, there is a new question and answer site: TopAnswers.xyz/tex. The platform itself is open source and developed with the community in mind and not for-profit. The TeX community is still small, but growing and would be more than happy to see new users. So if this sounds interesting to you, just drop by and have a look yourself.\r\n\r\n(the last text block is a suggested fragment for the next Duckboat, a regular column by @CarLaTeX in the journal of the TeX Users Group, https://tug.org/TUGboat/)	2020-01-23 18:02:01.765017+00	6	4	3	\N	0	0	0	\N	\N	Some suggestions:	f	f
443	435	14	2018-11-21 12:59:09+00	Assuming you're using the command `ALTER DATABASE [YourDatabaseName] SET QUERY_STORE CLEAR` to "clean the query store," I decided to see what queries are run behind the scenes when that command executes.  \r\n\r\nTo that end, I set up a basic Extended Events session by using the "Query Detail Tracking" XE template that's included with SSMS.  This captures several events that should cover most queries running on a system:\r\n\r\n - error_reported\r\n - module_end\r\n - rpc_completed\r\n - sp_statement_completed\r\n - sql_batch_completed\r\n - sql_statement_completed\r\n\r\nBy default it excludes queries running in the context of the system databases (`database_id > 4`), which is fine for this test since query store lives within the user database where it's enabled.\r\n\r\nAfter starting that XE session, I ran the `ALTER DATABASE...SET QUERY_STORE CLEAR` command, and the following queries were picked up by the session:\r\n\r\n    TRUNCATE table sys.plan_persist_runtime_stats;\r\n    TRUNCATE table sys.plan_persist_runtime_stats_interval;\r\n    TRUNCATE table sys.plan_persist_plan;\r\n    TRUNCATE table sys.plan_persist_query;\r\n    TRUNCATE table sys.plan_persist_query_text;\r\n\r\nAdding the optional "ALL" parameter after "CLEAR" ran this additional statement:\r\n\r\n    TRUNCATE table sys.plan_persist_context_settings;\r\n\r\nYou may notice that these are the base tables for the [Query Store catalog views][2].\r\n\r\n`TRUNCATE TABLE` resets identity values:\r\n\r\n> If the table contains an identity column, the counter for that column is reset to the seed value defined for the column. If no seed was defined, the default value 1 is used. To retain the identity counter, use DELETE instead.\r\n\r\nSo this behavior is "by design" - you can't depend on the identity values in the query store tables across "query store clear" operations.\r\n\r\nWhat problem are you solving by clearing the query store?  Perhaps we could suggest a different option if you provide that context in a new question.\r\n\r\nSome potential options for removing *some* queries without truncating the tables are found here:\r\n\r\n[Best Practice with the Query Store - Keep the most relevant data in Query Store][1]\r\n\r\n - Configure time-based policy to activate auto-cleanup\r\n - Activate size-based cleanup policy\r\n - Configure Query Capture Mode to Auto\r\n\r\nThe first two options cause rows in the Query Store to be deleted by id based on their age and / or how expensive the queries were.\r\n\r\nThe second option limits the amount of data being added to the Query Store in the first place.\r\n\r\n[1]: https://docs.microsoft.com/en-us/sql/relational-databases/performance/best-practice-with-the-query-store?view=sql-server-2017#keep-the-most-relevant-data-in-query-store\r\n[2]: https://docs.microsoft.com/en-us/sql/relational-databases/system-catalog-views/query-store-catalog-views-transact-sql?view=sql-server-2017	2019-12-13 09:47:50.118358+00	2	4	1	223093	0	0	0	2019-12-13 09:47:50.118358+00	\N	Assuming you're using the command `ALTER DATABASE [YourDatabaseName] SET QUERY_STORE CLEAR` to "clean the query store," I decided to see what queries are run behind the scenes when that command executes.	f	f
781	398	2	2020-02-10 15:48:23.479855+00	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/18) so work can begin, and will update the post here when it is complete.	2020-02-10 15:48:23.479855+00	0	1	1	\N	0	0	0	\N	\N	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/18) so work can begin, and will update the post here when it is complete.	t	f
786	417	2	2020-02-10 17:04:45.306145+00	Current thinking is to limit height by default (without scroll), but add a 'show more' link to see the full contents.\r\n\r\nHow does that sound?\r\n\r\n@@@ answer 563	2020-02-10 17:30:06.759568+00	0	1	1	\N	0	0	0	\N	\N	Current thinking is to limit height by default (without scroll), but add a 'show more' link to see the full contents.	t	f
63	89	751	2012-04-11 22:22:44+00	**Contents:**\r\n\r\n[toc]\r\n\r\n# Caveat\r\n\r\nThis answer discusses "classic" table variables introduced in SQL Server 2000. SQL Server 2014 in memory OLTP introduces Memory-Optimized Table Types. Table variable instances of those are different in many respects to the ones discussed below! ([more details][2]). \r\n\r\n# Storage Location\r\n\r\nNo difference. Both are stored in `tempdb`. \r\n\r\nI've seen it suggested that for table variables this is not always the case but this can be verified from the below\r\n\r\n    DECLARE @T TABLE(X INT)\r\n\r\n    INSERT INTO @T VALUES(1),(2)\r\n\r\n    SELECT sys.fn_PhysLocFormatter(%%physloc%%) AS [File:Page:Slot]\r\n    FROM @T\r\n\r\nExample Results (showing location in `tempdb` the 2 rows are stored)\r\n    \r\n    File:Page:Slot\r\n    ----------------\r\n    (1:148:0)\r\n    (1:148:1)\r\n\r\n\r\n# Logical Location\r\n\r\n`@table_variables` behave more as though they were part of the current database than `#temp` tables do. For table variables (since 2005) column collations if not specified explicitly will be that of the current database whereas for `#temp` tables it will use the default collation of `tempdb` ([More details][3]). Additionally User-defined data types and XML collections must be in tempdb to use for `#temp` tables but table variables can use them from the current database ([Source][4]).\r\n\r\nSQL Server 2012 introduces contained databases. [the behavior of temporary tables in these differs][5] (h/t Aaron)\r\n\r\n> In a contained database temporary table data is collated in the collation of the contained database.\r\n> \r\n>  - All metadata associated with temporary tables (for example, table and column names, indexes, and so on) will be in the catalog collation.\r\n>  - Named constraints may not be used in temporary tables.\r\n>  - Temporary tables may not refer to user-defined types, XML schema collections, or user-defined functions.\r\n\r\n# Visibility to different scopes\r\n\r\n`@table_variables` can only be accessed within the batch and scope in which they are declared. `#temp_tables` are accessible within child batches (nested triggers, procedure, `exec` calls). `#temp_tables` created at the outer scope (`@@NESTLEVEL=0`) can span batches too as they persist until the session ends. Neither type of object can be created in a child batch and accessed in the calling scope however as discussed next (global `##temp` tables *can* be though).\r\n\r\n# Lifetime\r\n\r\n`@table_variables` are created implicitly when a batch containing a `DECLARE @.. TABLE` statement is executed (before any user code in that batch runs) and are dropped implicitly at the end.\r\n\r\nAlthough the parser will not allow you to try and use the table variable before the `DECLARE` statement the implicit creation can be seen below.\r\n\r\n    IF (1 = 0)\r\n    BEGIN\r\n    DECLARE @T TABLE(X INT)\r\n    END\r\n\r\n    --Works fine\r\n    SELECT *\r\n    FROM @T\r\n\r\n`#temp_tables` are created explicitly when the TSQL `CREATE TABLE` statement is encountered and can be dropped explicitly with `DROP TABLE` or will be dropped implicitly when the batch ends (if created in a child batch with `@@NESTLEVEL > 0`) or when the session ends otherwise.\r\n\r\nNB: Within stored routines both types of object [can be cached][6] rather than repeatedly creating and dropping new tables. There are restrictions on when this caching can occur however that are possible to violate for `#temp_tables` but which the restrictions on `@table_variables` prevent anyway. The maintenance overhead for cached `#temp` tables is *slightly* greater than for table variables [as illustrated here][7].\r\n\r\n# Object Metadata\r\n\r\nThis is essentially the same for both types of object. It is stored in the system base tables in `tempdb`. It is more straightforward to see for a `#temp` table however as  `OBJECT_ID('tempdb..#T')` can be used to key into the system tables and the internally generated name is more closely correlated with the name defined in the `CREATE TABLE` statement. For table variables the `object_id` function does not work and the internal name is entirely system generated with no relationship to the variable name. The below demonstrates the metadata is still there however by keying in on a (hopefully unique) column name. For tables without unique column names the object_id can be determined using [`DBCC PAGE`][8] as long as they are not empty.\r\n\r\n\t/*Declare a table variable with some unusual options.*/\r\n\tDECLARE @T TABLE\r\n\t(\r\n\t[dba.se] INT IDENTITY PRIMARY KEY NONCLUSTERED,\r\n\tA INT CHECK (A > 0),\r\n\tB INT DEFAULT 1,\r\n\tInRowFiller char(1000) DEFAULT REPLICATE('A',1000),\r\n\tOffRowFiller varchar(8000) DEFAULT REPLICATE('B',8000),\r\n\tLOBFiller varchar(max) DEFAULT REPLICATE(cast('C' as varchar(max)),10000),\r\n\tUNIQUE CLUSTERED (A,B) \r\n\t\tWITH (FILLFACTOR = 80, \r\n\t\t\t IGNORE_DUP_KEY = ON, \r\n\t\t\t DATA_COMPRESSION = PAGE, \r\n\t\t\t ALLOW_ROW_LOCKS=ON, \r\n\t\t\t ALLOW_PAGE_LOCKS=ON)\r\n\t)\r\n\r\n\tINSERT INTO @T (A)\r\n\tVALUES (1),(1),(2),(3),(4),(5),(6),(7),(8),(9),(10),(11),(12),(13)\r\n\r\n\tSELECT t.object_id,\r\n\t\t   t.name,\r\n\t\t   p.rows,\r\n\t\t   a.type_desc,\r\n\t\t   a.total_pages,\r\n\t\t   a.used_pages,\r\n\t\t   a.data_pages,\r\n\t\t   p.data_compression_desc\r\n\tFROM   tempdb.sys.partitions AS p\r\n\t\t   INNER JOIN tempdb.sys.system_internals_allocation_units AS a\r\n\t\t\t ON p.hobt_id = a.container_id\r\n\t\t   INNER JOIN tempdb.sys.tables AS t\r\n\t\t\t ON t.object_id = p.object_id\r\n\t\t   INNER JOIN tempdb.sys.columns AS c\r\n\t\t\t ON c.object_id = p.object_id\r\n\tWHERE  c.name = 'dba.se'\r\n\r\nOutput\r\n\r\n    Duplicate key was ignored.\r\n\r\n     +-----------+-----------+------+-------------------+-------------+------------+------------+-----------------------+\r\n    | object_id |   name    | rows |     type_desc     | total_pages | used_pages | data_pages | data_compression_desc |\r\n    +-----------+-----------+------+-------------------+-------------+------------+------------+-----------------------+\r\n    | 574625090 | #22401542 |   13 | IN_ROW_DATA       |           2 |          2 |          1 | PAGE                  |\r\n    | 574625090 | #22401542 |   13 | LOB_DATA          |          24 |         19 |          0 | PAGE                  |\r\n    | 574625090 | #22401542 |   13 | ROW_OVERFLOW_DATA |          16 |         14 |          0 | PAGE                  |\r\n    | 574625090 | #22401542 |   13 | IN_ROW_DATA       |           2 |          2 |          1 | NONE                  |\r\n    +-----------+-----------+------+-------------------+-------------+------------+------------+-----------------------+\r\n\r\n# Transactions\r\n\r\nOperations on `@table_variables` are carried out as system transactions, independent of any outer user transaction, whereas the equivalent `#temp` table operations would be carried out as part of the user transaction itself. For this reason a `ROLLBACK` command will affect a `#temp` table but leave the `@table_variable` untouched.\r\n\r\n    DECLARE @T TABLE(X INT)\r\n    CREATE TABLE #T(X INT)\r\n\r\n    BEGIN TRAN\r\n\r\n    INSERT #T\r\n    OUTPUT INSERTED.X INTO @T\r\n    VALUES(1),(2),(3)\r\n\r\n    /*Both have 3 rows*/\r\n    SELECT * FROM #T\r\n    SELECT * FROM @T\r\n\r\n    ROLLBACK\r\n\r\n    /*Only table variable now has rows*/\r\n    SELECT * FROM #T\r\n    SELECT * FROM @T\r\n    DROP TABLE #T\r\n\r\n# Logging\r\n\r\nBoth generate log records to the `tempdb` transaction log. A common misconception is that this is not the case for table variables so a script demonstrating this is below, it declares a table variable, adds a couple of rows then updates them and deletes them. \r\n\r\nBecause the table variable is created and dropped implicitly at the start and the end of the batch it is necessary to use multiple batches in order to see the full logging. \r\n\r\n    USE tempdb;\r\n    \r\n    /*\r\n    Don't run this on a busy server.\r\n    Ideally should be no concurrent activity at all\r\n    */\r\n    CHECKPOINT;\r\n\r\n    GO\r\n\r\n    /*\r\n    The 2nd column is binary to allow easier correlation with log output shown later*/\r\n    DECLARE @T TABLE ([C71ACF0B-47E9-4CAD-9A1E-0C687A8F9CF3] INT, B BINARY(10))\r\n    \r\n    INSERT INTO @T\r\n    VALUES (1, 0x41414141414141414141), \r\n           (2, 0x41414141414141414141)\r\n    \r\n    UPDATE @T\r\n    SET    B = 0x42424242424242424242\r\n    \r\n    DELETE FROM @T\r\n    \r\n    /*Put allocation_unit_id into CONTEXT_INFO to access in next batch*/\r\n    DECLARE @allocId BIGINT, @Context_Info VARBINARY(128)\r\n    \r\n    SELECT @Context_Info = allocation_unit_id,\r\n           @allocId = a.allocation_unit_id \r\n    FROM   sys.system_internals_allocation_units a\r\n           INNER JOIN sys.partitions p\r\n             ON p.hobt_id = a.container_id\r\n           INNER JOIN sys.columns c\r\n             ON c.object_id = p.object_id\r\n    WHERE  ( c.name = 'C71ACF0B-47E9-4CAD-9A1E-0C687A8F9CF3' )\r\n    \r\n    SET CONTEXT_INFO @Context_Info\r\n    \r\n    /*Check log for records related to modifications of table variable itself*/\r\n    SELECT Operation,\r\n           Context,\r\n           AllocUnitName,\r\n           [RowLog Contents 0],\r\n           [Log Record Length]\r\n    FROM   fn_dblog(NULL, NULL)\r\n    WHERE  AllocUnitId = @allocId\r\n    \r\n    GO\r\n    \r\n    /*Check total log usage including updates against system tables*/\r\n    DECLARE @allocId BIGINT = CAST(CONTEXT_INFO() AS BINARY(8));\r\n    \r\n\tWITH T\r\n\t\t AS (SELECT Operation,\r\n\t\t\t\t\tContext,\r\n\t\t\t\t\tCASE\r\n\t\t\t\t\t  WHEN AllocUnitId = @allocId THEN 'Table Variable'\r\n\t\t\t\t\t  WHEN AllocUnitName LIKE 'sys.%' THEN 'System Base Table'\r\n\t\t\t\t\t  ELSE AllocUnitName\r\n\t\t\t\t\tEND AS AllocUnitName,\r\n\t\t\t\t\t[Log Record Length]\r\n\t\t\t FROM   fn_dblog(NULL, NULL) AS D)\r\n\tSELECT Operation = CASE\r\n\t\t\t\t\t\t WHEN GROUPING(Operation) = 1 THEN 'Total'\r\n\t\t\t\t\t\t ELSE Operation\r\n\t\t\t\t\t   END,\r\n\t\t   Context,\r\n\t\t   AllocUnitName,\r\n\t\t   [Size in Bytes] = COALESCE(SUM([Log Record Length]), 0),\r\n\t\t   Cnt = COUNT(*)\r\n\tFROM   T\r\n\tGROUP  BY GROUPING SETS( ( Operation, Context, AllocUnitName ), ( ) )\r\n\tORDER  BY GROUPING(Operation),\r\n\t\t\t  AllocUnitName \r\n\r\nReturns \r\n\r\n**Detailed view:**\r\n\r\n![Screenshot of results][9]\r\n\r\n**Summary View (includes logging for implicit drop and system base tables):**\r\n    \r\n\r\n![Screenshot of results][10]\r\n\r\n[As far as I've been able to discern][11] operations on both generate roughly equal amounts of logging.\r\n\r\nWhilst the *quantity* of logging is very similar one important difference is that log records related to `#temp` tables can not be cleared out until any containing user transaction finishes so a long running transaction that at some point writes to `#temp` tables will prevent log truncation in `tempdb` whereas the autonomous transactions spawned for table variables do not.\r\n\r\nTable variables do not support `TRUNCATE` so can be at a logging disadvantage when the requirement is to remove all rows from a table (though for very small tables `DELETE` [can work out better anyway][12])\r\n\r\n# Cardinality\r\n\r\nMany of the execution plans involving table variables will show a single row estimated as the output from them. Inspecting the table variable properties shows that SQL Server believes the table variable has **zero** rows (Why it estimates 1 row will be emitted from a zero row table is explained by @Paul White [here][13]). \r\n\r\nHowever the results shown in the previous section do show an accurate `rows` count in `sys.partitions`. The issue is that on most occasions the statements referencing table variables are compiled while the table is empty. If the statement is (re)compiled after `@table_variable` is populated then this will be used for the table cardinality instead (This might happen due to an explicit `recompile` or perhaps because the statement also references another object that causes a deferred compile or a recompile.)\r\n\r\n    DECLARE @T TABLE(I INT);\r\n    \r\n    INSERT INTO @T VALUES(1),(2),(3),(4),(5)\r\n    \r\n    CREATE TABLE #T(I INT)\r\n    \r\n    /*Reference to #T means this statement is subject to deferred compile*/\r\n    SELECT * FROM @T WHERE NOT EXISTS(SELECT * FROM #T)\r\n    \r\n    DROP TABLE #T\r\n\r\n**Plan shows accurate estimated row count following deferred compile:**\r\n\r\n![Shows accurate row count][15]\r\n\r\nIn SQL Server 2012 SP2, trace flag 2453 is introduced. More details are under "Relational Engine" [here][16].\r\n\r\nWhen this trace flag is enabled it can cause automatic recompiles to take account of changed cardinality as discussed further very shortly.\r\n\r\nNB: On Azure in compatibility level 150 compilation of the statement is [now deferred until first execution][14]. This means that it will no longer be subject to the zero row estimate problem.\r\n\r\n# No column statistics\r\n\r\nHaving a more accurate table cardinality doesn't mean the estimated row count will be any more accurate however (unless doing an operation on all rows in the table). SQL Server does not maintain column statistics for table variables at all so will fall back on guesses based upon the comparison predicate (e.g. that 10% of the table will be returned for an `=` against a non unique column or 30% for a `>` comparison). In contrast column statistics **are** maintained for `#temp` tables.\r\n\r\nSQL Server maintains a count of the number of modifications made to each column. If the number of modifications since the plan was compiled exceeds the recompilation threshold (RT) then the plan will be recompiled and statistics updated. The RT depends on table type and size.\r\n\r\nFrom [Plan Caching in SQL Server 2008][17]\r\n\r\n> RT is calculated as follows. (n refers to a table's cardinality when a query plan is compiled.)\r\n\r\n> **Permanent table**\r\n> -  If n <= 500, RT = 500.\r\n> -  If n > 500, RT = 500 + 0.20 * n.\r\n\r\n> **Temporary table**\r\n> -   If n < 6, RT = 6.\r\n> -   If 6 <= n <= 500, RT = 500.\r\n> -   If n > 500, RT = 500 + 0.20 * n.\r\n> **Table variable**\r\n> -   RT does not exist. Therefore, recompilations do not happen because of changes in  cardinalities of table variables.\r\n> (But see note about TF 2453 below)\r\n\r\nthe `KEEP PLAN` hint can be used to set the RT for `#temp` tables the same as for permanent tables. \r\n\r\nThe net effect of all this is that often the execution plans generated for `#temp` tables are orders of magnitudes better than for `@table_variables` when many rows are involved as SQL Server has better information to work with.\r\n\r\nNB1: Table variables do not have statistics but can still incur a "Statistics Changed" recompile event under trace flag 2453 (does not apply for "trivial" plans) This appears to occur under the same recompile thresholds as shown for temp tables above with an additional one that if `N=0 -> RT = 1`. i.e. all statements compiled when the table variable is empty will end up getting a recompile and corrected `TableCardinality` the first time they are executed when non empty. The compile time table cardinality is stored in the plan and if the statement is executed again with the same cardinality (either due to flow of control statements or reuse of a cached plan) no recompile occurs.\r\n\r\nNB2: For cached temporary tables in stored procedures the recompilation story is much more complicated than described above. See [Temporary Tables in Stored Procedures][18] for all the gory details.\r\n\r\n# Recompiles\r\n\r\nAs well as the modification based recompiles described above `#temp` tables can also be associated with [additional compiles][19] simply because they allow operations that are prohibited for table variables that trigger a compile (e.g. DDL changes `CREATE INDEX`, `ALTER TABLE`)\r\n\r\n# Locking\r\n\r\nIt [has been stated][20] that table variables do not participate in locking. This is not the case. Running the below outputs to the SSMS messages tab the details of locks taken and released for an insert statement.\r\n\r\n    DECLARE @tv_target TABLE (c11 int, c22 char(100))\r\n    \r\n    DBCC TRACEON(1200,-1,3604)\r\n    \r\n    INSERT INTO @tv_target (c11, c22)\r\n    \r\n    VALUES (1, REPLICATE('A',100)), (2, REPLICATE('A',100))\r\n    \r\n    DBCC TRACEOFF(1200,-1,3604)\r\n\r\nFor queries that `SELECT` from table variables Paul White points out in the comments that these automatically come with an implicit `NOLOCK` hint. This is shown below\r\n\r\n\tDECLARE @T TABLE(X INT); \r\n\r\n\tSELECT X\r\n\tFROM @T \r\n\tOPTION (RECOMPILE, QUERYTRACEON 3604, QUERYTRACEON 8607)\r\n\t\r\n**Output:**\r\n\r\n\t*** Output Tree: (trivial plan) ***\r\n\r\n\t\t\tPhyOp_TableScan TBL: @T Bmk ( Bmk1000) IsRow: COL: IsBaseRow1002  Hints( NOLOCK )\r\n\r\n\r\nThe impact of this on locking might be quite minor however.\r\n\r\n    SET NOCOUNT ON;\r\n    \r\n    CREATE TABLE #T( [ID] [int] IDENTITY NOT NULL,\r\n                     [Filler] [char](8000) NULL,\r\n                     PRIMARY KEY CLUSTERED ([ID] DESC))\r\n    \r\n    \r\n    DECLARE @T TABLE ( [ID] [int] IDENTITY NOT NULL,\r\n                     [Filler] [char](8000) NULL,\r\n                     PRIMARY KEY CLUSTERED ([ID] DESC))\r\n    \r\n    DECLARE @I INT = 0\r\n    \r\n    WHILE (@I < 10000)\r\n    BEGIN\r\n    INSERT INTO #T DEFAULT VALUES\r\n    INSERT INTO @T DEFAULT VALUES\r\n    SET @I += 1\r\n    END\r\n    \r\n    /*Run once so compilation output doesn't appear in lock output*/\r\n    EXEC('SELECT *, sys.fn_PhysLocFormatter(%%physloc%%) FROM #T')\r\n    \r\n    DBCC TRACEON(1200,3604,-1)\r\n    SELECT *, sys.fn_PhysLocFormatter(%%physloc%%)\r\n    FROM @T\r\n    \r\n    PRINT '--*--'\r\n    \r\n    EXEC('SELECT *, sys.fn_PhysLocFormatter(%%physloc%%) FROM #T')\r\n    \r\n    DBCC TRACEOFF(1200,3604,-1)\r\n    \r\n    DROP TABLE #T\r\n\r\nNeither of these return results in index key order indicating that SQL Server used an [allocation ordered scan][21] for both. \r\n\r\nI ran the above script twice and the results for the second run are below\r\n\r\n\r\n    Process 58 acquiring Sch-S lock on OBJECT: 2:-1325894110:0  (class bit0 ref1) result: OK\r\n    \r\n    --*--\r\n    Process 58 acquiring IS lock on OBJECT: 2:-1293893996:0  (class bit0 ref1) result: OK\r\n    \r\n    Process 58 acquiring S lock on OBJECT: 2:-1293893996:0  (class bit0 ref1) result: OK\r\n    \r\n    Process 58 releasing lock on OBJECT: 2:-1293893996:0 \r\n\r\nThe locking output for the table variable is indeed extremely minimal as SQL Server just acquires a schema stability lock on the object. But for a `#temp` table it is nearly as light in that it takes out an object level `S` lock. A `NOLOCK` hint or `READ UNCOMMITTED` isolation level can of course be specified explicitly when working with `#temp` tables as well.\r\n\r\nSimilarly to the issue with logging a surrounding user transaction can mean that the locks are held longer for `#temp` tables. With the script below\r\n\r\n        --BEGIN TRAN;   \r\n        \r\n        CREATE TABLE #T (X INT,Y CHAR(4000) NULL);\r\n            \r\n        INSERT INTO #T (X) VALUES(1) \r\n       \r\n        SELECT CASE resource_type\r\n                 WHEN  'OBJECT' THEN OBJECT_NAME(resource_associated_entity_id, 2)\r\n                 WHEN  'ALLOCATION_UNIT' THEN (SELECT OBJECT_NAME(object_id, 2)\r\n                                               FROM  tempdb.sys.allocation_units a \r\n                                               JOIN tempdb.sys.partitions p ON a.container_id = p.hobt_id\r\n                                               WHERE  a.allocation_unit_id = resource_associated_entity_id)\r\n    \t\t\t WHEN 'DATABASE' THEN DB_NAME(resource_database_id)\t\t\t\t\t\t\t\t\t\t \r\n                 ELSE (SELECT OBJECT_NAME(object_id, 2)\r\n                       FROM   tempdb.sys.partitions\r\n                       WHERE  partition_id = resource_associated_entity_id)\r\n               END AS object_name,\r\n               *\r\n        FROM   sys.dm_tran_locks\r\n        WHERE  request_session_id = @@SPID\r\n         \r\n        DROP TABLE #T\r\n        \r\n       -- ROLLBACK  \r\n\r\nwhen run outside of an explicit user transaction for both cases the only lock returned when checking `sys.dm_tran_locks` is a shared lock on the `DATABASE`. \r\n\r\nOn uncommenting the `BEGIN TRAN ... ROLLBACK` 26 rows are returned showing that locks are held both on the object itself and on system table rows to allow for rollback and prevent other transactions from reading uncommitted data. The equivalent table variable operation is not subject to rollback with the user transaction and has no need to hold these locks for us to check in the next statement but tracing locks acquired and released in Profiler or using trace flag 1200 shows plenty of locking events do still occur.\r\n\r\n# Indexes\r\n\r\nFor versions prior to SQL Server 2014 indexes can only be created implicitly on table variables as a side effect of adding a unique constraint or primary key. This does of course mean that only unique indexes are supported. A non unique non clustered index on a table with a unique clustered index can be simulated however by simply declaring it `UNIQUE NONCLUSTERED` and adding the CI key to the end of the desired NCI key (SQL Server would [do this behind the scenes anyway][22] even if a non unique NCI could be specified)\r\n\r\nAs demonstrated earlier various `index_option`s can be specified in the constraint declaration including `DATA_COMPRESSION`, `IGNORE_DUP_KEY`, and `FILLFACTOR` (though there is no point in setting that one as it would only make any difference on index rebuild and you can't rebuild indexes on table variables!)\r\n\r\nAdditionally table variables do not support `INCLUDE`d columns, filtered indexes (until 2016) or partitioning, `#temp` tables do (the partition scheme must be created in `tempdb`). \r\n\r\n## Indexes in SQL Server 2014\r\n\r\nNon unique indexes can be declared inline in the table variable definition in SQL Server 2014. Example syntax for this is below.\r\n\r\n    DECLARE @T TABLE (\r\n    C1 INT INDEX IX1 CLUSTERED, /*Single column indexes can be declared next to the column*/\r\n    C2 INT INDEX IX2 NONCLUSTERED,\r\n           INDEX IX3 NONCLUSTERED(C1,C2) /*Example composite index*/\r\n    );\r\n\r\n\r\n## Indexes in SQL Server 2016\r\n\r\nFrom CTP 3.1 it is now possible to declare filtered indexes for table variables. By RTM it *may* be the case that included columns are also allowed albeit they [will likely not make it into SQL16 due to resource constraints][23]\r\n\r\n    DECLARE @T TABLE\r\n    (\r\n    c1 INT NULL INDEX ix UNIQUE WHERE c1 IS NOT NULL /*Unique ignoring nulls*/\r\n    )\r\n\r\n# Parallelism\r\n\r\nQueries that insert into (or otherwise modify) `@table_variables` cannot have a parallel plan, `#temp_tables` are not restricted in this manner.\r\n\r\nThere is an apparent workaround in that rewriting as follows does allow the `SELECT` part to take place in parallel but that ends up using a hidden temporary table [(behind the scenes)][24]\r\n\r\n    INSERT INTO @DATA ( ... ) \r\n    EXEC('SELECT .. FROM ...')\r\n\r\nThere is no such limitation in queries that **select** from table variables [as illustrated in my answer here](https://stackoverflow.com/a/8242831/73226)\r\n\r\n# Other Functional Differences\r\n\r\n - `#temp_tables` cannot be used inside a function. `@table_variables` can be used inside scalar or multi-statement table UDFs.\r\n - `@table_variables` cannot have named constraints.\r\n - `@table_variables` cannot be `SELECT`-ed `INTO`, `ALTER`-ed, `TRUNCATE`d or be the target of `DBCC` commands such as `DBCC CHECKIDENT` or of `SET IDENTITY INSERT` and do not support table hints such as `WITH (FORCESCAN)` \r\n - `CHECK` constraints on table variables are not considered by the optimizer for simplification, implied predicates or contradiction detection.\r\n - Table variables don't seem to qualify for the [rowset sharing optimisation][25] meaning that delete and update plans against these can encounter more overhead and `PAGELATCH_EX` waits. ([Example][26])\r\n\r\n# Memory Only?\r\n\r\nAs stated at the beginning both get stored on pages in `tempdb`. However I didn't address whether there was any difference in behaviour when it comes to writing these pages to disc.\r\n\r\nI've done a small amount of testing on this now and so far have seen no such difference. In the specific test I did on my instance of SQL Server 250 pages seems to be the cut off point before the data file gets written to. \r\n\r\n> NB: The behavior below no longer occurs in SQL Server 2014 or [SQL\r\n> Server 2012 SP1/CU10 or SP2/CU1][27] the eager writer is no longer as\r\n> eager to write pages to disc. More details on that change at [SQL\r\n> Server 2014: tempdb Hidden Performance Gem][28].\r\n\r\nRunning the below script\r\n\r\n    CREATE TABLE #T(X INT, Filler char(8000) NULL)\r\n    INSERT INTO #T(X)\r\n    SELECT TOP 250 ROW_NUMBER() OVER (ORDER BY @@SPID)\r\n    FROM master..spt_values\r\n    DROP TABLE #T\r\n\r\nAnd monitoring writes to the `tempdb` data file with Process Monitor I saw none (except occasionally ones to the database boot page at offset 73,728). After changing `250` to `251`I began to see writes as below.\r\n\r\n![ProcMon][29]\r\n\r\nThe screenshot above shows 5 * 32 page writes and one single page write indicating that 161 of the pages were written to disc. I got the same cut off point of 250 pages when testing with table variables too. The script below shows it a different way by looking at `sys.dm_os_buffer_descriptors`\r\n\r\n\r\n    DECLARE @T TABLE (\r\n      X        INT,\r\n      [dba.se] CHAR(8000) NULL)\r\n    \r\n    INSERT INTO @T\r\n                (X)\r\n    SELECT TOP 251 Row_number() OVER (ORDER BY (SELECT 0))\r\n    FROM   master..spt_values\r\n    \r\n    SELECT is_modified,\r\n           Count(*) AS page_count\r\n    FROM   sys.dm_os_buffer_descriptors\r\n    WHERE  database_id = 2\r\n           AND allocation_unit_id = (SELECT a.allocation_unit_id\r\n                                     FROM   tempdb.sys.partitions AS p\r\n                                   INNER JOIN tempdb.sys.system_internals_allocation_units AS a\r\n                                              ON p.hobt_id = a.container_id\r\n                                            INNER JOIN tempdb.sys.columns AS c\r\n                                              ON c.object_id = p.object_id\r\n                                     WHERE  c.name = 'dba.se')\r\n    GROUP  BY is_modified \r\n\r\n**Results:**\r\n\r\n    is_modified page_count\r\n    ----------- -----------\r\n    0           192\r\n    1           61\r\n\r\nShowing that 192 pages were written to disc and the dirty flag cleared. It also shows that being written to disc doesn't mean that pages will be evicted from the buffer pool immediately. The queries against this table variable could still be satisfied entirely from memory. \r\n\r\nOn an idle server with `max server memory` set to `2000 MB` and `DBCC MEMORYSTATUS` reporting Buffer Pool Pages Allocated as approx 1,843,000 KB (c. 23,000 pages) I inserted to the tables above in batches of 1,000 rows/pages and for each iteration recorded.\r\n\r\n    SELECT Count(*)\r\n    FROM   sys.dm_os_buffer_descriptors\r\n    WHERE  database_id = 2\r\n           AND allocation_unit_id = @allocId\r\n           AND page_type = 'DATA_PAGE' \r\n\r\n\r\nBoth the table variable and the `#temp` table gave nearly identical graphs and managed to pretty much max out the buffer pool before getting to the point that they weren't entirely held in memory so there doesn't seem to be any particular limitation on how much memory either can consume.\r\n\r\n![Pages in Buffer Pool][30]\r\n\r\n  [1]: https://i.stack.imgur.com/K2zgk.png\r\n  [2]: https://blogs.technet.com/b/dataplatforminsider/archive/2014/01/07/sql-server-2014-in-memory-oltp-memory-optimized-table-types-and-table-variables.aspx\r\n  [3]: https://voluntarydba.com/post/2012/12/20/Collation-of-character-fields-in-temporary-objects.aspx\r\n  [4]: https://www.sqlservercentral.com/articles/Temporary+Tables/66720/\r\n  [5]: https://msdn.microsoft.com/en-us/library/ff929143.aspx\r\n  [6]: https://www.itprotoday.com/microsoft-sql-server/caching-temporary-objects\r\n  [7]: https://dba.stackexchange.com/a/13412/3690\r\n  [8]: https://stackoverflow.com/a/8561729/73226\r\n  [9]: https://i.stack.imgur.com/rjVln.png\r\n  [10]: https://i.stack.imgur.com/LRTBy.png\r\n  [11]: https://stackoverflow.com/a/8204184/73226\r\n  [12]: https://stackoverflow.com/a/2646780/73226\r\n  [13]: https://dba.stackexchange.com/a/10220/3690\r\n  [14]: https://blogs.msdn.microsoft.com/sqlserverstorageengine/2018/07/16/public-preview-of-table-variable-deferred-compilation-in-azure-sql-database/\r\n  [15]: https://i.stack.imgur.com/ibPGT.png\r\n  [16]: https://support.microsoft.com/KB/2958429\r\n  [17]: https://technet.microsoft.com/en-us/library/cc293623.aspx\r\n  [18]: https://sql.kiwi/2012/08/temporary-tables-in-stored-procedures.html\r\n  [19]: https://support.microsoft.com/kb/308737\r\n  [20]: https://blogs.msdn.com/b/sqlserverstorageengine/archive/2008/03/30/sql-server-table-variable-vs-local-temporary-table.aspx\r\n  [21]: https://blogs.msdn.microsoft.com/sqlserverstorageengine/2006/11/08/when-can-allocation-order-scans-be-used/\r\n  [22]: https://web.archive.org/web/20190308085816/http://sqlblog.com/blogs/kalen_delaney/archive/2010/03/07/more-about-nonclustered-index-keys.aspx\r\n  [23]: https://connect.microsoft.com/SQLServer/Feedback/Details/2079552\r\n  [24]: http://dataeducation.com/revisiting-isnull-coalesce-and-the-perils-of-micro-optimization/\r\n  [25]: https://sqlperformance.com/2015/12/sql-plan/optimizing-update-queries\r\n  [26]: https://stackoverflow.com/q/48828286/73226\r\n  [27]: https://support.microsoft.com/kb/2958012?wa=wsignin1.0\r\n  [28]: https://blogs.msdn.com/b/psssql/archive/2014/04/09/sql-server-2014-tempdb-hidden-performance-gem.aspx\r\n  [29]: https://i.stack.imgur.com/4zBig.png\r\n  [30]: https://i.stack.imgur.com/zAdpP.png	2019-11-28 15:41:40.468774+00	3	4	1	16386	0	0	0	2019-11-24 09:19:05.230301+00	\N	**Contents:**	f	f
124	185	12	2017-03-14 11:10:19+00	> An example query would be great.\r\n\r\nUsing a numbers table (integers 1...n, where n needs to be at least 1000 for this example):\r\n\r\n    SELECT N.n % 10, SPACE(100) \r\n    FROM dbo.Numbers AS N \r\n    WHERE N.n BETWEEN 1 AND 1000\r\n    UNION\r\n    SELECT 999, SPACE(100);\r\n\r\nResults:\r\n\r\n``` none\r\n999\r\n6\r\n4\r\n8\r\n3\r\n1\r\n0\r\n7\r\n5\r\n9\r\n2\r\n```\r\n\r\n[![hash union][1]][1]\r\n\r\n> When is it used and when is it better than the alternatives?\r\n\r\nHash union is not very common. It is preferred when one table is wide and has many duplicates, while the other is small (relatively few rows) and known to be distinct. A wide build side *with lots of duplicates* plays to the hash table's strengths because it is immediately only stored once per dupe.\r\n\r\n### How it works\r\n\r\nThe Hash Union operator builds a hash table on the upper (build) input eliminating duplicates as it goes (like a hash aggregate performing a distinct). It then reads rows from the lower (probe) input. If there is no match in the hash table, the row is returned. When the probe input is exhausted, the operator returns each row in the hash table.\r\n\r\nHash Union does not add rows from the probe side to the hash table, so it cannot eliminate duplicates from that input. The optimizer either has to have a guarantee of uniqueness, or add a grouping operator to the probe side.\r\n\r\n  [1]: https://i.stack.imgur.com/9tWBw.png	2019-11-28 13:53:19.907774+00	2	4	1	167113	0	0	0	2019-11-28 13:52:38.768903+00	\N	> An example query would be great.	f	f
656	373	702	2020-01-22 22:13:32.288856+00	### The TA Triangle Revised\r\n\r\nJust an attempt to spruce up the current favicon:\r\n\r\n![ta-revised-triangle-logo.png](/image?hash=3e4905cd2796847d0884c9904fa94ce874f12890394242c8d927662814dd9411)\r\n\r\nI have no investment in the colour choice! Just thinking out loud...\r\n\r\n---\r\n\r\n*commentary*: I have some half-baked ideas about icon/logo alternatives for TA. I will post them here, thus exposing my creative deficit and lack of skills with graphics tools. The ones I will post here have been created in Inkscape, fwiw.\r\n\r\nConsider these as the roughest of concept mock-ups....\r\n\r\n	2020-01-22 22:21:20.663902+00	6	1	1	\N	0	0	0	\N	\N	The TA Triangle Revised	f	f
658	373	702	2020-01-22 22:19:11.81166+00	### The TA "Interrobang"\r\n\r\nI feel that the TA concept is well captured in this crafty little [punctuation mark][1]:\r\n\r\n![ta-vollkorn-interrobang-logo.png](/image?hash=86cf345a5b97ade1fbbb85d83aec854b5cc0f3768a4442465f6c019a048d4d70)\r\n\r\nThe colours are traffic-light red-gold-green, with some hazy metaphor of getting a "green light"... I dunno.\r\n\r\nBut the `!` + `?` mashup I thought made a good representation for a Q&A site. The character I've used here (bold and tweaked, not only for colour) comes from the lovely [Vollkorn][v] font, btw.\r\n\r\n[1]: https://en.wikipedia.org/wiki/Interrobang\r\n[v]: http://vollkorn-typeface.com/\r\n[b]: https://topanswers.xyz/meta?q=373#a656\r\n\r\n---\r\n\r\n~\\[See~ [~"TA~ ~Triangle~ ~Revised"~][b] ~for~ ~a~ ~bit~ ~of~ ~introductory~ ~comment.\\]~	2020-01-23 20:57:50.961962+00	4	1	1	\N	0	0	0	\N	\N	The TA "Interrobang"	f	f
657	373	702	2020-01-22 22:15:55.01376+00	### `@` Backwards is TA\r\n\r\n![ta-backwards-at.png](/image?hash=002ddfd1ff792586c7cb97ce1ddb36e713a8590c587a9bc09b972d5ec8e88c13)\r\n\r\nThe `@` symbol is sometimes pronounced "at". "at" backwards is TA. The triangle motif sitting atop it is meant to reflect the answer's trajectory, rising to the top. This horizontally flipped and doctored `@` comes from [Andika New Basic][a].\r\n\r\nNo investment in colours, but they're in the neighbourhood of what Jack has for the current favicon.\r\n\r\n[a]: https://fontlibrary.org/en/font/andika\r\n[b]: https://topanswers.xyz/meta?q=373#a656\r\n\r\n---\r\n\r\n~\\[See~ [~"TA~ ~Triangle~ ~Revised"~][b] ~for~ ~a~ ~bit~ ~of~ ~introductory~ ~comment.\\]~	2020-01-23 20:55:40.170742+00	1	1	1	\N	0	0	0	\N	\N	`@` Backwards is TA	f	f
675	373	811	2020-01-26 07:36:36.133225+00	How about a "T" and an "A" overlaid:\r\n![TA.png](/image?hash=4246c1596608ef6bcfe5d9e1e742959f22a814516b726dd9f3443df84cd2c7cd)\r\n\r\nIt incorporates the name, while also showing an up-arrow to the top bar.	2020-01-26 07:36:36.133225+00	4	1	1	\N	0	0	0	\N	\N	How about a "T" and an "A" overlaid:	f	f
62	80	43	2019-11-23 19:22:28.952545+00	OK, I'm assuming really small problems can be just raised in The Tavern (chat) where they can be quickly addressed (possibly with a change or fix). Larger issues should probably be raised as a question and tagged appropriately: "bug" or "feature request" (not sure how I can find the list of tags or even use them here).	2019-11-24 12:13:55.728891+00	3	4	1	\N	0	0	0	\N	\N	OK, I'm assuming really small problems can be just raised in The Tavern (chat) where they can be quickly addressed (possibly with a change or fix). Larger issues should probably be raised as a question and tagged appropriately: "bug" or "feature request" (not sure how I can find the list of tags or even use them here).	f	f
152	80	96	2019-11-30 13:25:47.632866+00	update Feb 2020: the GitHub issue tracker is now available:\r\n\r\n@@@ answer 612\r\n\r\nAnything that requires discussion or voting should be raised here on meta first, but bugs and issues that don't can go directly on the GitHub issue tracker.\r\n\r\n---\r\n\r\nSpeaking from experience, neither chat nor forums nor a Q&A format are really well suited to tracking bugs and feature requests. It turns out Q&A is pretty good at helping people _use_ a site and understand its features (and to a lesser extent discussing their merits) but it quite frankly sucks for tracking progress, especially when the progress is code in an open source project or being worked on by a distributed team.\r\n\r\nI would like to see the code for this release in a git repository with a proper **issue tracker** such as Github or Gitlab. This meta can still be used for asking questions, but bug reports and specific feature requests should then be posted there.\r\n\r\nI say "specific" feature requests, because many feature requests start out life as less nuts and bolts technical requests and more philosophical inquires. Some feature requests may start out like on meta:\r\n\r\n> "Should we add downvotes?"\r\n\r\n...should be asked here. After the inevitable mountain of opiniouns that brews settles down towards a concencus, the actual feature request can hit the issue tracker as:\r\n\r\n> "Add downvote system that subtracts N points from user rep"\r\n	2020-02-10 15:17:11.341846+00	3	4	1	\N	0	0	0	\N	\N	update Feb 2020: the GitHub issue tracker is now available:	f	f
405	403	88	2015-09-23 17:26:22+00	You should not make any assumptions about how SQL Server ***will*** process your query, except this: you should always assume that SQL Server ***can*** process your query in a way that is different from how it is explicitly written on the screen. And also this behavior can change based on any of the factors that can influence whether a new plan will be used for the next execution of even the same query, so if you apply a hint or change the query in any way or add or remove an index and the error goes away, don't assume the error won't come back tomorrow.\r\n\r\nIn this case, SQL Server is processing a calculation before it is eliminating rows from the `WHERE` clause. The way you avoid this is, like you said, ensuring that those rows are also filtered out inside the `CASE` expression (not statement).\r\n\r\nA more common but similar approach is this kind of thing:\r\n\r\n    SELECT DATEPART(MONTH, varchar_column)\r\n    FROM dbo.some_table\r\n    WHERE ISDATE(varchar_column) = 1;\r\n\r\nIn many scenarios you will get an error message because SQL Server tried to apply the date functions against some values in the column that didn't turn out to be dates (and this attempt occurred prior to filtering). The workaround is tedious - use the `CASE` expression - but necessary unless you have some other way to verify the worthiness of the column (e.g. a computed column or fixing the data type in the first place). Just keep in mind that even this [can fail to "short circuit" in some scenarios](https://dba.stackexchange.com/questions/12941/does-sql-server-read-all-of-a-coalesce-function-even-if-the-first-argument-is-no/12945#12945).\r\n\r\n    SELECT CASE WHEN ISDATE(varchar_column) = 1 \r\n      THEN DATEPART(MONTH, varchar_column) END\r\n    FROM dbo.some_table\r\n    WHERE ISDATE(varchar_column) = 1;\r\n\r\nThis is explained more thoroughly in the following feedback item by Erland Sommarskog:\r\n\r\n- [SQL Server should not raise illogical errors](https://feedback.azure.com/forums/908035-sql-server/suggestions/32912431-sql-server-should-not-raise-illogical-errors)	2019-12-11 04:20:51.234255+00	2	4	1	115952	0	0	0	2019-12-11 04:20:51.234255+00	\N	You should not make any assumptions about how SQL Server ***will*** process your query, except this: you should always assume that SQL Server ***can*** process your query in a way that is different from how it is explicitly written on the screen. And also this behavior can change based on any of the factors that can influence whether a new plan will be used for the next execution of even the same query, so if you apply a hint or change the query in any way or add or remove an index and the error goes away, don't assume the error won't come back tomorrow.	f	f
44	72	2	2019-11-20 11:30:32.319346+00	We are using [markdown-it](https://github.com/markdown-it/markdown-it), which:\r\n\r\n* Has a useful [demo] page that you linked to in the question.\r\n\r\n* Follows the [CommonMark] spec, except that on TopAnswers we ***[don't allow HTML input at all][safe]***, so no `<br>`, `sup` etc [that you can use on SE][sehtml][^pluginforhtml][^plugin].\r\n\r\n* Isn't otherwise a radical departure from Markdown on SE, but there are a few more gotcha's such as:\r\n\r\n    1. Strikethrough in SE chat is `---strikethrough---` but CommonMark does not support strikethrough (however see below for our plugin support for `~~strikethrough~~`).\r\n\r\n    1. Markdown-it doesn't collapse adjacent blockquotes into a single blockquote:\r\n    \r\n        ```\r\n        > this is two\r\n\r\n        > blockquotes\r\n\r\n        > this is a single\r\n        >\r\n        > blockquote with two paragraphs\r\n        ```\r\n\r\n    1. Various other minor details from [this official CommonMark list][cmdiffs].\r\n    \r\n* Has some very sensible optional extensions (plugins) built-in — these are not part of the CommonMark spec but we have them switched on globally:\r\n    1. GitHub-style [strikethrough]\r\n    1. GitHub-style [tables]\r\n    \r\n* Has extensive support for other extensions, and we can add more or even create our own[^plugin].\r\n\r\n    * The following 'official'[^moreplugins] extensions are currently supported globally on TopAnswers:\r\n        1. [linkify]\r\n        1. [syntax highlighting][highlight][^syntax]\r\n        1. [superscript][^supersub]\r\n        1. [subscript][^supersub]\r\n        1. [abbreviations][abbr]\r\n        1. [definition lists][deflist]\r\n        1. [footnotes]\r\n        1. [emoji]\r\n        \r\n    * The following custom extensions are also currently supported globally:\r\n\r\n        * tweaking 'fuzzy' `dba.se` links to point to `dba.stackexchange.com`\r\n        \r\n    * Not actually a markdown plugin presently, but may be implemented as a plugin in future, and behaves similar to a plugin now:\r\n        * integration with [db<>fiddle], using this block level syntax:\r\n            ```\r\n            <>https://dbfiddle.uk/?rdbms=postgres_11&fiddle=71940aadda50f55bfede87606cd1dc2c`\r\n            ```\r\n    \r\n[^pluginforhtml]: If you miss something that you can only do in with inline HTML on SE, it may be available as a plugin for markdown-it, or we can write our own.\r\n[^plugin]: We'll need to be careful about adding non-standard syntax, but all requests for plugin support will be carefully considered — please ask here on meta for global support or with a meta question on the community site if it is site-specific like [MathML] or [Mathjax].\r\n[^moreplugins]: There are a lot of other plugins, probably of widely varying quality and sanity, [listed on npm][npm]\r\n[^syntax]: To chose a language for a code block, you need to use a [fenced code block](https://spec.commonmark.org/0.27/#fenced-code-blocks) and specify the language after the fence (e.g. `~~~none`, or the same with backticks ` ```none`)\r\n[^supersub]: Note that spaces currently [need to be escaped](https://github.com/markdown-it/markdown-it-sup/issues/3) if you want to super/subscript multiple words (demo [here](https://markdown-it.github.io/#md3=%7B%22source%22%3A%22-%20not%20%5Eup%2C%20up%2C%20and%20away%5E%20%28%3D%20superscript%20ignored%29%5Cn-%20%26nbsp%3B%5Cn-%20but%20%5Eup%2C%5C%5C%20up%2C%5C%5C%20and%5C%5C%20away%5E%20%5Cn%5Cnor%20%5Cn%5Cn-%20not%20~swing%20low~%20%28%3D%20subscript%20ignored%29%5Cn-%20but%20~swing%5C%5C%20low~%22%2C%22defaults%22%3A%7B%22html%22%3Afalse%2C%22xhtmlOut%22%3Afalse%2C%22breaks%22%3Afalse%2C%22langPrefix%22%3A%22language-%22%2C%22linkify%22%3Atrue%2C%22typographer%22%3Atrue%2C%22_highlight%22%3Atrue%2C%22_strict%22%3Afalse%2C%22_view%22%3A%22html%22%7D%7D)).\r\n\r\n[demo]: https://markdown-it.github.io\r\n[CommonMark]: https://commonmark.org\r\n[safe]: https://talk.commonmark.org/t/make-commonmark-safe-by-default/1265\r\n[sehtml]: https://meta.stackexchange.com/a/135909\r\n[cmdiffs]: https://github.com/commonmark/commonmark-spec#user-content-differences-from-original-markdown\r\n[MathML]: https://github.com/runarberg/markdown-it-math\r\n[Mathjax]: https://github.com/classeur/markdown-it-mathjax\r\n[strikethrough]: https://help.github.com/articles/basic-writing-and-formatting-syntax/#styling-text\r\n[tables]: https://help.github.com/articles/organizing-information-with-tables\r\n[linkify]: https://github.com/markdown-it/markdown-it#linkify\r\n[highlight]: https://github.com/markdown-it/markdown-it#syntax-highlighting\r\n[superscript]: https://github.com/markdown-it/markdown-it-sup\r\n[subscript]: https://github.com/markdown-it/markdown-it-sub\r\n[abbr]: https://github.com/markdown-it/markdown-it-abbr\r\n[deflist]: https://github.com/markdown-it/markdown-it-deflist\r\n[footnotes]: https://github.com/markdown-it/markdown-it-footnote \r\n[emoji]: https://github.com/markdown-it/markdown-it-emoji\r\n[db<>fiddle]: https://dbfiddle.uk\r\n[npm]: https://www.npmjs.com/search?q=keywords:markdown-it-plugin	2020-01-30 20:53:51.975031+00	7	1	1	\N	0	0	0	\N	\N	We are using [markdown-it](https://github.com/markdown-it/markdown-it), which:	f	f
434	411	96	2019-12-12 09:31:41.656136+00	On mobile the situation is even more dire. Not just the dismiss link but sometimes the place the notification links to is not even shown.\r\n\r\nIn general I think there is a fundamental problem with the way HTML + CSS are being written here. I'm sure with some more UI expertise and eyes on the code (hint hint) we could help fix these things, but here's the gist: `overflow: hidden` and `wrap: none` type properties should _never_ be used in any user interface where both the size and content varries wildly. Any interface that assumes those properties will always be playing whack-a-mole with display glitches like this. The line should wrap normally.\r\n\r\nOther concesions can be made to minimise the screen realestate userped by any given element, but nothing in an HTML user interface should ever be restricted from wrapping, flowing, and growing as needed to contain it's content. Styles can change responsively depending on the available space, but trucating anywhere active UI elements like links could appear should not be one of the tricks in the toolbox.	2019-12-12 09:31:41.656136+00	1	4	2	\N	0	0	0	\N	\N	On mobile the situation is even more dire. Not just the dismiss link but sometimes the place the notification links to is not even shown.	f	f
795	411	1	2020-02-10 17:39:38.884329+00	This has now been fixed. There is now a small red `x` button on the left of every notification:\r\n\r\n![Screenshot.png](/image?hash=6a9552a0fb748c5e510cd9a75f7927e534bcb61107075bc2b340793eeaaeb14c)\r\n\r\nThe time a notification was recieved now comes next, and then the rest of the notification.\r\n\r\nI also took the opportunity to upgrade some other notification styles, and add a notification count.	2020-02-10 17:39:38.884329+00	6	1	1	\N	0	0	0	\N	\N	This has now been fixed. There is now a small red `x` button on the left of every notification:	f	f
416	412	96	2019-12-11 15:18:48.414294+00	The current state of play is "most recently active first".\r\n\r\nSearch probably _should_ be the first line of defense if you are looking for something specific.\r\n\r\nThere is not any filter  or browse by tag, but I have [requested such a thing here](https://topanswers.xyz/meta?q=369#question).\r\n\r\nGiven that questions do not currently have votes, sorting them by votes doesn't make any sense. I think the main sort method that would make some sense to add is "sort by question date" so that the question list is chronological by question date and skips the noise produced by question and answer posts and edits constantly bumping everything.	2019-12-11 15:18:48.414294+00	2	4	2	\N	0	0	0	\N	\N	The current state of play is "most recently active first".	f	f
398	395	96	2019-12-10 07:16:43.333086+00	I agree, the shadow seems to have been added and I would consider it a visual regression. It shows up a few other places too. Even for those of us without vision challenges it doesn't improve readablity. It does balance the visual weight of elements on the page a bit, but with all due respect I think that problem would be better solved with an overhaul of the UI to cleanup the design, tweak the relative size and weights, and make better use of whitespace.	2019-12-10 07:16:43.333086+00	3	4	2	\N	0	0	0	\N	\N	I agree, the shadow seems to have been added and I would consider it a visual regression. It shows up a few other places too. Even for those of us without vision challenges it doesn't improve readablity. It does balance the visual weight of elements on the page a bit, but with all due respect I think that problem would be better solved with an overhaul of the UI to cleanup the design, tweak the relative size and weights, and make better use of whitespace.	f	f
403	395	2	2019-12-10 21:09:20.045269+00	In hindsight making the text a little blurry doesn't really add anything of value. We've removed the text shadow as of now.	2019-12-10 21:09:20.045269+00	5	1	1	\N	0	0	0	\N	\N	In hindsight making the text a little blurry doesn't really add anything of value. We've removed the text shadow as of now.	f	f
138	69	96	2019-11-29 14:04:12.839349+00	I would nominate [Hack](https://sourcefoundry.org/hack/). For many of the same reasons outlined for Deja Vu Sans, but with many improvements for use in source code.\r\n\r\n*Caveat:* This should only be used for code blocks, body text should be a Serif font as discussed elsewhere.	2019-11-29 14:05:56.914491+00	3	4	1	\N	0	0	0	\N	\N	I would nominate [Hack](https://sourcefoundry.org/hack/). For many of the same reasons outlined for Deja Vu Sans, but with many improvements for use in source code.	f	f
41	69	8	2019-11-18 12:03:20.002383+00	If the [Bitstream Vera](https://en.wikipedia.org/wiki/Bitstream_Vera#Licensing_and_expansion) license is acceptable I would vote for the [Deja Vu Sans](https://fontlibrary.org/en/font/dejavu-sans) font.\r\n\r\n\r\n![Sample](https://i.imgur.com/0wSCEdD.png)\r\n\r\n\r\n\r\n\r\nIt resembles the widely used Verdana when it comes to font width and supports many languages and symbols as well as a monospace variant for code blocks\r\n\r\n\r\n    Full Language Support\r\n        Afrikaans, Arabic, Archaic Greek Letters, Armenian, Baltic, Basic Cyrillic, \r\n        Basic Greek, Basic Latin, Catalan, Central European, Chess Symbols, \r\n        Claudian Letters, Dutch, Esperanto, Euro, Farsi, Georgian, Hebrew, Igbo Onwu, \r\n        IPA, Lao, Latin Ligatures, Mathematical Operators, Ogham, \r\n        Pan African Latin, Pinyin, Polytonic Greek, Romanian, Tifinagh, \r\n        Turkish, Uighur, Vietnamese, Western European \r\n        \r\n        \r\nIt also just looks nice and is already included in a lot of linux distros	2019-11-18 15:02:50.586442+00	4	4	1	\N	0	0	0	\N	\N	If the [Bitstream Vera](https://en.wikipedia.org/wiki/Bitstream_Vera#Licensing_and_expansion) license is acceptable I would vote for the [Deja Vu Sans](https://fontlibrary.org/en/font/dejavu-sans) font.	f	f
42	69	12	2019-11-18 12:17:50.169841+00	I would like to suggest [Source Sans Pro](https://fontlibrary.org/en/font/source-sans-pro).\r\n\r\n### Sample\r\n\r\n![sample][1]\r\n\r\nFeatures:\r\n\r\n| Item | Description |\r\n|----------|-------------|\r\n| Category | Sans-serif |\r\n| License  | OFL (SIL Open Font License) |\r\n| Family   | Source Sans Pro |\r\n| Designer | Paul D. Hunt |\r\n| Foundry  | Adobe Systems Incorporated |\r\n\r\n**Description**\r\n\r\n: Sans serif font family from Adobe for user interface environments. http://adobe-fonts.github.io/source-sans-pro/ Roman fonts version 2.020 and Italic fonts version 1.075 (OTF, TTF, WOFF, WOFF2, EOT)\r\n\r\n**Full Language Support**\r\n\r\n: Afrikaans, Baltic, Basic Cyrillic, Basic Greek, Basic Latin, Catalan, Central European, Dutch, Esperanto, Euro, Igbo Onwu, Pinyin, Polytonic Greek, Romanian, Turkish, Vietnamese, Western European\r\n\r\n[1]: https://i.imgur.com/XPvweju.png	2019-11-19 11:35:28.94568+00	4	1	1	\N	0	0	0	\N	\N	I would like to suggest [Source Sans Pro](https://fontlibrary.org/en/font/source-sans-pro).	f	f
461	444	208	2017-08-09 13:00:17+00	I don't know if you can prove that any observed behavior is always guaranteed, one way or the other, unless you can manufacture a counter-example. In the absence of that, the way to fix the order that results are returned, of course, is to add an `ORDER BY`.\r\n\r\nI don't know if there is a "fix", or that there exists a need for a fix, if you can demonstrate that in some scenarios the queries are processed in a different order.\r\n  \t \t\r\nThe lack of any explicit, official documentation suggests to me that you should not depend on this. This is exactly the kind of thing that got people into trouble with `ORDER BY` in a view, and `GROUP BY` without `ORDER BY`, 8 years ago when SQL Server 2005's optimizer was released.\r\n\r\nWith all of the new features in the newer versions of SQL Server (with more coming), even if you think you can guarantee a specific behavior today, I wouldn't expect it to hold true (until it is documented to do so).\r\n  \t \t\r\nEven if you're not depending on this behavior, what are you going to do with the results? Anyway, I wouldn't call a Simple Talk article by an outsider *official*. For all we know this is just a guess based on observation.\r\n\r\nMicrosoft is never going to publish official documentation saying 'x' is *not* guaranteed to do 'y'. This is one of the reasons we still, almost a decade later, have trouble convincing people that they can't rely on observed ordering without `ORDER BY` - there is no documentation that states "it is not guaranteed."	2019-12-15 06:00:58.460763+00	1	4	1	183081	0	0	0	2019-12-15 06:00:18.830832+00	\N	I don't know if you can prove that any observed behavior is always guaranteed, one way or the other, unless you can manufacture a counter-example. In the absence of that, the way to fix the order that results are returned, of course, is to add an `ORDER BY`.	f	f
454	444	90	2014-06-22 12:30:49+00	According to [Craig Freedman](http://blogs.msdn.com/b/craigfr/about.aspx) the order of execution for the concatenation operator is guaranteed.\r\n\r\nFrom his blog post [Viewing Query Plans](http://blogs.msdn.com/b/craigfr/archive/2006/06/13/629615.aspx) on MSDN Blogs:\r\n\r\n> Note that when an operator has more than one child, the order of the\r\n> children matters.  The topmost child is the first child while the\r\n> bottommost child is the second.  The concatenation operator processes\r\n> the children in this order.\r\n\n\nAnd from books online [Showplan Logical and Physical Operators Reference](http://msdn.microsoft.com/en-us/library/ms191158.aspx)\n\n> The Concatenation physical operator has two or more inputs and one\n> output. Concatenation copies rows from the first input stream to the\n> output stream, then repeats this operation for each additional input\n> stream.	2019-12-14 07:01:13.846676+00	6	4	1	68777	0	0	0	2019-12-14 07:01:13.846676+00	\N	According to [Craig Freedman](http://blogs.msdn.com/b/craigfr/about.aspx) the order of execution for the concatenation operator is guaranteed.	f	f
453	444	609	2013-04-08 16:55:52+00	**No**, there is no documentation from Microsoft guaranteeing the behavior, therefore it is **not guaranteed**.\r\n\r\nAdditionally, assuming that the Simple Talk article is correct, and that the Concatenation physical operator always processes inputs in the order shown in the *plan* (very likely to be true), then without a guarantee that SQL Server will **always** generate plans that keep the same the order between the query text and the query plan, you're only slightly better off.\r\n\r\nWe can investigate this further though.  If the query optimizer was able to reorder the Concatenation operator input, there should exist rows in the undocumented DMV, `sys.dm_exec_query_transformation_stats` corresponding to that optimization.\r\n    \r\n    SELECT * FROM sys.dm_exec_query_transformation_stats \r\n        WHERE name LIKE '%CON%' OR name LIKE '%UNIA%'\r\n\r\nOn SQL Server 2012 Enterprise Edition, this produces 24 rows. Ignoring the false matches for transformations related to constants, there is one transformation related to the Concatenation Physical Operator `UNIAtoCON` (Union All to Concatenation).  So, at the physical operator level, it appears that once a concatenation operator is selected, it will be processed in the order of the logical Union All operator it was derived from.\r\n\r\n---\r\n\r\nIn fact that is not quite true. Post-optimization rewrites exist that can reorder the inputs to a physical Concatenation operator after cost-based optimization has completed. One example occurs when the Concatenation is subject to a row goal (so it may be important to read from the cheaper input first). See [`UNION ALL` Optimization][2] by Paul White for more details.\r\n\r\nThat late physical rewrite was functional up to and including SQL Server 2008 R2, but a regression meant it no longer applied to SQL Server 2012 and later. A [fix has been issued][1] that reinstates this rewrite for SQL Server 2014 and later (not 2012) with query optimizer hotfixes enabled (e.g. trace flag 4199). \r\n\r\n---\r\n\r\nBut about the Logical Union All operator (`UNIA`)?  There is a `UNIAReorderInputs` transformation, which can reorder the inputs.  There are also two physical operators that can be used to implement a logical Union All, `UNIAtoCON` and `UNIAtoMERGE` (Union All to Merge Union).\r\n\r\nTherefore it appears that the query optimizer *can* reorder the inputs for a `UNION ALL`; however, it doesn't appear to be a common transformation (zero uses of `UNIAReorderInputs` on the SQL Servers I have readily accessible. We don't know the circumstances that would make the optimizer use `UNIAReorderInputs`; though it is certainly used when a plan guide or use plan hint is used to force a plan generated using the row goal physical reordered inputs mentioned above.\r\n\r\n>Is there a way to have the engine process more than one input at a time?\r\n\r\nThe Concatenation physical operator can exist within a parallel section of a plan.  With some difficulty, I was able to produce a plan with parallel concatenations using the following query:\r\n\r\n    SELECT userid, regdate  FROM (  --Users table is around 3mil rows\r\n        SELECT  userid, RegDate FROM users WHERE userid > 1000000\r\n        UNION \r\n        SELECT  userid, RegDate FROM users WHERE userid < 1000000\r\n        UNION all\r\n        SELECT userid, RegDate FROM users WHERE userid < 2000000\r\n        ) d ORDER BY RegDate OPTION (RECOMPILE)\r\n\r\nSo, in the strictest sense, the physical Concatenation operator does seem to always process inputs in a consistent fashion (top one first, bottom second); however, the optimizer could switch the order of the inputs before choosing the physical operator, or use a Merge union instead of a Concatenation.\r\n\r\n  [1]: https://support.microsoft.com/en-us/help/4023419\r\n  [2]: https://sqlperformance.com/2017/05/sql-plan/union-all-optimization	2019-12-14 07:01:13.572218+00	5	4	1	39513	0	0	0	2019-12-14 07:01:13.572218+00	\N	**No**, there is no documentation from Microsoft guaranteeing the behavior, therefore it is **not guaranteed**.	f	f
647	409	2	2020-01-22 00:18:06.455966+00	You can now link your SE accounts to your account here. If content has already been imported, it will transfer to your account.\r\n\r\nUnder 'community settings', you should find either a button to link your accounts:\r\n\r\n![link](/image?hash=c79ad81fce967f7876da4d35349204296f199c2b6725ff512b9590517e2a3a3a)\r\n\r\nor, if you are already connected, a link to your SE account profile:\r\n\r\n![already linked](/image?hash=c93761fc6f2a208261ded88a59531c19813f73254380c1f03e24aa72814aa746)\r\n\r\nThere is one unexpected quirk: you need an SO account for this to work. The linking process on the SE side will prompt to create one. This is annoying for those of us who have deleted their SO accounts, but seems to be unavoidable.	2020-01-22 00:21:32.203088+00	7	1	1	\N	0	0	0	\N	\N	You can now link your SE accounts to your account here. If content has already been imported, it will transfer to your account.	f	f
406	404	87	2016-02-17 08:23:42+00	Your intermittent errors probably stem from the explicit `CAST` conversions that you're performing, combined with the `WHERE` filters. For any given query, SQL Server will create an execution plan, which you can think of as a roadmap of operations in a given order. The execution plan for any query can often vary in a number of ways, and SQL Server tries to choose the plan that will be the most efficient for every particular scenario. In this choice, lots of factors will influence how the plan is generated.\r\n\r\nOnce created, the plan is cached, so it can be re-used. If the (verbatim) query isn't re-used any time soon, the data that it depends on is changed significantly or the server is restarted, the cache is cleared and the query plan will be regenerated the next time you run the query. This may very well result in a completely different plan.\r\n\r\nNow, for your query, consider the following example table:\r\n\r\n    Year  DateAsText\r\n    ----  ----------\r\n    ...\r\n    2015  2015-02-28\r\n    2015  2015-02-29\r\n    2015  2015-03-01\r\n    ...\r\n    2016  2016-02-28\r\n    2016  2016-02-29\r\n    2016  2016-03-01\r\n\r\nLet's assume we're running this query:\r\n\r\n    SELECT CAST(DateAsText AS date) AS ProperDate\r\n    FROM dbo.demoTable\r\n    WHERE [Year]=2016;\r\n\r\nNote that we have an invalid date in the table on the "29th" of february 2015, stored as a varchar. But since we're querying 2016, we don't expect to touch that row. However, there are multiple ways to solve this query:\r\n\r\n* Retrieve all the rows where `Year` is 2016, then convert `DateAsText` to a date.\r\n* Retrieve every row, convert `DateAsText` to a date, then filter only rows with `Year` 2016.\r\n\r\nThe example is simplified, but it still illustrates what's going on.\r\n\r\n**Your query**\r\n\r\nTry the following, then go through the results, looking for invalid dates like 31st of november, 0th of january, 29th of february on non-leap-years, negative values (I'm assuming `IS_RESOLVED_DATE` is numeric), etc.\r\n\r\n    SELECT DISTINCT STR(IS_RESOLVED_DATETIME, 16)\r\n    WHERE ISTY_ISSUE_TYPE_NAME LIKE '%HEALTH%' OR\r\n          ISTY_ISSUE_TYPE_NAME LIKE '%COURTESY%' OR\r\n          ISTY_ISSUE_TYPE_NAME LIKE '%INTERNAL%'\r\n    ORDER BY 1;\r\n\r\n**Solution**\r\n\r\nStore dates as dates, store numbers as numbers. It's really that simple. Speak to your software people about this. After all, they type all their variables in C#, why so lax in the database?\r\n\r\n**Short-term**\r\n\r\nYou could replace `CAST()` with `TRY_CAST()` if you're on SQL Server 2012 or newer. This returns a `NULL` value instead of an error whenever the input won't convert.\r\n\r\nOn older SQL Server versions, there's no easy way to do this - you could try a lot of `CASE` conditions *inside* the `CAST()`, making sure that invalid dates are passed to `CASE` as `NULL` values.\r\n\r\n    CAST((CASE WHEN doesntlookright=1 THEN NULL ELSE datecolumn END) AS date)\r\n\r\nPlease note that `TRY_CAST()` (and similar manual workarounds) will produce silent errors in those cases where you actually *do* have an invalid date in your varchar column.\r\n\r\nBased on this question, I also wrote a [blog post][1] that may be of interest.\r\n\r\n\r\n  [1]: http://sqlsunday.com/2016/02/17/intermittent-conversion-issues/	2019-12-11 04:22:13.515322+00	2	4	1	129500	0	0	0	2019-12-11 04:22:13.515322+00	\N	Your intermittent errors probably stem from the explicit `CAST` conversions that you're performing, combined with the `WHERE` filters. For any given query, SQL Server will create an execution plan, which you can think of as a roadmap of operations in a given order. The execution plan for any query can often vary in a number of ways, and SQL Server tries to choose the plan that will be the most efficient for every particular scenario. In this choice, lots of factors will influence how the plan is generated.	f	f
780	396	2	2020-02-10 15:44:17.995768+00	At some point this was partially actioned, but I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/17) so work can begin on the rest, and will update the post here when it is complete.	2020-02-10 15:44:17.995768+00	3	1	1	\N	0	0	0	\N	\N	At some point this was partially actioned, but I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/17) so work can begin on the rest, and will update the post here when it is complete.	t	f
391	385	96	2019-12-09 12:47:07.805437+00	`dot-top`\r\n\r\n...as in dba.top or tex.top.	2019-12-09 12:47:07.805437+00	1	4	2	\N	0	0	0	\N	\N	`dot-top`	f	f
390	385	96	2019-12-09 12:45:40.360948+00	`dot-ta`\r\n\r\n...as in dba.ta or tex.ta.	2019-12-09 12:46:52.557579+00	7	4	2	\N	0	0	0	\N	\N	`dot-ta`	f	f
392	385	96	2019-12-09 12:49:53.666659+00	`dot-xyz`\r\n\r\n...as in dba.xyz or tex.xyz.	2019-12-09 12:49:53.666659+00	0	4	2	\N	0	0	0	\N	\N	`dot-xyz`	f	f
785	385	2	2020-02-10 16:59:58.610224+00	how about reddit style:\r\n\r\nta/databases	2020-02-10 16:59:58.610224+00	0	1	1	\N	0	0	0	\N	\N	how about reddit style:	t	f
756	408	811	2020-02-05 05:43:42.800388+00	Let's allow changing the type as long as the post has neither votes, nor answers.\r\n\r\nAfter that, it requires a mod (or deletion + reposting).	2020-02-05 05:43:42.800388+00	5	1	1	\N	0	0	0	\N	\N	Let's allow changing the type as long as the post has neither votes, nor answers.	f	f
407	405	12	2014-05-10 19:38:53+00	The safest way to do this is to enforce your business rules using the built-in referential integrity constraints, as Alexander Kuznetsov describes in his article, ["Storing intervals of time with no overlaps"][1].\r\n\r\nApplying the techniques listed there to your sample table results in the following script:\r\n\r\n    CREATE TABLE [Z_STORE_TEAM](\r\n        [STORENUM] [int] NOT NULL,\r\n        [TEAM] [varchar](10) NULL,\r\n        [EFFECTIVE] [date] NOT NULL,\r\n        [FINISHED] [date] NULL,\r\n        PRIMARY KEY CLUSTERED \r\n        (\r\n            [STORENUM] ASC,\r\n            [EFFECTIVE] ASC\r\n        )\r\n    ) ON [PRIMARY];\r\n    \r\n    INSERT [Z_STORE_TEAM] \r\n        ([STORENUM], [TEAM], [EFFECTIVE], [FINISHED]) \r\n    VALUES \r\n        (1, N'1', CAST(0x01380B00 AS Date), CAST(0x81380B00 AS Date)),\r\n        (1, N'2', CAST(0x81380B00 AS Date), NULL),\r\n        (2, N'1', CAST(0x01380B00 AS Date), NULL);\r\n\r\nModifications:\r\n\r\n    -- New column to hold the previous finish date\r\n    ALTER TABLE dbo.Z_STORE_TEAM \r\n    ADD PreviousFinished date NULL;\r\n    GO\r\n    -- Populate the previous finish date\r\n    UPDATE This\r\n    SET PreviousFinished = Previous.FINISHED\r\n    FROM dbo.Z_STORE_TEAM AS This\r\n    CROSS APPLY\r\n    (\r\n        SELECT TOP (1) \r\n            Previous.FINISHED\r\n        FROM dbo.Z_STORE_TEAM AS Previous\r\n        WHERE \r\n            Previous.STORENUM = This.STORENUM\r\n            AND Previous.FINISHED <= This.EFFECTIVE\r\n        ORDER BY \r\n            Previous.FINISHED DESC\r\n    ) AS Previous;\r\n    GO\r\n    ALTER TABLE dbo.Z_STORE_TEAM \r\n    ADD CONSTRAINT UQ_STORENUM_PreviousFinished\r\n    UNIQUE (STORENUM, PreviousFinished);\r\n    GO\r\n    ALTER TABLE dbo.Z_STORE_TEAM\r\n    ADD CONSTRAINT CK_PreviousFinished_NotAfter_Effective\r\n    CHECK (PreviousFinished = EFFECTIVE);\r\n    GO\r\n    ALTER TABLE dbo.Z_STORE_TEAM\r\n    ADD CONSTRAINT UQ_STORENUM_FINISHED\r\n    UNIQUE (STORENUM, FINISHED);\r\n    GO\r\n    ALTER TABLE dbo.Z_STORE_TEAM\r\n    ADD CONSTRAINT FK_STORENUM_PreviousFinished\r\n    FOREIGN KEY (STORENUM, PreviousFinished)\r\n    REFERENCES dbo.Z_STORE_TEAM (STORENUM, FINISHED);\r\n    GO\r\n    ALTER TABLE dbo.Z_STORE_TEAM\r\n    ADD CONSTRAINT CK_EFFECTIVE_Before_FINISHED\r\n    CHECK (EFFECTIVE < FINISHED);\r\n\r\nAn attempt to insert the fourth row of sample data now fails with an error message:\r\n\r\n    INSERT [Z_STORE_TEAM] \r\n        ([STORENUM], [TEAM], [EFFECTIVE], [FINISHED]) \r\n    VALUES \r\n        (2, N'2', '20140201', NULL);\r\n\r\n![Error message][2]\r\n\r\nPlease read Alex's article to understand how these constraints ensure your table data will always be valid. Having a set of constraints enforce your data integrity means no trigger code is required.\r\n\r\nRelated article by the same author:\r\n\r\n[Modifying Contiguous Time Periods in a History Table][3]\r\n\r\n\r\n  [1]: https://web.archive.org/web/20180424214356/http://sqlblog.com/blogs/alexander_kuznetsov/archive/2009/03/08/storing-intervals-of-time-with-no-overlaps.aspx\r\n  [2]: https://i.stack.imgur.com/JRjsX.png\r\n  [3]: https://www.simple-talk.com/content/article.aspx?article=1191	2019-12-11 04:26:56.484777+00	1	4	1	64902	0	0	0	2019-12-11 04:26:56.484777+00	\N	The safest way to do this is to enforce your business rules using the built-in referential integrity constraints, as Alexander Kuznetsov describes in his article, ["Storing intervals of time with no overlaps"][1].	f	f
545	486	115	2019-12-27 21:04:39.452346+00	I'm just someone who stumbled across this site already in progress.  So others can provide better answers, but as a start:\r\n\r\n[This blog post](https://topanswers.xyz/meta?q=1) (thanks samcarter for pointing it out in chat) lays out the general goals, including:\r\n\r\n> We will not go down the for-profit route, and will apply for charitable status as soon as any income from donations exceeds the basic level needed to run a CIO in the UK as well as paying our hosting costs. This is not a high bar.\r\n\r\n> We will not diversify into other profit-seeking areas and try to leverage the community here to promote them. If we branch out at all, those activities will:\r\n\r\n> - be compatible with focussed Q&A and building a library of knowledge for the good of everyone (for example, services like db<>fiddle)\r\n\r\n> - also be not for profit\r\n\r\n> If you join this community and contribute your time and energy towards the Q&A here, we will never treat you as 'the product'. We want a real community with genuinely shared priorities.\r\n\r\n> As much as possible of the platform will be open-source and publically available.\r\n\r\nThis isn't an SE clone, but it's a Q&A site and Q&A sites have some common properties.  Here are some things I noticed, as an SE user, being different and interesting here:\r\n\r\n- Chat is tightly integrated.  Each question (or blog post) has its own chat, which is displayed alongside the main page.  This replaces comments.\r\n\r\n- Blog posts and (per-site) meta posts are integrated into the main site, not separate areas.  (This meta here is more of a network meta.  While you only see one site, Databases, there's at least one private beta running and presumably there will be more.  TopAnswers aims to be a network.)\r\n\r\n- Instead of upvotes and downvotes you can star posts, and a user's score is just the total number of stars.  Yes that means no downvotes, and there's a meta thread about whether that should be true.  Interesting difference from other sites: as your score increases you can give *more than one star* to a post.  I personally find this a fascinating experiment; it more or less grants experts (to the extent that total score, or reputation on SE, means expertise) the *ability* but not the *requirement* to cast weighted votes.\r\n\r\n- Account management is currently, um, primitive IMO.  I hope this will improve.  The reason for the roundabout stuff with tokens is so that TopAnswers retains zero personal information about you, but I think this will need to evolve to draw in users who are less geeky/hardcore.\r\n\r\nI'm excited by TopAnswers and also by Codidact.  (And yes, the teams are aware of each other and cooperating.)  I'm here (and there) because I want to see *community-driven* Q&A, where the community is the reason for a site's existence and not a byproduct.  I don't think either of these sites is trying to *compete* with SE; we're doing something different, and all who are interested should feel free to join in.\r\n\r\nJack, is that about right?\r\n\r\n	2019-12-27 21:05:06.773111+00	15	4	1	\N	0	0	0	\N	\N	I'm just someone who stumbled across this site already in progress.  So others can provide better answers, but as a start:	f	f
556	501	167	2020-01-01 19:05:38.032638+00	One can use the `totcount` package to store the footnote counter in an auxiliary file:\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\usepackage{totcount}\r\n\\regtotcounter[auxfile=\\jobname.tot]{footnote}\r\n\r\n\\begin{document}\r\n\r\n\r\ntest\\footnote{text}\r\n\r\n\\newpage\r\n\r\ntest\\footnote{text}\r\n\r\n\\end{document}\r\n```\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\makeatletter\r\n\\input{document.tot}\r\n\\addtocounter{footnote}{\\thefootnote@totc}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\r\ntest\\footnote{text}\r\n\r\n\\end{document}\r\n```	2020-01-01 19:05:38.032638+00	0	4	3	\N	0	0	0	\N	\N	One can use the `totcount` package to store the footnote counter in an auxiliary file:	f	f
364	353	2	2019-12-06 17:13:00.487026+00	This is now pretty much covered by flagging and the 'post cleanup crew' who can confirm flags.\r\n\r\nMore details on [this other meta post](https://topanswers.xyz/meta?q=182#a402):\r\n\r\n\r\n> In normal use I expect something like the following to happen:  \r\n>   \r\n> 1. Several users flag a post drawing it to the attention of the 'crew'. Comments are made in the question chat room  if necessary.  \r\n> 2. The first two 'crew' members to visit the post confirm the flags and the post is then effectively deleted (invisible to everyone except 'crew' and the OP).	2019-12-24 09:53:30.970958+00	8	1	1	\N	0	0	0	\N	\N	This is now pretty much covered by flagging and the 'post cleanup crew' who can confirm flags.	f	f
372	372	262	2019-12-07 14:52:53.872583+00	How about "**The Ducks' Pond**"?	2019-12-07 14:52:53.872583+00	2	4	3	\N	0	0	0	\N	\N	How about "**The Ducks' Pond**"?	f	f
371	372	2	2019-12-07 13:15:24.24633+00	I've changed the room title to TopTeX, but please ping me in here if you end up voting another suggestion higher!	2019-12-07 13:15:24.24633+00	7	1	1	\N	0	0	0	\N	\N	I've changed the room title to TopTeX, but please ping me in here if you end up voting another suggestion higher!	f	f
387	372	234	2019-12-08 17:15:26.030485+00	Anything is fine as long as it contains "burrow". ;-)	2019-12-08 17:15:26.030485+00	1	4	3	\N	0	0	0	\N	\N	Anything is fine as long as it contains "burrow". ;-)	f	f
242	280	16	2019-06-04 16:10:26+00	This is how I'd do it:\r\n\r\n    SELECT      *\r\n    FROM        #MyTable AS mt\r\n    CROSS APPLY (   SELECT COUNT(DISTINCT mt2.Col_B) AS dc\r\n                    FROM   #MyTable AS mt2\r\n                    WHERE  mt2.Col_A = mt.Col_A\r\n                    -- GROUP BY mt2.Col_A \r\n                ) AS ca;\r\n\r\nThe `GROUP BY` clause is redundant given the data provided in the question, but may give you a better execution plan. See the follow-up Q & A [CROSS APPLY produces outer join](https://topanswers.xyz/databases?q=256).\r\n\r\nConsider voting for [OVER clause enhancement request - DISTINCT clause for aggregate functions][1] on the feedback site if you would like that feature added to SQL Server.\r\n\r\n  [1]: https://feedback.azure.com/forums/908035-sql-server/suggestions/32890510-over-clause-enhancement-request-distinct-clause	2019-12-04 14:41:59.313828+00	2	4	1	239795	0	0	0	2019-12-04 14:24:44.393344+00	\N	This is how I'd do it:	f	f
643	589	37	2020-01-21 17:38:23.078852+00	You could modify either the entire database to use a case-sensitive collation, or just the single column involved.  Probably easiest to modify just the column involved, but if case sensitivity is a concern, probably safest to modify the entire database.\r\n\r\nCheck the following example:\r\n\r\n```\r\nDROP TABLE #sheep;\r\nDROP TABLE #names;\r\ncreate table #names (fct_name varchar(255) COLLATE SQL_Latin1_General_CP1_CS_AS)\r\ninsert into #names (fct_name)\r\nvalues \r\n('Angus McFife'),\r\n('Angus Mcfife'),\r\n('Willie McNulty')\r\n\r\ncreate table #sheep (farmer varchar(255) COLLATE SQL_Latin1_General_CP1_CS_AS, sheep int)\r\ninsert into #sheep (farmer, sheep)\r\nvalues\r\n('Angus McFife', 35),\r\n('Willie McNulty', 52),\r\n('Angus Mcfife', 16)\r\n\r\nselect fct_name, sum(s.sheep) as herd\r\nfrom #names n\r\ninner join #sheep s on n.fct_name = s.farmer \r\ngroup by n.fct_name;\r\n```\r\n\r\nThe results show:\r\n\r\n```\r\n╔════════════════╦══════╗\r\n║    fct_name    ║ herd ║\r\n╠════════════════╬══════╣\r\n║ Angus McFife   ║   35 ║\r\n║ Angus Mcfife   ║   16 ║\r\n║ Willie McNulty ║   52 ║\r\n╚════════════════╩══════╝\r\n```\r\n\r\nAs you can see, the two Angus' have discrete rows. \r\n\r\nHowever, since you don't want to modify the table structures, you could instead modify the query to be case-sensitive, like this:\r\n\r\n```\r\nDROP TABLE #sheep;\r\nDROP TABLE #names;\r\ncreate table #names (fct_name varchar(255))\r\ninsert into #names (fct_name)\r\nvalues \r\n('Angus McFife'),\r\n('Angus Mcfife'),\r\n('Willie McNulty')\r\n\r\ncreate table #sheep (farmer varchar(255), sheep int)\r\ninsert into #sheep (farmer, sheep)\r\nvalues\r\n('Angus McFife', 35),\r\n('Willie McNulty', 52),\r\n('Angus Mcfife', 16)\r\n\r\nselect fct_name COLLATE SQL_Latin1_General_CP1_CS_AS, sum(s.sheep) as herd\r\nfrom #names n\r\ninner join #sheep s on n.fct_name = s.farmer COLLATE SQL_Latin1_General_CP1_CS_AS\r\ngroup by n.fct_name COLLATE SQL_Latin1_General_CP1_CS_AS\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n\r\n╔══════════════════╦══════╗\r\n║ (No column name) ║ herd ║\r\n╠══════════════════╬══════╣\r\n║ Angus McFife     ║   35 ║\r\n║ Angus Mcfife     ║   16 ║\r\n║ Willie McNulty   ║   52 ║\r\n╚══════════════════╩══════╝\r\n```	2020-01-21 17:40:16.399633+00	2	4	1	\N	0	0	0	\N	\N	You could modify either the entire database to use a case-sensitive collation, or just the single column involved.  Probably easiest to modify just the column involved, but if case sensitivity is a concern, probably safest to modify the entire database.	f	f
502	464	167	2019-12-19 15:03:56.926679+00	As a workaround one could use a simple user script\r\n\r\n```\r\n// ==UserScript==\r\n// @name         toptex\r\n// @match        https://topanswers.xyz/*\r\n// @run-at       document-start\r\n// @grant        none\r\n// ==/UserScript==\r\n\r\n(function() {\r\n    'use strict';\r\n    \r\n    document.cookie = "uuid=abcdef";\r\n        \r\n})();\r\n```\r\n\r\nin which `abcdef` is the login key taken from an existing cookie.\r\n\r\n\r\nThis will create the login cookie every time you visit topanswers.xyz, so if you are not logged in, reload the page and you will be logged in. \r\n\r\nTo get the key from an existing cookie, one can for example use https://github.com/Rob--W/cookie-manager\r\n	2019-12-19 15:10:49.416939+00	6	4	2	\N	0	0	0	\N	\N	As a workaround one could use a simple user script	f	f
591	255	37	2019-05-30 19:42:49+00	In order for SQL Server to create a *non-unique* clustered index, a hidden "column" is added to the physical structure of the clustered index.  That hidden column is known as the uniqifier, and as its name implies, provides a mechanism to ensure that every row in the clustered index is unique.\r\n\r\nWhen you see that column show up in a query plan, it's a great indicator that the clustering key columns have not been defined as unique.  Possibly that's because the combination of columns is known to be not unique.  It's also possible the designer of the table simply forgot to add the `UNIQUE` qualifier to the `CREATE CLUSTERED INDEX` statement.\r\n\r\nIn fact, if we recreate the repro above with a unique clustered index, the `Uniq1002` column no longer appears in the query plan:\r\n\r\n```\r\nUSE tempdb;\r\n\r\nIF OBJECT_ID(N'dbo.t', N'U') IS NOT NULL\r\nDROP TABLE dbo.t\r\nGO\r\nCREATE TABLE dbo.t\r\n(\r\n    id int NOT NULL \r\n        PRIMARY KEY \r\n        NONCLUSTERED \r\n        IDENTITY(1,1)\r\n    , col1 datetime NOT NULL\r\n    , col2 varchar(800) NOT NULL\r\n    , col3 int NULL\r\n    , col4 sysname NULL\r\n);\r\n\r\nINSERT INTO dbo.t (\r\n      col1\r\n    , col2\r\n    , col3\r\n    , col4\r\n    ) \r\nSELECT TOP(100000) \r\n      CONVERT(datetime, DATEADD(DAY, CONVERT(int, CRYPT_GEN_RANDOM(1)), '2000-01-01 00:00:00'))\r\n    , replicate('A', 800)\r\n    , CONVERT(int, CRYPT_GEN_RANDOM(4))\r\n    , CONVERT(sysname, CHAR(65 + CRYPT_GEN_RANDOM(1) % 26) \r\n        + CHAR(65 + CRYPT_GEN_RANDOM(1) % 26) \r\n        + CHAR(65 + CRYPT_GEN_RANDOM(1) % 26))\r\nFROM sys.syscolumns sc\r\n    CROSS JOIN sys.syscolumns sc2;\r\n```\r\n\r\n\r\nHere's the UNIQUE clustered index:\r\n\r\n```\r\nCREATE UNIQUE CLUSTERED INDEX t_cx \r\nON dbo.t (col1, col2, col3);\r\n\r\nCREATE INDEX t_c1 ON dbo.t(col4); \r\n```\r\n\r\nAnd the query:\r\n```\r\nSELECT id\r\n    , col1\r\n    , col2\r\n    , col3\r\nFROM dbo.t aad WITH (INDEX = t_c1)\r\nWHERE col4 = N'JSB'\r\n    AND col1 > N'2019-05-30 00:00:00';\r\n```\r\n\r\nThe plan now shows this for the non-clustered index scan output columns:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nWhen a non-unique clustered index is created, the uniqifier is automatically added.  The uniqifier is also added to every non-clustered index, even though you can't "see" it by looking at the properties of the index, or by "scripting" the index.\r\n\r\nThe uniqifier is a four-byte column containing an integer that is automatically incremented behind the scenes for each row inserted into the table.  The first row inserted doesn't require a uniqifier; only rows added after the first row have the uniqifier present.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/DrXA6.png	2020-01-10 14:32:25.158948+00	3	4	1	239490	0	0	0	2020-01-10 14:32:25.158948+00	\N	In order for SQL Server to create a *non-unique* clustered index, a hidden "column" is added to the physical structure of the clustered index.  That hidden column is known as the uniqifier, and as its name implies, provides a mechanism to ensure that every row in the clustered index is unique.	f	f
201	255	12	2019-05-31 11:33:51+00	>Ostensibly, this represents the uniqifier used in the non-unique clustered index. Is that the case?\r\n\r\nYes.\r\n\r\n>What is the purpose of this Uniq1002 column in this index scan?\r\n\r\nEach row in the nonclustered index must be associated with *exactly one row* in the base table so that *Bookmark Lookups* (RID or Key) work correctly. This mapping is provided by the "row locator".\r\n\r\nFor heap tables, the row locator is the RID. For clustered row store tables, it is the clustering key (including the *uniquifier* where necessary).\r\n\r\nFor the *Key Lookup* in your plan to work, it must have access to the row locator. This includes the *uniquifier*, so it must be emitted by the nonclustered index scan.\r\n\r\nThe *uniquifier* is stored in the variable-length portion of the row so it only takes up space when needed (i.e. when a duplicate key actually exists).\r\n\r\n>Is a column named like that always the clustered index uniquifier?\r\n\r\nYes. The uniquifier column is always named `UniqXXXX`. The row locator associated with heap tables is named `BmkXXXX`. The row locator for a columnstore table is named `ColStoreLocXXXX`.\r\n\r\n---\r\n\r\n## Observing the uniquifier\r\n\r\nIt is possible to directly observe the values of the uniquifier on SQL Server versions that contain a functional [`query_trace_column_values` Extended Event][1].\r\n\r\nThis undocumented and unsupported event is in the *Debug* channel. It was introduced with SQL Server 2016, and stopped working around CU11 of SQL Server 2017.\r\n\r\nFor example:\r\n\r\n```\r\nCREATE TABLE #T (c1 integer NULL INDEX ic1 CLUSTERED, c2 integer NULL INDEX ic2 UNIQUE, c3 integer NULL);\r\nGO\r\nINSERT #T\r\n    (c1, c2, c3)\r\nVALUES \r\n    (100, 101, 0),\r\n    (100, 102, 1),\r\n    (100, 103, 2);\r\nGO\r\nDBCC TRACEON (2486);\r\nSET STATISTICS XML ON;\r\nSELECT T.* FROM #T AS T WITH (INDEX(ic2));\r\nSET STATISTICS XML OFF;\r\nDBCC TRACEOFF (2486);\r\nGO\r\nDROP TABLE #T;\r\n```\r\n\r\nHas the plan:\r\n\r\n[![Plan][2]][2]\r\n\r\nIt produces event output like the following on SQL Server 2016:\r\n\r\n[![Event output][3]][3]\r\n\r\n\r\n  [1]: https://www.sqlshack.com/query-trace-column-values/\r\n  [2]: https://i.stack.imgur.com/e1liR.png\r\n  [3]: https://i.stack.imgur.com/W5knS.png	2020-01-10 15:55:23.277739+00	5	4	1	239548	0	0	0	2019-12-04 01:10:13.406865+00	\N	>Ostensibly, this represents the uniqifier used in the non-unique clustered index. Is that the case?	f	f
283	308	16	2018-08-04 18:50:35+00	This will Work Perfectly®\r\n\r\n    WITH XMLNAMESPACES ( 'http://schemas.microsoft.com/sqlserver/2004/07/showplan' AS p )\r\n    SELECT qp.plan_name, \r\n           qp.query_xml.value('count(//p:RelOp/p:NestedLoops/@WithUnorderedPrefetch)', 'int') AS operator_count\r\n    FROM dbo.query_plans AS qp;\r\n\r\nIf you prefer, you could use\r\n\r\n    WITH XMLNAMESPACES (DEFAULT 'http://schemas.microsoft.com/sqlserver/2004/07/showplan')\r\n\r\n...and omit the namespace prefix `p:` from the XQuery expression.	2019-12-05 13:50:25.938954+00	2	4	1	214075	0	0	0	2019-12-05 13:50:25.938954+00	\N	This will Work Perfectly®	f	f
250	287	45	2017-05-27 02:05:01+00	If the data must be stored in SQL Server for whatever reason I can think of a few benefits to storing it in a separate table. Some are more convincing than others.\r\n\r\n 1. Putting the data in a separate table means you can store it in a separate database. This can have advantages for scheduled maintenance. For example, you can run `DBCC CHECKDB` only on the database that contains the BLOB data.\r\n\r\n 2. If you don't always put more than 8000 bytes into the BLOB then it's possible for it to [be stored in-row][1] for some rows. You may not want that because it will slow down queries that access data using the clustered index even if the column isn't needed by the query. Putting the data in a separate table removes this risk.\r\n\r\n 3. When stored off row SQL Server uses an up to 24 byte pointer to point to the new page. That takes up space and limits the total number of BLOB columns you can add to a single table. See srutzky's answer for more details.\r\n\r\n 4. A clustered columnstore index cannot be defined on a table containing a BLOB column. This limitation has been removed will be removed in SQL Server 2017.\r\n\r\n 5. If you eventually decide that the data should be moved outside of SQL Server it may be easier to make that change if the data is already in a separate table.\r\n\r\n  [1]: https://technet.microsoft.com/en-us/library/ms186981(v=sql.105).aspx	2019-12-04 14:30:17.799681+00	2	4	1	174723	0	0	0	2019-12-04 14:30:17.799681+00	\N	If the data must be stored in SQL Server for whatever reason I can think of a few benefits to storing it in a separate table. Some are more convincing than others.	f	f
251	287	16	2017-05-26 13:51:32+00	While I disagree that BLOBs should just be in another table -- they should [not be in the database at all][1]. Store a pointer to where the file lives on disk, and then just get that from the database...\r\n\r\n\r\nThe primary issue they cause (for me) is with indexing. Using XML with query plans, because everyone's got'em, let's make a table:\r\n\r\n\r\n    SELECT TOP 1000\r\n    ID = IDENTITY(INT,1,1),\r\n    deq.query_plan\r\n    INTO dbo.index_test\r\n    FROM sys.dm_exec_cached_plans AS dec\r\n    CROSS APPLY sys.dm_exec_query_plan(dec.plan_handle) AS deq\r\n\r\n    ALTER TABLE dbo.index_test ADD CONSTRAINT pk_id PRIMARY KEY CLUSTERED (ID)\r\n\r\n\r\nIt's only 1000 rows, but [checking on the size][2]...\r\n\r\n\r\n    sp_BlitzIndex @DatabaseName = 'StackOverflow', @SchemaName = 'dbo', @TableName = 'index_test'\r\n\r\n\r\nIt's over 40 MB for just 1000 rows. Assuming you add 40 MB every 1000 rows, that can get pretty ugly pretty quickly. What happens when you hit 1 million rows? That's just about 1 TB of data, there.\r\n\r\n\r\n[![NUTS][3]][3]\r\n\r\n\r\nAny queries that need to use your clustered index now need to read all of that BLOB data into memory *clarification:* when the BLOB data column is referenced.\r\n\r\n\r\nCan you think of better ways to use SQL Server memory than storing BLOBs? Because I sure can.\r\n\r\n\r\nExpanding it to nonclustered indexes:\r\n\r\n\r\n    CREATE INDEX ix_noblob ON dbo.index_test (ID)\r\n    \r\n    CREATE INDEX ix_returnoftheblob ON dbo.index_test (ID) INCLUDE (query_plan)\r\n\r\n\r\nYou can design your nonclustered indexes to largely avoid the BLOB column so regular queries can avoid the clustered index, but as soon as you need that BLOB column, you need the clustered index. \r\n\r\n\r\nIf you add it as an `INCLUDED` column to a nonclustered index to avoid a key lookup scenario, you end up with gigantic nonclustered indexes:[![enter image description here][4]][4]\r\n\r\nMore problems they cause:\r\n\r\n- If anyone runs a `SELECT *` query, they get all that BLOB data.\r\n- They take up space in backups and restores, slowing them down\r\n- They slow down `DBCC CHECKDB`, because I know you're checking for corruption, right?\r\n- And if you do any index maintenance, they slow that down as well.\r\n\r\nHope this helps!\r\n\r\n  [1]: https://www.brentozar.com/archive/2015/03/no-more-blobs/\r\n  [2]: http://firstresponderkit.org\r\n  [3]: https://i.stack.imgur.com/Ug9ST.jpg\r\n  [4]: https://i.stack.imgur.com/N1CMi.jpg	2019-12-04 14:30:18.115158+00	2	4	1	174680	0	0	0	2019-12-04 14:30:18.115158+00	\N	While I disagree that BLOBs should just be in another table -- they should [not be in the database at all][1]. Store a pointer to where the file lives on disk, and then just get that from the database...	f	f
249	287	158	2017-05-30 15:25:07+00	How large are these images, and how many do you expect to have? While I mostly agree with [@sp_BlitzErik][1], I think there are some scenarios where it is ok to do this, and so it would help to have a clearer picture of what is actually being requested here.\r\n\r\nSome options to consider that alleviate most of the negative aspects pointed out by Erik are:\r\n\r\n* [FILESTREAM][2] (starting in SQL Server 2008)\r\n* [FileTables][3] (starting in SQL Server 2012)\r\n\r\nBoth of these options are designed to be a middle-ground between storing BLOBs either fully in SQL Server or fully outside (except for a string colun to retain the path). They allow for BLOBs to be a part of the data model and participate in Transactions while not wasting space in the buffer pool (i.e. memory). The BLOB data is still included in backups, which does make them take up more space and take longer to backup _and_ to restore. However, I have a hard time seeing this as a true negative given that if it is part of the app then it needs to be backed up somehow, and having only a string column containing the path is completely disconnected and allows for BLOBs files to get deleted with no indication of that in the DB (i.e. invalid pointers / missing files). It also allows for files to be "deleted" within the DB but still exist on the file system which will need to eventually be cleaned up (i.e. headache). But, if the files are HUGE, then maybe it is best to leave entirely outside of SQL Server except for the path column.\r\n\r\nThat helps with the "inside or outside" question, but does not touch on the single table vs multiple table question. I can say that, beyond this specific question, there are certainly valid cases for splitting tables into groups of columns based on usage patterns. Often when one has 50 or more columns there are some that are accessed frequently and some that are not. Some columns are written to frequently while some are mostly read. Separating frequently access vs infrequently accessed columns into multiple tables having a 1:1 relationship is quite often beneficial because why waste the space in the Buffer Pool for data you probably aren't using (similar to why storing large images in regular `VARBINARY(MAX)` columns is a problem)? You also increase the performance of the frequently access columns by reducing the row size and hence fitting more rows onto a data page, making reads (both physical and logical) more efficient. Of course, you also introduce some inefficiency by needing to duplicate the PK, and now sometimes you need to join the two tables, which also complicates (even if only slightly) some queries.\r\n\r\nSo, there are several approaches you could take, and what is best depends on your environment and what you are trying to accomplish.\r\n\r\n---\r\n\r\n> I was under the impression that SQL Server only stores a pointer to some dedicated BLOB data structure in the table\r\n\r\nNot so simple. You can find some good info here, [What is the Size of the LOB Pointer for (MAX) Types Like Varchar, Varbinary, Etc?][4], but the basics are:\r\n\r\n* `TEXT`, `NTEXT`, and `IMAGE` datatypes (by default): 16 byte pointer\r\n* `VARCHAR(MAX)`, `NVARCHAR(MAX)`, `VARBINARY(MAX)` (by default):\r\n  * If the data can fit in the row, then it will be placed there\r\n  * If the data is less than approx. 40,000 bytes (the linked blog post shows 40,000 as the upper limit but my testing showed a slightly higher value) _AND_ if there is room on the row for this structure, then there will be between 1 and 5 direct links to LOB pages, starting at 24 bytes for the first link to the first 8000 bytes, and going up by 12 bytes per each additional link for each additional set of 8000 bytes, up to 72 bytes max.\r\n  * If the data is over approx. 40,000 bytes _OR_ there is not enough room to store the appropriate number of direct links (e.g. only 40 bytes left on the row and a 20,000 byte value needs 3 links which is 24 bytes for the first plus 12 for the two additional links for 48 bytes total required in-row space), then there will just be a 24 byte pointer to a text tree page which contains the links to the LOB pages).\r\n\r\n\r\n  [1]: https://dba.stackexchange.com/a/174680/30859\r\n  [2]: https://docs.microsoft.com/en-us/sql/relational-databases/blob/filestream-sql-server\r\n  [3]: https://docs.microsoft.com/en-us/sql/relational-databases/blob/filetables-sql-server\r\n  [4]: http://improve.dk/what-is-the-size-of-the-lob-pointer-for-max-types-like-varchar-varbinary-etc/	2019-12-04 14:30:17.5258+00	2	4	1	174921	0	0	0	2019-12-04 14:30:17.5258+00	\N	How large are these images, and how many do you expect to have? While I mostly agree with [@sp_BlitzErik][1], I think there are some scenarios where it is ok to do this, and so it would help to have a clearer picture of what is actually being requested here.	f	f
446	437	751	2012-03-12 11:49:11+00	The constant scans each produce a single in-memory row with no columns. The top compute scalar outputs a single row with 3 columns\r\n\r\nExpr1005 |   Expr1006 |   Expr1004\r\n---------| -----------| -----------\r\nNULL     |   NULL     |   60\r\n\r\nThe bottom compute scalar outputs a single row with 3 columns\r\n\r\nExpr1008   | Expr1009  |  Expr1007\r\n-----------| ----------| -----------\r\nNULL       | 1048576   |     10\r\n\r\nThe concatenation operator Unions these 2 rows together and outputs the 3 columns but they are now renamed \r\n\r\nExpr1010  |  Expr1011  |  Expr1012\r\n----------| -----------| -----------\r\nNULL      |  NULL      |  60\r\nNULL      |  1048576   |  10\r\n\r\nThe `Expr1012` column is a set of flags [used internally to define certain seek properties for the Storage Engine][1].\r\n\r\nThe next compute scalar along outputs 2 rows\r\n\r\nExpr1010   | Expr1011   | Expr1012  |  Expr1013  |  Expr1014  |  Expr1015\r\n-----------| -----------| ----------| -----------| -----------| -----------\r\nNULL       | NULL       | 60        |  True      |  4         |  16            \r\nNULL       | 1048576    | 10        |  False     |  0         |  0      \r\n\r\n\r\nThe last three columns are defined as follows and are just used for sorting purposes prior to presenting to the Merge Interval Operator\r\n\r\n``` none\r\n[Expr1013] = Scalar Operator(((4)&[Expr1012]) = (4) AND NULL = [Expr1010]), \r\n[Expr1014] = Scalar Operator((4)&[Expr1012]), \r\n[Expr1015] = Scalar Operator((16)&[Expr1012])\r\n```\r\n\r\n`Expr1014` and `Expr1015` just test whether certain bits are on in the flag. \r\n`Expr1013` appears to return a boolean column true if both the bit for `4` is on and `Expr1010` is `NULL`. \r\n\r\nFrom trying other comparison operators in the query I get these results:\r\n\r\n| Operator | Expr1010 | Expr1011 | Flags (Dec) | 32 | 16 | 8 | 4 | 2 | 1 |\r\n|----------|----------|----------|-------------|----|----|---|---|---|---|\r\n| >        | 1048576  | NULL     |           6 |  0 |  0 | 0 | 1 | 1 | 0 |\r\n| >=       | 1048576  | NULL     |          22 |  0 |  1 | 0 | 1 | 1 | 0 |\r\n| <=       | NULL     | 1048576  |          42 |  1 |  0 | 1 | 0 | 1 | 0 |\r\n| <        | NULL     | 1048576  |          10 |  0 |  0 | 1 | 0 | 1 | 0 |\r\n| =        | 1048576  | 1048576  |          62 |  1 |  1 | 1 | 1 | 1 | 0 |\r\n| IS NULL  | NULL     | NULL     |          60 |  1 |  1 | 1 | 1 | 0 | 0 |\r\n\r\nFrom which I infer that Bit 4 means "Has start of range" (as opposed to being unbounded) and Bit 16 means the start of the range is inclusive.\r\n\r\nThis 6 column result set is emitted from the `SORT` operator sorted by \r\n`Expr1013 DESC, Expr1014 ASC, Expr1010 ASC, Expr1015 DESC`. Assuming `True` is represented by `1` and `False` by `0` the previously represented resultset is already in that order.\r\n\r\nBased on my previous assumptions the net effect of this sort is to present the ranges to the merge interval in the following order\r\n\r\n     ORDER BY \r\n              HasStartOfRangeAndItIsNullFirst,\r\n              HasUnboundedStartOfRangeFirst,\r\n              StartOfRange,\r\n              StartOfRangeIsInclusiveFirst\r\n\r\nThe merge interval operator outputs 2 rows\r\n\r\nExpr1010   | Expr1011  |  Expr1012\r\n-----------| ----------| -----------\r\nNULL       | NULL      |  60\r\nNULL       | 1048576   |  10\r\n\r\n\r\nFor each row emitted a range seek is performed \r\n\r\n``` none\r\nSeek Keys[1]: Start:[dbo].[t].c2 > Scalar Operator([Expr1010]), \r\n              End: [dbo].[t].c2 < Scalar Operator([Expr1011])\r\n```\r\n\r\nSo it would appear as though two seeks are performed. One apparently `> NULL AND < NULL` and one  `> NULL AND < 1048576`. However the flags that get passed in appear to modify this to `IS NULL` and `< 1048576` respectively.\r\n\r\n60 is for a comparison with NULL.  The range boundary expressions use NULL to represent 'unbounded' at either end.  The seek is always exclusive i.e. `Start: > Expr & End: < Expr` rather than inclusive using `>=` and `<=`.\r\n\r\nIf you change the query slightly to:\r\n\r\n    select *\r\n    from t \r\n    where \r\n          c2 > 1048576 \r\n       or c2 = 0\r\n    ;\r\n\r\nThen the plan looks much simpler with an index seek with multiple seek predicates. \r\n\r\nThe plan shows `Seek Keys` \r\n\r\n``` none\r\nStart: c2 >= 0, End: c2 <= 0, \r\nStart: c2 > 1048576\r\n```\r\n\r\nThe explanation for why this simpler plan cannot be used for the case in the OP is given by Paul White in the comments to the [earlier linked blog post][1].\r\n\r\nAn index seek with multiple predicates cannot mix different types of comparison predicate (ie. `Is` and `Eq` in the case in the OP). This is just a current limitation of the product (and is presumably the reason why the equality test in the last query `c2 = 0` is implemented using `>=` and `<=` rather than just the straightforward equality seek you get for the query `c2 = 0 OR c2 = 1048576`.\r\n\r\n  [1]: https://sqlkiwi.blogspot.com/2012/01/dynamic-seeks-and-hidden-implicit-conversions.html\r\n  [2]: https://dba.stackexchange.com/users/1192/paul-white	2019-12-13 10:07:07.602751+00	8	4	1	14812	0	0	0	2019-12-13 09:53:54.056734+00	\N	The constant scans each produce a single in-memory row with no columns. The top compute scalar outputs a single row with 3 columns	f	f
505	469	12	2019-12-21 09:06:23+00	## Documentation\r\n\r\nThere is no documentation I am aware of for metadata-only `ALTER COLUMN` operations on compressed tables, beyond the [very general statement][1]:\r\n\r\n>Changes you specify in ALTER TABLE implement immediately. If the changes require modifications of the rows in the table, ALTER TABLE updates the rows.\r\n\r\nOne could interpret that as meaning that SQL Server may avoid a size-of-data operation if the particular storage format in use means the data does not change. Continuing in that generous vein, one could argue that whether a particular change is metadata-only or not depends on what has been implemented as well as what is logically possible.\r\n\r\nDocumentation notes were added when SQL Server 2012 added support for adding a `NOT NULL` column with a runtime constant default value, and the documentation has been updated for new `ONLINE` operations. If you feel documentation would be useful for metadata-only data type changes on compressed tables, you should [submit feedback][2]. I would not personally want to be the person tasked with that effort, or keeping it up to date.\r\n\r\nI asked my contacts at Microsoft about this, and they said they are looking at ways to improve the documentation. It is tricky to document well given the number of variables involved. Still, I hope we will see improvements soon.\r\n\r\n## Does SQL Server 2017 improve this?\r\n\r\nFrom my testing, the new behaviour was enabled in SQL Server 2016. It can be disabled with undocumented startup or global trace flag 3618:\r\n\r\n```\r\nDBCC TRACEON (3618, -1);\r\n```\r\n\r\nA demo:\r\n\r\n```\r\nCREATE TABLE dbo.test1\r\n(\r\n  c1 bigint NOT NULL,\r\n  c2 integer NOT NULL,\r\n  c3 char(4) NOT NULL\r\n  primary key (c1)\r\n)\r\nWITH (DATA_COMPRESSION = ROW);\r\n\r\nINSERT dbo.test1 WITH (TABLOCK)\r\n(\r\n    c1,\r\n    c2,\r\n    c3\r\n)\r\nSELECT\r\n    N.n,\r\n    CONVERT(integer, N.n),\r\n    LEFT(N.n, 4)\r\nFROM\r\n(\r\n    SELECT \r\n        n = ROW_NUMBER() OVER (ORDER BY @@SPID) \r\n    FROM sys.all_columns AS AC1\r\n    CROSS JOIN sys.all_columns AS AC2\r\n    ORDER BY n\r\n    OFFSET 0 ROWS\r\n    FETCH FIRST 5 * 1000 * 1000 ROWS ONLY\r\n) AS N;\r\n\r\n-- Both metadata only\r\nALTER TABLE dbo.test1 ALTER COLUMN c2 bigint NOT NULL;\r\nALTER TABLE dbo.test1 ALTER COLUMN c3 char(8) NOT NULL;\r\n\r\nDROP TABLE dbo.test1;\r\n```\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/statements/alter-table-transact-sql#locks-and-alter-table\r\n  [2]: https://docs.microsoft.com/en-us/sql/sql-server/sql-server-get-help	2020-01-28 13:00:59.924471+00	4	4	1	256094	0	0	0	2019-12-21 12:41:29.095289+00	\N	Documentation	f	f
17	29	1	2019-11-13 14:15:22.713074+00	# New New Chat\r\n\r\n[First referenced][1] on 2019-11-06 in [stop the merry-go-round][2].\r\n\r\nA call for replacing this site's chat anytime a bug or limitation is discovered.  An arbitrary number of "New"s can be added as needed.\r\n\r\n[1]: https://topanswers.xyz/transcript?room=4&year=2019&month=11&day=6#c770\r\n[2]: https://topanswers.xyz/databases?room=4	2019-11-13 20:53:48.111625+00	6	1	1	\N	0	0	0	\N	\N	New New Chat	f	f
15	29	1	2019-11-13 14:14:18.638294+00	# 6 to 8 days\r\n\r\n[First referenced][1] on 2019-11-12 in [stop the merry-go-round][2].\r\n\r\nA not so subtle dig at Stack Overflow's turnaround time for new features and bug fixes.\r\n\r\n[1]: /transcript?room=4&id=2063#c2063\r\n[2]: /databases?room=4	2019-11-14 00:39:42.044189+00	9	1	1	\N	0	0	0	\N	\N	6 to 8 days	f	f
43	29	14	2019-11-18 15:35:53.829095+00	# Needs Refresh\r\n\r\n[First referenced][1] on 2019-10-31 in [The Heap][2].\r\n\r\nA reminder from the development team to refresh a webpage in order to see a specific bug fix or feature change.  Implies the reader doesn't know how websites work.\r\n\r\n[1]: https://topanswers.xyz/transcript?room=2&year=2019&month=10#c553\r\n[2]: https://topanswers.xyz/databases?room=2	2019-11-18 15:35:53.829095+00	9	4	1	\N	0	0	0	\N	\N	Needs Refresh	f	f
16	29	1	2019-11-13 14:14:39.628553+00	# Room 3\r\n\r\n[First referenced][1] on 2019-11-07 in [stop the merry-go-round][2].\r\n\r\nAn inaccessible, and obviously nefarious, chatroom. A clear sign that TopAnswers has already become an evil corporation.\r\n\r\n[1]: https://topanswers.xyz/transcript?room=4&id=859#c859\r\n[2]: https://topanswers.xyz/databases?room=4	2019-11-13 23:12:03.310783+00	10	1	1	\N	0	0	0	\N	\N	Room 3	f	f
472	428	2	2019-12-15 09:10:24.25057+00	This (mostly) isn't an issue right now, because…\r\n\r\n> 1. Helping potential attacks gain further vectors\r\n\r\nProbably not — the only information that can leak would be information that the logged in user would be able to see anyway (for example their own login key). Still not ideal but nowhere near as bad.\r\n\r\nOne reason for this is that the front-end code has restricted access to the database — it can't see the actual tables underneath, only views and functions that are 'aware' of who the logged in user is. For example see this table and view:\r\n\r\n```sql\r\ncreate table chat_notification(\r\n  chat_id bigint references chat\r\n, account_id integer references account\r\n, chat_notification_at timestamptz not null default current_timestamp\r\n, primary key (chat_id,account_id)\r\n);\r\n```\r\n\r\n```sql\r\ncreate view chat_notification with (security_barrier) as\r\nselect chat_id,chat_notification_at\r\nfrom db.chat_notification\r\nwhere account_id=current_setting('custom.account_id',true)::integer;\r\n```\r\n\r\n> 2. Confusing the user\r\n\r\nYes — and perhaps we should dump a generic message instead if the user isn't registered.\r\n\r\n> 3. Lack of "What am I meant to do?" from the perspective of the user?\r\n\r\nIt's very useful having that error message in full with bug reports (like the one you linked to). Even if the user doesn't know what the error means, as long as they pass it back to us in full, we have a much better chance of diagnosing and fixing the problem than if we just returned a 500 'something went wrong' message.\r\n\r\nOf course further down the line we'll have to do that but for now I think the balance between risk and reward is towards keeping the full error message dumps.	2019-12-15 14:52:03.491211+00	3	1	1	\N	0	0	0	\N	\N	This (mostly) isn't an issue right now, because…	f	f
487	454	12	2014-12-11 19:08:42+00	>*Are actual stored procedures the only mechanism that implements temp table caching or do system stored procedures such as `sp_executeSQL` / `sp_execute` also take advantage of them?*\r\n\r\nYou need a real stored procedure (`CREATE PROCEDURE`) to benefit from temporary table caching. This includes *temporary* stored procedures (`#procname`).\r\n\r\n>*Point #3 on this blog post suggests that EXEC cannot use temp table caching, but leaves out whether sp_executeSQL can.*\r\n\r\nNotice that `EXECUTE` is used to run `sp_executesql`.\r\n\r\nTesting: There are many ways to check whether caching is occurring. Some of them are listed in my original article referenced in the question, some more methods are shown in my follow-up post, [Temporary Table Caching Explained][1], for example:\r\n\r\n    SELECT \r\n    \tDOMCC.name,\r\n    \tDOMCC.pages_kb,\r\n    \tDOMCC.pages_in_use_kb,\r\n    \tDOMCC.entries_count,\r\n    \tDOMCC.entries_in_use_count\r\n    FROM sys.dm_os_memory_cache_counters AS DOMCC\r\n    WHERE DOMCC.[type] = N'CACHESTORE_TEMPTABLES';\r\n\r\nStored procedure input TVPs are also cached, and [starting with SQL Server 2012][2], these can also be cached when used with `sp_executesql`. See the linked CSS blog post for details.\r\n\r\n  [1]: https://sqlkiwi.blogspot.com/2012/08/temporary-object-caching-explained.html\r\n  [2]: http://blogs.msdn.com/b/psssql/archive/2013/02/26/temp-table-caching-improvement-for-table-valued-parameters-in-sql-server-2012.aspx	2019-12-17 10:06:06.519525+00	3	4	1	85957	0	0	0	2019-12-17 10:06:06.519525+00	\N	>*Are actual stored procedures the only mechanism that implements temp table caching or do system stored procedures such as `sp_executeSQL` / `sp_execute` also take advantage of them?*	f	f
237	277	16	2019-02-21 01:44:38+00	Clumsy\r\n-\r\nI couldn't remember if I included these in [my original answer][1], so here's another couple. \r\n\r\nSpools!\r\n--\r\nSQL Server has lots of different spools, which are temporary data structures stored off in tempdb. Two examples are Table and Index spools.\r\n\r\nWhen they occur in a query plan, the writes to those spools will be associated with the query.\r\n\r\n[![NUTS][2]][2]\r\n\r\nThese will also be registered as writes in DMVs, profiler, XE, etc.\r\n\r\nIndex Spool\r\n-\r\n[![NUTS][3]][3]\r\n\r\nTable Spool\r\n-\r\n[![NUTS][4]][4]\r\n\r\nThe amount of writes performed will go up with the size of the data spooled, obviously.\r\n\r\nSpills\r\n-\r\nWhen SQL Server doesn't get enough memory for certain operators, it may spill some pages to disk. This primarily happens with sorts and hashes. You can see this in actual execution plans, and in newer versions of SQL server, spills are also tracked in [dm_exec_query_stats](https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-query-stats-transact-sql?view=sql-server-2017).\r\n\r\n    SELECT deqs.sql_handle,\r\n           deqs.total_spills,\r\n           deqs.last_spills,\r\n           deqs.min_spills,\r\n           deqs.max_spills\r\n    FROM sys.dm_exec_query_stats AS deqs\r\n    WHERE deqs.min_spills > 0;\r\n\r\n[![NUTS][5]][5]\r\n\r\n[![NUTS][6]][6]\r\n\r\nTracking\r\n-\r\nYou can use a similar XE session as the one I used above to see these in your own demos.\r\n\r\n    CREATE EVENT SESSION spools_and_spills\r\n        ON SERVER\r\n        ADD EVENT sqlserver.sql_batch_completed\r\n        ( ACTION ( sqlserver.sql_text ))\r\n        ADD TARGET package0.event_file\r\n        ( SET filename = N'c:\\temp\\spools_and_spills' )\r\n        WITH ( MAX_MEMORY = 4096KB,\r\n               EVENT_RETENTION_MODE = ALLOW_SINGLE_EVENT_LOSS,\r\n               MAX_DISPATCH_LATENCY = 1 SECONDS,\r\n               MAX_EVENT_SIZE = 0KB,\r\n               MEMORY_PARTITION_MODE = NONE,\r\n               TRACK_CAUSALITY = OFF,\r\n               STARTUP_STATE = OFF );\r\n    GO\r\n\r\n\r\n  [1]: https://dba.stackexchange.com/a/191833/22336\r\n  [2]: https://i.stack.imgur.com/Scexv.png\r\n  [3]: https://i.stack.imgur.com/Xu2e4.png\r\n  [4]: https://i.stack.imgur.com/0ska3.png\r\n  [5]: https://i.stack.imgur.com/GPerd.png\r\n  [6]: https://i.stack.imgur.com/Ox3FB.png	2019-12-04 14:23:02.550114+00	3	4	1	230338	0	0	0	2019-12-04 14:23:02.550114+00	\N	Clumsy	f	f
238	277	16	2017-11-27 19:41:24+00	There's another time when this may happen, and that's with an automatic stats update.\r\n\r\nHere's the XE session we'll be looking at:\r\n\r\n    CREATE EVENT SESSION batches_and_stats\r\n        ON SERVER\r\n        ADD EVENT sqlserver.auto_stats\r\n        ( ACTION ( sqlserver.sql_text )),\r\n        ADD EVENT sqlserver.sql_batch_completed\r\n        ( ACTION ( sqlserver.sql_text ))\r\n        ADD TARGET package0.event_file\r\n        ( SET filename = N'c:\\temp\\batches_and_stats' )\r\n        WITH ( MAX_MEMORY = 4096KB,\r\n               EVENT_RETENTION_MODE = ALLOW_SINGLE_EVENT_LOSS,\r\n               MAX_DISPATCH_LATENCY = 30 SECONDS,\r\n               MAX_EVENT_SIZE = 0KB,\r\n               MEMORY_PARTITION_MODE = NONE,\r\n               TRACK_CAUSALITY = OFF,\r\n               STARTUP_STATE = OFF );\r\n    GO\r\n\r\nThen we'll use this to collect information:\r\n\r\n    USE tempdb\r\n    \r\n    DROP TABLE IF EXISTS dbo.SkewedUp\r\n    \r\n    CREATE TABLE dbo.SkewedUp (Id INT NOT NULL, INDEX cx_su CLUSTERED (Id))\r\n    \r\n    INSERT dbo.SkewedUp WITH ( TABLOCK ) ( Id )\r\n    SELECT CASE WHEN x.r % 15 = 0 THEN 1\r\n                WHEN x.r % 5 = 0 THEN 1000\r\n                WHEN x.r % 3 = 0 THEN 10000\r\n                ELSE 100000\r\n           END AS Id\r\n    FROM   (   SELECT     TOP 1000000 ROW_NUMBER() OVER ( ORDER BY @@DBTS ) AS r\r\n               FROM       sys.messages AS m\r\n               CROSS JOIN sys.messages AS m2 ) AS x;\r\n    \r\n    \r\n    ALTER EVENT SESSION [batches_and_stats] ON SERVER STATE = START\r\n    \r\n    SELECT su.Id, COUNT(*) AS records\r\n    FROM dbo.SkewedUp AS su\r\n    WHERE su.Id > 0\r\n    GROUP BY su.Id\r\n    \r\n    ALTER EVENT SESSION [batches_and_stats] ON SERVER STATE = STOP\r\n\r\nSome of the interesting results from the XE Session:\r\n\r\n[![NUTS][1]][1]\r\n\r\nThe auto stats update doesn't show any writes, but the query shows one write immediately after the stats update.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/3Oa22.jpg	2019-12-04 14:23:02.821708+00	2	4	1	191833	0	0	0	2019-12-04 14:23:02.821708+00	\N	There's another time when this may happen, and that's with an automatic stats update.	f	f
236	277	180	2017-11-27 19:08:52+00	In some cases Query Store can cause writes to occur as an effect of a select statement, and in the same session.\r\n\r\nThis can be reproduced as follows:\r\n\r\n    USE master;\r\n    GO\r\n    CREATE DATABASE [Foo];\r\n    ALTER DATABASE [Foo] SET QUERY_STORE (OPERATION_MODE = READ_WRITE, \r\n      CLEANUP_POLICY = (STALE_QUERY_THRESHOLD_DAYS = 30), \r\n      DATA_FLUSH_INTERVAL_SECONDS = 900, \r\n      INTERVAL_LENGTH_MINUTES = 60, \r\n      MAX_STORAGE_SIZE_MB = 100, \r\n      QUERY_CAPTURE_MODE = ALL, \r\n      SIZE_BASED_CLEANUP_MODE = AUTO);\r\n    USE Foo;\r\n    CREATE TABLE Test (a int, b nvarchar(max));\r\n    INSERT INTO Test SELECT 1, 'string';\r\n\r\nCreate an Extended Events session for monitoring:\r\n\r\n    CREATE EVENT SESSION [Foo] ON SERVER \r\n    ADD EVENT sqlserver.rpc_completed(SET collect_data_stream=(1)\r\n        ACTION(sqlserver.client_app_name,sqlserver.client_hostname,sqlserver.client_pid,sqlserver.database_name,sqlserver.is_system,sqlserver.server_principal_name,sqlserver.session_id,sqlserver.session_server_principal_name,sqlserver.sql_text)\r\n        WHERE ([writes]>(0))),\r\n    ADD EVENT sqlserver.sql_batch_completed(SET collect_batch_text=(1)\r\n        ACTION(sqlserver.client_app_name,sqlserver.client_hostname,sqlserver.client_pid,sqlserver.database_name,sqlserver.is_system,sqlserver.server_principal_name,sqlserver.session_id,sqlserver.session_server_principal_name,sqlserver.sql_text)\r\n        WHERE ([writes]>(0)))\r\n    ADD TARGET package0.event_file(SET filename=N'C:\\temp\\FooActivity2016.xel',max_file_size=(11),max_rollover_files=(999999))\r\n    WITH (MAX_MEMORY=32768 KB,EVENT_RETENTION_MODE=ALLOW_MULTIPLE_EVENT_LOSS,MAX_DISPATCH_LATENCY=30 SECONDS,MAX_EVENT_SIZE=0 KB,MEMORY_PARTITION_MODE=NONE,TRACK_CAUSALITY=ON,STARTUP_STATE=OFF);\r\n\r\n\r\nNext run the following:\r\n\r\n    WHILE @@TRANCOUNT > 0 COMMIT\r\n    SET IMPLICIT_TRANSACTIONS ON;\r\n    SET NOCOUNT ON;\r\n    GO\r\n    DECLARE @b nvarchar(max);\r\n    SELECT @b = b FROM dbo.Test WHERE a = 1;\r\n    WAITFOR DELAY '00:00:01.000';\r\n    GO 86400\r\n\r\nAn implicit transaction may or may not be necessary to reproduce this.\r\n\r\nBy default, at the top of the next hour Query Store's statistics collection job will write out data. This appears to (sometimes?) occur as part of the first user query executed during the hour. The Extended Events session will show something similar to the following:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nThe transaction log shows the writes that have occurred:\r\n\r\n    USE Foo;\r\n    SELECT [Transaction ID], [Begin Time], SPID, Operation, \r\n      [Description], [Page ID], [Slot ID], [Parent Transaction ID] \r\n    FROM sys.fn_dblog(null,null) \r\n    /* Adjust based on contents of your transaction log */\r\n    WHERE [Transaction ID] IN ('0000:0000042c', '0000:0000042d', '0000:0000042e')\r\n    OR [Parent Transaction ID] IN ('0000:0000042c', '0000:0000042d', '0000:0000042e')\r\n    ORDER BY [Current LSN];\r\n\r\n[![enter image description here][2]][2]\r\n\r\nInspecting the page with `DBCC PAGE` shows that the writes are to `sys.plan_persist_runtime_stats_interval`.\r\n\r\n    USE Foo;\r\n    DBCC TRACEON(3604); \r\n    DBCC PAGE(5,1,344,1); SELECT\r\n    OBJECT_NAME(229575856);\r\n\r\nNote that the log entries show three nested transactions but only two commit records. In a similar situation in production, this led to an arguably faulty client library that used implicit transactions unexpectedly starting a write transaction, preventing the transaction log from clearing. The library was written to only issue a commit after running an update, insert, or delete statement, so it never issued a commit command and left a write transaction open.\r\n\r\n  [1]: https://i.stack.imgur.com/1v4lR.png\r\n  [2]: https://i.stack.imgur.com/ldvj0.png	2019-12-04 14:23:02.112459+00	2	4	1	191829	0	0	0	2019-12-04 14:23:02.112459+00	\N	In some cases Query Store can cause writes to occur as an effect of a select statement, and in the same session.	f	f
277	304	155	2019-09-02 09:27:44+00	Though at the moment I don't have the title of the actual hotfix, the better query plan will be used when enabling the query optimizer hotfixes on your version (SQL Server 2012). \r\n\r\nSome other methods are:\r\n\r\n - Using `OPTION(RECOMPILE)` so the filtering happens earlier, on the\r\n   literal value.\r\n - On SQL Server 2016 or higher the hotfixes before this version are\r\n   applied automatically and the query should also run equivalent to the\r\n   better execution plan.\r\n\r\n----------\r\n\r\n\r\n**Query optimizer Hotfixes**\r\n\r\nYou can enable these fixes with \r\n\r\n - Traceflag 4199 before SQL Server 2016\r\n - `ALTER DATABASE SCOPED CONFIGURATION SET QUERY_OPTIMIZER_HOTFIXES=ON;` starting from SQL Server 2016. (not needed for your fix)\r\n\r\nThe filtering on `@id` is applied earlier to both the recursive and anchor members in the execution plan with the hotfix enabled.\r\n\r\nThe traceflag can be added at the query level:\r\n\r\n    OPTION(QUERYTRACEON 4199)\r\n\r\nWhen running the query on SQL Server 2012 SP4 GDR or SQL Server 2014 SP3  with Traceflag 4199 the better query plan is chosen:\r\n\r\n    ALTER PROCEDURE #c (@Id BIGINT) AS BEGIN;\r\n        WITH descendants AS (SELECT\r\n             t.ParentId Id\r\n            ,t.Id DescendantId\r\n        FROM #tree t \r\n        WHERE t.ParentId IS NOT NULL\r\n        UNION ALL \r\n    \tSELECT\r\n             d.Id\r\n            ,t.Id DescendantId\r\n        FROM descendants d\r\n        JOIN #tree t ON d.DescendantId = t.ParentId)\r\n        SELECT d.*\r\n        FROM descendants d\r\n    \tWHERE d.Id = @Id\r\n        ORDER BY d.Id, d.DescendantId\r\n        OPTION( QUERYTRACEON 4199 );\r\n         \r\n    END;\r\n    GO\r\n    EXEC #c 24;\r\n\r\n[Query Plan on SQL Server 2014 SP3 With traceflag 4199][1] \r\n\r\n[Query Plan on SQL Server 2012 SP4 GDR With traceflag 4199][2]\r\n\r\n[Query Plan on SQL Server 2012 SP4 GDR without traceflag 4199][3]\r\n\r\n\r\nThe main consensus is to enable traceflag 4199 globally when using a version before SQL Server 2016. Afterwards it is open for discussion whether to enable it or not. A Q/A on that [here][4].\r\n\r\n----------\r\n\r\n\r\n**Compatibility level 130 or 140**\r\n\r\nWhen testing the parameterized query on a database with `compatibility_level` = 130 or 140, the filtering happens earlier:\r\n\r\n[![enter image description here][5]][5]\r\n\r\nDue to the fact that the 'old' fixes from traceflag 4199 are enabled on SQL Server 2016 and higher.\r\n\r\n\r\n----------\r\n\r\n\r\n**OPTION(RECOMPILE)**\r\n\r\nEven though a procedure is used, SQL Server will be able to filter on the literal value when adding `OPTION(RECOMPILE);`. \r\n\r\n    ALTER PROCEDURE #c (@Id BIGINT) AS BEGIN;\r\n        WITH descendants AS (SELECT\r\n             t.ParentId Id\r\n            ,t.Id DescendantId\r\n        FROM #tree t \r\n        WHERE t.ParentId IS NOT NULL\r\n        UNION ALL \r\n    \tSELECT\r\n             d.Id\r\n            ,t.Id DescendantId\r\n        FROM descendants d\r\n        JOIN #tree t ON d.DescendantId = t.ParentId)\r\n        SELECT d.*\r\n        FROM descendants d\r\n    \tWHERE d.Id = @Id\r\n        ORDER BY d.Id, d.DescendantId\r\n    OPTION(\r\n    RECOMPILE )\r\n             \r\n    END;\r\n    GO\r\n\r\n[![enter image description here][6]][6]\r\n\r\n[Query Plan on SQL Server 2012 SP4 GDR With OPTION(RECOMPILE)][7]\r\n\r\n\r\n  [1]: https://www.brentozar.com/pastetheplan/?id=By2NJvqHB\r\n  [2]: https://www.brentozar.com/pastetheplan/?id=BJnJePqrB\r\n  [3]: https://www.brentozar.com/pastetheplan/?id=BJEmevcrS\r\n  [4]: https://dba.stackexchange.com/questions/102292/trace-flag-4199-enable-globally\r\n  [5]: https://i.stack.imgur.com/GsLZH.png\r\n  [6]: https://i.stack.imgur.com/DqCBp.png\r\n  [7]: https://www.brentozar.com/pastetheplan/?id=SJcEQKcrH	2019-12-05 02:41:29.626456+00	2	4	1	246748	0	0	0	2019-12-05 02:41:29.626456+00	\N	Though at the moment I don't have the title of the actual hotfix, the better query plan will be used when enabling the query optimizer hotfixes on your version (SQL Server 2012).	f	f
278	304	12	2019-09-03 05:42:53+00	Randi Vertongen's [answer](https://topanswers.xyz/databases?q=304#a277) correctly addresses how you can get the plan you want with the parameterized version of the query. This answer supplements that by addressing the title of the question in case you are interested in the details.\r\n\r\nSQL Server rewrites tail-recursive common table expressions (CTEs) as iteration. Everything from the *Lazy Index Spool* down is the runtime implementation of the iterative translation. I wrote a detailed account of how this section of an execution plan works in [answer][2] to [Using EXCEPT in a recursive common table expression](https://dba.stackexchange.com/q/9638/1192).\r\n\r\nYou want to specify a predicate (filter) *outside* the CTE and have the query optimizer *push this filter down* inside the recursion (rewritten as iteration) and have it applied to the anchor member. This will mean the recursion starts with only those records that match `ParentId = @Id`.\r\n\r\nThis is quite a reasonable expectation, whether a literal value, variable, or parameter is used; however, the optimizer can only do things for which rules have been written. Rules specify how a logical query tree is modified to achieve a particular transformation. They include logic to make sure the end result is safe - i.e. it returns exactly the same data as the original query specification in all possible cases.\r\n\r\nThe rule responsible for pushing predicates on a recursive CTE is called `SelOnIterator` - a relational selection (= predicate) on an iterator implementing recursion. More precisely, this rule can copy a selection down to the *anchor* part of recursive iteration:\r\n\r\n```none\r\nSel(Iter(A,R)) -> Sel(Iter(Sel(A),R))\r\n```\r\n\r\nThis rule can be disabled with the undocumented hint `OPTION(QUERYRULEOFF SelOnIterator)`. When this is used, the optimizer can no longer push predicates with a literal value down to the anchor of a recursive CTE. You don't want that, but it illustrates the point.\r\n\r\nOriginally, this rule was limited to working on predicates with literal values only. It could also be made to work with variables or parameters by specifying `OPTION (RECOMPILE)`, since that hint enables the *Parameter Embedding Optimization*, whereby the runtime literal value of the variable (or parameter) is used when compiling the plan. The plan is not cached, so the downside of this is a fresh compilation on each execution.\r\n\r\nAt some point, the `SelOnIterator` rule was improved to also work with variables and parameters. To avoid unexpected plan changes, this was protected under the 4199 trace flag, database compatibility level, and query optimizer hotfix compatibility level. This is quite a normal pattern for optimizer improvements, which are not always documented. Improvements are normally good for most people, but there is always a chance that any change will introduce a regression for someone.\r\n\r\n> I want to put the CTE in a reusable view\r\n\r\nYou could use an inline table-valued function instead of a view. Provide the value you want to push down as a parameter, and place the predicate in the recursive anchor member.\r\n\r\nIf you prefer, enabling trace flag 4199 globally is also an option. There are many optimizer changes covered by this flag, so you would need to carefully test your workload with it enabled, and be prepared to handle regressions.	2019-12-05 02:44:52.478551+00	2	4	1	246814	0	0	0	2019-12-05 02:41:29.901378+00	\N	Randi Vertongen's [answer](https://topanswers.xyz/databases?q=304#a277) correctly addresses how you can get the plan you want with the parameterized version of the query. This answer supplements that by addressing the title of the question in case you are interested in the details.	f	f
146	202	12	2017-10-03 15:14:01+00	### You can't. The feature is disabled in 2017 RTM.\r\n\r\n---\r\n\r\nThat said, you can...\r\n\r\nUsing AdventureWorks:\r\n\r\n    CREATE VIEW dbo.TH\r\n    WITH SCHEMABINDING\r\n    AS\r\n    SELECT P.ProductID, COUNT_BIG(*) AS cbs\r\n    FROM Production.Product AS P\r\n    JOIN Production.TransactionHistory AS TH\r\n        ON TH.ProductID = P.ProductID\r\n    GROUP BY P.ProductID;\r\n    GO\r\n    CREATE UNIQUE CLUSTERED INDEX cuq ON dbo.TH (ProductID)\r\n    WITH (SNAPSHOT_MATERIALIZATION = ON);\r\n\r\nChanges to the underlying tables are not immediately reflected in the view (as is normally the case with SQL Server). Likewise, data modifications against the underlying tables do not have to maintain the snapshot indexed view.\r\n\r\nTo refresh the view contents, one needs to call one of the new stored procedures:\r\n\r\n    EXECUTE sys.sp_refresh_single_snapshot_view\r\n        @view_name = N'dbo.TH',\r\n        @rgCode = 0; -- don't know what this is for yet\r\n\r\nThis produces the execution plan:\r\n\r\n[![Plan][1]][1]\r\n\r\nThis likely won't work for you, because either an undocumented trace flag is needed, or you need to do the particularly nasty thing I did: writing to the memory location holding the feature flag (using a debugger) to enable this feature.\r\n\r\nIf you're curious, the feature flag is the byte at `sqllang!g_featureSwitchesLangSvc+0x10f`. It is checked during `sqllang!SpRefreshSingleSnapshotView`.\r\n\r\nIf you want to play along, and are fully prepared to accept the consequences of hacking about in SQL Server's code while it is running, and using a feature that Microsoft does not think is ready yet:\r\n\r\n1. Attach a debugger to the SQL Server 2017 process. I use WinDbg.\r\n2. Set a breakpoint:\r\n\r\n        bp sqllang!SpRefreshSingleSnapshotView\r\n\r\n3. Resume SQL Server using the Go command (`g`)\r\n4. Create the view above, but not the unique clustered index yet\r\n5. Run the `sys.sp_refresh_single_snapshot_view` command above\r\n6. When the breakpoint is hit, step through until you see the code line:\r\n\r\n        cmp byte ptr [sqllang!g_featureSwitchesLangSvc+0x10f (00007fff`328dfbcf)],0\r\n\r\n The offset may be different in other builds, for example in 2017 RTM CU3 it is `sqllang!g_featureSwitchesLangSvc+0x114`\r\n\r\n7. The memory address inside the parentheses may be different. Use the one you see.\r\n8. Use the display memory command to see the current value at the memory address you found:\r\n\r\n        db 00007fff`328dfbcf L1\r\n\r\n9. This should show a zero, indicating that the feature is disabled.\r\n10. Change the zero to a one, using the enter values command (again with your memory address):\r\n\r\n        eb 00007fff`328dfbcf 1\r\n\r\n11. Disable the breakpoint and resume running SQL Server.\r\n12. The feature is now enabled.\r\n13. Build the unique clustered index on the view.\r\n14. Play around.\r\n\r\n---\r\n\r\nNote `SNAPSHOT_MATERIALIZATION` allows us to materialize a snapshot of a query specification that ordinarily could not be indexed, for example the below uses `MAX`:\r\n\r\n    CREATE VIEW dbo.TH2\r\n    WITH SCHEMABINDING\r\n    AS\r\n    SELECT TH.ProductID, MaxTransactionID = MAX(TH.TransactionID)\r\n    FROM Production.TransactionHistory AS TH\r\n    GROUP BY TH.ProductID;\r\n    GO\r\n    CREATE UNIQUE CLUSTERED INDEX cuq ON dbo.TH2 (ProductID)\r\n    WITH (SNAPSHOT_MATERIALIZATION = ON);\r\n\r\nResult:\r\n\r\n```none\r\nCommands completed successfully.\r\n```\r\n\r\n  [1]: https://i.stack.imgur.com/RX266.png	2019-11-30 07:52:15.524121+00	1	4	1	187536	0	0	0	2019-11-30 07:50:29.291866+00	\N	You can't. The feature is disabled in 2017 RTM.	f	f
81	109	12	2019-11-25 22:55:38.455548+00	>*I think my understanding of the second estimate is incorrect and the differing numbers seems to indicate that. What am I missing?*\r\n\r\nUsing the SQL Server 2012 cardinality estimator, the selectivity of the join drives the estimated number of rows on the inner side of the nested loops join, and not the other way around.\r\n\r\nThe 11.4867 number is *derived* (for display in showplan) by dividing the computed estimated cardinality of the join output (30.0919) by the number of iterations (2.61972). The result, using single-precision floating-point arithmetic, is **11.4867**.\r\n\r\nIt really is as simple as that. Note that the (logical) join selectivity is independent of the choice of physical join operator. It remains the same whether the join is ultimately performed using a Nested Loops, Hash, or Merge Join physical operator.\r\n\r\nIn SQL Server 2012 and earlier, the join selectivity (as a whole) is estimated using the `SalesOrderID` histograms from each table (computed for each histogram step, after step boundary alignment using linear interpolation as necessary). The `SalesOrderID` histogram associated with the `SalesOrderHeader` table is also adjusted for the scaling effect of the independent `CustomerID` filter.\r\n\r\nThat is not to say there is anything fundamentally 'wrong' with the alternate calculation proposed in the question; it just makes a different set of assumptions. There will always be different ways to compute or combine estimates for a given sequence of logical operations. There is no general guarantee that different statistical methods applied to the same data will produce the same answers, or that one method would always be superior to the other. Inconsistencies resulting from the application of different statistical methods can even appear within a single final execution plan, though they are rarely noticed.\r\n\r\nAs a side-note, the SQL Server 2014 cardinality estimator takes a different approach to combining the independent-filter-adjusted histogram information (["coarse alignment"][1]), which results in a different final estimate of **10.1006** rows for this query:\r\n\r\nPlan for computation:\r\n\r\n```none\r\n  CSelCalcExpressionComparedToExpression\r\n  (QCOL: [s].SalesOrderID x_cmpEq QCOL: [d].SalesOrderID)\r\n    \r\nLoaded histogram for column QCOL: [s].SalesOrderID from stats with id 1\r\nLoaded histogram for column QCOL: [d].SalesOrderID from stats with id 1\r\n    \r\nStats collection generated: \r\n    \r\n  CStCollJoin(ID=4, **CARD=10.1006** x_jtInner)\r\n      CStCollFilter(ID=3, CARD=2.61972)\r\n          CStCollBaseTable(ID=1, CARD=31465 TBL: Sales.SalesOrderHeader AS TBL: s)\r\n      CStCollBaseTable(ID=2, CARD=121317 TBL: Sales.SalesOrderDetail AS TBL: d)\r\n```\r\n\r\nThis happens to be the same result as the calculation in the question, though the detailed reasoning is different (i.e. it is not based on an assumed nested loops implementation).\r\n\r\n\r\n  [1]: https://sqlperformance.com/2018/11/sql-optimizer/histogram-coarse-alignment	2019-11-25 23:01:13.921041+00	1	4	2	\N	0	0	0	\N	\N	>*I think my understanding of the second estimate is incorrect and the differing numbers seems to indicate that. What am I missing?*	f	f
8	18	2	2019-11-10 00:59:24.436261+00	I'm not sure there is any need for the site to mandate an option. Every contribution could specify what the author wants to grant, with the default for each user set in their profile.\r\n\r\nThe question would then become "what grant should we default to for new accounts". I suggest the default should be CC0 for the entire contribution. Either of the others would be OK too though.\r\n\r\nI would suggest that once a contribution has been made, the grants cannot be changed (perhaps unless no-one has edited it except the OP).\r\n\r\n---\r\n\r\nupdate:\r\n\r\nEvery question and answer now specifies the license granted. Once granted you can't change the license and all edits to posts, by anyone, are explicitly done under the same license as the original post, as indicated on the buttons that submit the changes, like this:\r\n\r\n![Screenshot 2019-11-16 at 14.51.37.png](/image?hash=41b5930f262b21abf124d5b9a65b1b9c46ac5f17607f8762b8914d9a8429b7fc)\r\n\r\nThe options currently available are:\r\n\r\n* [CC0 1.0](https://creativecommons.org/publicdomain/zero/1.0)\r\n* [CC BY 4.0](https://creativecommons.org/licenses/by/4.0)\r\n* [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0)\r\n* [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)\r\n* [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)\r\n\r\nAnd for an (optional) [additional license for original code](/meta?q=24):\r\n\r\n* [CC0 1.0](https://creativecommons.org/publicdomain/zero/1.0)\r\n\r\nPlease ask here on meta if you'd like to see a license added to either list.\r\n\r\nThe site default is currently CC BY-SA 4.0 but this may change in future. If it does change **it will *never* change the account default license that you have set** — the site-wide default is only for *new* accounts. I can see good arguments for several of the other licenses in the current list to be the default — I like CC0, but CC BY-NC-SA would be an interesting choice.	2019-11-16 15:06:22.005663+00	3	1	1	\N	0	0	0	\N	\N	I'm not sure there is any need for the site to mandate an option. Every contribution could specify what the author wants to grant, with the default for each user set in their profile.	f	f
137	18	96	2019-11-29 13:37:37.245539+00	Raw "Public Domain" declarations are problematic as the only license available for code snippets. In some countries PD is not a recognized thing and this actually restricts the use of code more than frees it up. The same goes for undecaled or "do whatever" style licenses. CC0 seems be the best attempt to address those tricky legal issues while still being as explicitly open as possible. That's probably a good default.\r\n\r\nHowever it does not address all concerns and other licenses should be an option. For example:\r\n\r\n* To questioners, it will be difficult to ask questions if they are forced to rewrite snippets to be CC0. If my sofware is MIT or GPL or whatever licensed and I want to ask about why something is going wrong in some function, and if I didn't write all the code and hence can't just relicence it myself, it's problematic to not be able to post the snippet in a question.\r\n\r\n* Most of the time when posting something online in answer to a question I don't have any reason to care. But every once in a while I do care that my contributions in a certain problem are only usable in the OSS world and not useful to people building proprietary software unsess they rewrite them. Personally I'm far more inclined to go the extra mile to help out some poor soul by fixing their code up for them if I know it will remain a lasting OSS afair and _not_ just be gobbled up.\r\n\r\nWith those points in mind I'd suggest adding a pretty full suite of licence choices for code snippets, and probably default answers to 1st to whatever the question was posted as and 2nd to the users preference.	2019-11-30 14:08:48.979163+00	2	4	1	\N	0	0	0	\N	\N	Raw "Public Domain" declarations are problematic as the only license available for code snippets. In some countries PD is not a recognized thing and this actually restricts the use of code more than frees it up. The same goes for undecaled or "do whatever" style licenses. CC0 seems be the best attempt to address those tricky legal issues while still being as explicitly open as possible. That's probably a good default.	f	f
143	199	2	2015-07-28 17:56:35+00	I posted this [to pgsql-bugs](http://www.postgresql.org/message-id/20150728162823.25043.27625@wrigleys.postgresql.org) and [the reply there](http://www.postgresql.org/message-id/17665.1438105627@sss.pgh.pa.us) from Tom Lane indicates this is a lock escalation issue, disguised by the mechanics of the way SQL language functions are processed. Essentially, **the lock generated by the `insert` is obtained *before* the exclusive lock on the table**:\r\n\r\n> I believe the issue with this is that a SQL function will do parsing (and\r\nmaybe planning too; don't feel like checking the code right now) for the\r\nentire function body at once.  This means that due to the INSERT command\r\nyou acquire RowExclusiveLock on the "test" table during function body\r\nparsing, before the LOCK command actually executes.  So the LOCK\r\nrepresents a lock escalation attempt, and deadlocks are to be expected.\r\n>\r\n> This coding technique would be safe in plpgsql, but not in a SQL-language\r\nfunction.\r\n>\r\n> There have been discussions of reimplementing SQL-language functions so\r\nthat parsing occurs one statement at a time, but don't hold your breath\r\nabout something happening in that direction; it doesn't seem to be a\r\nhigh priority concern for anybody.\r\n>\r\n> regards, tom lane\r\n\r\nThis also explains why locking the table outside the function in a wrapping plpgsql block (as [suggested by](http://chat.stackexchange.com/transcript/message/23065138#23065138) @ypercube) prevents the deadlocks.	2019-11-29 17:46:16.810405+00	0	4	1	108349	0	0	0	2019-11-29 17:43:58.537858+00	\N	I posted this [to pgsql-bugs](http://www.postgresql.org/message-id/20150728162823.25043.27625@wrigleys.postgresql.org) and [the reply there](http://www.postgresql.org/message-id/17665.1438105627@sss.pgh.pa.us) from Tom Lane indicates this is a lock escalation issue, disguised by the mechanics of the way SQL language functions are processed. Essentially, **the lock generated by the `insert` is obtained *before* the exclusive lock on the table**:	f	f
540	484	8	2015-05-19 12:14:42+00	That is just documented behavior. I don't think anyone messed with the settings.\r\n\r\nSee [data type precedence](https://msdn.microsoft.com/en-us/library/ms190309.aspx) on MSDN.\r\n\r\n> When an operator combines two expressions of different data types, the\r\n> rules for data type precedence specify that the data type with the\r\n> lower precedence is converted to the data type with the higher\r\n> precedence.\r\n\r\nThe empty string gets converted to 0 in any numeric type and to 1900-01-01 00:00:00.000 when converted to a date.\r\n\r\nI think your real problem is that your design is so that you have to join on fields of a different data type. The only way to get around this is to have a convert on your join clause which will hurt query performance. The main problem is probably with the schema design.\r\n\r\nHowever illogical it may seem, converting an empty string to other data types produces arbitrary values.\r\n\r\nThis code:\r\n\r\n    SELECT CONVERT(int, '')\r\n    SELECT CONVERT(float, '')\r\n    SELECT CONVERT(date, '')\r\n    SELECT CONVERT(datetime, '')\r\n\r\nProduces this output:\r\n\r\n    0\r\n    0\r\n    1900-01-01\r\n    1900-01-01 00:00:00.000\r\n\r\nYou could expect then that this behavior is consistent between other preceding datatypes and expect that converting 0 to a date would produce the same arbitrary value but it doesn't.\r\n\r\n    SELECT CONVERT(date, 0)\r\n\r\nProduces\r\n\r\n> Explicit conversion from data type int to date is not allowed.\r\n\r\nBecause it's not a [supported conversion](https://msdn.microsoft.com/en-us/library/ms191530.aspx)\r\n\r\nwhile\r\n\r\n    SELECT CONVERT(datetime, 0)\r\n\r\nReturns \r\n\r\n> January, 01 1900 00:00:00\r\n\r\nSo yes, it's weird and arbitrary, but actually documented and explainable.	2019-12-26 10:42:01.558405+00	9	4	1	101887	0	0	0	2019-12-25 09:35:27.217819+00	\N	That is just documented behavior. I don't think anyone messed with the settings.	f	f
499	455	2	2019-12-18 22:34:27.600544+00	Summary: we have a solution that works well with Firefox and OK-ish with Chrome.\r\n\r\n> …would it be possible to change the question/answer editor in such a way that browsers could spell check it?\r\n\r\nSadly there isn't a very good solution to this — CodeMirror is otherwise great (and even works well on mobile), and if we just use a textarea instead (like the chat input box) then we lose the syntax highlighting which would be a shame.\r\n\r\nThere is an 'out of the box' solution for CodeMirror, and there are some plugins as @Josh [mentioned](https://topanswers.xyz/transcript?room=466&id=9630#c9630) in the comments. The former appears to be quite flaky (it works pretty well in Firefox but not so well in Chrome), and the latter are separate from the Browser's built-in dictionaries etc, which isn't ideal either.\r\n\r\nAs an experiment we are enabling the native method and asking for feedback. Is it better than nothing on Chrome? Does it do any harm on any other browsers? Please let us know.\r\n\r\n---\r\n\r\nThis seems to be at least moderately successful so we will stick with it for now.	2019-12-24 20:00:10.390908+00	2	1	1	\N	0	0	0	\N	\N	Summary: we have a solution that works well with Firefox and OK-ish with Chrome.	f	f
105	150	12	2016-10-28 16:50:39+00	The guess for `LIKE` *in your case* is based on:\r\n\r\n* `G`: The standard 9% guess (`sqllang!x_Selectivity_Like`)\r\n* `M`: A factor of 6 (magic number)\r\n* `D`: Average data length in bytes (from statistics), rounded down to integer\r\n\r\nSpecifically, `sqllang!CCardUtilSQL7::ProbLikeGuess` uses:\r\n\r\n> `Selectivity (S) = G / M * LOG(D)`\r\n\r\nNotes:\r\n\r\n* The `LOG(D)` term is omitted if `D` is between 1 and 2.\r\n* If `D` is less than 1 (including for missing or `NULL` statistics):  \r\n`D = FLOOR(0.5 * maximum column byte length)`\r\n\r\nThis sort of quirkiness and complexity is quite typical of the original CE.\r\n\r\nIn the question example, the average length is 5 (5.6154 from `DBCC SHOW_STATISTICS` rounded down):\r\n\r\n```none\r\nEstimate = 10,000 * (0.09 / 6 * LOG(5)) = <b>241.416</b>\r\n```\r\n\r\nOther example values:\r\n\r\n```none\r\n D  = Estimate using formula for S\r\n 15 = 406.208\r\n 14 = 395.859\r\n 13 = 384.742\r\n 12 = 372.736\r\n 11 = 359.684\r\n 10 = 345.388\r\n 09 = 329.584\r\n 08 = 311.916\r\n 07 = 291.887\r\n 06 = 268.764\r\n 05 = 241.416\r\n 04 = 207.944\r\n 03 = 164.792\r\n 02 = 150.000 (LOG not used)\r\n 01 = 150.000 (LOG not used)\r\n 00 = 291.887 (LOG 7) /* FLOOR(0.5 * 15) [15 since lastname is varchar(15)] */\r\n```\r\n\r\n### Test rig\r\n\r\n    DECLARE\r\n        @CharLength integer = 5, -- Set length here\r\n        @Counter integer = 1;\r\n    \r\n    CREATE TABLE #T (c1 varchar(15) NULL);\r\n    \r\n    -- Add 10,000 rows\r\n    SET NOCOUNT ON;\r\n    SET STATISTICS XML OFF;\r\n    \r\n    BEGIN TRANSACTION;\r\n    WHILE @Counter <= 10000\r\n    BEGIN\r\n        INSERT #T (c1) VALUES (REPLICATE('X', @CharLength));\r\n        SET @Counter = @Counter + 1;\r\n    END;\r\n    COMMIT TRANSACTION;\r\n    \r\n    SET NOCOUNT OFF;\r\n    SET STATISTICS XML ON;\r\n    \r\n    -- Test query\r\n    DECLARE @Like varchar(15);\r\n    SELECT * FROM #T AS T \r\n    WHERE T.c1 LIKE @Like;\r\n    \r\n    DROP TABLE #T;\r\n\r\n	2019-11-27 10:41:32.161212+00	2	4	1	153618	0	0	0	2019-11-27 10:40:20.594797+00	\N	The guess for `LIKE` *in your case* is based on:	f	f
230	274	16	2019-03-26 12:45:24+00	There are a lot of different ways to do this. \r\n\r\nI don't usually recommend inserting into a `#temp` table, since any tempdb load or autogrowth may impact the results, and I definitely don't recommend using a `@table` variable, since modifications to those are forced serial (no parallel plan can be used), which may change actual query times.\r\n\r\n\r\nVariable Assignment\r\n--\r\n\r\nYou can declare a variable and assign your columns to it, like this:\r\n\r\n    DECLARE @Start datetime\r\n    DECLARE @End datetime\r\n    DECLARE @blob_eater SQL_VARIANT;\r\n    \r\n    SELECT @StartTimeWA=GETDATE() \r\n    \r\n    SELECT \r\n           @blob_eater = [id] \r\n          ,@blob_eater = [database_id]\r\n          ,@blob_eater = [proc_name]\r\n          ,@blob_eater = [exec_t] from\r\n      [DB].[dbo].[STAT] \r\n    \r\n    SELECT @End=GETDATE()\r\n    \r\n    SELECT DATEDIFF(MS,@Start,@End) AS [Duration]\r\n\r\nThough doing this may prevent some parameter embedding optimizations. See [Parameter Sniffing, Embedding, and the RECOMPILE Options][1] under "An Embedding Restriction".\r\n\r\nNote that this method may trigger plan warnings about implicit conversions, but they're not the kind you have to worry about. See this Q&A for background: What Triggers This Warning: [Type Conversion in Expression May Affect “CardinalityEstimate” in Query Plan Choice][2].\r\n\r\n\r\nIn SSMS\r\n--\r\nYou can change the settings to discard query results.\r\n\r\n[![NUTS][3]][3]\r\n\r\n\r\nSQL Query Stress\r\n--\r\n[SQL Query Stress][4] is an open source tool that allows you to run queries against a SQL Server to simulate load. No query results are returned to the application when they're run.\r\n\r\nYou can read some instructions on it [here][5].\r\n\r\nostress (RML Utilities)\r\n--\r\n[ostress][6] is a similar tool, published by Microsoft, which also doesn't return results to the client, unless you choose to do it.\r\n\r\nI've written some about it [here][7].\r\n\r\nPlan Explorer\r\n--\r\nSentryOne's [Plan Explorer][8] is a free alternative to view SQL Server execution plans and deadlocks with. \r\n\r\nYou can also use it as a client to query SQL Serve to some degree:\r\n\r\n[![NUTS][9]][9]\r\n\r\nThis will also discard results.\r\n\r\n[![NUTS][10]][10]\r\n\r\nHope this helps!\r\n\r\n\r\n  [1]: https://sqlperformance.com/2013/08/t-sql-queries/parameter-sniffing-embedding-and-the-recompile-options\r\n  [2]: https://dba.stackexchange.com/questions/231948/what-triggers-this-warning-type-conversion-in-expression-may-affect-cardinalit\r\n  [3]: https://i.stack.imgur.com/ILrKc.png\r\n  [4]: https://github.com/ErikEJ/SqlQueryStress\r\n  [5]: https://www.brentozar.com/archive/2015/05/how-to-fake-load-tests-with-sqlquerystress/\r\n  [6]: https://www.microsoft.com/en-us/download/details.aspx?id=4511\r\n  [7]: https://www.brentozar.com/archive/2017/02/simulating-workload-ostress-agent-jobs/\r\n  [8]: https://www.sentryone.com/plan-explorer\r\n  [9]: https://i.stack.imgur.com/35GzY.png\r\n  [10]: https://i.stack.imgur.com/ppl4f.png	2019-12-04 14:18:12.510678+00	2	4	1	233134	0	0	0	2019-12-04 14:18:12.510678+00	\N	There are a lot of different ways to do this.	f	f
245	283	16	2018-09-26 16:42:29+00	There are many reasons why you may not have missing index requests!\r\n===================================================================\r\n\r\nWe'll look at a few of the reasons in more detail, and also talk about some of the general limitations of the feature.\r\n\r\nGeneral Limitations\r\n===================\r\n\r\nFirst, from: [Limitations of the Missing Indexes Feature][1]:\r\n\r\n>  - It does not specify an order for columns to be used in an index.\r\n\r\nAs noted in this Q&A: [How does SQL Server determine key column order in missing index requests?][2], the order of columns in the index definition is dictated by Equality vs Inequality predicate, and then column ordinal position in the table. \r\n\r\nThere are no guesses at selectivity, and there may be a better order available. It's your job to figure that out.\r\n  \r\n**Special Indexes**\r\n      \r\nMissing index requests also don't cover 'special' indexes, like:\r\n\r\n - Clustered\r\n - Filtered\r\n - Partitioned\r\n - Compressed\r\n - XML-ed\r\n - Spatial-ed\r\n - Columnstore-d\r\n - Indexed View-ed\r\n\r\nWhat columns are considered?\r\n=============================\r\n\r\nMissing Index key columns are generated from columns used to filter results, like those in:\r\n\r\n - JOINs\r\n - WHERE clause\r\n\r\nMissing Index Included columns are generated from columns required by the query, like those in:\r\n\r\n - SELECT\r\n - GROUP BY\r\n - ORDER BY\r\n\r\nEven though quite often, columns you're ordering by or grouping by can be beneficial as key columns. This goes back to one of the Limitations:\r\n\r\n> - It is not intended to fine tune an indexing configuration.\r\n\r\nFor example, this query will not register a missing index request, even though adding an index on LastAccessDate would prevent the need to Sort (and spill to disk).\r\n\r\n    SELECT TOP (1000) u.DisplayName\r\n    FROM dbo.Users AS u\r\n    ORDER BY u.LastAccessDate DESC;\r\n\r\n[![NUTS][3]][3]\r\n\r\nNor does this grouping query on Location.\r\n\r\n    SELECT TOP (20000) u.Location\r\n    FROM dbo.Users AS u\r\n    GROUP BY u.Location\r\n\r\n[![NUTS][4]][4]\r\n\r\nThat doesn't sound very helpful!\r\n================================\r\n\r\nWell, yeah, but it's better than nothing. Think of missing index requests like a crying baby. You know there's a problem, but it's up to you as an adult to figure out what that problem is.\r\n\r\nYou still haven't told me why I don't have them, though...\r\n==========================================================\r\n\r\nRelax, bucko. We're getting there.\r\n\r\nTrace Flags\r\n===========\r\n\r\nIf you enable [TF 2330][5], missing index requests won't be logged. To find out if you have this enabled, run this:\r\n\r\n    DBCC TRACESTATUS;\r\n\r\nIndex Rebuilds\r\n==============\r\n\r\n[Rebuilding indexes][6] will clear missing index requests. So before you go Hi-Ho-Silver-Away rebuilding every index the second an iota of fragmentation sneaks in, think about the information you're clearing out every time you do that.\r\n\r\nYou may also want to think about [Why Defragmenting Your Indexes Isn’t Helping][7], anyway. Unless you're using [Columnstore][8].\r\n\r\nAdding, Removing, or Disabling Indexes\r\n==================\r\nAdding, removing, or disabling an index will clear all of the missing index requests for that table.  If you're working through several index changes on the same table, make sure you script them all out before making any.\r\n\r\nTrivial Plans\r\n==============\r\n\r\nIf a plan is simple enough, and the index access choice is obvious enough, and the cost is low enough, you'll get a trivial plan. \r\n\r\nThis effectively means there were no cost based decisions for the optimizer to make.\r\n\r\nVia [Paul White][9]:\r\n\r\n>  The details of which types of query can benefit from Trivial Plan change frequently, but things like joins, subqueries, and inequality predicates generally prevent this optimization.\r\n\r\nWhen a plan is trivial, additional optimization phases are not explored, and [missing indexes are not requested][10].\r\n\r\nSee the difference between these queries and [their plans][11]:\r\n\r\n    SELECT *\r\n    FROM dbo.Users AS u\r\n    WHERE u.Reputation = 2;\r\n    \r\n    SELECT *\r\n    FROM dbo.Users AS u\r\n    WHERE u.Reputation = 2\r\n    AND 1 = (SELECT 1);\r\n\r\n[![NUTS][12]][12]\r\n\r\nThe first plan is trivial, and no request is shown. There may be cases where bugs prevent missing indexes from appearing in query plans; they are usually more reliably logged in the missing index DMVs, though.\r\n\r\nSARGability\r\n============\r\n\r\nPredicates where the optimizer wouldn't be able to use an index efficiently even with an index may prevent them from being logged.\r\n\r\nThings that are generally not SARGable are:\r\n\r\n - Columns wrapped in functions\r\n - Column + SomeValue = SomePredicate\r\n - Column + AnotherColumn = SomePredicate \r\n - Column = @Variable OR @Variable IS NULL\r\n\r\nExamples:\r\n---------\r\n\r\n    SELECT *\r\n    FROM dbo.Users AS u\r\n    WHERE ISNULL(u.Age, 1000) > 1000;\r\n\r\n\r\n    SELECT *\r\n    FROM dbo.Users AS u\r\n    WHERE DATEDIFF(DAY, u.CreationDate, u.LastAccessDate) > 5000\r\n\r\n\r\n    SELECT *\r\n    FROM dbo.Users AS u\r\n    WHERE u.UpVotes + u.DownVotes > 10000000\r\n\r\n\r\n    DECLARE @ThisWillHappenWithStoredProcedureParametersToo NVARCHAR(40) = N'Eggs McLaren'\r\n    SELECT *\r\n    FROM dbo.Users AS u\r\n    WHERE u.DisplayName LIKE @ThisWillHappenWithStoredProcedureParametersToo \r\n          OR @ThisWillHappenWithStoredProcedureParametersToo IS NULL;\r\n\r\nNone of these queries will register missing index requests. For more information on these, check out the following links:\r\n\r\n - [Optional Parameters and Missing Index Requests](https://www.brentozar.com/archive/2017/09/optional-parameters-missing-index-requests/)\r\n - [SARGable WHERE clause for two date columns](https://dba.stackexchange.com/questions/132437/sargable-where-clause-for-two-date-columns)\r\n - [What are different ways to replace ISNULL() in a WHERE clause that uses only literal values?](https://dba.stackexchange.com/questions/168276/what-are-different-ways-to-replace-isnull-in-a-where-clause-that-uses-only-lit)\r\n\r\n\r\nYou Already Have An Okay Index\r\n==============================\r\n\r\nTake this index:\r\n\r\n`CREATE INDEX ix_whatever ON dbo.Posts(CreationDate, Score) INCLUDE(OwnerUserId);`\r\n\r\nIt looks okay for this query:\r\n\r\n    SELECT p.OwnerUserId, p.Score\r\n    FROM dbo.Posts AS p\r\n    WHERE p.CreationDate >= '20070101'\r\n    AND   p.CreationDate < '20181231'\r\n    AND   p.Score >= 25000\r\n    AND 1 = (SELECT 1)\r\n    ORDER BY p.Score DESC;\r\n\r\nThe plan is a simple Seek...\r\n\r\n[![NUTS][13]][13]\r\n\r\nBut because the leading key column is for the less-selective predicate, we end up doing more work than we should:\r\n\r\n> Table 'Posts'. Scan count 13, logical reads 136890\r\n\r\nIf we change the index key column order, we do a lot less work:\r\n\r\n`CREATE INDEX ix_whatever ON dbo.Posts(Score, CreationDate) INCLUDE(OwnerUserId);`\r\n\r\n[![NUTS][14]][14]\r\n\r\nAnd significantly fewer reads:\r\n\r\n> Table 'Posts'. Scan count 1, logical reads 5\r\n\r\n\r\nSQL Server Is Creating Indexes For you\r\n==============================\r\n\r\nIn certain cases, SQL Server will choose to create an index on the fly via an index spool. When an index spool is present, a missing index request won't be. Surely adding the index yourself could be a good idea, but don't count on SQL Server helping you figure that out.\r\n\r\n[![NUTS][15]][15]\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/ms345485(v=sql.105)\r\n  [2]: https://dba.stackexchange.com/questions/208947/how-does-sql-server-determine-key-column-order-in-missing-index-requests\r\n  [3]: https://i.stack.imgur.com/jAZhj.jpg\r\n  [4]: https://i.stack.imgur.com/OksAJ.jpg\r\n  [5]: https://www.brentozar.com/archive/2015/11/trace-flag-2330-who-needs-missing-index-requests/\r\n  [6]: https://littlekendra.com/2016/05/18/index-usage-stats-bug-fixed-in-sql-server-2012-sp2-cu12-sp3-cu3/\r\n  [7]: https://groupby.org/conference-session-abstracts/why-defragmenting-your-indexes-isnt-helping/\r\n  [8]: https://blogs.msdn.microsoft.com/sqlserverstorageengine/2016/03/07/columnstore-index-defragmentation-using-reorganize-command/\r\n  [9]: http://sqlblog.com/blogs/paul_white/archive/2012/04/28/query-optimizer-deep-dive-part-1.aspx\r\n  [10]: https://www.brentozar.com/archive/2013/07/dude-who-stole-my-missing-index-recommendation/\r\n  [11]: https://www.brentozar.com/pastetheplan/?id=BkPun7YFX\r\n  [12]: https://i.stack.imgur.com/4tTLd.jpg\r\n  [13]: https://i.stack.imgur.com/oVu5D.jpg\r\n  [14]: https://i.stack.imgur.com/fWnL2.jpg\r\n  [15]: https://i.stack.imgur.com/Xu2e4.png	2019-12-04 14:25:50.936871+00	2	4	1	218642	0	0	0	2019-12-04 14:25:50.936871+00	\N	There are many reasons why you may not have missing index requests!	f	f
388	380	2	2019-12-08 19:00:04.113196+00	Update Feb 2020: I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/15) so work can begin, and will update the post here when it is complete.\r\n\r\n---\r\n\r\nSomeone also mentioned this in chat (can't find it now), so it is already on the todo list.\r\n\r\nIf this is possible (and from what I understand of [this post on SO](https://stackoverflow.com/a/7784202) it seems like it is for 'modern browsers'), it would be very nice to have at minimal cost.	2020-02-10 15:37:04.501264+00	5	1	1	\N	0	0	0	\N	\N	Update Feb 2020: I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/15) so work can begin, and will update the post here when it is complete.	f	f
497	463	620	2013-02-14 12:01:49+00	Latin1 is code page 1252, in which [178 is 'SUPERSCRIPT TWO'][1]. This is an Unicode [superscript][2]: is **the character "2" as superscript**. According to the [Unicode Technical Standard #10][3] it should compare equal to 2, see [8.1 Collation Folding][4]:\r\n\r\n> Map compatibility (tertiary) equivalents, **such as full-width and\r\n> superscript characters**, to representative character(s)\r\n\r\nThe bug would be if superscript 2 would compare different from 2! Before you say 'but my column is not Unicode', rest assured: according to [MSDN](http://msdn.microsoft.com/en-us/library/ms143726.aspx) (see Windows Collations) all string comparison and sorting are done according to the Unicode rules, even when the on-disk representation is CHAR.\r\n\r\nAs for the other characters in your example, like `VULGAR FRACTION ONE QUARTER` and the like they do not compare equal to any number, but, as Mark already showed, they do sort properly between 0 and 9.\r\n\r\nAnd, of course, if you'd change the code page you would get different results. Eg. with `Greek_CS_AS` ([code page 1253][5]) you would get the characters with code 178, 179 and 189. \r\n\r\n\r\n  [1]: http://www.microsoft.com/typography/unicode/1252.htm\r\n  [2]: http://en.wikipedia.org/wiki/Unicode_subscripts_and_superscripts#Superscripts_and_subscripts_block\r\n  [3]: http://www.unicode.org/reports/tr10/\r\n  [4]: http://www.unicode.org/reports/tr10/#Collation_Folding\r\n  [5]: http://www.microsoft.com/typography/unicode/1253.htm	2019-12-18 07:44:13.325223+00	2	4	1	34733	0	0	0	2019-12-18 07:44:13.325223+00	\N	Latin1 is code page 1252, in which [178 is 'SUPERSCRIPT TWO'][1]. This is an Unicode [superscript][2]: is **the character "2" as superscript**. According to the [Unicode Technical Standard #10][3] it should compare equal to 2, see [8.1 Collation Folding][4]:	f	f
496	463	751	2013-02-14 11:46:34+00	`[0-9]` is not some type of regular expression defined to just match digits. \r\n\r\nAny range in a `LIKE` pattern matches characters between the start and end character according to collation sort order.\r\n\r\n    SELECT CodePoint,\r\n           Symbol,\r\n           RANK() OVER (ORDER BY Symbol COLLATE Latin1_General_CI_AS) AS Rnk\r\n    FROM   #CodePage\r\n    WHERE  Symbol LIKE '[0-9]' COLLATE Latin1_General_CI_AS\r\n    ORDER  BY Symbol COLLATE Latin1_General_CI_AS \r\n\r\nReturns\r\n\r\n    CodePoint            Symbol Rnk\r\n    -------------------- ------ --------------------\r\n    48                   0      1\r\n    188                  ¼      2\r\n    189                  ½      3\r\n    190                  ¾      4\r\n    185                  ¹      5\r\n    49                   1      5\r\n    50                   2      7\r\n    178                  ²      7\r\n    179                  ³      9\r\n    51                   3      9\r\n    52                   4      11\r\n    53                   5      12\r\n    54                   6      13\r\n    55                   7      14\r\n    56                   8      15\r\n    57                   9      16\r\n\r\nSo you get these results because under your default collation these characters sort after `0` but before `9`. \r\n\r\nIt looks as though the collation is defined to actually sort them in mathematical order with the fractions in the correct order between `0` and `1`.\r\n\r\nYou could also use a set rather than a range. To avoid `2` matching `²` you would need a `CS` collation\r\n\r\n    SELECT CodePoint, Symbol\r\n    FROM #CodePage\r\n    WHERE Symbol LIKE '[0123456789]' COLLATE Latin1_General_CS_AS	2019-12-18 07:44:13.062312+00	4	4	1	34731	0	0	0	2019-12-18 07:44:13.062312+00	\N	`[0-9]` is not some type of regular expression defined to just match digits.	f	f
50	73	2	2019-11-21 20:49:28.520608+00	Originally you could only vote on questions of type 'meta question', or 'blog post'. This meant that normal questions on the 'meta' community could not be voted on.\r\n\r\nTo keep things simple, we have disabled 'meta questions' in the 'meta' community, and allowed voting on all normal questions instead (again, just in the 'meta' community). ***So now you can vote on everything 'meta'*** (except your own posts of course). This is a feature — the use of meta helps build the site and keep things organised, so we want to encourage it.\r\n\r\n	2019-11-21 20:49:28.520608+00	2	1	1	\N	0	0	0	\N	\N	Originally you could only vote on questions of type 'meta question', or 'blog post'. This meant that normal questions on the 'meta' community could not be voted on.	f	f
456	441	168	2019-12-14 10:28:43.961708+00	I don't really consider myself an expert in this field, but at least I did some\r\n`expl3` coding myself in some of my packages, so I might be able to get you\r\nstarted.\r\n\r\n1. The document `texdoc expl3` (`expl3.pdf`) gives an introduction to LaTeX3 on\r\n   16 pages. Described are:\r\n\r\n    - The layers of LaTeX3 (some of which might not be fully implemented). The\r\n      basic concept is that there should be different languages to achieve\r\n      different things. Programming is done in the programmer layer which is the\r\n      `expl3`-syntax. Atop that programmer layer there should be a designer\r\n      layer, which is implemented in `expl3`, but the language used there will\r\n      most likely not be `expl3` (to be seen, as far as I know). And the\r\n      user-facing layer is the document mark-up.\r\n    \r\n    - The naming conventions/scheme used in `expl3`'s programmer's layer.\r\n    \r\n    - The LaTeX3 approach to expansion. There shouldn't be many `\\expandafter`s\r\n      (which's equivalent in `expl3` is `\\exp_after:wN`) but instead we use\r\n      special function variants that expand their argument. Of course, this is\r\n      internally implemented using `\\exp_after:wN`, but this makes code much\r\n      more readable, compare:\r\n        ```\r\n        \\exp_after:wN \\tl_set:Nn \\exp_after:wN \\l_tmpa_tl \\exp_after:wN { \\l_tmpb_tl }\r\n        \\tl_set:No \\l_tmpa_tl { \\l_tmpb_tl }\r\n        ```\r\n        Both lines do the same, but the second is much easier to read.\r\n        \r\n    - The distributions components and a bit about stability (I'll get back to\r\n      that later)\r\n    \r\n    - Notes for people writing code for LaTeX2ε using the currently available\r\n      LaTeX3 facilities.\r\n    \r\n    - Loadtime options of the `expl3` package\r\n    \r\n    - Usage of `expl3` in other formats than LaTeX2ε\r\n\r\n    - Requirements\r\n\r\n2. This might be best suited to be answered by Frank Mittelbach or Joseph\r\n   Wright, but here are my 2 cents: The ultimate purpose is to create a new\r\n   format, which will be called LaTeX3. LaTeX3 aims to be split up in more parts\r\n   than LaTeX2ε with a more sensible distinction between different tasks. The\r\n   result will be a clean programming language for the internal stuff, a design\r\n   language to define how document elements will be formatted, and a language to\r\n   mark-up documents, which might not differ from LaTeX2ε's document mark-up at\r\n   all or just in some aspects, but (to my knowledge) this isn't completely\r\n   worked out yet. So this will make the usual programming syntax of LaTeX2ε and\r\n   its internal macros obsolete, but most likely not the document mark-up\r\n   syntax.\r\n\r\n3. The code focuses on stability over performance, so the performance might not\r\n   always be top-notch. That being said, LaTeX3 still has the fastest\r\n   space-stripping code I know of and most likely in other parts is pretty\r\n   optimized as well. There are some aspects of it that can be made much faster,\r\n   and means to do so are evaluated by the team, but speed gain shouldn't affect\r\n   stability and predictability (they want document-level code to have\r\n   predictable outcome in the code-layer, e.g., macros created with `xparse`'s\r\n   `\\NewDocumentCommand` always expands to the same structure, even if it might\r\n   be faster to just expand to the code if they don't grab arguments -- but they\r\n   have a branch that shortcuts in cases like this to make the overhead\r\n   smaller). All in all the performance isn't too bad, compared to what the code\r\n   does, after all they implemented something coming close to a general purpose\r\n   language in TeX, imho.\r\n\r\n4. The code loaded by `\\usepackage{expl3}` is pretty stable. There might be\r\n   internal changes in the future, but functions documented in\r\n   `texdoc interface3` (`interface3.pdf`) can be considered stable and should\r\n   work in years, too. There are experimental functions and modules, but those\r\n   are contained in `l3experimental` (`l3benchmark.sty` is one of those packages\r\n   for example). To give you an idea, `\\ExplSyntaxOn` and `\\ExplSyntaxOff` were\r\n   last changed in 2011, some functions were not changed in ages, some were\r\n   added kind of recently. In `interface3.pdf` functions added recently (meaning\r\n   in this millennium) or changed are marked containing the date of the last\r\n   change. Citing from `expl3.pdf`:\r\n\r\n   > Wile `expl3` is still experimental, the bundle is now regarded as broadly\r\n   > stable. The syntax conventions and functions provided are now ready for\r\n   > wider use. There may still be changes to some functions, but these will be\r\n   > minor when compared to the scope of `expl3`.\r\n   >\r\n   > New modules will be added to the distributed version of `expl3` as they\r\n   > reach maturity. At present, the `expl3` bundle consists of a number of\r\n   > modules, most of which are loaded by including the line:\r\n   >\r\n   > `\\RequirePackage{expl3}`\r\n   >\r\n   > in a LaTeX2ε package, class or other file. The `expl3` modules regarded as\r\n   > stable, and therefore suitable for basing real code on, are as follows:\r\n   >\r\n   > `l3basics`, `l3box`, `l3clist`, `l3coffins`, `l3expan`, `l3int`, `l3keys`,\r\n   > `l3msg`, `l3names`, `l3prg`, `l3prop`, `l3quark`, `l3seq`, `l3skip`,\r\n   > `l3tl`, `l3token`\r\n\r\n   I've changed the layout of that list to remove the scope of the modules.\r\n\r\n5. The assumption that users can fix their own codes I'd consider bold.\r\n   Nevertheless I'd say no. The aim of all this is to get a stable and solid\r\n   platform on which designers and document authors can build. The average user\r\n   can't code in LaTeX2ε's internal level, and the internals of LaTeX2ε are\r\n   pretty obfuscated by `\\v@w@lr@pl@c@m@nt` (vowel replacement) and the\r\n   inconsistent usage of `@`. That's going to turn a lot better with `expl3`,\r\n   but users need to learn the new conventions if they want to grasp the meaning\r\n   of code. But I'd personally say that these conventions are easy to understand\r\n   and understanding `expl3` code is much easier than LaTeX2ε code once you're\r\n   accustomed to it (this is coming from someone who codes more in (La)TeX than\r\n   he writes documents with it, so I think I know what I'm saying here). Note\r\n   that with `expl3` code I don't mean the inner workings of `expl3` functions,\r\n   but the outcome of the used functions.\r\n\r\nThere is a not so short video on youtube from TUG conference 2011 of a speech given by Frank Mittelbach about the architecture of LaTeX3: https://www.youtube.com/watch?v=-lr6KEPGLDs\r\n	2019-12-14 14:30:33.699209+00	7	4	1	\N	0	0	0	\N	\N	I don't really consider myself an expert in this field, but at least I did some	f	f
515	456	2	2019-12-23 21:50:33.827627+00	Thanks for the suggestion, we've added pagination.\r\n\r\n> without pagination I feel kinda lost, because I have no idea how much more posts there are.\r\n\r\nThis is one benefit I didn't think of — I like the way the pagination control gives us an at-a-glance measure of the total number of questions:\r\n\r\n![Screenshot 2019-12-23 at 21.49.17.png](/image?hash=6f75b8de2cf99025dfc3c088a61531c98b8aa78a7153d8f113ad53466f15cfcd)	2019-12-23 21:50:33.827627+00	6	1	1	\N	0	0	0	\N	\N	Thanks for the suggestion, we've added pagination.	f	f
570	495	2	2020-01-04 21:31:40.481338+00	You can now see who imported a post on the history page.\r\n\r\nIn other words, we've done about 10% of what I think you are asking here — but I think it is the only part we will implement. The issues of what new users think will fade over time, and now that we have [the original dates](/meta?q=457) for imported posts, I think it is clearer what is new and what is not.	2020-01-04 21:31:40.481338+00	1	1	1	\N	0	0	0	\N	\N	You can now see who imported a post on the history page.	f	f
564	514	2	2020-01-04 08:05:57.604892+00	We now have a new landing page — it is the default page for topanswers.xyz, and gives an overview of our aims, and links to further information on meta.\r\n\r\n> …some kind of landing page would be really nice to help explain the purpose of the site and help new users get familiar…\r\n\r\nI hope it is going to meet that purpose, but comments and suggestions for improvement are welcome.\r\n\r\n> …this would likely be a fast changing page, so keeping it succinct would be best initially…\r\n\r\nPlease also let us know if you spot things that are out of date as time goes by, or new information that really needs to be added when things change.	2020-01-05 12:10:37.986353+00	4	1	1	\N	0	0	0	\N	\N	We now have a new landing page — it is the default page for topanswers.xyz, and gives an overview of our aims, and links to further information on meta.	f	f
578	529	709	2020-01-06 13:43:20.013208+00	I didn't see this in my initial search, but it looks like [this has already been requested](/meta?q=233)\r\n\r\nTo expand further, I might add that since there aren't many public sites yet on TA, initially a single "Activity Log" would be sufficient, but at some point site-specific Activity Logs would be ideal.	2020-01-06 14:51:46.679632+00	2	4	1	\N	0	0	0	\N	\N	I didn't see this in my initial search, but it looks like [this has already been requested](/meta?q=233)	f	f
738	529	2	2020-02-02 20:49:37.284887+00	@@@ answer 187	2020-02-02 20:49:37.284887+00	0	1	1	\N	0	0	0	\N	\N	@@@ answer 187	t	f
280	97	96	2019-12-05 07:36:39.531678+00	This will be quite easy to do...in the context of native apps. Generating the code and showing it is trivial, the tricky part is reading it. I'm pretty sure it's possible but don't think it's very easy to do at the moment using mobile web browsers.\r\n\r\nGiven the marginally better experience this would create, I don't think it's worth much developer time at this stage. If and when native aps are developed for accessing the site API this will be a great little feature.\r\n\r\nFor MFA[^note1], providing QR codes makes a lot more sense since TOTP[^note2] storage apps typically already have built-in functions for reading them.\r\n\r\n[^note1]: Multi Factor Authentication\r\n[^note2]: Timed One Time Password	2019-12-05 12:33:20.722827+00	2	4	2	\N	0	0	0	\N	\N	This will be quite easy to do...in the context of native apps. Generating the code and showing it is trivial, the tricky part is reading it. I'm pretty sure it's possible but don't think it's very easy to do at the moment using mobile web browsers.	f	f
586	536	2	2020-01-09 18:22:00.322843+00	This was an adverse side-effect of the way the room 'unread message' counters were updated.\r\n\r\n> Would it be possible to enable these links also to be opened in a new tab?\r\n\r\nYou can now open them in new tabs like normal links, and the same applies to the room links in notifications that also had the problem previously.\r\n\r\nThis fix has been part of a major revamp of how 'unread' counters are calculated and shown, so please let us know if you notice any quirks in that area.	2020-01-12 21:24:39.458404+00	3	1	1	\N	0	0	0	\N	\N	This was an adverse side-effect of the way the room 'unread message' counters were updated.	f	f
639	586	234	2020-01-20 16:13:46.001701+00	A path always extends the bounding box as if it had `line cap=rect`, regardless of whether or not it really has. So one may want to dial that option, and subtract `\\pgflinewidth` from its length to arrive at\r\n\r\n\r\n```\r\n\\documentclass[tikz]{standalone}\r\n\r\n\\begin{document}\r\n\r\n\\begin{tikzpicture}\r\n\t\\draw[line width=5cm, green,line cap=rect] (0,0) --\r\n\t(\\paperwidth-\\pgflinewidth,0);\r\n\\end{tikzpicture}\r\n\r\n\\begin{tikzpicture}\r\n\t\\draw[line width=1cm, green,line cap=rect] (0,0) -- (\\paperwidth-\\pgflinewidth,0);\r\n\\end{tikzpicture}\r\n\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2020-01-20 at 8.13.30 AM.png](/image?hash=a565c43f4ab327045f23e5c201066068f3a57d6c0e049dddfa50288989b7f7e0)	2020-01-20 18:48:46.810319+00	4	4	3	\N	0	0	0	\N	\N	A path always extends the bounding box as if it had `line cap=rect`, regardless of whether or not it really has. So one may want to dial that option, and subtract `\\pgflinewidth` from its length to arrive at	f	f
553	485	2	2019-12-29 13:57:30.671327+00	update 17 Feb 2020: you can now search for tags, along with a couple of other search features that have been implemented.\r\n\r\n---\r\n\r\n~~No there is no way to browse tags or to search by tags yet.~~\r\n\r\nThis feature request is now on GitHub, referencing this question and this similar question:\r\n\r\n@@@ answer 365\r\n	2020-02-17 00:33:09.43298+00	5	1	1	\N	0	0	0	\N	\N	update 17 Feb 2020: you can now search for tags, along with a couple of other search features that have been implemented.	f	f
631	579	2	2020-01-17 00:08:27.158385+00	Definitely off-putting, thanks for drawing attention to this. We've added a landing page for private beta communities as suggested.\r\n\r\n> * just present a static webpage saying that the page is not yet out of beta\r\n\r\nWe've chosen this option for now, with a link to the meta chat room where access can be manually requested. We might well refine this at some point with a more specific link to information about the particular community, and I'll update this post when that happens.	2020-01-17 00:08:27.158385+00	6	1	1	\N	0	0	0	\N	\N	Definitely off-putting, thanks for drawing attention to this. We've added a landing page for private beta communities as suggested.	f	f
630	579	772	2020-01-16 21:45:16.745325+00	I was just recommend to come to top answers and check out the private beta for TeX and I got the same page. I knew I wouldn't get into the site, but that was a pretty off putting message. I was really expecting some sort of directions of how to get access. It would be nice to have something more than just the error page. You almost lost me, a somewhat tech competent individual, to the void because of the error page.	2020-01-16 21:45:16.745325+00	6	4	3	\N	0	0	0	\N	\N	I was just recommend to come to top answers and check out the private beta for TeX and I got the same page. I knew I wouldn't get into the site, but that was a pretty off putting message. I was really expecting some sort of directions of how to get access. It would be nice to have something more than just the error page. You almost lost me, a somewhat tech competent individual, to the void because of the error page.	f	f
402	182	2	2019-12-10 21:04:35.841244+00	We don't have a complete answer to this question yet. What we do have is our first 'moderation'-like feature, just released.\r\n\r\nOur hope is that this will be a model of other features as they are slowly rolled out in response to need. However it is also a trial of sorts, and subject to significant changes based on feedback here, after people have tried it out for a while. The idea is to break the overall 'moderator' role down, into finer-grained areas of expertise and trust.\r\n\r\nBefore getting into the feature itself, one thing needs to be said very clearly: ***all the following actions will be public***. Your name and identicon will be attached to posts you flag, and visible to others, including the OP.\r\n\r\nHere's how the new feature works right now:\r\n\r\n* Every post[^1] initially carries a 'flag' button next to the 'subscribe button':  \r\n   ![Screenshot 2019-12-10 at 20.33.29.png](/image?hash=21a5ee117c4df5dd87e9937840564784f9bee54ddabf600e00aca96c91ba7e0a)  \r\n   \r\n   Each community can decide it's own 'flagging' guidelines, but the expection is that it would be for content you'd want deleted: spam, off-topic questions or junk for example.\r\n   \r\n   flagging a post has two effects:\r\n   1. The post is immediately hidden from all unregistered users\r\n   2. A notification is sent to members of the post cleanup 'crew'\r\n   \r\n* Unlike regular registered users, members of the cleanup crew flag in a different way:\r\n   * As soon as a 'crew' member flags, the post is hidden from all non-crew members\r\n   * Crew members also have the option to 'counterflag' — this overrides regular flags user and makes the post visible again\r\n   * If multiple crew members flag and counterflag, the overall action is based on the majority.\r\n   * if a clear majority forms, outstanding notifications are cleared.\r\n\r\nIn normal use I expect something like the following to happen:\r\n\r\n1. Several users flag a post drawing it to the attention of the 'crew'. Comments are made in the question chat room  if necessary.\r\n2. The first two 'crew' members to visit the post confirm the flags and the post is then effectively deleted (invisible to everyone except 'crew' and the OP).\r\n\r\nFinally, I anticipate that each community will decide on the rules for an automatic background job. The job will flag questions that meet certain criteria (e.g. no answers or votes after 4 weeks).\r\n\r\n[^1]: except your own — and right now it is only working on questions (answer will follow if the feature is well-received, otherwise it is back to the drawing board!)	2019-12-24 09:52:26.700852+00	9	1	1	\N	0	0	0	\N	\N	We don't have a complete answer to this question yet. What we do have is our first 'moderation'-like feature, just released.	f	f
206	182	115	2019-12-04 01:44:03.384036+00	I don't know what *will* be done, but I'd like to offer some proposals (recognizing that this question is pretty broad).\r\n\r\n**Who will moderate?** That should be up to each community.  From a platform perspective, we shouldn't care whether they choose founders, hold elections for permanent positions, draw straws every six months, or make "moderator" just another privilege that is gained through site activity.  The platform's job is to provide a means of designating someone a moderator.\r\n\r\n**What tools do we need?** This will be constantly evolving.  We should start with the minimum that gets the job done.  I think that means:\r\n\r\n- A way for users to flag content for moderator attention.\r\n\r\n- A way for somebody to hide/delete content.  Initially, this probably means a manual process.  Later it could be community-driven (enough flags, for example).\r\n\r\nOn day one I think that's enough.  Soon after I think we will also need:\r\n\r\n- A way to lock content -- prevent edits to a post, prevent new messages in a chat room, maybe other things.  When faced with vandalism like a spew of rude comments/chat messages on a post, there should be an option that's milder than deleting the post to which the activity is attached.\r\n\r\n- A way to address users who are behaving in ways that are disrupting the community.  This could mean suspensions, but it could also mean revoking specific privileges, depending on what the privilege system ends up looking like.  Or it could mean imposing rate limits.\r\n\r\nI propose that moderators be the ones who wield these tools and make these decisions.  Things baked into the system tend to be good fits for some communities and bad fits for others.  Initially, give the power to the human moderators, and then see what patterns emerge.\r\n\r\nAt some point we're going to be successful enough to be facing spambots coming through Tor, and that'll need better tools.  I don't know how early we need to consider that; my gut feeling is that our earliest concerns are about disruptive humans who can prevent communities from taking root by setting a bad mood, rather than insurance spammers that everybody knows to ignore.\r\n\r\n	2019-12-04 01:44:03.384036+00	9	4	1	\N	0	0	0	\N	\N	I don't know what *will* be done, but I'd like to offer some proposals (recognizing that this question is pretty broad).	f	f
552	494	167	2019-12-29 13:56:20.169808+00	Not sure if this counts as "not drawing it manually", but at the moment, the line below the origin comes from `enlarge y limits={value=.02,lower}`. You could  instead set `ymin` to the same value as your tick at VaR.\r\n\r\nThe reason `enlarge y limits={value=.02,lower}` did not work as expected is that this seems to be a relative value. From the `pgfplots` docu:\r\n\r\n> `\\pgfplotsset{enlarge x limits={value=0.2,upper}}` will enlarge (only) the upper axis limit by 20% of the axis range.\r\n\r\n\r\n```\r\n\\documentclass[11pt]{standalone}\r\n% tikz\r\n\\usepackage{tikz}\r\n\\usepackage{pgfplots}\r\n\\pgfplotsset{compat=newest}\r\n\\usetikzlibrary{arrows.meta}\r\n\\usetikzlibrary{positioning}\r\n\\usetikzlibrary{backgrounds}\r\n\t\\pgfplotsset{\r\n\t\tassi/.style={\r\n\t\t\taxis lines=middle,\r\n\t\t\taxis line style={-Stealth},\r\n\t\t\tticks=none,\r\n\t\t\tsamples=300,\r\n\t\t\tclip=false,\r\n\t\t\t},\r\n\t}\r\n\\usetikzlibrary{math}\r\n\\tikzmath{%\r\n  function h1(\\x, \\lx) { return (9*\\lx + 3*((\\lx)^2) + ((\\lx)^3)/3 + 9); };\r\n  function h2(\\x, \\lx) { return (3*\\lx - ((\\lx)^3)/3 + 4); };\r\n  function h3(\\x, \\lx) { return (9*\\lx - 3*((\\lx)^2) + ((\\lx)^3)/3 + 7); };\r\n  function skewnorm(\\x, \\l) {\r\n    \\x = (\\l < 0) ? -\\x : \\x;\r\n    \\l = abs(\\l);\r\n    \\e = exp(-(\\x^2)/2);\r\n    return (\\l == 0) ? 1 / sqrt(2 * pi) * \\e: (\r\n      (\\x < -3/\\l) ? 0 : (\r\n      (\\x < -1/\\l) ? \\e / (8 * sqrt(2 * pi)) * h1(\\x, \\x*\\l) : (\r\n      (\\x <  1/\\l) ? \\e / (4 * sqrt(2 * pi)) * h2(\\x, \\x*\\l) : (\r\n      (\\x <  3/\\l) ? \\e / (8 * sqrt(2 * pi)) * h3(\\x, \\x*\\l) : (\r\n      sqrt(2/pi) * \\e)))));\r\n  };\r\n}\r\n\\begin{document}\r\n\\begin{tikzpicture}\r\n\\begin{axis}[\r\n\tassi,\r\n\tenlarge x limits={value=.04,upper},\r\n%\tenlarge y limits={value=.03,lower},\r\n\tymin=-0.02,\r\n\tymax=.73,\r\n\txscale=1.2,\r\n\txlabel = {Loss},\r\n\tx label style={at=(current axis.right of origin),\r\n\t\tanchor=west},\r\n\tylabel = {Probability\\\\ density},\r\n\ty label style={%at=(current axis.above origin),\r\n\t\tanchor=north east, align=center, %inner sep=0pt\r\n\t\t},\r\n\t]\r\n\t\\pgfmathsetmacro{\\limiteinf}{-1.9}\r\n\t\\pgfmathsetmacro{\\limitesup}{2.8}\r\n\t\\pgfmathsetmacro{\\VaRx}{1.6}\r\n\t\\pgfmathsetmacro{\\VaR}{skewnorm(\\VaRx+.6,2)}\r\n\t\\begin{scope}[on background layer]\r\n\t\t\\addplot[fill=gray!30, draw=none, domain=\\VaRx:\\limitesup] \r\n\t\t\t{skewnorm((x+.6), 2)} \\closedcycle;\r\n\t\\end{scope}\r\n\t\\addplot[very thick, domain=\\limiteinf:\\limitesup] {skewnorm((x+.6), 2)};\r\n\t\\draw (axis cs:\\VaRx,0) -- (axis cs:\\VaRx,\\VaR);\r\n\t\\draw (axis cs:\\VaRx,0) -- (axis cs:\\VaRx,-.02) \r\n\t\tnode[below=4pt, inner sep=0pt] {$\\mathrm{VaR}_\\alpha$};\r\n\t\\node (unomenoalfa) at (axis cs:2.3,0.12) {$1-\\alpha$}; \r\n\t\\draw[-Stealth] (unomenoalfa) -- (axis cs:1.7,0.015);\r\n\\end{axis}\r\n\\end{tikzpicture}\r\n\\end{document}\r\n```\r\n\r\n![document5.png](/image?hash=3f7c1ca7f7964f7c5afe96a77f5369aa516ac300f88a8fd1987fdd494a626876)	2019-12-29 14:52:31.960232+00	6	4	3	\N	0	0	0	\N	\N	Not sure if this counts as "not drawing it manually", but at the moment, the line below the origin comes from `enlarge y limits={value=.02,lower}`. You could  instead set `ymin` to the same value as your tick at VaR.	f	f
49	76	12	2019-11-21 20:34:13.105847+00	> * Is the memory grant *always* the same? i.e. is is always 1024kb (making a number up, here).\r\n\r\nThe minimal size for a hash table is 16KB (single partition, 512 row estimate).\r\n\r\nIt is possible that a local aggregate with many aggregations could get a larger fixed memory allocation, but I have not encountered this.\r\n\r\nThere is a separate instance of the local aggregate on each thread.\r\n\r\n> * Is the memory grant derived from the query's total memory grant?\r\n\r\nNo it is acquired separately at query startup.\r\n\r\nThe *Memory Fractions* for a local aggregate are zero (input and output).\r\n\r\nGrant-affecting options like `MAX_GRANT_PERCENT` have no effect.\r\n\r\n> * Is the memory grant a runtime decision based on how much memory is left over from other memory consuming operators?\r\n\r\nNo.\r\n\r\n> I've noticed that in repeated runs of the same query with no changes made, where it receives the same memory grant, different numbers of rows will emit from the Partial Aggregate\r\n\r\nThis depends on the order in which rows arrive, which will generally be different on each run due to timing differences in the plan operators below the local aggreate.\r\n\r\nEach local aggregate adds a new entry to its hash table when it encounters a new grouping value. The entry maintains the local aggregate results associated with that key value. In your case, the key is `DisplayName`, and the maintained aggregations are:\r\n\r\n```\r\n[partialagg1011] = Scalar Operator(MAX([StackOverflow2013].[dbo].[Users].[Reputation]\r\n[partialagg1012] = Scalar Operator(COUNT_BIG([Expr1007]))\r\n[partialagg1014] = Scalar Operator(SUM([Expr1007]))\r\n[partialagg1016] = Scalar Operator(COUNT_BIG([Expr1008]))\r\n[partialagg1018] = Scalar Operator(SUM([Expr1008]))\r\n```\r\n\r\nAs new rows arrive, they either contribute to the subtotals for an existing key, or add a new hash table entry. The number of unaggregated rows therefore depends on which group keys are encountered first. If key values with more duplicates are encountered early, fewer rows will be left unaggregated.\r\n\r\nIn this case, the subtotals are all fixed-length, but they do not have to be.  The hash table stores in-row data types directly. Longer length subtotals (e.g. strings) would take more space and reduce the number of total entries that would fit in the available space.	2019-11-21 20:34:13.105847+00	3	4	1	\N	0	0	0	\N	\N	> * Is the memory grant *always* the same? i.e. is is always 1024kb (making a number up, here).	f	f
587	539	2	2020-01-09 23:18:21.254169+00	Yes, just like on SE, there are public, gallery and private rooms.\r\n\r\n> …is there a way to request access to rooms such as the "Stop the Merry Go Round" room?\r\n\r\n[Stop the Merry Go Round](/databases?room=4) is a gallery room so all the chat is publicly visible, but write-access is invite-only.\r\n\r\nCurrently the only way to request access is to get someone from the room to ping me in chat — when that changes I'll update this post…	2020-01-09 23:18:21.254169+00	5	1	1	\N	0	0	0	\N	\N	Yes, just like on SE, there are public, gallery and private rooms.	f	f
258	292	115	2019-12-04 15:20:36.38818+00	Possibly related: I would like more room for message entry.  I know that as I type more the box expands, but that causes things to move around (worse with the preview) too.  I'd really like to have a box that's a few lines high right from the start, which also gives me the benefit of not having to look at the *very bottom right* of my browser to type at all.  (This is one of the less-comfortable places to focus, at least for me -- though I think I've seen research supporting the claim that it's a weaker zone in general.)\r\n	2019-12-04 15:20:36.38818+00	0	4	1	\N	0	0	0	\N	\N	Possibly related: I would like more room for message entry.  I know that as I type more the box expands, but that causes things to move around (worse with the preview) too.  I'd really like to have a box that's a few lines high right from the start, which also gives me the benefit of not having to look at the *very bottom right* of my browser to type at all.  (This is one of the less-comfortable places to focus, at least for me -- though I think I've seen research supporting the claim that it's a weaker zone in general.)	f	f
261	292	14	2019-12-04 18:36:56.624006+00	As an alternative suggestion to a per-user option (on the profile page), it might be nice to have a keyboard shortcut that toggles the preview on or off (ctrl + p comes to mind, but that's already used for printing in most browsers - but who prints anything these days :P).  \r\n\r\nThis way you could quickly toggle it on, to see how a message with complicated markdown renders for example, but otherwise it is dismissed.\r\n\r\nThis toggle could be "remembered" in the browser session without needing to be a persistent user profile option.	2019-12-04 18:36:56.624006+00	6	4	1	\N	0	0	0	\N	\N	As an alternative suggestion to a per-user option (on the profile page), it might be nice to have a keyboard shortcut that toggles the preview on or off (ctrl + p comes to mind, but that's already used for printing in most browsers - but who prints anything these days :P).	f	f
256	292	96	2019-12-04 15:00:12.355686+00	This won't be _quite_ as easy as just adding `display: none;` to the element (or I would do it myself in a user style) because the same pane is re-used to show one bit of information that is functionally useful: who if anybody is getting pinged by the message.	2019-12-04 15:00:12.355686+00	0	4	2	\N	0	0	0	\N	\N	This won't be _quite_ as easy as just adding `display: none;` to the element (or I would do it myself in a user style) because the same pane is re-used to show one bit of information that is functionally useful: who if anybody is getting pinged by the message.	f	f
257	292	12	2019-12-04 15:05:38.590413+00	Chat message preview was added based on user feedback and has been very well recieved.\r\n\r\nPlease don't take it away.\r\n\r\nMaking it an option in the profile seems sensible if it is not too much work.\r\n\r\nThere might be a thousand other things you want to do first.	2019-12-04 15:05:38.590413+00	7	4	2	\N	0	0	0	\N	\N	Chat message preview was added based on user feedback and has been very well recieved.	f	f
589	540	167	2020-01-10 10:25:05.577128+00	Maybe you could use something like `<+-.(n)>`. This would mean\r\n\r\n- `+` a new overlay is created \r\n- `-` until\r\n- `.` current overlay\r\n- `(n)` plus the next n overlays \r\n\r\n\r\n```\r\n\\documentclass{beamer}\r\n\\begin{document}\r\n\\begin{frame}[t]\r\n\\frametitle{Some extensive slide (overlap 1)}\r\n\\only<+-.(2)>{Statement A \\[A=B\\]}\r\n\\only<+-.(2)>{Statement B \\[B=C\\]}\r\n\\only<+-.(2)>{Statement C \\[C=D\\]}\r\n\\only<+-.(2)>{Statement D \\[D=E\\]}\r\n\\only<+-.(2)>{Statement E \\[E=F\\]}\r\n\\only<+-.(2)>{Statement F \\[F=G\\]}\r\n\\end{frame}\r\n\r\n\\begin{frame}[t]\r\n\\frametitle{Some extensive slide (overlap 2)}\r\n\\only<+-.(3)>{Statement A \\[A=B\\]}\r\n\\only<+-.(3)>{Statement B \\[B=C\\]}\r\n\\only<+-.(3)>{Statement C \\[C=D\\]}\r\n\\only<+-.(3)>{Statement D \\[D=E\\]}\r\n\\only<+-.(3)>{Statement E \\[E=F\\]}\r\n\\only<+-.(3)>{Statement F \\[F=G\\]}\r\n\\end{frame}\r\n\r\n\\end{document}\r\n```\r\n\r\n\r\n![document.gif](/image?hash=efac9943c5ed39bd54592f2656ff7206bb2d3541f62f930d9879025f26953040)	2020-01-10 10:29:09.481261+00	4	4	3	\N	0	0	0	\N	\N	Maybe you could use something like `<+-.(n)>`. This would mean	f	f
51	74	12	2019-11-21 20:50:56.898302+00	I would not be in a rush to implement question closing here, just because that's how SE works.\r\n\r\nIf a question is a poor fit for the site, in the worst case it will be ignored, which is arguably a worse fate than being deleted. Otherwise, people will use chat to guide the author to improve their post.\r\n\r\nIt seems to me that chat (or being ignored) is just as likely to produce the desired outcome of an improving edit as closing (putting "on hold") would be. Probably more so.\r\n\r\nWe might need a proper process to *delete* unsuitable posts at some stage, but I don't think we can make that call yet, and there are other feature priorities.\r\n\r\nThe main advantage of closing on SE is that it can eventually qualify a question for automatic deletion. Closing an answered question is less likely to lead to that outcome. In many cases, all closing does is preserve the content in a state that means better answers cannot be added. It also often leads to answer material being posted in comments.	2019-11-21 20:50:56.898302+00	5	1	1	\N	0	0	0	\N	\N	I would not be in a rush to implement question closing here, just because that's how SE works.	f	f
660	600	234	2020-01-23 18:17:10.798544+00	I think that the patch is not complete, for instance `\\colorlet` also leads to an error. So I am wondering if defining your own fill may be an option.\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\usepackage{xcolor}\r\n\\usepackage{xstring}\r\n\\usepackage{xpatch}\r\n\\usepackage{tikz}\r\n\r\n\\makeatletter\r\n\\xpatchcmd{\\XC@split@viii}{\\c@lor@error{`\\@@nam'}\\def\\@@nam{black}}{%\r\n\t\\def\\colorprefix{c}%\r\n\t\\StrSplit{#1}{1}{\\firstletter}{\\restletter}%\r\n\t\\if\\firstletter\\colorprefix\r\n\t\t\\definecolor{#1}{HTML}{\\restletter}%\r\n\t\t\\def\\@@nam{#1}%\r\n\t\\else\r\n\t\t\\c@lor@error{`\\@@nam'}\\def\\@@nam{black}%\r\n\t\\fi  \r\n}{\\typeout{patching xcolor sucess}}{\\typeout{patching xcolor failure}}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\r\n\\textcolor{cffa500}{\\rule{2cm}{2cm}}\r\n\\textcolor{c10a500}{\\rule{2cm}{2cm}}\r\n%\\colorlet{mycolor}{cffa500} %<also fails\r\n\r\n\\tikzset{Fill/.code={\\StrSplit{#1}{1}{\\firstletter}{\\restletter}%\r\n\\definecolor{#1}{HTML}{\\restletter}%\r\n\\tikzset{fill=#1}\r\n}}\r\n\r\n\\tikz{\\fill[Fill=cffa500] (0,0) circle[radius=2];}\r\n\r\n\\end{document}\r\n```\r\n\r\n![Screen Shot 2020-01-23 at 10.16.53 AM.png](/image?hash=dee8d749e53737ed737a1965fafc11b44f3584eb32fa73085a5c38d6c00afa66)	2020-01-24 11:26:14.650557+00	5	4	3	\N	0	0	0	\N	\N	I think that the patch is not complete, for instance `\\colorlet` also leads to an error. So I am wondering if defining your own fill may be an option.	f	f
662	600	167	2020-01-23 20:12:36.374953+00	@marmots great answer made me realize that I was totally barking up the wrong tree. Instead of trying to hack `xcolor`, one can tackle the problem from the `tikz` side. \r\n\r\nOne possibility (probably not the best) is to hook into the definition of `\\pgfsetfillcolor`\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\usepackage{xstring}\r\n\\usepackage{tikz}\r\n\\usepackage{tikzlings}\r\n\r\n\\makeatletter\r\n\\def\\pgfsetfillcolor#1{%\r\n\t\\def\\colorprefix{c}%\r\n\t\\StrLen{#1}[\\colorlenght]%\r\n\t\\StrSplit{#1}{1}{\\firstletter}{\\restletter}%\r\n\t\\if\\firstletter\\colorprefix%\r\n\t\t\t\\ifnum\\colorlenght=7%\r\n\t\t\t\t\\definecolor{#1}{HTML}{\\restletter}%\r\n\t\t\t\\fi%\r\n\t\\fi%\r\n  \\pgfutil@colorlet{pgffillcolor}{#1}%\r\n  \\pgfutil@ifundefined{applycolormixins}{}{\\applycolormixins{pgffillcolor}}%\r\n  \\expandafter\\let\\expandafter\\pgf@temp\\csname\\string\\color@pgffillcolor\\endcsname\r\n  % for arrow tips:\r\n  \\global\\let\\pgf@fillcolor@global=\\pgf@temp\r\n  \\expandafter\\pgf@setfillcolor\\pgf@temp\r\n}\r\n\\makeatother\r\n\r\n\\begin{document}\r\n\r\n\r\n\\begin{tikzpicture}\r\n\r\n\\marmot\r\n\r\n\\path[fill=cyan] (2,0) circle[radius=2];\r\n\r\n\\path[fill=cffa500] (4,0) circle[radius=2];\r\n\\end{tikzpicture}\r\n\r\n\\end{document}\r\n```\r\n\r\nCaveats:\r\n\r\n- this will only influence fill colours, but similar could be done with strokes etc.\r\n\r\n- it will run into a problem if combined with non hex colours that also start with `c` and also have 7 characters	2020-01-23 20:12:36.374953+00	4	4	3	\N	0	0	0	\N	\N	@marmots great answer made me realize that I was totally barking up the wrong tree. Instead of trying to hack `xcolor`, one can tackle the problem from the `tikz` side.	f	f
633	583	168	2020-01-17 13:36:52.194167+00	As a generalized approach, independent on the used editor, one can invoke the compiling binary with something like the following from the command line:\r\n\r\n```sh\r\n<tex> -jobname="<file>" "<definitions>\\\\input{<file>}"\r\n```\r\n\r\nSo adapted to your use case this would look like\r\n\r\n```sh\r\npdflatex -jobname="<file>-noanswer" "\\\\newcommand*\\\\version{noanswer}\\\\input{<file>}"\r\npdflatex -jobname="<file>" "\\\\newcommand*\\\\version{}\\\\input{<file>}"\r\n```\r\n\r\nWith the same modifications to the file as you did this would create two PDFs, one `<file>.pdf` and one `<file>-noanswer.pdf`.\r\n\r\n\r\n```tex\r\n\\documentclass{article}\r\n\r\n% setting a default value in case it is compiled without the magic comment\r\n\\unless\\ifdefined\\version\r\n\\def\\version{noanswer}\r\n\\fi\r\n\r\n\\usepackage[\\version]{exercise}\r\n\r\n\\begin{document}\r\n\r\n    \\begin{Exercise}[title={Title},label=ex1]\r\n\r\n        question text\r\n\r\n    \\end{Exercise}\r\n\r\n    \\begin{Answer}[ref={ex1}]\r\n\r\n        solution\r\n\r\n    \\end{Answer}\r\n\r\n\\end{document}\r\n```	2020-01-17 13:36:52.194167+00	4	6	3	\N	0	0	0	\N	\N	As a generalized approach, independent on the used editor, one can invoke the compiling binary with something like the following from the command line:	f	f
632	583	167	2020-01-17 10:03:25.08019+00	With texstudio one can use the following magic comment to automatically compile two versions:\r\n\r\n```\r\n% !TeX program = latexmk -pdf -pdflatex="pdflatex -synctex=1 -interaction=nonstopmode -shell-escape" -jobname=% -pretex="\\newcommand{\\version}{noanswer}" -usepretex % | latexmk -pdf -pdflatex="pdflatex -synctex=1 -interaction=nonstopmode -shell-escape" -jobname=%_solution -pretex="\\newcommand{\\version}{}" -usepretex % | txs:///view-pdf "?am)_solution.pdf"\r\n\r\n\\documentclass{article}\r\n\r\n% setting a default value in case it is compiled without the magic comment\r\n\\unless\\ifdefined\\version\r\n\\def\\version{noanswer}\r\n\\fi\r\n\r\n\r\n\\usepackage[\\version]{exercise}\r\n\r\n\\begin{document}\r\n\r\n    \\begin{Exercise}[title={Title},label=ex1]\r\n\r\n        question text\r\n\r\n    \\end{Exercise}\r\n\r\n    \\begin{Answer}[ref={ex1}]\r\n\r\n        solution\r\n\r\n    \\end{Answer}\r\n\r\n\\end{document}\r\n```\r\nThe magic comment consists of three separate steps:\r\n\r\n1. - `latexmk` creating the student version with latexmk, which will automatically determine the required number of latex runs and other tools\r\n   - `-pdf` telling latexmk to create a pdf\r\n   - `-pdflatex="pdflatex -synctex=1 -interaction=nonstopmode -shell-escape"` passing some options pdflatex, for example in my real world example I need `-shell-escape` for some diagrams. If you don't need it, better remove this option.\r\n   - `-jobname=%` setting a job name\r\n   - `-pretex="\\newcommand{\\version}{noanswer}"` passing the `noanswer` option to the document\r\n   - `-usepretex %` make sure latexmk will use the pretex option which we just set \r\n\r\n2. - `latexmk` creating the version for myself including answers\r\n   - `-pdf` see above\r\n   - `-pdflatex="pdflatex -synctex=1 -interaction=nonstopmode -shell-escape"` see above\r\n   - `-jobname=%_solution` changing the jobname to not overwrite the student version\r\n   - `-pretex="\\newcommand{\\version}{}"` include the answers\r\n   - `-usepretex %` see above\r\n\r\n3. `txs:///view-pdf "?am)_solution.pdf"` will display the pdf with the solution in the pdf viewer\r\n\r\n	2020-01-17 13:47:26.911484+00	2	4	3	\N	0	0	0	\N	\N	With texstudio one can use the following magic comment to automatically compile two versions:	f	f
345	344	2	2019-12-06 10:40:30.769067+00	Right now the way to add tags is to give me a list[^1].\r\n\r\nWe will of course at some point cater for user-added tags, but in the meantime as a community TeX needs to decide how it will be using tags:\r\n\r\n1. Just a few, carefully chosen, tags that cover the areas people will want to filter questions by\r\n1. As many as tex.se (about 2000)\r\n1. Something in between\r\n\r\nMy advice is to focus on the *reason* for tagging. Things like "I want to filter questions by X" and "We don't want every question title to contain Y or Z" seem to me to be good reasons. On the other hand "Tags help Google" seems to be to be very tenuous, and a lot of effort for the community to maintain.\r\n\r\nYou can of course change your mind later on whatever you decide now.\r\n\r\n[^1]: TopAnswers has the concept of 'implied' tags. Each tag can 'imply' a maximum of one other tag.	2019-12-06 10:40:30.769067+00	0	1	1	\N	0	0	0	\N	\N	Right now the way to add tags is to give me a list[^1](TopAnswers has the concept of 'implied' tags. Each tag can 'imply' a maximum of one other tag.).	f	f
346	344	96	2019-12-06 10:54:04.907691+00	We don't need all of the SE ones (yet), but given the nature of engine specific expertise and packages, filtering on those things is going to be a very common desire. If nothing else as an end user it is _very_ useful to be able to limit searches to a tag of the package or type of thing I'm looking for when hunting for previous questions, and all of these will fit into that usage pattern.\r\n\r\nHere is a list to get us started. Feel free to edit this to add...\r\n\r\n## TeX Engines\r\n\r\n* pdftex\r\n* luatex\r\n* xetex\r\n* context\r\n* latex3\r\n\r\n## Bundles of stuff that are their own ecosystem\r\n\r\n* koma\r\n* texlive\r\n* ctan\r\n\r\n## Common packages\r\n\r\n* tikz\r\n* fancyhdr\r\n* graphicx\r\n* biblatex\r\n\r\n## Areas of expertise\r\n\r\n* math\r\n* fonts\r\n* figures\r\n* classes\r\n* tables\r\n* beamer\r\n\r\n## Build tools\r\n\r\n* arara\r\n* latexmk\r\n* pandoc	2019-12-06 10:54:04.907691+00	3	4	2	\N	0	0	0	\N	\N	We don't need all of the SE ones (yet), but given the nature of engine specific expertise and packages, filtering on those things is going to be a very common desire. If nothing else as an end user it is _very_ useful to be able to limit searches to a tag of the package or type of thing I'm looking for when hunting for previous questions, and all of these will fit into that usage pattern.	f	f
563	513	2	2020-01-04 08:03:38.327689+00	Update Feb 2020: we've implemented this for long code blocks but not yet for chat messages.\r\n\r\nThat is still on the cards, and added as another [GitHub issue](https://github.com/topanswers/topanswers/issues/27), but not very high priority.\r\n\r\n---\r\n\r\nWe do need to think about this.\r\n\r\nIt's part of a wider class of problems. In the questions and answers we don't restrict the height of code blocks, some of which are very long. There is an argument for doing what SE do and what we currently do in comments, and simply set a max height and scroll, but then:\r\n\r\n> the user then has to move their cursor over the message to scroll through the message.\r\n\r\nSo that's not ideal either. Generally I think scrollbars within a scrolling section should be avoided — some people are fine with them on some devices, others really struggle with them.\r\n\r\nMy current thinking is like one of your options:\r\n\r\n> have some indication that the post has been truncated (such as three dots at the bottom of the post). When the user clicks on the post (or perhaps just the three dots), the post would expand to full size for the reader's viewing pleasure.\r\n\r\nI think we can do that in chat and in Q&A.\r\n\r\nI've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/24) so work can begin, and will update the post here when it is complete.	2020-02-12 15:24:26.864168+00	5	1	1	\N	0	0	0	\N	\N	Update Feb 2020: we've implemented this for long code blocks but not yet for chat messages.	f	f
625	575	168	2020-01-16 08:52:24.41881+00	If you're running TeXlive you can add a folder to the list of folders to be searched (without needing to be indexed, so like the `$TEXMFHOME`) by issuing the following command (this will be permanent, should be run as root -- on \\*nix that is, no idea about Windows):\r\n\r\n```sh\r\ntlmgr conf auxtrees add /path/to/folder\r\n```\r\n\r\nA folder added this way might be removed using\r\n\r\n```sh\r\ntlmgr conf auxtrees remove /path/to/folder\r\n```\r\n\r\n---\r\n\r\nAdditionally the readme of https://github.com/pgf-tikz/pgf states the following possibility (but I never tried that one) for an inclusion for the current shell session (for example to test building the pgfmanual, this replaces your `$TEXMFHOME`):\r\n\r\n```sh\r\ntlmgr init-usertree --usertree <folder>\r\nexport TEXMFHOME=`realpath <folder>`\r\n```	2020-01-16 09:17:52.099837+00	3	6	3	\N	0	0	0	\N	\N	If you're running TeXlive you can add a folder to the list of folders to be searched (without needing to be indexed, so like the `$TEXMFHOME`) by issuing the following command (this will be permanent, should be run as root -- on \\*nix that is, no idea about Windows):	f	f
619	575	262	2020-01-15 13:31:58.992297+00	I think I figured it out.\r\n\r\nWhen looking at `/path/to/texlive/2019/texmf-dist/web2c/texmf.cnf`, I found this line:\r\n\r\n```none\r\nTEXMF = {$TEXMFAUXTREES$TEXMFCONFIG,$TEXMFVAR,$TEXMFHOME,!!$TEXMFLOCAL,!!$TEXMFSYSCONFIG,!!$TEXMFSYSVAR,!!$TEXMFDIST}\r\n```\r\n\r\nSo I just add the following lines to my custom `texmf.cnf` file (`/path/to/texlive/2019/texmf.cnf`):\r\n\r\n```\r\nTEXMFCUSTOMONE = /path/to/texmf/first\r\nTEXMFCUSTOMTWO = /path/to/texmf/second\r\nTEXMFCUSTOMTHREE = /path/to/texmf/third\r\n%...\r\nTEXMF = {$TEXMFAUXTREES$TEXMFCONFIG,$TEXMFVAR,$TEXMFHOME,!!$TEXMFLOCAL,!!$TEXMFSYSCONFIG,!!$TEXMFSYSVAR,!!$TEXMFDIST,$TEXMFCUSTOMONE,$TEXMFCUSTOMTWO,$TEXMFCUSTOMTHREE}\r\n```\r\n\r\nNow if I put a `.sty` file in `/path/to/texmf/first/tex/latex/a`, it works:\r\n\r\n```\r\n$ kpsewhich a.sty\r\n/path/to/texmf/first/tex/latex/a/a.sty\r\n```\r\n\r\nHowever, I am not sure if I have completed everything. Is there anything I need to do now?	2020-01-15 13:31:58.992297+00	2	4	1	\N	0	0	0	\N	\N	I think I figured it out.	f	f
582	530	96	2020-01-08 08:21:28.003001+00	Yes.\r\n\r\nMy suggestion would be to make the scope pretty broad, maybe even a little broader than the [U&L Stack Exchange site](https://unix.stackexchange.com/) was, covering Open Source operating systems and user environments (desktop  and cli):\r\n\r\n* All distributions, both Linux, BSD, and old fashioned Unix. I would even go so far as to say that other Open Source kernels and platforms should be included when they crop up (e.g. [Haiku](https://www.haiku-os.org/) or [Plan 9](https://9p.io/plan9/)).\r\n* All related OSS user space tool sets such as GNU tools.\r\n* All desktop environments that are not specific to non-\\*nix platforms.\r\n* All shells that are not specific to another platform (no PowerShell).\r\n\r\nNote I would include the above even in isolation on non-\\*nix platforms, such as `bash` when run under the Windows Subsystem for Linux or X11 on MacOS (*ala* XQartz).	2020-01-08 08:22:01.441009+00	5	4	2	\N	0	0	0	\N	\N	Yes.	f	f
554	497	2	2019-12-29 21:56:35.980567+00	I'm unsure if we ever want something exactly like Community Wiki the way it works on SE — but I'm interested to hear arguments for and against.\r\n\r\n> That is, no one editing it, including the first one to write it, earns stars for their personal score.\r\n\r\nThis is possible already in a roundabout way that you may like to consider. If you post the question or answer with a dummy account (for example by 'joining' from a private browser session), then no-one real will get the 'credit':\r\n\r\n1. open private window \r\n2. click 'join'\r\n3. go to profile and change account name to 'Wiki'\r\n4. post blank answer\r\n5. close private window.\r\n\r\nThere is no need to keep access to the wiki account, one per q/a would be sensible I think. And there is no need to post any actual content as the wiki user either — that is best done with your real account.\r\n\r\nIt would be reasonably easy to fold those accounts into an official 'Wiki' feature if we decided to implement one at a later date.\r\n\r\nIn general I think this should only be used for those rare cases where an answerable question is posted that absolutely requires a group effort to answer. Normally if you post an answer, I think you should take the credit for it, even if you are just quoting documentation.	2020-01-01 20:44:53.870177+00	3	1	1	\N	0	0	0	\N	\N	I'm unsure if we ever want something exactly like Community Wiki the way it works on SE — but I'm interested to hear arguments for and against.	f	f
584	497	96	2020-01-09 12:01:53.36868+00	A CW style implementation that might work for this site would be to make it a different _question post_ type (just like blog / meta questions) with a unique property that:\r\n\r\n1. A blank answer is automatically added, owned by *nobody*.\r\n1. Only one answer post would ever be allowed. Much  like the blog posts don't allow answers at all, CW questions would only have one answer.\r\n\r\nThis is different from the SE implementation that allows multiple answer posts and a mix of post types which I've always found confusing and very rarely useful. Either a question is best answered by multiple people taking a stab at it and the best post being sorted at the top, or it is best answered by lots of edits/users over time adding to a list. Mixing and matching is almost always bad.	2020-01-09 12:01:53.36868+00	6	4	2	\N	0	0	0	\N	\N	A CW style implementation that might work for this site would be to make it a different _question post_ type (just like blog / meta questions) with a unique property that:	f	f
580	221	2	2020-01-07 13:50:17.352047+00	The balance of pros and cons favours GitLab, but we've opted for GitHub instead because of the potential crossover with [Codidact](https://codidact.org/) contributors.\r\n\r\nYou can find the current (7th Jan 2020) source [on Github](https://github.com/topanswers/topanswers) now. We plan to keep it up to date as part of our release process.	2020-01-07 13:50:17.352047+00	14	1	1	\N	0	0	0	\N	\N	The balance of pros and cons favours GitLab, but we've opted for GitHub instead because of the potential crossover with [Codidact](https://codidact.org/) contributors.	f	f
157	221	96	2019-12-01 07:20:14.82922+00	(Answer is WIP)\r\n\r\n# Considerations\r\n\r\n* Feature set\r\n* Ideology\r\n* Familiarity\r\n* Developer buy-in\r\n\r\n# Options\r\n\r\n* Github\r\n* Gitlab\r\n* Gitlab (self-hosted)\r\n* Sourcehut\r\n* Bitbucket\r\n* Gogs (self-hosted)\r\n* Gitea (self-hosted)\r\n\r\nNot considered for lack of first hand knowledge: Buddy, Phabricator, Beanstalk, Gitbucket, Allura, Rhodecode, ...\r\n\r\nRejected out of hand: Sourceforge (even though they do have git these days their community track record is pretty abysmal), Launchpad (because gag), Trac (because git strapped on to something designed for Subversion and never overhauled), ...\r\n\r\n## Github\r\n\r\nhttps://github.com \r\n\r\n* :thumbsup: Good feature set\r\n* :thumbsup: Good integrations (CI, code review, bots)\r\n* :thumbsup: Highest familiarity and developer buy in.\r\n* :thumbsdown: Microsoft\r\n* :thumbsdown: Proprietary\r\n\r\n## Gitlab\r\n\r\nhttps://gitlab.com\r\n\r\n* :thumbsup: Great features\r\n* :thumbsup: Great integrations (Pipelines for CI/CD, code review, bots)\r\n* :thumbsup: Feature parity and UX similarity to Github mean most developers can jump right in.\r\n* :thumbsup: Easy migration\r\n* :thumbsup: Hosted or self-hosted\r\n\r\n## Gitlab (self-hosted)\r\n\r\nSame as above, but self hosted. Easy migration between instances.\r\n\r\n* :thumbsdown: Self hosted deploy is a bear if not running a dedicated host, Ruby stack is hefty with lots of dependencies.\r\n\r\n## Sourcehut\r\n\r\nhttps://git.sr.ht\r\n\r\n* :thumbsup: ideological alignment\r\n* :thumbsup: light weight\r\n* :thumbsup: versatile build / deploy integration\r\n* :thumbsdown: UI/UX is as bad as @Jack's initial design for topanswers.xyz	2020-01-15 10:40:15.327845+00	6	4	2	\N	0	0	0	\N	\N	(Answer is WIP)	f	f
571	518	2	2020-01-04 23:03:20.247065+00	I read the article but I personally didn't find many of the assertions it makes very compelling[^1].\r\n\r\nFor example:\r\n\r\n> …producers, by adopting Open Source, have locked themselves into a situation where they lose all control over their code once it’s released to the wild.\r\n>\r\n> This is pretty absurd…\r\n\r\nThe author doesn't make much of a big deal about the fact that Open Source is all about *choosing* to grant rights, and the implication that anyone who does so is making an absurd choice is a little over the top.\r\n\r\nBy analogy, if you've just invented the wheel, and choose to profit from that, fine — you have done something that will eventually benefit everyone and chosen to benefit yourself too, I see no reason to fault you for that. However if you choose to tell everyone that they can just go ahead and use your invention right now without paying anyone:\r\n\r\n1. Wheels are going to help more people more quickly\r\n2. There is going to be less waste on lawyers and marketing\r\n3. You aren't going to get so rich\r\n\r\nMaybe you made a bad economic choice there, but *absurd*? I think 'nice' would be a better word. And the whole 'but bad people can then use wheels too' argument feels a bit ridiculous to me — bad people are generally the most likely to just go right ahead and steal your invention without paying anyway.\r\n\r\n> The key is that Open Source is literally about exploiting unpaid labor.\r\n\r\nOnly if you have a very broad view of what exploitation is. I don't think it's very helpful to include 'activities that people undertake completely free from all compulsion' under the heading 'literally exploiting'.\r\n\r\nApart from all that (and I could have added quite a number of further examples of what I regard as very suspect presuppositions), the ['Ethical' alternative](https://ethicalsource.dev/definition/) that the author suggests, as a step in the right direction, seems to have some very serious practical flaws, for example:\r\n\r\n> Creators have the right to prohibit use by individuals or organizations engaged in human rights violations or other behavior deemed unethical.\r\n\r\nThis opens the door to endless interminable arguments between contributors about which individuals or organizations are the ones engaging in unethical behaviour.\r\n\r\nSociety needs to have those discussions, but do they really need to be had inside the community developing Postgresql, and JQuery etc etc? Would it really make the world a better place if every productive community had to be made up of not just people good at producing the particular product they are working on, but also people who are like-minded ideologically? I don't think it would. More than that, I'm quite sure the rate of progress would slow dramatically. \r\n\r\n> Creators have the right to solicit reasonable and voluntary compensation from the communities or institutions that benefit from the software\r\n\r\nEither this really means 'voluntary', in which case the situation is no different to normal open-source software (there is no prohibition on *asking* for donations), or it means something else that is going to include lawyers and complex negotiations that will put everyone off using your software in the first place.\r\n\r\nIf we really want to focus on people, we need to think practically about the outcomes that will benefit people. making this platform not-for-profit and open source will help prevent contributors from being productised 10 years down the line. We are all here to give away our knowledge, and that isn't absurd — quite the opposite; it's better to give than receive[^2].\r\n\r\n[^1]: N.b. this is just my 2c, and I have no qualification to say any of it with any particular authority.\r\n[^2]: Which is not *at all* to say that receiving isn't good, just that giving is even better. I have lost count of the times I've been helped by something I found on SE, but the biggest benefit they have given us is the ability to *contribute*.	2020-01-04 23:03:20.247065+00	5	1	1	\N	0	0	0	\N	\N	I read the article but I personally didn't find many of the assertions it makes very compelling[^1](N.b. this is just my 2c, and I have no qualification to say any of it with any particular authority.).	f	f
437	432	96	2019-12-12 15:09:24.553804+00	I'm not sure this is a good idea. Already the "real rooms" (top level room(s) belonging to sites) are being drowned out in the bar by the somewhat less important and transient question rooms. I would almost argue that all question rooms should just have a solid color (matching their parent site theme) with no icons at all, or that the identicon should be dimmed out or tinted or something so than the rooms with _REAL_ icons stand out more.	2019-12-12 15:09:24.553804+00	5	4	2	\N	0	0	0	\N	\N	I'm not sure this is a good idea. Already the "real rooms" (top level room(s) belonging to sites) are being drowned out in the bar by the somewhat less important and transient question rooms. I would almost argue that all question rooms should just have a solid color (matching their parent site theme) with no icons at all, or that the identicon should be dimmed out or tinted or something so than the rooms with _REAL_ icons stand out more.	f	f
451	432	2	2019-12-14 01:11:03.682072+00	> Would it be pragmatic to add a "custom icon" option for questions to make them more distinctive in the nav bar?\r\n\r\nThey already do!\r\n\r\nIf you find the right room :). I've added icons to a few — basically those I expect will get chat traffic for years rather than days/weeks:\r\n\r\n* https://topanswers.xyz/meta?q=1  \r\n* https://topanswers.xyz/databases?q=30\r\n\r\nFeel free to post suggestions for other long-lived question rooms (like this one?) as answers here — I've subscribed to this question so I should see them.	2019-12-14 16:17:28.773339+00	5	1	1	\N	0	0	0	\N	\N	> Would it be pragmatic to add a "custom icon" option for questions to make them more distinctive in the nav bar?	f	f
562	429	691	2020-01-03 20:13:33.723915+00	I offer two possible solutions to the parasite problem.\r\n\r\n\r\nA) The votes of parasite users maybe should only count (be credited) when the known users have upvoted the topic already. I.e., the parasites can reinforce the "thanks for the help", but cannot singlehandedly push something into a "good answer" status. This allows the known community to establish credibility, but also allows the good passives to be differentiated from vote mongers. \r\n\r\nB) What if people can only vote using stars that they've earned? I.e., whatever a person's total lifetime star count is, that is their weekly limit for voting on other stuff. You have to ask at least one good question or provide one good answer before you can start voting on other stuff. And you won't be able to do much damage until you've shown a track record of such voting.	2020-01-03 20:13:33.723915+00	9	4	1	\N	0	0	0	\N	\N	I offer two possible solutions to the parasite problem.	f	f
442	429	12	2019-12-13 06:30:25.247235+00	This is the wrong solution to a problem that hasn't arisen (naturally) yet.\r\n\r\nNew users (or readers that never write a starred answer) shouldn't be prevented from adding one star to posts they find useful. The system already rewards longer-term contributors with the ability to award multiple stars at their discretion. The benefit of allowing wide feedback outweighs any downside.\r\n\r\nProblematic abuse of the star system should be detected and handled in other ways, should the need ever arise. Also, remember that privileges are merit-based, not tied to star count.\r\n\r\nI think we should resist knee-jerk implementation based on Stack Exchange experiences. The system there does not avoid difficulties either - one only has to look at the effect of Hot Network Questions.	2019-12-13 14:38:45.187368+00	4	4	1	\N	0	0	0	\N	\N	This is the wrong solution to a problem that hasn't arisen (naturally) yet.	f	f
435	429	96	2019-12-12 12:47:41.174499+00	*If you aren't convinced...*\r\n\r\nBefore you try to argue that drive-by voters somehow add value to the system even if the noise level might be higher, first explain to me why the opinion of clowns[^1] who [voted to get rid of the vote table entirely](https://topanswers.xyz/databases?q=200#a183) should be given equal weight with all the database experts who have **offered evidence that their voices add signal not noise by answering real questions**.\r\n\r\nP.S. If you have to run your own SQL query on the topanswers database to answer the question of who those clowns are, then please allow me to suggest that anything you do with that information will be a bandaid on a bigger problem that won't get fixed until the wider user community can vet who is allowed to vote by requiring at least a nominal number of contributions first.\r\n\r\n[^1]: I've done the math and *nobody* is being called a clown who has not read and approved of this message.	2019-12-13 07:20:56.954956+00	1	4	2	\N	0	0	0	\N	\N	*If you aren't convinced...*	f	f
574	429	715	2020-01-05 05:38:06.703121+00	The proposed solution would create a bootstrap problem.\r\n\r\nIf "Require receiving at least 1 star to earn the privilege of giving stars" were in place on day one, then nobody would have started with privileges to give out the first star. Other question and answer sites have solved this "cast the first stone" bootstrap problem in various ways:\r\n\r\n- Stack Exchange allows the asker to accept an answer, and an accepted answer grants enough reputation for the upvote privilege.\r\n- Stack Exchange also runs "private beta" rules for the first week of a new site, when voting and tag creation are free. (Private beta participants come from other SE sites, and I have a bit of an accessibility beef with how private beta access for Stack Overflow itself was handed out.)\r\n- Codidact, as I understand it, will allow the asker to upvote answers to the asker's own questions. (I see this as analogous to accepting on SE.) There is also a [plan to add "young community" rules](https://forum.codidact.org/t/bootstrapping-trust-levels-for-young-communities/534?u=pinobatch) analogous to SE's private beta.	2020-01-07 17:22:04.503734+00	6	4	1	\N	0	0	0	\N	\N	The proposed solution would create a bootstrap problem.	f	f
483	429	234	2019-12-15 17:10:41.49635+00	I personally do not see anything wrong with "drive by voters". For a long time, I was a "parasite user" on TeX.SE, and was using answers provided there to solve some of my problems, but didn't have a user account. At a given point I decided to join, asked a question, got reputation and so on. However, in retrospect I think I *should* have joined earlier and just upvoted the posts I was using. We all know that this is impossible on the SE sites since there is a reputation threshold. However, I feel that it would have been perfectly OK if I, as a drive-by voter, would have upvoted those posts which worked for me. Therefore I think we should allow folks to become "passive users", i.e. join and do nothing but award stars whenever they think something was useful for them. Recall also that in the system here they can only award one star, while high reputation users can award many.	2019-12-16 05:47:44.658135+00	7	4	3	\N	0	0	0	\N	\N	I personally do not see anything wrong with "drive by voters". For a long time, I was a "parasite user" on TeX.SE, and was using answers provided there to solve some of my problems, but didn't have a user account. At a given point I decided to join, asked a question, got reputation and so on. However, in retrospect I think I *should* have joined earlier and just upvoted the posts I was using. We all know that this is impossible on the SE sites since there is a reputation threshold. However, I feel that it would have been perfectly OK if I, as a drive-by voter, would have upvoted those posts which worked for me. Therefore I think we should allow folks to become "passive users", i.e. join and do nothing but award stars whenever they think something was useful for them. Recall also that in the system here they can only award one star, while high reputation users can award many.	f	f
450	429	2	2019-12-13 23:50:42.279283+00	This is a hard problem, but I think we have a better answer.\r\n\r\n> Allowing votes before any other activity makes votes meaningless \r\n\r\nI think that is putting the case much too strongly — the fact that it is *possible* to game a rating system doesn't immediately mean the whole rating system is meaningless.\r\n\r\nNonetheless it's a serious issue, and you've provided proof that it's currently pretty easy to 'cheat'. Cheating is going to be one of the two main problems we will face from the bad guys, the other being spam.\r\n\r\n> Drive by voters water down the voting system.\r\n\r\nOnly if all/most drive by voting is abuse, or if we can't clean up the bad votes effectively.\r\n\r\nI'm not sure if you mean to imply that legitimate 'drive by' votes aren't generally useful. There isn't strong evidence either way as yet, but anecdotally, I know I've often clicked the 'vote up' button on an SE site that I'm not registered to. Those 'drive by' (failed) votes would be helpful for sorting those posts if they were allowed.\r\n\r\nIn other words, it would be nice to keep them if we can. It's also an 'on ramp' to greater involvement with the site — someone who joins up just to vote and doesn't hit a load of friction, might save their login key and come back to contribute in some other way.\r\n\r\nSo what it really hinges on is whether we can effectively invalidate bad votes. And whether we can do that together as a community in a way that scales. I think that may be possible without going in the direction you are suggesting[^1].\r\n\r\nIt may be reasonably easy to spot bad votes. For example, your sock votes stand out like a sore thumb with this naive query:\r\n\r\n```sql\r\nwith v as (select answer_id,question_id\r\n                , sum(answer_vote_votes) answer_votes\r\n                , coalesce(sum(case when communicant_votes>0 then answer_vote_votes end),0) established_votes\r\n                , count(*) answer_vote_count\r\n           from answer_vote\r\n                natural join (select answer_id,question_id from answer) a\r\n                natural join (select question_id,community_id from question) q\r\n                natural join (select account_id,community_id,communicant_votes from communicant) c\r\n           where answer_vote_votes>0\r\n           group by answer_id, question_id )\r\nselect answer_votes,established_votes,answer_vote_count\r\n     , (established_votes+1)::float/(answer_votes+1)::float trust_score\r\nfrom v\r\norder by trust_score limit 10;\r\n```\r\n\r\n|answer_votes|established_votes|answer_vote_count|trust_score\r\n|--|--|--|--|--\r\n|50 |0 |50 |0.0196078431372549\r\n|8 |1 |8 |0.222222222222222\r\n|1 |0 |1 |0.5\r\n|1 |0 |1 |0.5\r\n|1 |0 |1 |0.5\r\n|1 |0 |1 |0.5\r\n|7 |4 |6 |0.625\r\n|2 |1 |2 |0.666666666666667\r\n|5 |3 |5 |0.666666666666667\r\n|2 |1 |2 |0.666666666666667\r\n\r\nThis is making use of the same information you are suggesting (whether a vote is from someone 'established'), but in a different way. Rather that invalidating votes for non-established accounts, we are using them to sift out bad results.\r\n\r\nI think it would be good to add something like the 'trust' score above, in the hover text for posts with stars. Then we can spot suspicious voting by working together with information that is available to everyone, and flagging things that are suspicious. Initially clean up of bad votes will be done by devs (I know some still is on SE), but eventually we'll make some of that available to trusted members of the communities.\r\n\r\nI'm tagging this `wont-fix` for now, but we can revisit later if necessary.\r\n\r\n[^1]: …but I'll happily admit I may be wrong and only time will tell. It's also worth noting that it was the original intention to dissalow voting by users without 'stars' of their own — there was a user id limit of 100 hard coded into the early source that was supposed to do that. Sadly all it did was prevent people signing up, due to a bug!	2019-12-15 14:53:45.951965+00	7	1	1	\N	0	0	0	\N	\N	This is a hard problem, but I think we have a better answer.	f	f
116	28	96	2019-11-28 11:32:30.227548+00	**AGPL.**\r\n\r\nEspecially given the history that led to this site existing in the first place and the way Q&A 'communities' work better when more people are involved in the same site rather than being fragmented all over the web, I would highly recommend a share-alike license of some kind. This means avoiding MIT and even BSD and Apache style licenses that do not have share-alike clauses. Anybody should be welcome to run and modify the code to their liking, but any modifications should be made available to others.\r\n\r\nStarting a competing site with the same software should be an option, but the advantages of that site should be the people and community that chooses to use it not any technical superiority. Any technical advancements should be available to those that originally started or cantributed along the way.\r\n\r\nThe GPL is the pimary option that comes to mind, and the [Affero clause added in the AGPL in specific](https://www.gnu.org/licenses/why-affero-gpl.html) seems to suit these goals well, but there might be others that meet this goal too.	2019-11-30 11:30:50.085623+00	7	4	1	\N	0	0	0	\N	\N	**AGPL.**	f	f
597	487	715	2020-01-11 03:27:30.271733+00	If the user has chosen to enable JavaScript, the site could offer use of the [Notifications API](https://developer.mozilla.org/en-US/docs/Web/API/Notifications_API/Using_the_Notifications_API).	2020-01-11 03:27:30.271733+00	3	4	1	\N	0	0	0	\N	\N	If the user has chosen to enable JavaScript, the site could offer use of the [Notifications API](https://developer.mozilla.org/en-US/docs/Web/API/Notifications_API/Using_the_Notifications_API).	f	f
543	487	2	2019-12-27 19:29:45.756479+00	> I saw the commentary about not keeping too much personal information\r\n\r\nIt's not that we are avoiding PII, but that we don't want for *force* you to hand over anything more than the bare minimum.\r\n\r\n> So, question: What are the plans for sending notifications if you're not allowing us to submit e-mail addresses?\r\n\r\nEmail notifications and account recovery are two reasons for having the option to add a verified email addresses to your profile.\r\n\r\nI've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/25) so work can progress, and will update the post here when it is complete.	2020-02-10 17:37:36.778343+00	4	1	1	\N	0	0	0	\N	\N	> I saw the commentary about not keeping too much personal information	f	f
664	606	814	2020-01-24 14:28:54.085369+00	# What is an MWE?\r\n\r\nA minimal working example (MWE) is a short test document that contains everything necessary to compile (e.g. `\\documentclass{...}`, the necessary packages etc.) while still producing the error/problem you are asking about. \r\n\r\n# Some important points\r\n\r\n- remember that we don't have access to your computer. Try to avoid images or included files in your MWE which we don't have. If you need things like images in your document etc to reproduce the problem, have a look at the files from packages like `MWE` or `duckuments`. These are included in most latex installations\r\n\r\n- try to avoid custom classes and packages if you can also reproduce the problem with standard classes and packages. If some custom class or package that is not available from ctan is essential to reproduce the problem, please add a link where we can download the same version of the class/package you are using. \r\n\r\n- If you are using special fonts, right-to-left typesetting, special engines etc. check if you can also reproduce your problem without these things, this will increase the number of users who can try to help you.\r\n\r\n- you might have concerns sharing you unpublished text on the internet. That is totally fine, we are not interested in the content of your document, probably most of can be removed in the creation of an MWE. If you need text to reproduce the problem, you can replace your own words with dummy text like lorem lipsum. There are several latex packages that will help you with that, for example `duckuments`, `lipsum`, `blindtext` and many more (https://ctan.org/topic/dummy-gen)\r\n\r\n- "Is this a bad joke, my document is not working, how should I create a working example? If I were to know how to make it working, I would not be asking here" The name is probably a bit misleading, your document does not need to be working without error, but it should have all the necessary bits to work in theory if the error would not be there.\r\n\r\n# Why should I add an MWE?\r\n\r\nBy adding an MWE you help both the people who try to help you and yourself\r\n\r\n- An MWE is often essential to be able to solve a TeX problem. If you just describe the symptoms (for example "My document is suddenly in italic"), there might be many different causes and users can only guess what might or might not be the specific problem in your document. With an MWE the users trying to help you can see in your code what the problem really is.\r\n\r\n- During the process of creating an MWE you will often be able to solve the problem yourself. Even though your problem is solved, please consider writing a questions and self answer if you think other users might face the same problem.\r\n\r\n- Showing an MWE has the additional benefit that it often also shows your experience level in TeX. This might help users to tailor their answers to your needs.\r\n\r\n- With an MWE you use the time of the users willing to help you efficiently. They don't have to spend time trying to reproduce your problem before even starting to work on a solution and you save them from the boring task of typing things like `\\documentclass` for the 1000th time. In exchange this will cost you a bit more time, yes, but please remember that the other users invest their spare time for free when they try to help you, so it is a nice gesture to make it as easy as possible for them. It also helps to avoid unnecessary duplication of the work if multiple users try to help you and each of them would have to create their own MWE.\r\n\r\n- With an MWE more people will be able help you because this combines the knowledge from the users willing to help you with yours. For example if you face a problem with some exotic package, then an MWE will show the other users how this package works and they can immediately start working on a solution for you. Without an MWE you are limited to either the users that already know your exotic package or are willing to spend some time learning it before even starting to work on an answer.\r\n\r\n# How to create an MWE\r\n\r\nThere are at least two approaches\r\n\r\n- Divide and conquer: Make a copy of your document and start to remove things unnecessary to reproduce the problem. A good strategy is bisecting: remove one half of the document and test if the problem is still there. If yes, repeat this step and remove half of the remaining document etc. If the error is gone, undo the last step and try to remove the other half. Repeat this until you have an MWE.\r\n\r\n- Build from scratch: Create a simple test document that compiles without problems. Then start copying bits and pieces from your real document into the test document until your problem also appears in your test document. This should give a good idea where the problem comes from and you can again remove everything from your test document that is not necessary to reproduce the problem.\r\n\r\n# Example\r\n\r\n```\r\n\\documentclass{article}\r\n\\usepackage{graphicx}\r\n\\usepackage{lipsum} % to generate dummy text in this MWE\r\n\r\n\\begin{document}\r\n\r\n\\lipsum\r\n\r\n\\begin{figure}[htb]\r\n \\centering\r\n \\includegraphics{example-image-duck}% <- use some graphics that ship with the \r\n \\caption{A figure.} % usual TeX installations rather than some others won't have\r\n\\end{figure}\r\n\\end{document}\r\n```	2020-01-28 15:42:54.651271+00	10	4	1	\N	0	0	0	\N	\N	What is an MWE?	f	f
694	606	814	2020-01-28 14:14:02.669234+00	# MWE with bibliographies (MWEB)\r\n\r\nIn general the tips from https://topanswers.xyz/tex?q=606#a664 also apply to MWEs with bibliographies (MWEB). For many users it might seem harder to create an MWEB, because they often depend on external `.bib`. Therefore this answer will give some concrete examples differentiating between different bibliography tools.\r\n\r\nWhile creating an MWEB remember to compile often enough (including running bibliography tools like bibtex/biber if applicable) because many changes will only take effect multiple compilations.\r\n\r\n## `thebibliography` environment\r\n\r\nThis is the easiest case because no additional files are involved. \r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\begin{document}\r\n\r\ntext \\cite{texbook} text\r\n\r\n\\begin{thebibliography}{9}\r\n\\bibitem{texbook} \r\n  Donald Ervin Knuth. \r\n  \\textit{The \\TeX book}. \r\n  Addison-Wesley, Reading, Massachusetts, 1983.\r\n\\end{thebibliography}\r\n\r\n\\end{document}\r\n```\r\n\r\n## BibTeX\r\n\r\n### Sample `.bib` file\r\n\r\nIf the problem is not related to the content of the `.bib` file, most tex distributions come with several sample `.bib` files that can be used to demonstrate your problem. One possibility is `xampl.bib`[^*] :  \r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\begin{document}\r\n\r\ntext \\cite{book-full}\r\n \r\n\\bibliographystyle{unsrt}\r\n\\bibliography{xampl}\r\n\r\n\\end{document}\r\n```\r\n\r\n### Own `.bib` file\r\n\r\nIf the problem is connected to a specific bib entry, the users trying to help you will need to see this entry. To save them from having to manually create a new `.bib` file and copy the entry there, it is very helpful to use the `filecontents`[^**] environment that will automatically create a new file with the desired content.\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\begin{filecontents}{\\jobname.bib}\r\n@book{knuth:ct:a,\r\n  author       = {Knuth, Donald E.},\r\n  title        = {The {\\TeX book}},\r\n  year         = 1984,\r\n  maintitle    = {Computers \\& Typesetting},\r\n  volume       = {A},\r\n  publisher    = {Addison-Wesley},\r\n}\r\n\\end{filecontents}\r\n\r\n\\begin{document}\r\n\r\ntext \\cite{knuth:ct:a} text\r\n \r\n\\bibliographystyle{unsrt}\r\n\\bibliography{\\jobname}\r\n\r\n\\end{document}\r\n```\r\n\r\n## Biblatex\r\n\r\n### Sample `.bib` file\r\n\r\nIf the problem is not related to the content of the `.bib` file, one can use `biblatex-examples.bib` in the MWEB[^*]. This is a demo `.bib` file included in most latex installations which has examples for many different types of entries. \r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\usepackage{biblatex}\r\n\\addbibresource{biblatex-examples.bib}\r\n\r\n\\begin{document}\r\n\r\ntext \\cite{knuth:ct:a} text\r\n\r\n\\printbibliography\r\n\r\n\\end{document}\r\n```\r\n\r\n### Own `.bib` file\r\n\r\nIf the problem is connected to a specific bib entry, the users trying to help you will need to see this entry. To save them from having to manually create a new `.bib` file and copy the entry there, it is very helpful to use the `filecontents`[^**] environment that will automatically create a new file with the desired content.\r\n\r\n```\r\n\\documentclass{article}\r\n\r\n\\usepackage{biblatex}\r\n\r\n\\begin{filecontents}{\\jobname.bib}\r\n@book{knuth:ct:a,\r\n  author       = {Knuth, Donald E.},\r\n  title        = {The {\\TeX book}},\r\n  date         = 1984,\r\n  maintitle    = {Computers \\& Typesetting},\r\n  volume       = {A},\r\n  publisher    = {Addison-Wesley},\r\n  location     = {Reading, Mass.},\r\n  langid       = {english},\r\n  langidopts   = {variant=american},\r\n  sorttitle    = {Computers & Typesetting A},\r\n  indexsorttitle= {The TeXbook},\r\n  indextitle   = {\\protect\\TeX book, The},\r\n  shorttitle   = {\\TeX book},\r\n}\r\n\\end{filecontents}\r\n\r\n\\addbibresource{\\jobname.bib}\r\n\r\n\\begin{document}\r\n\r\ntext \\cite{knuth:ct:a} text\r\n\r\n\\printbibliography\r\n\r\n\\end{document}\r\n```\r\n\r\n[^*]: In order to inspect the sample `.bib` one can use `kpsewhich <filename>.bib`, e.g. `kpsewhich biblatex-examples.bib`, from a command line to get the location of the file and then use your favourite editor to open it\r\n\r\n[^**]: The `filecontents` environment doesn't overwrite existing files. The proposed filename `\\jobname.bib` will result in a `.bib` file with the same base name as your main `.tex`.	2020-01-28 15:53:42.44029+00	7	4	3	\N	0	0	0	\N	\N	MWE with bibliographies (MWEB)	f	f
411	339	168	2019-12-11 09:11:53.110502+00	I didn't test this, so I'm entirely unsure whether it does what it should in your context.\r\n\r\nThe idea is to measure the height of your score and test for a few things (I hope the comments are enough). One could of course test for other things (e.g. if this page is even and already started, will the score fit on the remaining space of this page plus the next page).\r\n\r\nAll you have to do is make sure that the scores are typeset inside the `myscores` environment, the environment should handle the rest.\r\n\r\n```tex\r\n\\makeatletter\r\n\\newsavebox\\mybox  \r\n\\newlength\\myabsheight\r\n% this is analogue to how `\\cleardoublepage' works\r\n\\newcommand*\\cleartoevenpage\r\n  {%\r\n    \\clearpage\r\n    \\if@twoside\r\n      \\ifodd\\c@page\r\n        \\hbox{}\\newpage\r\n        \\if@twocolumn\r\n          \\hbox{}\\newpage\r\n        \\fi\r\n      \\fi\r\n    \\fi\r\n  }\r\n\\newenvironment{myscores}  \r\n  {\\setbox\\mybox\\vbox\\bgroup}  \r\n  {%  \r\n    \\egroup  \r\n    \\myabsheight=\\dimexpr\\ht\\mybox+\\dp\\mybox\\relax\r\n    \\ifdim\\pagetotal=0pt\r\n      % we're at the start of a page  \r\n      \\ifdim\\myabsheight>\\textheight\r\n        % score will be bigger than one page, so we want to start on the next\r\n        % even page\r\n        \\cleartoevenpage\r\n      \\fi\r\n    \\else\r\n      \\ifdim\\myabsheight>\\dimexpr\\pagegoal-\\pagetotal\\relax\r\n        % score will not fit on the remaining space of this page\r\n        \\ifdim\\myabsheight>\\textheight\r\n          % score will not fit on a single page\r\n          \\cleartoevenpage\r\n        \\else\r\n          % score will fit on a single page\r\n          \\clearpage\r\n        \\fi\r\n      \\fi\r\n    \\fi\r\n    \\unvbox\\mybox\r\n  }\r\n\\makeatother\r\n```	2019-12-11 10:19:16.52318+00	2	4	3	\N	0	0	0	\N	\N	I didn't test this, so I'm entirely unsure whether it does what it should in your context.	f	f
172	236	131	2019-12-01 20:02:51.473736+00	> I assume we're doing the right things with attribution. What does that look like?\r\n\r\nThere are a number of imported questions over on Databases. Here's just one: [Why does changing the declared join column order introduce a sort?](https://topanswers.xyz/databases?q=225) Note the "imported from SE" in the header.	2019-12-01 20:46:50.733203+00	2	4	1	\N	0	0	0	\N	\N	> I assume we're doing the right things with attribution. What does that look like?	f	f
176	236	12	2019-12-01 20:59:27.116404+00	This answer will likely age quite quickly, but just to document what we have been doing so far:\r\n\r\n## Import\r\n\r\nImport has been done on an **ad-hoc basis** by individual users.\r\n\r\nOn request, they are granted access to an "import question from SE" button on the main page. This allows an existing question to be imported, and optionally one or more of the answers. Any answers by the importing user are automatically included.\r\n\r\nThe point of this was (a) to give people a way to archive their best content; and (b) seed the site with posts so we can see how the design and tooling might scale (or not).\r\n\r\nThere is currently no mechanism to import Q & A ***in bulk***.\r\n\r\nThe feature is quite basic at present. A dialog box pops up asking for a question id or url, and optionally one or more answer ids/urls, separated by a space:\r\n\r\n![topanswers.png](/image?hash=320a92a526ae0eb39c16b7927c7fdb86b9c28b2531d52de5b047974695af751c)\r\n\r\nThere is some automatic markdown translation, but it is best to review the result of the import anyway.\r\n\r\nThey can now link their SE accounts to their account here on TopAnswers by following the steps [here](/meta?q=409#a647). If they do that, every post imported from SE is automatically linked to their TA profile.\r\n\r\nThere is no *requirement* for the importer to have a post (question or answer) on the source page, but we have generally only been importing stuff where the importer has made a contribution.\r\n\r\n## Attribution\r\n\r\nAttribution is handled by links at the top of the Q & A here, with a link to the SE user, the original post, and the CC licence. [For example](https://topanswers.xyz/databases?q=227):\r\n\r\n![topanswers.png](/image?hash=eaa6dbf81fcceec35312f97f5fcac2591138754b62841e04f4d99c447405581d)\r\n\r\nWe don't currently link to the edit history since the [CC FAQ](https://wiki.creativecommons.org/wiki/Frequently_Asked_Questions#How_do_I_properly_attribute_a_Creative_Commons_licensed_work.3F) says:\r\n\r\n> Additionally, you may satisfy the attribution requirement by providing a link to a place where the attribution information may be found.\r\n\r\nThe link to SE is skipped if the importer is the original author of the SE post. For example, when I imported [my own DBA Q & A](https://topanswers.xyz/databases?q=207):\r\n\r\n![topanswers.png](/image?hash=e6b0c72f2f391dda7a94e55a019e4124d24231acbba81a05d8b23eb38c731cd8)	2020-01-23 23:00:05.282069+00	7	4	1	\N	0	0	0	\N	\N	This answer will likely age quite quickly, but just to document what we have been doing so far:	f	f
608	561	2	2020-01-12 22:13:04.980382+00	We've added the precise timestamp on hover.\r\n\r\n> Ideally, it would be great to have a tool-tip with human-readable date/time for those "relative" time-stamps.\r\n\r\nThe hover text is formatted the same way the chat timestamps display on hover, so just the time if today, and the full timestamp is reserved for older posts.\r\n\r\n> So, as a bonus, if someone could kindly explain what that "data-seconds" number represents, I will have learned something.\r\n\r\nIt's the 'age' of the timestamp — the number of seconds between the timestamp and right now. It doesn't need 6 decimal places of precision, that's just wasted bandwidth! The reference to `epoch` in the code is a bit misleading as it is being applied to an `interval` rather than a `timestamp`, so in that context it just means the size of the interval in seconds rather than the unix epoch.	2020-01-12 22:13:04.980382+00	8	1	1	\N	0	0	0	\N	\N	We've added the precise timestamp on hover.	f	f
140	197	2	2012-03-31 08:05:09+00	The aim when shutting down for maintenance (or cold backup) is that the database is left in a consistent state with no need for rollback/recovery on startup.\r\n\r\nThere are 3 SQL*Plus `shutdown` commands that achieve this in theory, all of which immediately prevent *new* sessions connecting to the instance:\r\n\r\n 0. [`shutdown normal`](http://docs.oracle.com/cd/E11882_01/server.112/e25494/start003.htm#ADMIN11157) or just `shutdown`: waits for all sessions to disconnect. This mode is rarely used in practice because it relies on well-behaved clients not leaving connections open. This used to be the only `shutdown` mode that did not cancel running transactions.\r\n 0. [`shutdown transactional`](http://docs.oracle.com/cd/E11882_01/server.112/e25494/start003.htm#ADMIN11159): disconnects sessions once currently running transactions complete, preventing new transactions from beginning.\r\n 0. [`shutdown immediate`](http://docs.oracle.com/cd/E11882_01/server.112/e25494/start003.htm#ADMIN11158): disconnects all sessions immedately and rolls back interrupted transactions before shutting down. Note that the disconnections are immediate, but *the shutdown may not be* as any interrupted transactions may take time to roll back.\r\n\r\nThe fourth mode of `shutdown` is [`shutdown abort`](http://docs.oracle.com/cd/E11882_01/server.112/e25494/start003.htm#ADMIN11160). This is like pulling the power cord - the instance stops *now* without any cleanup. You usually want to bring the database up again afterwards and shut down cleanly immediately afterwards as in your example. The concepts guide [says](http://docs.oracle.com/cd/E11882_01/server.112/e25789/startup.htm#CNCPT89041):\r\n\r\n> This mode is intended for emergency situations, such as when no other form of shutdown is successful.\r\n\r\nAll the examples you give [perform a checkpoint](http://docs.oracle.com/cd/E11882_01/server.112/e25789/startup.htm#CNCPT89041) as part of the `shutdown [normal]` or `shutdown immediate` so explicit checkpointing is presumably to [reduce the time required for recovery](http://docs.oracle.com/cd/E11882_01/server.112/e25789/startup.htm#CNCPT89043).\r\n\r\ngeneral advice:\r\n\r\n * Do not use `shutdown normal`.\r\n * Use `shutdown transactional` *for attended shutdown only*, when you want to minimise cancelled transactions (attended only because this kind of shutdown [is not guaranteed to shut the database down](http://docs.oracle.com/cd/E11882_01/server.112/e25494/start003.htm#ADMIN11161) at all if timeouts are breached).\r\n * Use `shutdown immediate` for unattended shutdown or when you do not care about currently running transactions.\r\n * Do not use `shutdown abort` (plus startup/shutdown) unless you have to - this was more common in much earlier versions of Oracle that it is today. In other situations (not patch/upgrade), if you have a need to [minimise downtime](http://www.speakeasy.org/~jwilton/oracle/shutdown-abort-bad.html) this mode may be appropriate.	2019-11-29 17:22:44.663276+00	1	4	1	15896	0	0	0	2019-11-29 17:22:44.663276+00	\N	The aim when shutting down for maintenance (or cold backup) is that the database is left in a consistent state with no need for rollback/recovery on startup.	f	f
117	183	12	2019-11-28 12:31:15.58366+00	It does make sense to allow a user to generate a new recovery token. That action should invalidate the previous token.\r\n\r\n>...but not working how people expect (how every other user account system works) will eventually have its own cost.\r\n\r\nI think the benefits of not storing an email address outweigh that cost.	2019-11-28 13:33:32.816612+00	3	4	2	\N	0	0	0	\N	\N	It does make sense to allow a user to generate a new recovery token. That action should invalidate the previous token.	f	f
126	183	2	2019-11-28 14:02:00.870607+00	> A mechanism is needed to invalidate account recovery codes.\r\n\r\nThanks for suggesting this, I think you were right that it's urgent to get a mechanism in place.\r\n\r\nWe've just rolled out the feature:\r\n\r\n![Screenshot 2019-11-28 at 13.56.54.png](/image?hash=8e5281e7c01cbd4f00af8976c4ab30c468edf7104371448eb14792fc3ea84570)\r\n\r\n> Yes that's my actual account recovery code — before I fiddled with it and typed over some hex values.\r\n\r\nI suggest you test the feature now :P	2019-11-28 14:02:00.870607+00	5	1	1	\N	0	0	0	\N	\N	> A mechanism is needed to invalidate account recovery codes.	f	f
621	505	2	2020-01-16 00:17:22.056618+00	We are trialling an alternative…\r\n\r\n> I would like to keep track of chat rooms like the Tavern or the main chat room of the tex site, but I don't always post messages in them, so they age away.\r\n\r\nRather than let you choose which rooms to pin manually, the system now shows rooms that you chat in more, for longer. If you leave a single comment, the room will hang around in the list for a few days. If you leave hundreds, the room will remain in the list for up to three months even if you stop chatting entirely, and forever if you comment at least once every three months.\r\n\r\n> Would it be possible to somehow pin them to my list of chat rooms at the top right?\r\n\r\nWe've also sorted the room list by total comments (and then most recent comment). This has the effect of keeping the sort order quite stable, especially for rooms we chat in fairly often. You should find that The Tavern and TopTeX are effectively pinned now.\r\n\r\nPlease note these changes are experimental and subject to being rolled back if feedback is negative! We can also tweak the algorithm in various directions if you need us to.	2020-01-16 10:31:41.384023+00	6	1	1	\N	0	0	0	\N	\N	We are trialling an alternative…	f	f
636	532	2	2020-01-18 23:47:43.129429+00	Blog posts are different to normal and 'meta' questions, as you have noticed.\r\n\r\nAnyone can post a 'blog post', but it should be on-topic for the community. I don't know if TopAnswers will catch on as a blogging platform — I imagine we'd need to (at least) ① be attracting a lot of traffic, ② have implemented a bunch of features around 'following' (or hiding) blog posts, and ③ develop the markdown editor here to quite a high level of sophistication, to attract bloggers to the platform.\r\n\r\nIn the meantime the 'blog' type of post serves as a useful way of keeping our own TopAnswers announcements 'in-universe' and subject to community feedback.\r\n\r\n> Differences Noticed:  \r\n>   \r\n> 1. Blog posts are not editable  \r\n> 1. Blog posts are not answerable\r\n\r\nOne further difference that you probably also noticed is that you can vote on blog posts. Of course it looks like you can vote on 'normal' questions here on the meta community too, but that's not really the case: all normal questions here are treated as 'meta' questions. The difference is clearer on the other communities where there are three question types:\r\n\r\n1) Normal question\r\n2) 'Meta' question\r\n3) Blog post\r\n\r\nWith that in mind, here is a table of the differences:\r\n\r\n|type|can star?|can answer?|editable?|\r\n|-|-|-|-|\r\n|normal[^1]|no|yes|yes|\r\n|meta|yes|yes|yes|\r\n|blog|yes|no|no[^2]|\r\n\r\nA fourth post type, 'Wiki', [is being considered](/meta?q=497#a584) and it is possible that more will be added in the future. It is also likely that communities will be able to decide which post types they would like to allow.\r\n\r\nAlso under consideration is allowing questions to be linked to a blog post — this would be a fifth type. This is not necessary now, but would be helpful on a busier site like meta.se.\r\n\r\nSo…\r\n\r\n> Can I make one? If so, how\r\n\r\nYes, sure, go ahead if you have something on-topic that you'd like to blog about.\r\n\r\n> Why not make them editable?\r\n\r\nBecause I think wouldn't be idiomatic for blog posts — if you spot something wrong in a blog post your options are typically the same as they are here: comment and suggest a change.\r\n\r\n[^1]: as noted, 'normal' question here on the 'meta' community are really disguised 'meta' questions, so can be voted on.\r\n[^2]: blog posts are editable by the OP only.	2020-01-21 15:26:21.170761+00	3	1	1	\N	0	0	0	\N	\N	Blog posts are different to normal and 'meta' questions, as you have noticed.	f	f
54	56	43	2019-11-22 09:35:36.626896+00	Would it work to allow the user to add their own custom CSS? This would solve the problem for those that care enough to know or work out how to change look and feel, while letting devs focus on other features.	2019-11-22 09:35:36.626896+00	2	4	1	\N	0	0	0	\N	\N	Would it work to allow the user to add their own custom CSS? This would solve the problem for those that care enough to know or work out how to change look and feel, while letting devs focus on other features.	f	f
158	56	96	2019-12-01 07:32:17.421674+00	Yes, absolutely. A few carefully selected options by default and a "write in" option for those who know what they want and have it on their computer would be great, possibly with the write in being a CSS field so Google Fonts or other providers could be sourced in an `@import`.\r\n\r\nContrary to a previous answer, having a limited selection is not benefit to support. Only having a fixed single selection would be a real benefit (same for everybody). As _soon as_ you get into allowing different fonts you pretty much open up Pandora's box — but then again allowing flexible width layouts does that too.\r\n\r\nThe sooner this becomes an option the less "debt" will be built up by posts that assume they know exactly how they will be presented. If a Markdown is enough to represent content, then a choice of fonts on the UI site should be included just to prove that the content isn't dependent on the font.\r\n\r\n*Note SE has avoided font options and even flexible width layouts over the years because they are very hard to add into the mix later (you break all the posts that assumed a fixed width and hacked their content to match).*	2019-12-01 07:34:50.293931+00	2	4	2	\N	0	0	0	\N	\N	Yes, absolutely. A few carefully selected options by default and a "write in" option for those who know what they want and have it on their computer would be great, possibly with the write in being a CSS field so Google Fonts or other providers could be sourced in an `@import`.	f	f
167	56	115	2019-12-01 17:52:53.969277+00	Please consider the possibility of per-*site* fonts and not just per-*person*.  On Stack Exchange, Mi Yodeya ran into a problem because the default font (post-redesign) had really terrible support for Hebrew, which we cared about.  I think the Mathjax-using sites also had some problems with the default font, though I don't recall the details.\r\n\r\nThis concern has two implications:\r\n\r\n- We might need to customize a font at the site level.\r\n\r\n- A per-user font setting *does not solve the problem*, because different sites might have different needs.  (I chose a different font here because I found the original one hard to read; if that original were to be the one that supported Hebrew, for instance, I'd have to choose between the Hebrew on one site and legibility on all sites.  That would be unfortunate.)\r\n	2019-12-01 17:52:53.969277+00	7	4	1	\N	0	0	0	\N	\N	Please consider the possibility of per-*site* fonts and not just per-*person*.  On Stack Exchange, Mi Yodeya ran into a problem because the default font (post-redesign) had really terrible support for Hebrew, which we cared about.  I think the Mathjax-using sites also had some problems with the default font, though I don't recall the details.	f	f
40	56	12	2019-11-18 11:52:08.584109+00	It's a reasonable idea, but the choices should be very limited.\r\n\r\nHaving multiple typefaces might make support more time-consuming, since issues might arise with one choice but not another.\r\n\r\nFor example, messages with mixed text and `code` inline formatting, sometimes appear with very small scrollbars in chat. Things like this might depend in the future on which typeface was chosen, along with the already myriad possibilities of OS, device, browser...and so on.\r\n\r\nI quite like the current typeface, but agree with @dezso that a sans-serif option would be welcome.	2019-11-18 11:52:08.584109+00	6	4	2	\N	0	0	0	\N	\N	It's a reasonable idea, but the choices should be very limited.	f	f
613	567	167	2020-01-14 13:46:26.266151+00	Just some thoughts I'd like to throw in here for discussion:\r\n\r\nIf there were tags for answers, maybe they could also be used to describe for which software versions/operating systems an answer works or does not work? \r\n\r\nThis is probably only be useful for some topics, but when searching for a solution, I often have the problem that there might be many different answers, but they might not all work for the particular software version/operating system/etc. that I have. So I usually test several ones until I find one that works for my setup. This is especially a problem when using non-standard versions, for example very new or very old ones.\r\n\r\nAs I'm probably not the only one doing this, how about preserving this information to save others from doing the same tests? User could add some tags to answers that says for which software versions/operating system the answer works and for which one it doesn't.\r\n\r\nSee a quick mock-up in the image below (sorry, it is based on stackexchange, but you probably get the idea)\r\n\r\n![5604152df06daabb93881b6dab4ecc79f927e91d.png](/image?hash=24207e63885752edbd69b5039e1babdcb6f6f2594f8c06de1e558c1a1bd2f813)\r\n\r\n*Disclaimer: This is based on a similar suggestion I made for codidact some time ago https://forum.codidact.org/t/categorize-answers-with-tags/185*	2020-01-14 14:06:13.697077+00	6	4	1	\N	0	0	0	\N	\N	Just some thoughts I'd like to throw in here for discussion:	f	f
642	384	2	2020-01-21 15:32:19.126086+00	shortlinks are all now case-insensitive.	2020-01-21 15:32:19.126086+00	5	1	1	\N	0	0	0	\N	\N	shortlinks are all now case-insensitive.	f	f
149	207	12	2018-03-27 15:19:01+00	The `StatementParameterizationType` attribute indicates the type of [parameterization][1] applied to the statement.\r\n\r\nThe values are documented in [`sys.query_store_query`][2]:\r\n\r\n* 0 = None\r\n* 1 = User\r\n* 2 = Simple\r\n* 3 = Forced\r\n\r\nIt appears on the root node of **post-execution** ("actual") plans only.\r\n\r\nThe [**Query Store**][3] must also be **enabled** to see this attribute (e.g. in SSMS).\r\n\r\n---\r\n\r\n[AdventureWorks][4] demo:\r\n\r\n    USE AdventureWorks2017;\r\n    GO\r\n    ALTER DATABASE SCOPED CONFIGURATION CLEAR PROCEDURE_CACHE;\r\n    ALTER DATABASE CURRENT SET PARAMETERIZATION SIMPLE;\r\n    GO\r\n    -- None (0)\r\n    SELECT COUNT_BIG(*)\r\n    FROM Production.Product AS P \r\n    WHERE P.Color = N'Red';\r\n    GO\r\n    -- User (1)\r\n    EXECUTE sys.sp_executesql\r\n        @stmt = N'\r\n            SELECT COUNT_BIG(*)\r\n            FROM Production.Product AS P \r\n            WHERE P.Color = @Color;',\r\n        @params = N'@Color nvarchar(15)',\r\n        @Color = N'Red';\r\n    GO\r\n    -- Simple (2)\r\n    SELECT A.AddressID\r\n    FROM Person.[Address] AS A\r\n    WHERE A.AddressLine1 = N'1 Smiling Tree Court';\r\n    GO\r\n    ALTER DATABASE SCOPED CONFIGURATION CLEAR PROCEDURE_CACHE;\r\n    ALTER DATABASE CURRENT SET PARAMETERIZATION FORCED;\r\n    GO\r\n    -- Forced (3)\r\n    SELECT COUNT_BIG(*)\r\n    FROM Production.Product AS P \r\n    WHERE P.Color = N'Red';\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/query-processing-architecture-guide#PlanReuse\r\n  [2]: https://docs.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-query-store-query-transact-sql\r\n  [3]: https://docs.microsoft.com/en-us/sql/relational-databases/performance/monitoring-performance-by-using-the-query-store\r\n  [4]: https://github.com/Microsoft/sql-server-samples/releases/tag/adventureworks	2019-11-30 12:54:33.535958+00	1	4	1	202425	0	0	0	2019-11-30 12:54:33.535958+00	\N	The `StatementParameterizationType` attribute indicates the type of [parameterization][1] applied to the statement.	f	f
120	184	99	2018-07-05 13:26:47+00	Rather than export, manually edit, and rerun, you could try do the job directly in the database with something like:\r\n\r\n    DECLARE C CURSOR FOR\r\n            SELECT sm.definition, so.type\r\n            FROM   sys.objects so\r\n            JOIN   sys.all_sql_modules sm ON sm.object_id = so.object_id\r\n            WHERE  so.type IN ('P', 'V')\r\n            ORDER BY so.name\r\n    DECLARE @SQL NVARCHAR(MAX), @ojtype NVARCHAR(MAX)\r\n    OPEN C\r\n    FETCH NEXT FROM C INTO @SQL, @ojtype\r\n    WHILE @@FETCH_STATUS = 0 BEGIN\r\n        IF @objtype = 'P' SET @SQL = REPLACE(@SQL, 'CREATE PROCEDURE', 'ALTER PROCEDURE') \r\n        IF @objtype = 'V' SET @SQL = REPLACE(@SQL, 'CREATE VIEW'     , 'ALTER VIEW'     ) \r\n        SET @SQL = REPLACE(@SQL, 'GETDATE()', '[dbo].[getlocaldate]()') \r\n        --PRINT @SQL\r\n        EXEC (@SQL)\r\n        FETCH NEXT FROM C INTO @SQL, @ojtype\r\n    END\r\n    CLOSE C\r\n    DEALLOCATE C\r\n\r\nof course extending it to deal with functions, triggers, and so forth too.\r\n\r\nThere are a few caveats:\r\n\r\n* You may need to be a bit brighter and deal with different/extra white-space between `CREATE` and `PROCEDURE`/`VIEW`/`<other>`. Rather than the `REPLACE` for that you might prefer to instead leave the `CREATE` in place and execute a `DROP` first, but this risks leaving `sys.depends` and friends out of kilter where `ALTER` may not, also if `ALTER` fails you at least have the existing object still in place where with `DROP`+`CREATE` you may not.\r\n\r\n* If you code has any "clever" smells like modifying its own schema with ad-hoc TSQL then you'll need to make sure the search and replace for `CREATE`->`ALTER` doesn't interfere with that.\r\n\r\n* You will be wanting to regression test the entire application(s) after the operation, whether you use the cursor or export+edit+run methods.\r\n\r\nI've used this method to make similar schema-wide updates in the past. It is a bit of a hack, and feels quite ugly, but sometimes it is the easiest/quickest way.\r\n\r\nDefaults and other constraints can be modified similarly too, though those can only be dropped and recreated rather than altered. Something like:\r\n\r\n    DECLARE C CURSOR FOR\r\n            SELECT AlterDefaultSQL = 'ALTER TABLE [' +st.name+ '] DROP CONSTRAINT [' + si.name + '];'\r\n                                   + CHAR(10)\r\n                                   + 'ALTER TABLE [' +st.name+ '] ADD CONSTRAINT [' + si.name + '] DEFAULT '+REPLACE(si.definition, 'GETDATE()', '[dbo].[getlocaldate]()')+' FOR '+sc.name+';'\r\n            FROM   sys.tables st\r\n            JOIN   sys.default_constraints si ON si.parent_object_id = st.object_id\r\n            JOIN   sys.columns sc ON sc.default_object_id = si.object_id\r\n    DECLARE @SQL NVARCHAR(MAX)\r\n    OPEN C\r\n    FETCH NEXT FROM C INTO @SQL\r\n    WHILE @@FETCH_STATUS = 0 BEGIN\r\n        --PRINT @SQL\r\n        EXEC (@SQL)\r\n        FETCH NEXT FROM C INTO @SQL\r\n    END\r\n    CLOSE C\r\n    DEALLOCATE C\r\n\r\nSome more fun that you may need to contend with: if you are partitioning by time then those parts may need altering too. While partitioning on time more granularly then by day is rare you could have issues where `DATETIME`s are interpreted by the partitioning function as being the previous or next day depending on timezine, leaving your partitions unaligned with your usual queries.	2019-11-28 13:30:11.290334+00	2	4	1	211442	0	0	0	2019-11-28 13:30:11.290334+00	\N	Rather than export, manually edit, and rerun, you could try do the job directly in the database with something like:	f	f
123	184	102	2018-07-13 10:35:57+00	> Dynamically alter all proc and udf to change value\r\n\r\n        DECLARE @Text   NVARCHAR(max), \r\n            @spname NVARCHAR(max), \r\n            @Type   CHAR(5), \r\n            @Sql    NVARCHAR(max) \r\n    DECLARE @getobject CURSOR \r\n    \r\n    SET @getobject = CURSOR \r\n    FOR SELECT sc.text, \r\n               so.NAME, \r\n               so.type \r\n        FROM   sys.syscomments sc \r\n               INNER JOIN sysobjects so \r\n                       ON sc.id = so.id \r\n        WHERE  sc.[text] LIKE '%getdate()%' \r\n    \r\n    --and type in('P','FN') \r\n    OPEN @getobject \r\n    \r\n    FETCH next FROM @getobject INTO @Text, @spname, @Type \r\n    \r\n    WHILE @@FETCH_STATUS = 0 \r\n      BEGIN \r\n          IF ( @Type = 'P' \r\n                OR @Type = 'FN' ) \r\n            SET @Text = Replace(@Text, 'getdate', 'dbo.getlocaldate') \r\n    \r\n          SET @Text = Replace(@Text, 'create', 'alter') \r\n    \r\n          EXECUTE Sp_executesql \r\n            @Text \r\n    \r\n          PRINT @Text \r\n    \r\n          --,@spname,@Type \r\n          FETCH next FROM @getobject INTO @Text, @spname, @Type \r\n      END \r\n    \r\n    CLOSE @getobject \r\n    \r\n    DEALLOCATE @getobject  \r\n\r\n　\r\n\r\n        CREATE PROCEDURE [dbo].[Testproc1] \r\n    AS \r\n        SET nocount ON; \r\n    \r\n      BEGIN \r\n          DECLARE @CurDate DATETIME = Getdate() \r\n      END\r\n\r\n \r\n\r\nNotice commented [sysobjects Type column][1] condition.My script will alter only proc and UDF.\r\n\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/relational-databases/system-compatibility-views/sys-sysobjects-transact-sql?view=sql-server-2017\r\n\r\nThis script will alter all `Default Constraint` which contain `GetDate()`\r\n\r\n\r\n        DECLARE @TableName      VARCHAR(300), \r\n            @constraintName VARCHAR(300), \r\n            @colName        VARCHAR(300), \r\n            @Sql            NVARCHAR(max) \r\n    DECLARE @getobject CURSOR \r\n    \r\n    SET @getobject = CURSOR \r\n    FOR SELECT ds.NAME, \r\n               sc.NAME AS colName, \r\n               so.NAME AS Tablename \r\n        --,ds.definition \r\n        FROM   sys.default_constraints ds \r\n               INNER JOIN sys.columns sc \r\n                       ON ds.object_id = sc.default_object_id \r\n               INNER JOIN sys.objects so \r\n                       ON so.object_id = ds.parent_object_id \r\n        WHERE  definition LIKE '%getdate()%' \r\n    \r\n    OPEN @getobject \r\n    \r\n    FETCH next FROM @getobject INTO @constraintName, @colName, @TableName \r\n    \r\n    WHILE @@FETCH_STATUS = 0 \r\n      BEGIN \r\n          SET @Sql = 'ALTER TABLE ' + @TableName \r\n                     + ' DROP CONSTRAINT ' + @constraintName + '; ' \r\n                     + Char(13) + Char(10) + '           ' + Char(13) + Char(10) + '' \r\n          SET @Sql = @Sql + ' ALTER TABLE ' + @TableName \r\n                     + ' ADD CONSTRAINT ' + @constraintName \r\n                     + '          DEFAULT dbo.GetLocaledate() FOR ' \r\n                     + @colName + ';' + Char(13) + Char(10) + '          ' + Char(13) \r\n                     + Char(10) + '' \r\n    \r\n          PRINT @Sql \r\n    \r\n          EXECUTE sys.Sp_executesql \r\n            @Sql \r\n    \r\n          --,@spname,@Type \r\n          FETCH next FROM @getobject INTO @constraintName, @colName, @TableName \r\n      END \r\n    \r\n    CLOSE @getobject \r\n    \r\n    DEALLOCATE @getobject   	2019-11-28 13:30:12.066175+00	1	4	1	212118	0	0	0	2019-11-28 13:30:12.066175+00	\N	> Dynamically alter all proc and udf to change value	f	f
121	184	100	2018-07-10 15:49:40+00	I really like David's answer and upvoted that for a programmatic way of doing things. \r\n\r\nBut you can try this today for a test-run in Azure via SSMS:\r\n\r\n**Right click your database --> Tasks --> Generate Scripts..**\r\n\r\n[Back Story] we had a junior DBA who upgraded all our test environments to SQL 2008 R2 while our production environments were at SQL 2008. It's a change that makes me cringe to this day. To migrate to production, from test, we had to generate scripts within SQL, using generate scripts, and  in the advanced options we used the 'Type of Data to script: Schema and Data' option to generate a massive text file. We were successfully able to move our test R2 databases to our legacy SQL 2008 servers -- where a database restore to a lower version would not have worked. We used sqlcmd to input the large file--as the files were often too big for the SSMS text buffer.\r\n\r\n**What I'm saying here is that this option would probably work for you as well. You'll just need to do one additional step and search and replace  getdate() with [dbo].[getlocaldate]() in the generated text file. (I would put your function into the database before migration though).**\r\n\r\n(I never wanted to be proficient at this band-aid of a database restore, but for a while it became a defacto way of doing things. And, it worked every time.)\r\n\r\nIf you choose this route, be sure and select the Advanced button and **select all the options you need (read each one)** to move from the old database to the new database--like the defaults you mentioned. But give it a few test runs in Azure. I bet you'll find that this is one solution that works -- with a modicum of effort.\r\n\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/O6OMW.png	2019-11-28 13:30:11.549177+00	0	4	1	211773	0	0	0	2019-11-28 13:30:11.549177+00	\N	I really like David's answer and upvoted that for a programmatic way of doing things.	f	f
119	184	98	2018-07-04 16:49:14+00	> What would be the best way to implement this change?\r\n\r\nI would work the other way around. Convert all your timestamps in the database to UTC, and just use UTC and go with the flow. If you need a timestamp in a different tz, you can create a generated column using `AT TIME ZONE` (as you did above) that renders the time stamp in that specified TZ (for the app). But, I would seriously consider just having UTC returned to the app, and writing that logic -- the display logic -- in the app.	2019-11-28 13:30:11.03172+00	0	4	1	211353	0	0	0	2019-11-28 13:30:11.03172+00	\N	> What would be the best way to implement this change?	f	f
118	184	97	2018-07-04 19:17:48+00	1. Use the SQL Server tool to export the database objects definition to a SQL file which should include: tables, views, triggers, SPs, functions, and so on \r\n\r\n2. Edit the SQL file (make a backup first) using any text editor that allows you to find the text `"GETDATE()"` and replace it with `"[dbo].[getlocaldate]()"`\r\n\r\n3. Run the edited SQL file in Azure SQL to create your database objects...\r\n\r\n4. Execute the migration of data.\r\n\r\nHere you have a reference from azure documentation: [Generating Scripts for SQL Azure][1]\r\n\r\n\r\n  [1]: https://azure.microsoft.com/en-us/blog/generating-scripts-for-sql-azure/	2019-11-28 13:30:10.758654+00	0	4	1	211361	0	0	0	2019-11-28 13:30:10.758654+00	\N	1. Use the SQL Server tool to export the database objects definition to a SQL file which should include: tables, views, triggers, SPs, functions, and so on	f	f
122	184	101	2019-01-24 07:42:04+00	I have upvoted Evan Carrolls answer, as I think this is the *best* solution. I have not been able to convince my colleagues that they should change a lot of C# code, so I had to use the code that David Spillett wrote. I have fixed a couple of problems with UDFs, Dynamic SQL and Schemas (not all code use "dbo.") like this:\r\n\r\n    DECLARE C CURSOR LOCAL STATIC FOR\r\n            SELECT sm.definition, so.type\r\n            FROM   sys.objects so\r\n            JOIN   sys.all_sql_modules sm ON sm.object_id = so.object_id\r\n            WHERE  so.type IN ('P', 'V')\r\n            AND CHARINDEX('getdate()', sm.definition) > 0\r\n            ORDER BY so.name\r\n    \r\n    DECLARE @SQL NVARCHAR(MAX), @objtype NVARCHAR(MAX)\r\n    OPEN C\r\n    WHILE 1=1 BEGIN\r\n        FETCH NEXT FROM C INTO @SQL, @objtype\r\n        IF @@FETCH_STATUS <> 0 BREAK\r\n    \r\n        IF @objtype = 'P' SET @SQL = REPLACE(@SQL, 'CREATE PROCEDURE', 'ALTER PROCEDURE') \r\n        IF @objtype = 'P' SET @SQL = REPLACE(@SQL, 'CREATE   PROCEDURE', 'ALTER PROCEDURE') /* when you write "create or alter proc" */\r\n        IF @objtype = 'V' SET @SQL = REPLACE(@SQL, 'CREATE VIEW'     , 'ALTER VIEW'     ) \r\n        IF CHARINDEX('getdate())''', @sql) > 0 BEGIN  /* when dynamic SQL is used */\r\n            IF CHARINDEX('utl.getdate())''', @sql) = 0 SET @SQL = REPLACE(@SQL, 'GETDATE()', 'utl.getdate()') \r\n        end\r\n        ELSE begin\r\n            SET @SQL = REPLACE(@SQL, 'GETDATE()', 'CONVERT(DATETIME, CONVERT(datetimeoffset,  SYSDATETIME()) AT TIME ZONE ''Central Europe Standard Time'')') \r\n        end\r\n        EXEC dbo.LongPrint @String = @sql    \r\n        EXEC (@SQL)\r\n    END\r\n    CLOSE C\r\n    DEALLOCATE C\r\n\r\nand the default constraints like this:\r\n\r\n    DECLARE C CURSOR LOCAL STATIC FOR\r\n            SELECT AlterDefaultSQL = 'ALTER TABLE [' +sch.name+ '].[' +st.name+ '] DROP CONSTRAINT [' + si.name + '];'\r\n                                   + CHAR(10)\r\n                                   + 'ALTER TABLE [' +sch.name+ '].[' +st.name+ '] ADD CONSTRAINT [' + si.name + '] DEFAULT '+REPLACE(si.definition, 'GETDATE()', 'CONVERT(DATETIME, CONVERT(datetimeoffset,  SYSDATETIME()) AT TIME ZONE ''Central Europe Standard Time'')')+' FOR '+sc.name+';'\r\n            FROM   sys.tables st\r\n            JOIN   sys.default_constraints si ON si.parent_object_id = st.object_id\r\n            JOIN   sys.columns sc ON sc.default_object_id = si.object_id\r\n            INNER JOIN sys.schemas sch ON sch.schema_id = st.schema_id\r\n            WHERE CHARINDEX('getdate()', si.definition) > 0\r\n            ORDER BY st.name, sc.name\r\n    \r\n    DECLARE @SQL NVARCHAR(MAX)\r\n    OPEN C\r\n    WHILE 1=1 BEGIN\r\n        FETCH NEXT FROM C INTO @SQL\r\n        IF @@FETCH_STATUS <> 0 BREAK\r\n    \r\n        EXEC dbo.LongPrint @String = @sql  \r\n        EXEC (@SQL)\r\n        FETCH NEXT FROM C INTO @SQL\r\n    END\r\n    CLOSE C\r\n    DEALLOCATE C\r\n\r\n**UDFs**\r\n\r\nThe suggestion to use a UDF that returns todays date and time looks nice, but I think there are still enough performance problems with UDFs, so I've chosen to use the very long and ugly `AT TIME ZONE` solution.	2019-11-28 14:20:22.303994+00	0	4	1	227937	0	0	0	2019-11-28 13:30:11.807811+00	\N	I have upvoted Evan Carrolls answer, as I think this is the *best* solution. I have not been able to convince my colleagues that they should change a lot of C# code, so I had to use the code that David Spillett wrote. I have fixed a couple of problems with UDFs, Dynamic SQL and Schemas (not all code use "dbo.") like this:	f	f
241	279	16	2018-06-14 20:35:29+00	It doesn't appear that they do, but this really only applies to nested CTEs.\r\n\r\nCreate two temp tables:\r\n\r\n    CREATE TABLE #t1 (id INT);\r\n    INSERT #t1 ( id )\r\n    VALUES ( 1 );\r\n    \r\n    CREATE TABLE #t2 (id INT);\r\n    INSERT #t2 ( id )\r\n    VALUES ( 1 );\r\n\r\nQuery 1:\r\n\r\n    WITH your_mom AS (\r\n    \tSELECT TOP 1 *\r\n    \tFROM #t1 AS t \r\n    ),\r\n    also_your_mom AS (\r\n    \tSELECT TOP 1 *\r\n    \tFROM #t2 AS t\r\n    )\r\n    SELECT *\r\n    FROM your_mom;\r\n\r\nQuery 2:\r\n\r\n    WITH your_mom AS (\r\n    \tSELECT TOP 1 *\r\n    \tFROM #t1 AS t \r\n    ),\r\n    also_your_mom AS (\r\n    \tSELECT TOP 1 *\r\n    \tFROM #t2 AS t\r\n    )\r\n    SELECT *\r\n    FROM also_your_mom;\r\n\r\nQuery plans:\r\n\r\n[![NUTS][1]][1]\r\n\r\nThere is an overhead, but the unnecessary portion of the query is eliminated very early (during parsing in this case; the simplification stage in more complex cases), so the additional work is truly minimal, and does not contribute to potentially expensive cost-based optimization.\r\n\r\n  [1]: https://i.stack.imgur.com/Ryx9a.jpg	2019-12-04 14:24:18.301161+00	2	4	1	209697	0	0	0	2019-12-04 14:24:18.301161+00	\N	It doesn't appear that they do, but this really only applies to nested CTEs.	f	f
240	279	158	2018-06-14 21:05:33+00	+1 to Erik, but wanted to add two things (which did not work well in a comment):\r\n\r\n1. You don't even need to look at execution plans to see that they are ignored when not used. The following should produce a "divide by 0" error but does not due to `cte2` not being selected from at all:\r\n\r\n        ;WITH cte1 AS\r\n        (\r\n          SELECT 1 AS [Bob]\r\n        ),\r\n        cte2 AS (\r\n          SELECT 1 / 0 AS [Err]\r\n          FROM cte1\r\n        )\r\n        SELECT *\r\n        FROM   cte1;\r\n\r\n1. CTE's can be ignored, even if they are the only CTE, and even if they are selected from, _if_ logically all rows would be excluded anyway. The following is a case where the query optimizer knows ahead of time that no rows could be returned from the CTE, so it doesn't even bother to execute it:\r\n\r\n        ;WITH cte AS\r\n        (\r\n          SELECT 1 / 0 AS [Bob]\r\n        )\r\n        SELECT TOP (1) [object_id]\r\n        FROM   sys.objects\r\n        UNION ALL\r\n        SELECT cte.[Bob]\r\n        FROM   cte\r\n        WHERE  1 = 0;\r\n\r\nRegarding performance, the unused CTE is parsed and compiled (or at least compiled in the case below), so it is not 100% ignored, but the cost would have to be negligible and not worth being concerned about.\r\n\r\nWhen only parsing, there is no error:\r\n\r\n    SET PARSEONLY ON;\r\n    \r\n    ;WITH cte1 AS\r\n    (\r\n      SELECT obj.[NotHere]\r\n      FROM   sys.objects obj\r\n    )\r\n    SELECT TOP (1) so.[name]\r\n    FROM   sys.objects so\r\n    \r\n    GO\r\n    SET PARSEONLY OFF;\r\n    GO\r\n    \r\n\r\nWhen doing everything just short of execution, then there is a problem:\r\n    \r\n    GO\r\n    SET NOEXEC ON;\r\n    GO\r\n    \r\n    ;WITH cte1 AS\r\n    (\r\n      SELECT obj.[NotHere]\r\n      FROM   sys.objects obj\r\n    )\r\n    SELECT TOP (1) so.[name]\r\n    FROM   sys.objects so\r\n    \r\n    GO\r\n    SET NOEXEC OFF;\r\n    GO\r\n    /*\r\n    Msg 207, Level 16, State 1, Line XXXXX\r\n    Invalid column name 'NotHere'.\r\n    */\r\n\r\n	2019-12-04 14:24:18.019498+00	1	4	1	209701	0	0	0	2019-12-04 14:24:18.019498+00	\N	+1 to Erik, but wanted to add two things (which did not work well in a comment):	f	f
797	568	2	2020-02-10 17:48:49.81182+00	We don't think there is enough space in the notification area to add a useful edit preview except for the most trivial edits.\r\n\r\nIt's also an edge case — edits are not very high volume, and you'll usually want to review them in detail.\r\n\r\nTagging as `wont-fix` for now…	2020-02-10 17:48:49.81182+00	3	1	1	\N	0	0	0	\N	\N	We don't think there is enough space in the notification area to add a useful edit preview except for the most trivial edits.	t	f
368	359	233	2019-12-07 05:47:07.928846+00	I think we should import only the useful posts (outstanding or frequently asked) when needed. No massive import at all.\r\n\r\n	2019-12-07 07:07:35.108141+00	6	4	1	\N	0	0	0	\N	\N	I think we should import only the useful posts (outstanding or frequently asked) when needed. No massive import at all.	f	f
156	210	2	2019-12-01 01:48:28.772753+00	Pending a major revamp of the profile page at some point in the future, we've prevented the dropping of data somewhat brutally, by simply disabling all fields except the one you are currently changing.	2019-12-01 01:48:28.772753+00	1	1	1	\N	0	0	0	\N	\N	Pending a major revamp of the profile page at some point in the future, we've prevented the dropping of data somewhat brutally, by simply disabling all fields except the one you are currently changing.	f	f
366	354	2	2019-12-06 17:27:23.507566+00	We have no plans to implement anything at like the migration feature on SE — once question deletion is implemented it's a simple thing for someone to manually move a post with cut and paste.\r\n\r\nResponsible folk within the communities should be nudging question posters to ask on main meta if necessary.	2019-12-06 17:27:23.507566+00	2	1	1	\N	0	0	0	\N	\N	We have no plans to implement anything at like the migration feature on SE — once question deletion is implemented it's a simple thing for someone to manually move a post with cut and paste.	f	f
609	563	2	2020-01-13 15:16:40.381779+00	We have added the italic (and [oblique](https://en.wikipedia.org/wiki/Oblique_type)) versions where available.\r\n\r\n> The "leaning" f, a, and g are the most striking in this example, but plenty of other Latin letter shapes are typically drawn differently for italics.\r\n\r\nWe had no idea that the italic versions were anything other than just slanted, and some of them are significantly different as you say.\r\n\r\nIf you are using a font that supplies an italic type, you should see some stylistic differences (not just slanting) now:\r\n\r\n* not italic\r\n* *italic*\r\n* **bold not italic**\r\n* ***bold italic***	2020-01-13 21:43:26.153034+00	3	1	1	\N	0	0	0	\N	\N	We have added the italic (and [oblique](https://en.wikipedia.org/wiki/Oblique_type)) versions where available.	f	f
440	379	234	2019-12-12 21:48:44.691523+00	`Oh My!` A "Ja mei" option, which can be used similarly to the smiley and so on. Those who understand what "Ja mei" means will know how to use it, and the other may just not use it.	2020-01-22 00:34:47.454337+00	2	4	3	\N	0	0	0	\N	\N	`Oh My!` A "Ja mei" option, which can be used similarly to the smiley and so on. Those who understand what "Ja mei" means will know how to use it, and the other may just not use it.	f	f
703	379	848	2020-01-29 11:21:55.571519+00	`completed` IMHO, it would be useful to have a checkbox or similar when choosing a license for one's messages, that would mean “or any later version” (of the license).\r\n\r\nIndeed, having code stuck to LPPL 1.3c could be a problem in the future when an improved version of the license comes out and the copyright holders are not all reachable anymore. If that was the copyright holders' intention, fine, but in my case, I would like to be able to choose this dual-licensing :\r\n\r\n* CC-BY-SA 4.0 or, at the licensee's option, any later version;\r\n\r\n* LPPL 1.3c or, at the licensee's option, any later version.\r\n\r\nwhich could be summarized as { CC-BY-SA 4.0+, LPPL 1.3c+ }.\r\n\r\n(I made up the brace notation, but not the '+' one, which is very commonly used, e.g. when saying that code is licensed under GPLv2+).	2020-02-02 22:43:34.210938+00	7	4	3	\N	0	0	0	\N	\N	`completed` IMHO, it would be useful to have a checkbox or similar when choosing a license for one's messages, that would mean “or any later version” (of the license).	f	f
410	379	96	2019-12-11 07:11:00.724237+00	`completed` A way for users with SE accounts to validate their identities so that imports are linked to their account without it being a manual process to ping @Jack for every case.\r\n\r\nRelated [feature request](https://topanswers.xyz/meta?q=409).	2020-01-22 00:32:33.19886+00	9	4	2	\N	0	0	0	\N	\N	`completed` A way for users with SE accounts to validate their identities so that imports are linked to their account without it being a manual process to ping @Jack for every case.	f	f
412	379	202	2019-12-11 09:32:50.883992+00	`in progress` Option to report an answer as a duplicate or in violation of `academic-honesty`.\r\n\r\nRelated [feature request](/meta?q=574).	2020-01-22 00:34:24.962362+00	3	4	2	\N	0	0	0	\N	\N	`in progress` Option to report an answer as a duplicate or in violation of `academic-honesty`.	f	f
376	379	234	2019-12-08 14:31:27.463586+00	`completed` Add some kind of moderation tools allowing us to remove propaganda/spam/offensive posts.\r\n\r\nRelated [feature request](https://topanswers.xyz/meta?q=182).	2020-01-22 00:32:11.031217+00	10	4	3	\N	0	0	0	\N	\N	`completed` Add some kind of moderation tools allowing us to remove propaganda/spam/offensive posts.	f	f
375	379	233	2019-12-08 13:37:15.751231+00	`completed` A delete option.\r\n\r\nRelated [feature request](https://topanswers.xyz/meta?q=353).	2020-01-22 00:32:02.171524+00	11	4	1	\N	0	0	0	\N	\N	`completed` A delete option.	f	f
377	379	234	2019-12-08 14:33:08.135358+00	Maybe add some refereeing tools that allow us to simply delete posts that are content-wise wrong (but do not need get moderated away).\r\n\r\nRelated [feature request](https://topanswers.xyz/meta?q=382).	2019-12-11 07:58:35.445912+00	6	4	3	\N	0	0	0	\N	\N	Maybe add some refereeing tools that allow us to simply delete posts that are content-wise wrong (but do not need get moderated away).	f	f
448	379	202	2019-12-13 10:48:27.310782+00	`won't fix` An option to report users, similar to flags for questions.	2020-01-22 00:35:07.650513+00	2	4	2	\N	0	0	0	\N	\N	`won't fix` An option to report users, similar to flags for questions.	f	f
612	537	8	2020-01-14 10:45:48.782699+00	> **Update 16 Jan, 2020** - The [TA GitHub issue tracker](https://github.com/topanswers/topanswers/issues) is now open.\r\n\r\n---\r\n\r\nYes to this.\r\n\r\nI came looking for this question as I noticed the Issues tab wasn't available on GitHub and would have asked for it to be enabled if I didn't find this question.\r\n\r\nOne of the benefits of using github issues is that potential contributors could just pick an issue if it's not in progress yet and create a pull request with a potential fix. (Although with the speed issues get fixed on this site it could be hard finding an issue to work on)	2020-01-16 11:09:04.278422+00	9	4	1	\N	0	0	0	\N	\N	> **Update 16 Jan, 2020** - The [TA GitHub issue tracker](https://github.com/topanswers/topanswers/issues) is now open.	f	f
624	537	709	2020-01-16 05:17:50.650083+00	I'll add a third voice of agreement. Github issue tracking is a key part of workflow on Github. You're likely to get plenty of developers happy to create tickets for bugs/features and you can use that to track future development and request/accept help for the site.	2020-01-16 05:17:50.650083+00	3	4	1	\N	0	0	0	\N	\N	I'll add a third voice of agreement. Github issue tracking is a key part of workflow on Github. You're likely to get plenty of developers happy to create tickets for bugs/features and you can use that to track future development and request/accept help for the site.	f	f
585	537	2	2020-01-09 18:20:18.06746+00	I know next to nothing about how GitHub issue trackers work, but based on what you have said here, it sounds like the way forward unless someone else has a sensible objection.\r\n\r\nInitially at least we can continue to work on issues raised directly in comments here, but it would seem to make sense for the devs to raise the issue on GitHub ourselves with a link to the comment here when that happens.\r\n\r\n> Note I'm happy to actually set this up…\r\n\r\nWonderful, it's great to have the extra expertise in areas we are weak, thank you.\r\n\r\n> …but I'd like to see consensus or approval here first...\r\n\r\nThere is no particular rush so please wait until you feel any discussion here has run its course.	2020-01-09 18:20:18.06746+00	8	1	1	\N	0	0	0	\N	\N	I know next to nothing about how GitHub issue trackers work, but based on what you have said here, it sounds like the way forward unless someone else has a sensible objection.	f	f
211	235	2	2019-12-04 08:52:37.395233+00	There is a lot more to do here, and we really haven't started on the mobile experience yet, but we have made a few cursory changes along the lines you suggest:\r\n\r\n* The questions pane is now the default, until you switch to the chat pane and then your last setting is remembered\r\n\r\n* We've added some padding on the bottom of the chat pane. This is ugly and still problematic, but slightly better than having crucial navigation elements hidden below the iOS nav bar\r\n	2019-12-04 08:52:37.395233+00	3	1	1	\N	0	0	0	\N	\N	There is a lot more to do here, and we really haven't started on the mobile experience yet, but we have made a few cursory changes along the lines you suggest:	f	f
56	81	12	2019-11-22 11:33:38.769244+00	The *Nested Loops Join* has the *Optimized* property:\r\n\r\n![Optimized property](https://i.imgur.com/QtpPDZy.png "Optimized Nested Loops Join")\r\n\r\nThis means SQL Server sorts rows arriving from the *Index Seek* to promote a sequential I/O data access pattern at the *RID Lookup*. The sort is invisible, and known as a *batch sort*. Craig Freedman discusses optimized nested loops and batch sorting in the article [OPTIMIZED Nested Loops Joins][1].\r\n\r\nThe invisible batch sort is unlike a normal *Sort* plan operator in that it can never spill to *tempdb*. A batch sort makes a best effort to sort rows into the desired order using the memory grant it is given. When full sorting cannot be achieved, the partially sorted output is still helpful to the *RID Lookup*. The optimizer chooses a batch sort when it thinks sorting could be useful, but it cannot justify the cost of a full Sort operator\r\n\r\nThe general idea is to sort the upcoming *RID Lookups* into heap page id order, to promote sequential I/O. The heap page numbers are stored in the nonclustered index as part of the heap bookmark (labelled `Bmk1000` in this plan). These page numbers are extracted from the bookmark by the intrinsic function `BmkToPage`.\r\n\r\nThe function itself is trivial, but you may encounter adverse performance effects due to the invisible sorting. Older versions of SQL Server could also suffer from an [excessive memory grant][3] for the hidden sort.\r\n\r\nOptimized nested loops can be disabled in modern SQL Server versions with the query hint:\r\n\r\n```sql\r\nOPTION (USE HINT ('DISABLE_OPTIMIZED_NESTED_LOOP'));\r\n```\r\n\r\nNote that this hint will also prevent an explicit *Sort* operator before the *Nested Loops Join*.\r\n\r\nDisabling optimized nested loops alone can be achieved with [documented trace flag 2340][5]:\r\n\r\n>Causes SQL Server not to use a sort operation (batch sort) for optimized Nested Loops joins when generating a plan.\r\n\r\n### Further reading\r\n\r\n- [Batch Sort and Nested Loops][2] by Dmitry Piliugin.\r\n- [Addressing large memory grant requests from optimized Nested Loops][3] by Pedro Lopes.\r\n- [Nested Loops Prefetching][4] by me.\r\n\r\n\r\n[1]: https://blogs.msdn.microsoft.com/craigfr/2009/03/18/optimized-nested-loops-joins/\r\n[2]: https://www.sqlshack.com/batch-sort-and-nested-loops/\r\n[3]: https://blogs.msdn.microsoft.com/sql_server_team/addressing-large-memory-grant-requests-from-optimized-nested-loops/\r\n[4]: https://www.sql.kiwi/2013/08/sql-server-internals-nested-loops-prefetching.html\r\n[5]: https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-traceon-trace-flags-transact-sql	2019-11-22 11:33:38.769244+00	4	4	2	\N	0	0	0	\N	\N	The *Nested Loops Join* has the *Optimized* property:	f	f
147	203	12	2017-10-18 07:16:39+00	>Truncating a temp table at the end of the stored procedure that creates it seems to cause the space the table uses in tempdb for the data to be released faster than if no truncate statement is used, despite expectations to the contrary. Why?\r\n\r\nIf the temporary table is large enough ([more than 128 extents][1]), the physical page deallocations are deferred, and performed by a background system task. This is true whether an explicit `TRUNCATE TABLE` is used or not.\r\n\r\nThe only difference is a tiny implementation detail. An explicit `TRUNCATE TABLE` happens to create a task with a **shorter timer** than the (otherwise identical) deferred drop task created by temporary table cleanup:\r\n\r\n[![Call stack because people like them][2]][2]\r\n\r\nWhether this is by accident or design is anyone's guess. It could of course change at any time, since this level of detail goes way beyond the supported product surface area.\r\n\r\nIf you disable deferred drop globally with a (mostly) undocumented trace flag:\r\n\r\n    DBCC TRACEON (671, -1);\r\n\r\n...the deallocations are performed synchronously in both cases, and you will see no difference in timing.\r\n\r\n>What would be the relative performance implications of using or not using such a truncate statement? When using SNAPSHOT isolation, tempdb is often stressed and I would think that releasing space used in tempdb from a large temp table as soon as possible would prevent otherwise unnecessary growth of tempdb. Would this potential space savings come at the cost of performance?\r\n\r\nI seriously doubt this would ever make much difference either way. If *tempdb* is sized appropriately to the peak needs of your workload, whether deferred drop occurs after one second or three should not matter. The same work is done; it is just a small difference in timing.\r\n\r\nOn the other hand: If you feel more comfortable using `TRUNCATE TABLE` on temporary tables at the end of your stored procedures, go with that. I'm not aware of any particular downside to doing this.\r\n\r\n\r\n  [1]: https://technet.microsoft.com/en-us/library/ms177495.aspx\r\n  [2]: https://i.stack.imgur.com/SDsv2.png	2019-11-30 07:57:43.852784+00	4	4	1	188727	0	0	0	2019-11-30 07:57:43.852784+00	\N	>Truncating a temp table at the end of the stored procedure that creates it seems to cause the space the table uses in tempdb for the data to be released faster than if no truncate statement is used, despite expectations to the contrary. Why?	f	f
652	582	752	2020-01-22 14:04:45.521384+00	If I'm understanding your question, the keystone is this bit:\r\n\r\n\r\n> I have a table "components". One of the attributes of this table is "type". "type" is restricted to only three values\r\n\r\nYou can use a foreign key constraint. In the design below, the components table will only accept a component type if it already exists in the component_types table.\r\n\r\n\t\tif object_id('component_types') is not null begin drop table components drop table component_types  end\r\n\r\n\t\tcreate table component_types (comp_type_id int identity (0,1) primary key, description varchar(255))\r\n\r\n\t\tinsert into component_types (description)\r\n\t\tvalues('Doodad'),\r\n\t\t('widget'),\r\n\t\t('gyrothingy')\r\n\r\n\t\tselect * from component_types\r\n\r\n\t\tcreate table components (comp_id int identity (0,1) primary key, comp_type_id int,\r\n\t\tforeign key(comp_type_id) references component_types(comp_type_id))\r\n\r\n\t\tinsert into components (comp_type_id)\r\n\t\tvalues (0),\r\n\t\t(1),\r\n\t\t(2),\r\n\t\t(2),\r\n\t\t(1),\r\n\t\t(0)\r\n\r\n\t\tselect * from components c\r\n\t\tinner join component_types t on c.comp_type_id = t.comp_type_id\r\n\r\n\t\t--this insert will fail\r\n\t\tinsert into components (comp_type_id)\r\n\t\tvalues (3),\r\n\t\t(4),\r\n\t\t(5)\r\n\r\n\t\tselect * from component_types	2020-01-22 14:05:16.162891+00	0	4	1	\N	0	0	0	\N	\N	If I'm understanding your question, the keystone is this bit:	f	f
335	349	96	2019-12-06 07:12:44.66443+00	The post change history page is actually now (as of sometime in the last couple days) showing diff highlighting to the previes revision so you can see what changed. The effect is pretty subtle, but added text gets a green highlight, removed gets a red, etc.\r\n\r\nWhat I see for this post:\r\n\r\n![2019-12-06_10-14.png](/image?hash=8599a69820b2bd358d36fa166a38bac22d4233dcf15155363b4724bb4086bd0f)	2019-12-06 07:14:51.469357+00	4	4	2	\N	0	0	0	\N	\N	The post change history page is actually now (as of sometime in the last couple days) showing diff highlighting to the previes revision so you can see what changed. The effect is pretty subtle, but added text gets a green highlight, removed gets a red, etc.	f	f
655	597	2	2020-01-22 22:04:50.965519+00	We are not currently planning to have a single 'moderator' role on TopAnswers. We want to break that down if possible and also shed some of the mystique around moderator actions — in practice that will mean keeping things as public as possible, and even avoiding words like 'moderator', 'reputation' etc that can carry a lot of unnecessary baggage.\r\n\r\nThat is not to rule out elections. We need some way of making the community itself responsible for choosing how it is run. My current thinking is that communities will elect people with one power: the power to delegate roles (such as 'post cleanup crew') from within the community. Perhaps they could simply be called a '[delegator](https://www.collinsdictionary.com/dictionary/english/delegator)'?\r\n\r\nIf you agree that is a good direction, I'm happy for us to do that when you decide you are ready — bearing in mind of course that the tools and platform will be evolving underneath us as we go along!	2020-01-22 22:04:50.965519+00	4	1	1	\N	0	0	0	\N	\N	We are not currently planning to have a single 'moderator' role on TopAnswers. We want to break that down if possible and also shed some of the mystique around moderator actions — in practice that will mean keeping things as public as possible, and even avoiding words like 'moderator', 'reputation' etc that can carry a lot of unnecessary baggage.	f	f
214	263	2	2019-12-04 12:21:52.101914+00	We now have a new landing page that meets these needs — it is the default page for topanswers.xyz, and gives an overview of our aims, and links to further information on meta.\r\n\r\n> Information I'd like to see before I create an account is, where the servers are located…\r\n\r\nThis is now covered in the footer.\r\n\r\n> …who owns the rights to the contents…\r\n\r\nWe touch on that in one of the bullet points.\r\n\r\n> …whether it is a corporate or private person who maintains the server…\r\n\r\nWe link to posts about our intention to incorporate as a CIO (a proper UK charity).\r\n\r\n> …and perhaps most important, what's the scope of the site.\r\n\r\nThis is the bit we flesh out most fully. Comments welcome!	2020-02-10 17:34:00.870245+00	9	1	1	\N	0	0	0	\N	\N	We now have a new landing page that meets these needs — it is the default page for topanswers.xyz, and gives an overview of our aims, and links to further information on meta.	f	f
187	233	2	2019-12-02 13:55:00.377031+00	update 1st Feb 2020: we've now displaying a rudimentary list of questions on your profile.\r\n\r\nThis will be expanded to answers and other contributions once we are happy with the way it works for questions (feedback would be most welcome).\r\n\r\n---\r\n\r\n> Is this already planned but you just haven't gotten to it?\r\n\r\nYes, absolutely, it's an essential feature. Very little thought on my part has yet gone into the details of implementation, but I don't have any doubt that we need to make this available.\r\n\r\n> If so, do you have any sense of when it might be possible?\r\n\r\nI suspect that at least something rudimentary will be released fairly soon (days or weeks but not months[^1]). We'll update this post when that happens, thanks for bringing it up!\r\n\r\n[^1]: In the event this slipped off our radar and did end up taking a couple of months, sorry about that!	2020-02-01 00:28:21.903072+00	9	1	1	\N	0	0	0	\N	\N	update 1st Feb 2020: we've now displaying a rudimentary list of questions on your profile.	f	f
279	228	96	2019-12-05 05:48:53.293336+00	These days assuming a wide screen by default is much better than assuming a narrow one. I _love_ the effective use of my screen real-estate using side panes like this.\r\n\r\nThat being said, I don't think the site should be making _any_ assumptions. Instead it should be adapting. Literally all major browsers these days have good support for flexible layouts (usually known as "responsive") that adapt to their shape and size. In this case I think the post compose screen should have a couple different modes and the overall available space would be used to select the best mode by default. When there is not realyl enough horizontal real-estate available, first the question and then the preview panes should revert to a verticle stacked layout.\r\n\r\nWhen this works properly, everything from mobile devices to wrap around monster desk screens will be first class citizens.	2019-12-05 05:49:51.245051+00	2	4	2	\N	0	0	0	\N	\N	These days assuming a wide screen by default is much better than assuming a narrow one. I _love_ the effective use of my screen real-estate using side panes like this.	f	f
170	228	12	2019-12-01 18:19:48.518476+00	In other views, the border between the questions and answers pane and the associated chat room is movable.\r\n\r\nThis feature should be replicated so people can configure the layout to their own preferences when writing a question or an answer.	2019-12-01 18:19:48.518476+00	5	4	1	\N	0	0	0	\N	\N	In other views, the border between the questions and answers pane and the associated chat room is movable.	f	f
776	228	2	2020-02-10 15:27:16.804569+00	The new plan is to adapt the layout according to the viewport width.\r\n\r\nI've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/12) so work can begin, and will update the post here when it is complete.	2020-02-10 15:27:16.804569+00	1	1	1	\N	0	0	0	\N	\N	The new plan is to adapt the layout according to the viewport width.	t	f
254	290	16	2017-12-21 19:45:01+00	The function that you're looking for is [`QUOTENAME`][1]!\r\n\r\nThrough the practical use of square bracket technology, you can safely encapsulate strings to aid in the prevention of hot SQL injection attacks.\r\n\r\nNote that just sticking square brackets around something does _not_ safely quote it out, though you can avoid your code erroring with invalid characters in object names.\r\n\r\n### Good code\r\n\r\n    DECLARE @sql NVARCHAR(MAX) = N''\r\n    SELECT @sql = 'SELECT ' + QUOTENAME(d.name) + ' FROM your_mom'\r\n    FROM sys.databases AS d\r\n\r\n### Bad code\r\n\r\n    DECLARE @sql NVARCHAR(MAX) = N''\r\n    SELECT @sql = 'SELECT [' + d.name + '] FROM your_mom'\r\n    FROM sys.databases AS d\r\n\r\n  [1]: https://docs.microsoft.com/en-us/sql/t-sql/functions/quotename-transact-sql\r\n\r\n**To give a specific example...**\r\n\r\nThe following works fine for the initial input\r\n\r\n    DECLARE @ObjectName SYSNAME = 'sysobjects';\r\n    \r\n    DECLARE @dynSql NVARCHAR(MAX) = 'SELECT COUNT(*) FROM [' + @ObjectName + ']';\r\n    \r\n    EXEC (@dynSql);\r\n    \r\nBut with malicious input it is vulnerable to SQL injection    \r\n    \r\n    DECLARE @ObjectName SYSNAME = 'sysobjects];SELECT ''This is some arbitrary code executed. It might have dropped a table or granted permissions''--'\r\n    \r\n    DECLARE @dynSql NVARCHAR(MAX) = 'SELECT  COUNT(*)  FROM [' + @ObjectName + ']';\r\n    \r\n    EXEC (@dynSql);\r\n    \r\nUsing `QUOTENAME` correctly escapes the embedded `]` and prevents the attempted SQL injection from happening.\r\n    \r\n    \r\n    DECLARE @ObjectName SYSNAME = 'sysobjects];SELECT ''This is some arbitrary code executed. It might have dropped a table or granted permissions''--'\r\n    \r\n    DECLARE @dynSql NVARCHAR(MAX) = 'SELECT  COUNT(*)  FROM ' + QUOTENAME(@ObjectName);\r\n    \r\n    EXEC (@dynSql);\r\n    \r\n\r\n> Invalid object name 'sysobjects];SELECT 'This is some arbitrary code\r\n> executed. It might have dropped a table or granted permissions'--'.\r\n\r\n\r\n	2019-12-04 14:32:01.491381+00	1	4	1	193768	0	0	0	2019-12-04 14:32:01.491381+00	\N	The function that you're looking for is [`QUOTENAME`][1]!	f	f
575	234	666	2020-01-05 11:21:23.617359+00	To recovery the login key, how about  arranging for it to be saved in (at the user's discretion) and recovered from the user's normal login depositories.  eg IOS Passwords and Accounts? 	2020-01-05 11:21:23.617359+00	1	4	1	\N	0	0	0	\N	\N	To recovery the login key, how about  arranging for it to be saved in (at the user's discretion) and recovered from the user's normal login depositories.  eg IOS Passwords and Accounts?	f	f
186	234	2	2019-12-02 12:49:24.096686+00	> However, the user does not appear to be presented with this information when they sign up. (I wasn't.)\r\n\r\nThanks for pointing out this omission. As our sign-up process is different (even if it is better), the onus is on us to give users extra prompting to help stop accidental loss of access.\r\n\r\nAs of now, a new sign up redirects to the profile page and highlights the login key and the message about protecting it. We've also moved the recovery info near the top of the login page:\r\n\r\n![Screenshot 2019-12-02 at 12.52.51.png](/image?hash=b0119f3971037879763aabb6f8924fd9f5597454f36ebb69a71ada6850a683e6)	2019-12-02 12:53:26.39258+00	2	1	1	\N	0	0	0	\N	\N	> However, the user does not appear to be presented with this information when they sign up. (I wasn't.)	f	f
171	234	115	2019-12-01 19:08:01.335764+00	I'd like to go a step farther: allow people to record a (private) email address on the profile, and add an "email me my recovery information" button.  This also doubles as "email me that text string I'm going to need to sign in on another device".\r\n\r\nWe want people to invest in our communities, and we're also reaching out to people who already *have* invested elsewhere and want to bring that investment here.  Let's please make it hard to be locked out of that investment.\r\n\r\nFurther, if we have an email address, then even if the user *didn't* click that button and then lost the token, we would in principle have a way to reunite the person with the lost account later.\r\n	2019-12-01 19:19:11.534231+00	4	4	1	\N	0	0	0	\N	\N	I'd like to go a step farther: allow people to record a (private) email address on the profile, and add an "email me my recovery information" button.  This also doubles as "email me that text string I'm going to need to sign in on another device".	f	f
181	234	96	2019-12-02 08:27:15.873064+00	I understand the desire to hold as little PII as possible, but account recovery is a problem, and making those with root DB access responsible for manually validating identities does not scale.\r\n\r\nFor example @Jack knows some other channels to verify this account is me if I claim to have lost access to it, but he will quickly run into people he doesn't know how to contact, hence puting those accounts at risk of being lost (at best) or handed over to bad actors (at worst).\r\n\r\nI would suggest:\r\n\r\n1. Having a huge bold red warning banner on all profiles about users being 100% responsible for their account recovery and explaining that they are NOT RECOVERABLE unless the user enables a recovery mechanism. Drop the banner only when at least one fallback mechanism is enabled.\r\n\r\n1. Not making any single method mandatory. Different information will be sensitive to different people. If would rather cross link _all_ of my social profiles, some people would rather cross link select sets, others would prefer none. Whatever PII is held should be up to the user.\r\n\r\n1. Allow cross linking other identities with or without marking them as usable for account recovery. Some people may want to list their Twitter profile without making that a point of failure.\r\n\r\nHere are some account recovery ideas.\r\n\r\n* GPG: Allow me to enter my public key, and if I lose my accont present me with an encrypted message with my recovery key than only I should  be able to decrypt (because only I have the private key). **Requires zero PII** but a lot of technical knowledge.\r\n\r\n* Bitcoin / Etherium / other blockchain currency: List a wallet address as a profile recovery option, users could recover accounts making a donation from that wallet that supports the site at the same time! **PII only revealesed on recovery request.**\r\n\r\n* Keybase:\r\n\r\n* Twitter / Github / Gitlab: These and many other systems provide federated login support so people can just authenticate with them as ID providers in the first place (incidentally without reveling very much PII), and they can also be added as backup mechanisms. A lot of us will want to advertise these anyway.\r\n\r\n* SE / Facebook / other social media link / website: account recovery could be done by giving the user a token to post anywhere on the profile page and then crawling for it (tricky to do securely on some sites, manual validation might be a first step).\r\n\r\n* FIDO: Using 2FA as a single factor fallback is weird, but possible.\r\n\r\n* Email: a very common way of doing this and email providers tend to take ID seriously enough it's a viable method of verifying people. Include a warning about this being visible to (potentially lots of) server and DB administrators. Even if the current set is low, people should be aware listing their email is not a secure proposition. Perhaps ONLY allow listing it publically to avoid people that want to keep their email private even using this method, with an option to use it for recovery or not, but not an option to keep it private.\r\n\r\n* SMS: Similar to above. I think it's a bad idea but lots of people rely on their phone provide as a fallback identity.\r\n\r\n	2019-12-02 08:33:25.737931+00	5	4	2	\N	0	0	0	\N	\N	I understand the desire to hold as little PII as possible, but account recovery is a problem, and making those with root DB access responsible for manually validating identities does not scale.	f	f
365	369	2	2019-12-06 17:24:07.458951+00	Update 17 Feb 2020: (1) is now implemented, and we are leaning towards 'wont-fix' for (2)\r\n\r\n---\r\n\r\nUpdate Feb 2020: This is starting to get increasingly necessary to have a comprehensive and flexible filtering mechanism. I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/14) so work can begin and will update the post here when it is complete.\r\n\r\n---\r\n\r\n~~It's [been suggested](https://topanswers.xyz/transcript?room=2&id=5124#c5124) that a simple drop-down might work.~~	2020-02-17 00:31:10.258415+00	6	1	1	\N	0	0	0	\N	\N	Update 17 Feb 2020: (1) is now implemented, and we are leaning towards 'wont-fix' for (2)	f	f
132	191	12	2017-06-16 00:47:39+00	In many respects, this is expected behaviour. Any set of compression routines will have widely ranging performance depending on input data distribution. We expect to trade data loading speed for storage size and runtime querying performance.\r\n\r\nThere is a definite limit to how detailed an answer you're going to get here, since VertiPaq is a proprietary implementation, and the details are a closely-guarded secret. Even so, we do know that VertiPaq contains routines for:\r\n\r\n* Value encoding (scaling and/or translating values to fit in a small number of bits)\r\n* Dictionary encoding (integer references to unique values)\r\n* Run Length Encoding (storing runs of repeated values as [value, count] pairs)\r\n* Bit-packing (storing the stream in as few bits as possible)\r\n\r\nTypically, data will be value or dictionary encoded, then RLE or bit-packing will be applied (or a hybrid of RLE and bit-packing used on different subsections of the segment data). The process of deciding which techniques to apply can involve generating a histogram to help determine how maximum bit savings can be achieved.\r\n\r\nCapturing the slow case with Windows Performance Recorder and analyzing the result with Windows Performance Analyzer, we can see that the vast majority of the execution time is consumed in looking at the clustering of the data, building histograms, and deciding how to partition it for best savings:\r\n\r\n[![WPA Analysis][1]][1]\r\n\r\nThe most expensive processing occurs for values that appear at least 64 times in the segment. This is a heuristic to determine when *pure* RLE is likely to be beneficial. The faster cases result in *impure* storage e.g. a bit-packed representation, with a larger final storage size. In the hybrid cases, values with 64 or more repetitions are RLE encoded, and the remainder are bit-packed.\r\n\r\nThe longest duration occurs when the maximum number of distinct values with 64 repetitions appear in the largest possible segment i.e. 1,048,576 rows with 16,384 sets of values with 64 entries each. Inspection of the code reveals a hard-coded time limit for the expensive processing. This can be configured in other VertiPaq implementations e.g. SSAS, but not in SQL Server as far as I can tell.\r\n\r\nSome insight into the final storage arrangement can be acquired using the [undocumented `DBCC CSINDEX` command][2]. This shows the RLE header and array entries, any bookmarks into the RLE data, and a brief summary of the bit-pack data (if any).\r\n\r\nFor more information, see:\r\n\r\n* [The VertiPaq Engine in DAX][3] by Alberto Ferrari and Marco Russo\r\n* [Microsoft Patent WO2015038442][4]: Processing datasets with a DBMS engine \r\n* [Microsoft Patent WO2010039898][5]: Efficient large-scale filtering and/or sorting for querying of column based data encoded structures\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/AKQ5I.png\r\n  [2]: http://www.queryprocessor.ru/dbcc-csindex/\r\n  [3]: https://www.microsoftpressstore.com/articles/article.aspx?p=2449192&seqNum=3\r\n  [4]: https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2015038442\r\n  [5]: https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2010039898	2019-11-29 08:29:28.915707+00	2	4	1	176417	0	0	0	2019-11-29 08:29:28.915707+00	\N	In many respects, this is expected behaviour. Any set of compression routines will have widely ranging performance depending on input data distribution. We expect to trade data loading speed for storage size and runtime querying performance.	f	f
420	309	96	2019-12-11 16:25:23.17371+00	This idea would have the bonus effect of mitigating any possible downsides to my [feature request for height-limiting code blocks](https://topanswers.xyz/meta?q=417).	2019-12-11 16:25:23.17371+00	0	4	2	\N	0	0	0	\N	\N	This idea would have the bonus effect of mitigating any possible downsides to my [feature request for height-limiting code blocks](https://topanswers.xyz/meta?q=417).	f	f
787	309	2	2020-02-10 17:07:10.361443+00	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/21) so work can begin, and will update the post here when it is complete.	2020-02-10 17:07:10.361443+00	0	1	1	\N	0	0	0	\N	\N	I've moved this on to [to a GitHub issue](https://github.com/topanswers/topanswers/issues/21) so work can begin, and will update the post here when it is complete.	t	f
393	386	2	2019-12-09 14:45:47.267504+00	this was a regression, now fixed!	2019-12-09 14:45:47.267504+00	1	1	1	\N	0	0	0	\N	\N	this was a regression, now fixed!	f	f
332	341	2	2019-12-06 06:10:32.257289+00	We've added TeX to our highlightjs highlighting library  as it [isn't in the list](https://highlightjs.org/download/) for a default download.\r\n\r\nWe've also made `tex` the default language for this community but please bear in mind that is you want unformatted code blocks you now need to declare that explicitly, so:\r\n\r\n~~~none\r\n```\r\n\\textbf{bob}\r\n```\r\n~~~\r\n\r\nproduces:\r\n\r\n```\r\n\\textbf{bob}\r\n```\r\n\r\nand:\r\n\r\n~~~none\r\n```none\r\n\\textbf{bob}\r\n```\r\n~~~\r\n\r\nproduces: \r\n\r\n```none\r\n\\textbf{bob}\r\n```\r\n\r\n\r\n	2019-12-06 06:17:36.740146+00	6	1	1	\N	0	0	0	\N	\N	We've added TeX to our highlightjs highlighting library  as it [isn't in the list](https://highlightjs.org/download/) for a default download.	f	f
224	270	16	2018-05-09 23:51:32+00	Simple Parameterization is attempted when a *trivial* plan is found. The parameterization attempt may be considered *safe* or *unsafe*.\r\n\r\nThe key point is that a trivial plan is *found* and considered *safe*. If the cost of the trivial plan exceeds the `cost threshold for parallelism`, the optimizer will go on to later stages of optimization, where parallel plans may be considered. Whether the final result is a serial or parallel plan, it will be simple parameterized if the safe trivial plan found (*but not ultimately used*) was parameterized.\r\n\r\nIn the question example, setting the `cost threshold for parallelism` higher than the cost of the trivial plan will allow the optimizer to stop at that stage.\r\n\r\n---\r\n\r\nLooking at the query plan isn't always enough to figure out if your query has actually been Simple Parameterized.\r\n\r\nThe safest way is to check some DMVs to verify:\r\n\r\n    /*Unsafe auto param*/\r\n    SELECT *\r\n    FROM sys.dm_os_performance_counters AS dopc\r\n    WHERE dopc.counter_name LIKE '%Unsafe Auto-Params/sec%';\r\n    \r\n    /*Safe auto param*/\r\n    SELECT *\r\n    FROM sys.dm_os_performance_counters AS dopc\r\n    WHERE dopc.counter_name LIKE '%Safe Auto-Params/sec%';\r\n    \r\n    /*Trivial Plans*/\r\n    SELECT * \r\n    FROM sys.dm_exec_query_optimizer_info AS deqoi \r\n    WHERE deqoi.counter = 'trivial plan';\r\n\r\n---\r\n\r\nAdditionally, you can also use undocumented trace flag 8607, but not as an `OPTION` clause hint. Using the `OPTION` clause prevents a trivial plan.\r\n\r\n\tDBCC TRACEON(8607, 3604);\r\n\t/*Wait*/\t\r\n\t\r\n\t/*Run*/\t\t\r\n\tSELECT u.CreationDate, u.Id\r\n\tFROM dbo.Users AS u\r\n\tWHERE u.Reputation = 2;\r\n\r\n\t/*Clean up*/\r\n\tDBCC TRACEOFF(8607, 3604);\r\n\r\nIf the plan is considered safe for Simple Parameterization, you'll see a message confirming it here.\r\n\r\n~~~\r\n********************\r\n\r\n** Query marked as Cachable\r\n    \r\n** Query marked as Safe for Auto-Param\r\n\r\n~~~\r\n	2019-12-04 14:53:45.7941+00	2	4	1	206305	0	0	0	2019-12-04 14:14:32.563666+00	\N	Simple Parameterization is attempted when a *trivial* plan is found. The parameterization attempt may be considered *safe* or *unsafe*.	f	f
339	355	12	2019-09-18 17:15:31+00	SQL Server uses different calculations in different situations. Your example is different from the linked Q & A because your range is entirely contained within a step; it does not cross a step boundary. It is also an interval with two ends rather than one. Writing `BETWEEN` is the same as writing two separate predicates with `>=` and `<=`.\r\n\r\n### Interval with two boundaries, within a single step\r\n\r\nThe formula is modified to perform linear interpolation within the step for the number of distinct values expected, and reflect that two range endpoints are now specified (and assumed to exist within the histogram step) rather than one.\r\n\r\nUsing the histogram steps given in the question:\r\n\r\n[![question histogram steps][3]][3]\r\n\r\nFor the query with `BETWEEN '20140615' AND '20140616'`, the calculation is:\r\n\r\n```\r\nDECLARE\r\n    @Q1 float = CONVERT(float, CONVERT(datetime, '2014-06-15')),\r\n    @Q2 float = CONVERT(float, CONVERT(datetime, '2014-06-16')),\r\n    @K1 float = CONVERT(float, CONVERT(datetime, '2014-06-14')),\r\n    @K2 float = CONVERT(float, CONVERT(datetime, '2014-06-18')),\r\n    @RANGE_ROWS float = 301,\r\n    @DISTINCT_RANGE_ROWS float = 3;\r\n\r\nDECLARE\r\n    @S1 float = (@Q1 - @K1) / (@K2 - @K1),\r\n    @S2 float = (@Q2 - @K1) / (@K2 - @K1);\r\n\r\nDECLARE\r\n    @F float = @S2 - @S1;\r\n\r\nDECLARE\r\n    @AVG_RANGE_ROWS float = @RANGE_ROWS / @DISTINCT_RANGE_ROWS;\r\n\r\nSELECT\r\n    @AVG_RANGE_ROWS * ((@F * (@DISTINCT_RANGE_ROWS - 2)) + 2);\r\n```\r\n\r\n...giving **225.75**. Changing `@Q2` from `'20140616'` to `'20140617'` gives a result of **250.833**.\r\n\r\nBoth results match those given in the question.\r\n\r\n  [1]: https://i.stack.imgur.com/ComWw.png\r\n  [2]: https://i.stack.imgur.com/PmHiv.png\r\n  [3]: https://i.stack.imgur.com/sMCyC.png	2019-12-06 09:12:22.316724+00	3	4	1	249080	0	0	0	2019-12-06 09:12:22.316724+00	\N	SQL Server uses different calculations in different situations. Your example is different from the linked Q & A because your range is entirely contained within a step; it does not cross a step boundary. It is also an interval with two ends rather than one. Writing `BETWEEN` is the same as writing two separate predicates with `>=` and `<=`.	f	f
\.